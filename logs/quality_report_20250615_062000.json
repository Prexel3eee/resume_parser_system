{
  "timestamp": "2025-06-15T06:20:00.076255",
  "summary": {
    "total_processed": 28,
    "successful_extractions": 28,
    "failed_extractions": 0,
    "success_rate": 100.0,
    "ocr_usage_percentage": 0.0,
    "avg_extraction_time": 31.096858509949275
  },
  "field_analysis": {
    "empty_fields": {
      "primary_email": 2,
      "secondary_email": 28,
      "phone": 12,
      "city": 8,
      "state": 8,
      "zip": 11,
      "work_authority": 23,
      "tax_term": 22,
      "source_by": 28,
      "experience": 13
    },
    "field_confidence": {
      "primary_email": [
        0.0,
        0.9,
        0.9,
        0.0,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9
      ],
      "secondary_email": [
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0
      ],
      "phone": [
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.9,
        0.0,
        0.9,
        0.9,
        0.0,
        0.9,
        0.0,
        0.9,
        0.9,
        0.0,
        0.9,
        0.9,
        0.0,
        0.9,
        0.9,
        0.0,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9
      ],
      "city": [
        0.0,
        0.0,
        0.7,
        0.0,
        0.0,
        0.7,
        0.9,
        0.7,
        0.7,
        0.9,
        0.7,
        0.7,
        0.0,
        0.7,
        0.7,
        0.0,
        0.7,
        0.7,
        0.7,
        0.0,
        0.7,
        0.7,
        0.0,
        0.7,
        0.9,
        0.7,
        0.7,
        0.7
      ],
      "state": [
        0.0,
        0.0,
        0.9,
        0.0,
        0.0,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.0,
        0.8,
        0.9,
        0.0,
        0.8,
        0.9,
        0.9,
        0.0,
        0.8,
        0.9,
        0.0,
        0.8,
        0.9,
        0.8,
        0.9,
        0.9
      ],
      "zip": [
        0.0,
        0.0,
        0.7,
        0.0,
        0.0,
        0.7,
        0.9,
        0.7,
        0.7,
        0.9,
        0.7,
        0.7,
        0.0,
        0.0,
        0.7,
        0.0,
        0.0,
        0.7,
        0.7,
        0.0,
        0.7,
        0.7,
        0.0,
        0.7,
        0.9,
        0.0,
        0.7,
        0.7
      ],
      "work_authority": [
        0.0,
        0.0,
        0.8,
        0.0,
        0.0,
        0.8,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.8,
        0.0,
        0.8,
        0.8
      ],
      "resume_link": [
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0
      ],
      "raw_resume": [
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0
      ],
      "tax_term": [
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.9,
        0.0,
        0.0,
        0.9,
        0.0,
        0.0,
        0.0,
        0.9,
        0.9,
        0.0,
        0.9,
        0.9,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0
      ],
      "source_by": [
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0
      ],
      "designation": [
        0.9,
        0.8,
        0.8,
        0.9,
        0.8,
        0.8,
        0.9,
        0.8,
        0.9,
        0.9,
        0.8,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.8,
        0.9,
        0.9,
        0.8,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9
      ],
      "experience": [
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.0,
        0.9,
        0.9,
        0.0,
        0.0,
        0.0,
        0.9,
        0.0,
        0.0,
        0.9,
        0.0,
        0.9,
        0.0,
        0.0,
        0.9,
        0.0,
        0.0,
        0.9,
        0.0,
        0.0
      ]
    }
  },
  "skills_analysis": {
    "categories": {
      "technical_skills": {
        "count": 0,
        "skills": []
      },
      "business_skills": {
        "count": 0,
        "skills": []
      },
      "creative_skills": {
        "count": 0,
        "skills": []
      },
      "communication_skills": {
        "count": 0,
        "skills": []
      },
      "industry_skills": {
        "count": 0,
        "skills": []
      },
      "soft_skills": {
        "count": 0,
        "skills": []
      },
      "other_skills": {
        "count": 0,
        "skills": []
      }
    },
    "total_skills": 0,
    "unique_skills": 0
  },
  "resume_details": [
    {
      "resume_path": "data\\input\\.Net - TX - Hassan.pdf",
      "confidence_score": 0.333,
      "used_ocr": false,
      "extracted_fields": {
        "skills": {
          "soft_skills": [
            {
              "name": "analysis",
              "confidence": 0.42,
              "context": "full_text",
              "positions": [
                294,
                1048,
                9011
              ],
              "experience_weight": 0.4,
              "importance_score": 0.7
            }
          ],
          "domain_specific": [
            {
              "name": "design",
              "confidence": 0.42,
              "context": "full_text",
              "positions": [
                303,
                1057,
                4522,
                5423,
                5940,
                6404,
                9020
              ],
              "experience_weight": 0.4,
              "importance_score": 0.7
            }
          ]
        },
        "first_name": "Hassan",
        "last_name": "Mwase",
        "primary_email": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "secondary_email": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "phone": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "city": {
          "value": "",
          "confidence": 0.0,
          "method": "ner",
          "structured_data": null
        },
        "state": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "zip": {
          "value": "",
          "confidence": 0.0,
          "method": "regex",
          "structured_data": null
        },
        "work_authority": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "resume_link": {
          "value": "data\\input\\.Net - TX - Hassan.pdf",
          "confidence": 1.0,
          "method": "file_path",
          "structured_data": null
        },
        "raw_resume": {
          "value": "  \nHassan Mwase  \n \nProudly presented by Hassan Mwase  IT professional with 1 6+ years of experience as a software developer/solution architect  on multiple \nlarge complex  project s and production support for a variety of clients  including most recent \nexperience developing Web applications . Proficient in analysis, design, development, testing, \nintegration and troubleshooting skills.  Result oriented back -end and front -end developer with \nexceptional analytical skills, team oriented, dependable and very flexible. Experience with \nWaterfall and JIRA Agile methodologies.  \n \nTechnical Skills  \n\uf0b7 Experienced Software development of Web applications using  Azure DevOps, Cloud, Git, \nASP.NET Web Applicati on, ASP.NET Web Forms , ASP.NET MVC,  C#, .NET \nFramework, Kendo UI , JavaScript,  Razor,  Angular  JS, CSS 3, jQuery , bootstrap, Ajax, \ndependency injection,  JSON, LINQ, HTML5,  Microsoft Visual Studio enterprise 201 7, \nActive Server  Pages extended files,  Eclipse , Test Driven Development (TDD ), Selenium \nFramework, Web Services, Postman, Visual Studio code , DevExpress, SQL Server \nManagement Studio . \n\uf0b7 Proficient in analysis, design, development, testing, integration and troubleshooting skills.  \n\uf0b7 Experience with Waterfal l and JIRA Agile methodologies .  \n\uf0b7 Result oriented back -end and front -end developer with exceptional analytical skills, team \noriented, dependable and very flexible.  \n\uf0b7 Familiarity with REST/ HTML/CSS 3.  \n\uf0b7 Excellent relational database experience using Oracle and  SQL Server  including writing \nPL/SQL, SQL stored procedures, functions, triggers and views.  \n\uf0b7 Experience developing using Oracle Database,  Oracle Forms, Oracle Reports,  SQL*Plus, \nPL/SQL, UNIX, Pro*C . \n\uf0b7 Knowledge of Java/J2EE architecture/technologies,  WebSphere , Test Driven \nDevelopment TDD  and Oracle ADF , ODBC  \n\uf0b7 Writing unit tests with NUnit and Moq  \n\uf0b7 Familiar with Regular Expression and Pattern Matching.  \n\uf0b7 SQL Server \u2013 Microsoft SQL Server Management Studio v.18.8 , Visual Studio 2017, \nMicrosoft SQL Server To ols for Visual Studio, GraphiViz 2.38 \u2013 This is used in the PMEF \nNEDAT and new NSIGHT application as an API for creating bubble visualization images.  \nMicrosoft Internet Information Services  (IIS) version 10  web server . OleDB API \nUncovered errors in timeshe et contracts and Provided guidance in resolving major issues \nfrom past years. Applied fix to application code.  \n \n \nProfessional Experience  \n \nSecret Security Clearance  11/2020 to 11/2030                        May. 2020  \u2013 Present  \nSenior Software Programmer/Analyst Consultant /Agile developer  \nTools : .NET, ASP.NET C#, Web Forms .Net Framework ver.4.6, JavaScript, CSS3,  jQuery, \nbootstrap v4.0.0 , JSON, LINQ, HTML5, Visual Basic 201 7, Microsoft Visual Studio enterprise,  \nHassan Mwase           Page 2 \nProudly presented by Hassan Mwase  Developer, Web Services, Web Forms, Databases: SQL Server  and Oracle , Microsoft team TSF  \ndevelopment environment  \n\uf0b7 Develop web applications using C#,  ASP.NET and JavaScript . \n\uf0b7 As a senior developer, I develop, enhance  and Support critical federal Government  \nprojects.  \n\uf0b7 Develop and maintain multiple web applications written in ASP.NET, C#, JavaScript,  \nand Web Forms.  \n\uf0b7 Participate in project meetings with other technical staff, business owners and subject \nmatter experts.  \n\uf0b7 Troubleshooting and debugging Defects  \n\uf0b7 Write automated UI test script using ServiceNow ATF  Framework . \n \n \nFarmers Insurance  \u2013 USA                                                                      Sept. 2018  \u2013 Aug. 2020  \nSenior Programmer/Analyst Consultant /Agile developer  \nTools : .NET, ASP.NET C#, MVC, Web Forms  .Net Framework ver.4.6, JavaScript,  CSS3,  jQuery, \nbootstrap  v4.0.0, JSON, LINQ, HTML5, Visual Basic 201 7,  Microsoft Visual Studio enterprise,  \nDeveloper, Selenium Framework, Web Services, Web Forms, Databases: SQL Server  and DB2 ,  \nMicrosoft team TSF  development environment  \n\uf0b7 Develop web applications using C#, Azure DevOps, Git, ASP.NET MVC , JavaScript, \njQuery, Razor MVC . \n\uf0b7 Develop and maintain multiple customer web applications written in C# , JavaScript and \nWeb Forms.  \n\uf0b7 Participate in project meetings with other technical staff, business owners and subject \nmatter experts.  \n\uf0b7 Troubleshooting and debugging Defects  \n\uf0b7 Write automated UI test script and fix defects using Selenium Framework . \n\uf0b7 Write unit tests with NUnit and Moq  \n \nBoeing  Company  \u2013 USA                                                                              Feb. 2018  \u2013 Sept.2018  \nSenior Programmer/Analyst Consultant /Agile developer  \nTools : .NET, ASP.NET MVC ,C#, MVC,  Dependency Injection DI,  .Net Framework ver.4.6, \nJavaScript, Razor,  Angular  JS,  CSS3, jQuery, bootstrap , JSON, LINQ, HTML5 ,  Microsoft Visual Studio \nenterprise 201 7, PL/SQL, SQL D eveloper, Databases: Oracle 12C and SQL Server  , JIRA  Agile , Microsoft \nteam TSF development environment  \n\uf0b7 Develop se rver side C# pages, create entities in the relation object mapping to modernize \nBoeing data ERP application  using  ASP.NET MVC framework .  \n\uf0b7 Manage Agile task board .  \n\uf0b7 Design and develop back -end Database stored procedures, functions, triggers and views . \n\uf0b7 Code C# interfaces, Classes, view models, API controllers  and data services web resources.  \n\uf0b7 Develop server side XHML5 web pages leveraging angular JS  and jQuery. \n\uf0b7 Code C# Dependency Injection processes \u2013 DI Manager interfaces, DI Manager classes, \nDependency  Injection configuration and API Controllers .   \nHassan Mwase           Page 3 \nProudly presented by Hassan Mwase   \n \nCliffs Natural resources  - Cleveland , OH                                                    July 2016  \u2013 Dec. 2017  \nSenior Programmer/Analyst Consultant  \nTool s: .NET, C#, MVC,  .Net Framework ver.4.6, JavaScript, Angular JS, CSS, JSON, LINQ, HTML5,  \nMicrosoft Visual Studio enterprise 2015,  PL/SQL, Postman , SQL Developer , Toad,  Databases: Oracle  \n12C and SQL Server   \n\uf0b7 Re-designed the Oracle ERP application and convert oracle  forms and reports  web application \nusing  ASP.NET MVC framework . \n\uf0b7 Create d Technical specs based on user desired functionality changes.  \n\uf0b7 Design and develop back -end Database stored procedures, functions, triggers and views . \n\uf0b7 Coded  C# interfaces, Classes, view models, JavaScript controllers  and data services web \nresources.   \n\uf0b7 Develop front end web content XHML 5 web pages  leveraging angular  JS. \n\uf0b7 Code system enhancements, defect fixes  and production support .   \n\uf0b7 Commit and publish finished web application solutions.  \n\uf0b7 Support project and business users to Create  Unit Test, integration, and UAT test scenarios and \nscripts, as well as, performing system and integration testing.   \n\uf0b7 Design, build and implement client and server side applications  as needed , as well as \nsupporting and enhancing custom applications.  \n \nGap Inc. - Columbus , OH                                               July 2015 \u2013 Nov. 2015 \nSenior Programmer/Analyst Consultant /Agile developer  \nTools : Oracle 11G, PL/SQL, SQL Developer, Toad, JDeveloper,  Oracle 11G Web logic environment, \nEclipse, JavaScript,  Ruby , Eclipse, WebSphere, Tomcat,  JIRA Agile development environment.  \n\uf0b7 Onsite team lead working with product managers and business users to re-design the on -\nline order fulfillment process from batch processing to on -demand and transform oracle \ninterface based application to Web UI .  \n\uf0b7 Technical lead for on -site/offshore developers and QA.  \n\uf0b7 Convert legacy forms application to Web  UI using Java Script. \n\uf0b7 Coded supporting stored  procedure s, functions, packages  and views . \n\uf0b7 Assisted to test conver sion of  character based SSH screens for RF devices to a stateless \nHTML user interface, update to the tech stack from Oracle Forms to Java6/HTML5, \ndesigned to  extend the life of a legacy Warehouse Management System(RWMS)   \n\uf0b7 Create  test cases , run automated system tests and validate  output of Ruby automated testing \nfor process online customer orders.  \n\uf0b7 Support warehouse order processing and shipment message queues (MQ) .  \n\uf0b7 Create d Unit Test , integration, and UAT test scenarios and  scripts, as well as, performing \nsystem and integration testing. Write integration testing cases.   \nCliffs Natural Resources - Cleveland , OH  June 2011  \u2013 April 2015  \nSenior Programmer  Consultant  \nTools : Oracle 11G, SQL Server, Oracle Forms, Oracle Forms, .NET, C#, MVC,.Net Framework, \nJavaScript, HTML5,  Microsoft Visual Studio enterprise 2015,  PL/SQL, SQL Developer, Toad  \n\uf0b7 Support business functions through the development, implementation, and maintenance \nof applications systems.  \nHassan Mwase           Page 4 \nProudly presented by Hassan Mwase  \uf0b7 Responsible for Maintenance and support of Company ERP application, support various \nenterprise applications enhancements and code extensions, build and apply fixes.  \n\uf0b7 Maintained and coded database objects such as procedures, functions, packages and \ninterfac e that support ERP applications, ASP.NET MVC, .Net Framework and Object \nBusiness Intelligence  Enterprise Edition (OBIEE).  \n\uf0b7 Coded PL/SQL subprograms that were called by the front -end application written in C#  \nand ORM Entity framework .  \n\uf0b7 Worked with applicatio n managers to g ather requirements and build solutions based off of \nuser requirements and build enhancements . \n\uf0b7 Proactively assess opportunities, risks, challenges for technology and business, define \nscope, plan and schedule of custom enhancements.  \n\uf0b7 Consistent ly exceeded project requirements and expectations on -time delivery.  \n \nWalt Disney World  - Orlando , FL Feb.2008  \u2013 Aug.2010  \nLead  Senior Programmer/Analyst Consultant  \nTools:  Oracle 10G, Oracle Retail supply chain, Oracle Retail Merchandizing, Oracle Database, Oracle \nForms, Oracle Forms, PL/SQL, S QL Developer, Toad  \n\uf0b7 As project Stream lead, managed  full lifecycle implementation effort. Engaged client managers to \nreview project plans, timelines and deliverables.  \n\uf0b7 SME lead Oracle WMS/RMS implementation/integration contractor and client developer  team . \n\uf0b7 Responsible for analysis, design and final delivery of functioning and technical integrated multiple \nfacility warehouse. Setup and configure environments.  \n\uf0b7 Designed, wrote code  and tested application enhancements, mod ifications, and interfaces. \nSupported post go -live including system integration stabilization, inventory stock on hand \nimbalance resolution between RWMS13.x and RMS13.x issue resolution and knowledge transfer \nto Disney IT personnel.  \nCato Corporation, 8100 Denmark Road, Charlotte, NC   Dec. 2007 \u2013 Aug. 2008  \nProject  Lead - Senior Programmer/Analyst Consultant  \nLead technical consultant, to upgrade  RMS . Plan and recom mend most efficient upgrade strategy of all \ncurrent Oracle retail application modules  to be web -based.   \nTesco Central Europe Prague , Czech Republic  Aug.2007 \u2013 Oct.2007  \nLead Senior Programmer/Analyst Consultant  \nLead functional and technical RWMS Project lead consultant,  build and deploy Oracle Retail \nWarehouse Management System for both Prevov Fresh DC and Ambient DC . Review foundation and \ntransactional data to ensure successful build, test and implementation process  of Slovakian DC in \nPresov  \nIBM - Circuit City Richmond , VA Aug.2006 \u2013 July 2007  \nSenior Analyst Consultant  \nTools: RWMS 10.3 , UNIX, Pro*C, Oracle Database, SQL*Plus, PL/SQL, Oracle Reports, Oracle Forms, \nOracle, RIB  \nLead technical analyst to assist RMS11 implementation and integration . Worked on Purchase \nOrder , Cost and Deals team .  \n \nGAP Inc.  \u2013 Columbus , OH 2005 \u2013 2006  \nSenior Analyst Consultant  \nHassan Mwase           Page 5 \nProudly presented by Hassan Mwase  Tools: RWMS 10.3 , UNIX, Pro*C, Oracle Database, SQL*Plus, PL/SQL, Oracle Reports, Oracle Forms, \nOracle, RIB  \nDevelopment team lead developer on the WMS system - a highly customized version of Oracle \nRetail WMS.   \n \nRetek /Oracle  \u2013 Minneapoli s, MN  2003 \u2013 2005 \nSenior Developer  \nTools: RWMS 10.3 , UNIX, Pro*C, Oracle Database, SQL*Plus, PL/SQL, Oracle Reports, Oracle Forms, \nOracle, RIB  \n\uf0b7 Technical lead/supervisor of Retek WMS Developer and customer product support.  \n\uf0b7 Developed RWMS 10.3 /RMS 10.3 and RIB hospital application support interface. On -site DC \noperation s support and technical lead for RWMS/RIB  at the Navy Exchange Norfolk \nVirginia   .  \n \n \n \nEducation  \n \nBachelor of Commerce  \nUniversity of Calgary, Calgary, Alberta  \n \nComputer Programming  \nYork University, Toronto, Ontario  \n \nCertification  \nOracle Certified Professional \u2013 Certified Oracle DBA  \n \n \n",
          "confidence": 1.0,
          "method": "full_text",
          "structured_data": null
        },
        "tax_term": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "source_by": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "designation": {
          "value": "software developer",
          "confidence": 0.9,
          "method": "ner",
          "structured_data": null
        },
        "experience": {
          "value": "6+",
          "confidence": 0.9,
          "method": "regex_total_experience_summary",
          "structured_data": null
        }
      }
    },
    {
      "resume_path": "data\\input\\Affzal_SrDeveloperResumeS.pdf",
      "confidence_score": 0.48500000000000004,
      "used_ocr": false,
      "extracted_fields": {
        "skills": {},
        "first_name": "AFFZAL",
        "last_name": "ABDUL",
        "primary_email": {
          "value": "Affzal.sql@gmail.com",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "secondary_email": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "phone": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "city": {
          "value": "",
          "confidence": 0.0,
          "method": "ner",
          "structured_data": null
        },
        "state": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "zip": {
          "value": "",
          "confidence": 0.0,
          "method": "regex",
          "structured_data": null
        },
        "work_authority": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "resume_link": {
          "value": "data\\input\\Affzal_SrDeveloperResumeS.pdf",
          "confidence": 1.0,
          "method": "file_path",
          "structured_data": null
        },
        "raw_resume": {
          "value": " \nAFFZAL ABDUL  \nDeveloper  Sr. \nEmail:Affzal.sql@gmail.com  \nM: 361 -330-0666  \n \nPROFESSIONAL SUMMARY  \n \n\u2022 Over 9 years  of experience ranging software development, testing, assurance, and implementation and working in \ncomplex engagements.  Prominently has experience in data warehousing and Business Intelligence technologies using \nMS SQL Server 2000/2005/2008R2/2012/2014 &  2016 on Azure.  \n\u2022 Solid 8 years in depth experience  in MS SQL Server database development and administration  in enterprise \nenvironment including server installation, configuration, upgrade, maintenance, performance tuning , optimization, \nbackup/restore, recovery, migration, monitoring, security planning and trouble -shooting.  \n\u2022 Good understanding in database and data warehousing concepts (OLTP & OLAP)  and also data analysis  along with \nvisualizations using Tableau  and Sisense . \n\u2022 Experience in bo th building and utilizing DevOps  pipelines for the projects which started brand new and legacy systems .  \n\u2022 Leverage strong communication and interpersonal skills to cultivate strong working relationships with clients, vendors \nand offshore partners.  \n\u2022 Worked in dependent and as a team with strong critical thinking, time  management  and proven handling of \nresponsibilities and projects.  \n\u2022 Experience in Information Technology field as a n SQL server Developer with strong expertise in  SQL server \ndevelopment, Physical/Logical design, designing Stored -Procedures/T -SQL coding, Troubleshooting, Backups and \nRestore of databases,  SSIS/SSRS, OLTP and OLAP.  \n\u2022 Experience in creating visualizations on the data and created cosmetic clear dashboards  on top of the analyzed data  in \nboth Tableau and PowerBI . \n\u2022 Transformed data from one source  to other destinations  using tools like SSIS , Informatica  PowerCenter . \n\u2022 Experience in Optimizing  Code and Improving Efficiency in databases including  Re-indexing, Updating Statistics, \nRecompiling Stored Procedures  and performing other maintenance tasks and DBCC Commands.  \n\u2022 Good understanding of Microsoft Reporting  Service  (SSRS) with Report authoring , Report management , Report \nformatting, Report distribution, Report delivery and Report security.  \n\u2022 Extensive time spent on POC for Sisense  to be used in data visualization purposes and thereby  have to stick with using \ntableau for visualization purposes of the business data.  \n\u2022 Experience in enhancing and deploying the SSIS Packages from development server to production server.  Migrated  \nDTS packages into SSIS packages using upgrade advisor tool.  \n\u2022 Used Database Monitoring tools like profiler, performance monitor  and Event viewer.  Good understanding in SQL user \nlogins, database user accounts, server roles, database roles and its permissions . \n\u2022 Experience in integrating the SSRS reports into web applications using Report viewer control, URL methods and \ncreating data driven subscriptions to different users using Report Manager.  \n\u2022 Expertise in PowerShell  Scripting / Python  and making REST  API calls to pull data and also load it to databases.   \n\u2022 Expert ex perience in handling huge volume of data and transforming and cleansing of data and raw data.  Involved in \nhuge data migrations, transfers using BCP, SSIS and Informatica PowerCenter  from SQL Analysis server, Teradata, \nOracle . \n\u2022 Experience in Agile methodology in the SDLC process ( both scrum & Kanban).  \n \n \nEducation Background  \n \n\u2022 Bachelor of Engineering (Information Technology) - Osmania University, India. (2005 -2009)  \n\u2022 Master\u2019s in Information Technology and Management \u2013 IIT, Chicago. (2010 -2012)  \n \n \nTECHNICA L SKILLS  \n \nSQL Server Tools   : Enterprise Manager, Profiler, Query Analyser, Memory Optimization Advisor  \nExport & Import (SSIS), SSRS . \nRDBMS   : MS SQL Server 2005/2008 , 2012 , 2014 , 2016.MS  Access, Oracle  11g, Teradata . \nETL   : SSIS, Informatica  Power Center (9.5 & 10.2) . \nProgramming Languages  : SQL, T -SQL, PL-SQL, Core Java, C #, UNIX , PowerShell  and Vbs  \nInternet Technologies  : IIS, XML, HTML.  \nProductivity Applications  : MS Office Suite along with Outlook.  JIRA . \n \nOperating System  : Windows95/98, Windows 2000/NT/XP , 2003 , 7, MS DOS.  \nReporting Tools   : Crystal Reports 11, SSRS , ARMS reporting , Sisense, and Tableau . \nDesign Documentation              :           UML, MS Visio  \nOther  Tools    :            Tidal , Control -M, VSS, TFS, GIT,  SVN.  \n \n \nPROFESSIONAL EXPERIENCE  \n \nAnthem Inc. , Norfolk , VA                                                                         Mar 2017\u2013 Till date  \nDeveloper  Sr. \n \nAnthem Inc . is an American  health insurance company  founded in the 1940s, prior to 2014 known as  WellPoint, Inc. It is \nthe largest for -profit  managed health care  company in the  Blue Cross and Blue Shield Association . It covers 7 .7 million \nseniors, people with disabilities, low -income families and other state and federally sponsored beneficiaries, and federal \nemployees in 26 states, making it the nation\u2019s largest provider of health care for public programs . \n \nResponsibilities:  \n \n\u2022 Expertise in Provider  dealing with legacy system to load files from states into the FACETS application.  Expertise in \nProvider business domai n area and the flow of the FACETS  application. in-depth knowledge of both providers in \nFACETS.  \n\u2022 Apply the Business rules on the data coming from multiple operating markets. Maintain the business rules in a table for \nvisibility to all required processes.  \n\u2022 Actively worked in gathering requirements with SA and PO to convert them to technical documents for the deve lopment \nto initiate in Jira.  \n\u2022 Used JIRA, SVN Tortoise  & Bit bucket  for code repository and logged / tracked the activities in the team. Also CTU tool \nfor code deployments.  \n\u2022 Used  Splunk  for parsing complex files and  Splunk language ( SPQL ) to make the searches and created dashboards on \nthe data. Also configured notifications as requested by the business.  \n\u2022 Did POC  for DevOps tools and deeper research on both Sisense  and Tableau  tools for utilizing them as data \nvisualization purposes across the provider department.  \n\u2022 Have hands on experience in pulling data from JIRA using REST calls thru PowerShell scripting and load them to SQL \ndb. Also did dashboards on the loaded data and provi ded support for the created dashboards in tableau . \n\u2022 Used Python  to massage the data in REST calls using JIRA libraries and loading them to database. Built dashboards \non top the loaded data for project management team.  \n\u2022 Checked the configuration and performance of the existing databases including data file allocation, index, fill factor, \nfragmentation and the impact on system performance. Analyzed potential problems (response delay, locking, server \ndown time, etc.) to avo id and optimize.  \n\u2022 Actively involved  with production support ( 24*7) and Development environment.  \n\u2022 Used Premise  to trace the slow running queries and tried to Optimize SQL queries for improved performance and \navailability. Also used precise tool for reviewin g the performance.  \n\u2022 Actively involved in code reviews and mentoring the developers. And also involved in development activities.  \n\u2022 Excellent Team Building, Project Management, Analytical, Interpersonal & Communication Skills.  \n\u2022 Extensively worked and did resear ch on Redgate  suite tool belt and implemented them across the whole team.  \n\u2022 Have hands on experience in pulling data from JIRA using REST API calls thru PowerShell scripting and load them to \nSQL db. Also did dashboards on the loaded data and provided support for the created dashboards in tableau.  \n\u2022 Participated in Informatica  DVO  trainings and started extensive use of the tool as part of the DevOps roadmap.  Also \ndid POC  on icedq  and attended trainings.  \n\u2022 Built SQL pipelines using Redgate tool belt  in bamboo and deeper knowledge of branching strategy.  \n\u2022 Used both SonarQube  and Veracode  in the DevOps pipelines using Bamboo . \n\u2022 Built the automation framework for reducing the regression test cycle efforts.  \n\u2022 Used Informatica  PowerCenter  and SSIS  to both move and modify data from different environments and optimized \nworkflows.  \n\u2022 Proficient in SSIS ETL packages  & Informatica workflow  designing and development for various complex solutions \nincluding files processing, Incremental loads, historical data maintena nce. Extensively used Configurations, Logging, \nDebugging using breakpoints, Check points, Transactions, Error and Event Handling . \n\u2022 Assisted team in adding new data concepts and developing new data model development methodologies, principles, \nstandards and g overnance considering process optimization aspects.  \n\u2022 Worked on Tidal to schedule the jobs needed and configure them to receive the notifications on demand basis.  \n \n\u2022 Actively in Cloud  CoE trainings and completed AWS Cloud practitioner course.  \n\u2022 Creation of POC and doing architecture POC for upcoming AWS  engagements.  \n \n \n \nEnvironment : MS SQL Server 2016, 2014/2012/2008/2005/2000(SSMS), Visual Studio 2010/2012 ,2017 , TFS, VSS, \nReporting Services (SSRS), Integration Services (SSIS) , Informatica  PowerCenter , T-SQL,  PL-SQL,  .Net, Redgate tools, \nPowerShell Scripting,  python, Excel , Sisense, Tableau.  \n \n \n \n \nAccordant, A CVS Caremark Company, Greensboro, NC                                                                May 2015 \u2013 Mar 2017  \nSr. SQL  DBA  /Developer  \n \nAccordant Health Services , a CVS Caremark company, is a recognized leader in delivering disease management and case \nmanagement services for people with rare chronic conditions.  It has been providing value -added services on behalf of our \ncontracted clients such as health plans, emplo yers, and third -party  administrators (TPAs) for more than 14 years.  \n \nResponsibilities:  \n\u2022 Installation of MS SQL 2012  enterprise edition  on production, Test and Development environments . \n\u2022 Configured many alerts for sql server ongoing health checks and disk space alerts.  \n\u2022 Took part in design and implementation of Log shipping on few instances for the development and testing teams.  \n\u2022 Involved in Business requirement gathering, Technical Design Documents, Business use cases and Data mapping.  \n\u2022 Extensively worked on SSIS , designed and created mappings using various SSIS transformations like  \nOLEDB Command, Conditional Split, Lookup, Aggregator, Multicast, lookup and fuzzy logic.  \n\u2022 Developed SQL scripts to Insert/Update and Delete data in production Environment. Develop a nd implement policies \nand standards for preserving the integrity and security of data.  \n\u2022 Configure SSIS Package for run time Parameters and Configuration file.  Set the Standards for ETL development for the \nteam to follow on naming standards and best practice s for the meta -data, event handling and logging  \n\u2022 Strong expertise in writing stored procedures and performance optimization for the poor running queries. Good T -SQL \nProgramming skills.  \n\u2022 Worked with the development team to implement data strategies, build data flows and develop data models.  \n\u2022 Used Reporting Services (SSRS) to schedule reports to be generated on predetermined time.  \n\u2022 Generated on -demand and scheduled reports for business analysis or management decision using SQL Server \nReportin g Services (SSRS).  \n\u2022 Used Team Foundation Server and Visual Source Safe as a version control tool.  Migrated objects and solutions from \nvisual source safe to TFS. \n\u2022 Experience in deploying created reports in various sources like Web browser, XML and PDF.  \n\u2022 Used S QL Server Profiler to trace the slow running queries and tried to Optimize SQL queries for improved performance \nand availability.  Implement table partitioning  to avoid table level locking issues when different operating company\u2019s \ndata process is initiated at the same time by locking at partition level.  \n\u2022 Good understanding of Microsoft Reporting Service (SSRS) with Report authoring, Report management, Report \nformatting, Report distribution, Report delivery and Report security.  \n\u2022 Extensive experience in Capacity  planning, Performance Tuning, Disaster Recovery, Troubleshooting procedures . \nActively took part in disaster recovery planning and handled the real time issues.  \n\u2022 Also worked on ARMS  reporting application where I have hands -on experiencing in formatting reports, scheduling and \ndelivery of reports.  \n\u2022 Has good exposure to deploy the solutions in cloud platform and also storage usage.  \n\u2022 Involved in implementing business rules on client files  and membership files from the client and mock the data to \nrequired frequencies and also developed mechanism to alert when frequencies are not met on the fields in \nimplementation.  \n\u2022 Has hands -on experience in deploying the ssis packages to both file and serv er levels.  \n \n \nEnvironment : MS SQL Server 2014/2012/2008/2005/2000(SSMS), Visual Studio 2010/2012, TFS, VSS, Reporting \nServices (SSRS), Integration Services (SSIS), T -SQL, .Net, Excel.  \n \n \n \nFirstView Financial LLC, Atlanta, GA                                                         July 2011 \u2013 May 2015  \nSr. SQL DBA  /Developer  \n \nFirstView Financial LLC, Atlanta, GA provides prepaid debit card solutions enabling clients to enjoy all the benefits of \noperating a fully custom prepaid d ebit card program.  \n                                                                                                                                             \nResponsibilities  \n\u2022 Installation of MS SQL 2008 R2 servers on production, Test and Development boxes . \n\u2022 Developed ETL solutions using SQL Server Integration Services (SSIS) to import data based on requirements for \neasy/less maintenance and easy upgrade.  \n\u2022 Employed condition -based  notifications to let the user know the status of the Agent job s. \n\u2022 Developed complex SSIS packages  using proper control flow tasks  and data flow  transformations as per business \nrequirements.  \n\u2022 Migrated data from EXCEL and Flat files using SSIS packages to load data to the relational database in SQL Server \n2008R2 supporti ng BI solutions.  \n\u2022 Implemented Jaro -Winkler distance algorithm to calculate the similarity between strings in Sql server.  \n\u2022 Developed custom vb scripting for use in building custom functionality in job alerts.  \n\u2022 Developed dynamic SQL and dynamic store procedures, views, indexes, CTEs, cursors for business needs.  \n\u2022 Developed test scripts and environment of the developed SSIS solutions and the loaded data.  \n\u2022 Developed complex SQL scripts based on the business requirements in SQL Server databases.  \n\u2022 Implemented Disaster Recovery plans using Database Mirroring and Log Shipping.  Participated  in disaster recovery \ndrills and played important role in DR restoration.  \n\u2022 Controlling day to day activities of Production database and troubleshooting the issues.  \n\u2022 Researched and implemented SQL Server Auditing 2008 on SQL Servers for Auditing needs.  \n\u2022 Supported SSAS cubes as part of production support and monitored the loading process.  \n\u2022 Involved in requirement gathering, technical documentation Phases, supporting testing . \n\u2022 Created ETL jobs to load and clean gigabytes of data and also handled partitioning of the tables in ETL as a part of \nperformance tuning while handling huge volume of data.  \n\u2022 Provided production support and resolved production tickets.  \n\u2022 Handled  user requested issues through Service Tickets.  \n\u2022 Involved in Change Control Management is the discipline of systematically identifying and controlling change requests \nto a project from both internal and external sources.  \n\u2022 Migrated SQL Server 2005 database to  MS SQL Server 2008  \n\u2022 Designed and implemented comprehensive Backup plan and disaster recovery strategies Implemented.  \n\u2022 Created database objects like tables, views, indexes, stored -procedures, triggers, cursors   \n\u2022 Successfully implemented Database Mirroring in  SQL Server 2008  \n\u2022 Successfully Configured Snapshot, Transactional and Transactional with updatable subscription in Replication.  \n\u2022 Created Maintenance Plans for production servers (Full, Differential and Transactional Backups).  \n\u2022 Monitored and modified Performan ce using execution plans and Index tuning.  \n\u2022 Installation of 32 bit and 64 Oracle 10g client and applied DST Patches for Oracle Linked servers  \n\u2022 Co-coordinating with the programmer analyst for optimizing query, writing stored procedures.  \n\u2022 Conducting Root Cause Analysis of application availability and narrow down to issues related to coding practices, \ndatabase bottlenecks, or network latency  \n\u2022 Creating logins, groups, users, roles, database devices, databases, mirroring  devices, checking for database \nconsiste ncy, fixing DBCC errors, monitoring  error logs, database space allocations, transaction log space allocations , \nfine tuning SQL performance.  \n\u2022 Resolving Locking and Blocking issues by using various SQL  server internal commands  \n \nEnvironment  \nMS SQL Server 2008R2 Enterprise Edition, Oracle, SSMS, SSIS, SSRS , TOAD, SQL Plus, OLAP , OLTP, T -SQL, MS Excel, \nMS Access, Unix, MS Visual Studio.Net, VB, XP professional,2003, Crystal Reports XI.  \n",
          "confidence": 1.0,
          "method": "full_text",
          "structured_data": null
        },
        "tax_term": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "source_by": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "designation": {
          "value": "Developer",
          "confidence": 0.8,
          "method": "regex",
          "structured_data": null
        },
        "experience": {
          "value": "9",
          "confidence": 0.9,
          "method": "regex_total_experience_summary",
          "structured_data": null
        }
      }
    },
    {
      "resume_path": "data\\input\\Data Entry - NY - Lauren.pdf",
      "confidence_score": 0.645,
      "used_ocr": false,
      "extracted_fields": {
        "skills": {},
        "first_name": "Lauren",
        "last_name": "Dubin",
        "primary_email": {
          "value": "laurendbrnx2005@aol.com",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "secondary_email": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "phone": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "city": {
          "value": "Bronx",
          "confidence": 0.7,
          "method": "ner",
          "structured_data": null
        },
        "state": {
          "value": "NY",
          "confidence": 0.9,
          "method": "zip_database",
          "structured_data": null
        },
        "zip": {
          "value": "10468",
          "confidence": 0.7,
          "method": "regex",
          "structured_data": null
        },
        "work_authority": {
          "value": "S",
          "confidence": 0.8,
          "method": "regex",
          "structured_data": null
        },
        "resume_link": {
          "value": "data\\input\\Data Entry - NY - Lauren.pdf",
          "confidence": 1.0,
          "method": "file_path",
          "structured_data": null
        },
        "raw_resume": {
          "value": "Lauren Dubin  \n2835 WEBB AVENUE APT. 8G  \nBRONX, NY 10468  \nCELL:  917 -392-2569  \nPHONE/FAX (718) 549 -3568  \nlaurendbrnx2005@aol.com  \n \nPROFESSIONAL SUMMARY  \n \nA skille d and dedicated Administrative, Data Entry, and Customer Service professional with  10 years experience \ncoordinating, planning, and supporting daily operational functions. Excellent written and oral communication skills. \nExperience handling sensitive and confidential business. Superior organizational and multi -tasking skills. Ability to \nwork independently with minimal supervision, as well as in team environment. Highly motivated individual with \nprofessional demeanor.  \n \nSKILLS  \n \nData Entry, Typing (80+ WPM), Transcription, Medical Scribe, fast long -hand, familiarity with ICD -10 medical \ncoding, insurance, pre -certifications/authorizations, Windows 10, Microsoft Office 365 which includes Word  2019 , \nExcel  2019 , PowerPoint  2019 , Access  2019 , Outlook , and Publisher, Eudora, EPIC, NextGen, Athena, Mozilla \nFirefox & Thunderb ird, Adobe applications , Mac computers, electronic tablets, QuarkXPress, PC DOCS, medical \nterminology, Electronic Medical Records, most transcription systems including internet applications, and voice \nrecognition software such as PowerScribe, Legal word pr ocessing centers, recep tion including Call Centers, clerical, \nscanning, data entry, customer service, computer tech support/Help Desk (problem solving & troubleshooting  \nabilities) , maintaining filing and database systems . \n \nPROFESSIONAL EXPERIENCE  \n \nMetropol itan Transportation Authority     August 2017 -PRESENT  \nBronx, NY \nData Entry/Office assistant  \n\u2022 Comparing data entered with source documents.  \n\u2022 Correcting errors when appropriate.  \n\u2022 Maintaining required records of all completed work and compiling weekly reports.  \n\u2022 Assist in maintaining filing system by organizing, purging, maintaining and retrieving files.  \n\u2022 Clerical duties as assigned  \n \nAcacia Network, Bronx, NY       December 2016 \u2013 June 2017  \nMedical Scribe  \n\u2022 Anticipate physician needs to facilitate the flow of clinics.  \n\u2022 Accurately and thoroughly record patient medical history an d physical exam, procedures and treatments \nperformed by healthcare professionals, including nurses and physician assistants.  \n\u2022 Patient education and explanations of risks and benefits, physician -dictated diagnoses, prescriptions and \ninstructions for patient or family members for self -care and follow -up. \n\u2022 Prepare referral letters as directed by the physician.  \n\u2022 Collect, organize and catalog data for physician quality reporting system and other quality im provement \nefforts and format for submission.  \n\u2022 Attend training s on diverse subjects such as information technology, legal, HIPAA and regulatory \ncompliance, billing and coding.  Quickly assimilate new knowledge into process and procedures.  \n\u2022 Proofread and edit all the physician\u2019s medical documents for accuracy, spelling , punctuation and grammar.  \n\u2022 Utilizing NextGen E lectronic Medical Record system.  \n\u2022 Use of Microsoft Word 2016, Outlook 2016  \n \nTunstall Medical Alert \u2013 Long Island, NY      March 2016 \u2013 July 2016  \nData En try Operator, Customer Service Representative | Temporary As signm ent \n\u2022 Entering alphanumeric information into computer system in preparation for medical billing  \n\u2022 Assisting with authorizations for installation of medical alert equipment  \n\u2022 Electronic and paper filing \n\u2022 Customer service duties  \n\u2022 Clerical duties as assigned  \n \nMill Basin Radiology Services \u2013 New York, NY     October 2013 \u2013 November 2017  \nRemote Medical Transcriptionist /Medical data entry |Part-time and  Per Diem ) \n\u2022 Home based transcription utilizing Word 20 13 of time sensitive material in formatted medical radiology  \nreports, mainly sonograms on an as needed basis, often with deadlines  \n\u2022 Entry of medical data in radiology and medical procedures into Microsoft Word  \n\u2022 Use of Windows 10, medical terminology, Office  2013 including Outlook  \n \nPatient Care Associates \u2013 Mamaronec k, NY    November 2012 \u2013 June 2013  \nMedical Data Entry Operator and Transcriptionist, Office Assistant  \n\u2022 Entering of medical data of radiology and cardiology formats  for acute care facilities involvi ng deadlines and \ntime sensitive material, record keeping of same, projects and spreadsheets recording productivity  \n\u2022 Use of medical terminology, internet incorporated transcription and dictaphone applications  \n\u2022 Reception and clerical support as needed  \n \nMill B asin Radiology Services \u2013 New York, NY     November 2011 \u2013 October 2012  \nSenior In -House Medica l Transcriptionist, Secretary  \n\u2022 Entry of medical data in radiology and medical procedures into Microsoft Word 2003, often with deadlines , \ntime sensitive material fo rmatted  into medical radio logy reports, mainly sonograms  \n\u2022 Filing \n\u2022 Record keeping   \n\u2022 Problem solving and troubleshooting  \n\u2022 Reception  and clerical support as needed    \n\u2022 Use of medical terminology,  Windows XP, Mic rosoft Word 2003 and Excel 2003  \n \nWeill Cornell Medical  College \u2013 New York, NY     May 2009 \u2013 September 2011  \nMedical Administrative Assistant  \n\u2022 Performed administrative support services for the Director and Faculty of the Myeloproliferative Diseases \nprogram in the Department of Medicine/ Division of Hematology an d Medical Oncology.  Maintained \ncomputerized calendar   \n\u2022 Coord inated scheduling appointment process for long -term scheduling monthly, weekly, and daily views   \n\u2022 Coordinated all necessary paperwork for daily meetings and events  \n\u2022 Dealt with press and broadcast media to set up arrangements for the Director's public appear ances    \n\u2022 Handled travel arrangements including local, domestic, and foreign travel  \n\u2022 Handled confidential information and situations on a daily basis   \n\u2022 Screened all phone calls   \n\u2022 Assisted in prepa ration and typing of research grants and contract application s and protocol submissions  \n\u2022 Data entry of clinical material as dictated into Epic electronic medical record  \n\u2022 Performed other job -related duties as required. Use of medical terminology, Windows XP,  Microsoft Office \n2007, Eudora, Mozilla Firefox & Thunderbird , Adobe, Outlook, and Epic  \n \nJCW Enterprises Typing Service \u2013 New York, NY    November 2005 \u2013 December 2010  \nRemote Medical Transcriptionist  \n\u2022 Home based typing and transcription, work was on a contr act basis and commission and consisted of \nmedical transcripti on and data entry  of radiology and surgical reports  \n\u2022 Use of Windows XP, Microsoft Office 2003: Word and Excel, Centricity and internet applications; RTAS \ndictaphone  \n \nEDUCATION  \n \nThe Rhodes School \u2013 New York, NY  \nHigh School Diploma  \n \nMandl School \u2013 The Colleg e of Allied Health \u2013 New York, NY  \nMedical Secretary Science Certificate  \n",
          "confidence": 1.0,
          "method": "full_text",
          "structured_data": null
        },
        "tax_term": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "source_by": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "designation": {
          "value": "Data",
          "confidence": 0.8,
          "method": "regex",
          "structured_data": null
        },
        "experience": {
          "value": "10",
          "confidence": 0.9,
          "method": "regex_total_experience_summary",
          "structured_data": null
        }
      }
    },
    {
      "resume_path": "data\\input\\David Buchberger - Security - OH.pdf",
      "confidence_score": 0.675,
      "used_ocr": false,
      "extracted_fields": {
        "skills": {
          "methodologies": [
            {
              "name": "itil",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                0
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "domain_specific": [
            {
              "name": "security",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                30
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "security_tools": [
            {
              "name": "nessus",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                127
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "mcafee",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                159
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ]
        },
        "first_name": "David",
        "last_name": "Buchberger",
        "primary_email": {
          "value": "david_buchberger@columbus.rr.com",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "secondary_email": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "phone": {
          "value": "6142029370",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "city": {
          "value": "Sugar Grove",
          "confidence": 0.9,
          "method": "address_pattern",
          "structured_data": null
        },
        "state": {
          "value": "OH",
          "confidence": 0.9,
          "method": "address_pattern",
          "structured_data": null
        },
        "zip": {
          "value": "43155",
          "confidence": 0.9,
          "method": "address_pattern",
          "structured_data": null
        },
        "work_authority": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "resume_link": {
          "value": "data\\input\\David Buchberger - Security - OH.pdf",
          "confidence": 1.0,
          "method": "file_path",
          "structured_data": null
        },
        "raw_resume": {
          "value": "Resume \u2013 David Buchberger \n 1 RR1 Box 592, Sugar Grove, OH 43155 - (614) 202-9370 -  david_buchberger@columbus.rr.com  PROFILE  Security professional and USAF veteran with 20+ years\u2019 IT experience (including at Battelle and State of Ohio), with 4+ years in security operations, seeks position that requires familiarity with security tools, ability to communicate with all levels of the organization, and experience with the implementation of cyber strategies from inception to execution.   EDUCATION  Embry Riddle Aeronautical University                                                                               B.S.  EMPLOYMENT HISTORY  August 2018 \u2013  February 2020 Infinite Contract State of Ohio \u2013 Office of Information Security and Privacy   Tier 1 Cyber Security Analyst \u2022 Respond to assigned tickets using ServiceNow \u2022 Remediate phishing attacks using Microsoft Defender Advanced Threat Protection, Office 365, and Symantec Management Center \u2022 Use Symantec Web Analytics to allow approved access to Web page URLS \u2022 Assist Tier 2 and Tier 3 with tickets as needed \u2022 Use McAffe ePO to detect malware on servers and desktops \u2022 Use Tanium to troubleshoot infected systems  2015 \u2013 2018 Battelle      Columbus, OH   Network Systems Analyst II (Security Operations Center/Vulnerability Management) \u2022 Scan Network with Nessus Manager and Professional \u2022 Create and monitor Scan schedule \u2022 Build Nessus Agent Groups \u2022 Perform credentialed, Non-credentialed and Agent scans on servers and workstations \u2022 Perform credentialed and Non-Credentialed WAS and DMZ scans \u2022 Perform internal and external Discovery scans \u2022 Work with clients to resolve Nessus plugin scan requirements \u2022 Prioritize network vulnerability threat with Kenna Security \u2022 Monitor Kenna Security Connector tunnel Nessus file upload integrity \u2022 Assign and track ServiceNow vulnerability remediation requests \u2022 Provide vulnerability remediation assistance to server Admins \u2022 Maintain Access Management CMDB \u2022 Create reports for upper management \u2022 Create detailed instructional documentation \u2022 SOC exposure to SIEM tools i.e. Carbon Black (Response and Defense), FireEye,   2013 \u2013 2015 Battelle      Columbus, OH   Network Systems Analyst II (Internal Network) \u2022 Maintain 2FA (RSA Token, Terminal Server and VMware) NIST FISMA and HIPAA environment \u2022 Verify VEEAM backup integrity, create reports and post to SharePoint for client review \u2022 Troubleshoot, install, and remove third party applications \u2022 Troubleshoot network connectivity issues \u2022 Troubleshoot printer issues \u2022 Install updates \u2022 Remediate vulnerabilities \u2022 Familiarity with Microsoft Group Policy objects as it applies to workstations \u2022 Experience with Microsoft Direct Access \u2022 Submit Firewall and Switch RFC \u2022 Assist remote Users \u2022 Create monthly internal network vulnerability report  2006 \u2013 2013 Battelle      Columbus, OH   Sr. Comm/Equip Spec III (20,000+ User environment)  \nResume \u2013 David Buchberger \n 2 2002 \u2013 2006  Arlington Computer Products (Discover Card)   Columbus, OH   PC Technician (3,500+ User environment) 2001 \u2013 2002  TEKsystems (Columbia Gas)    Columbus, OH   PC Technician (1,000+ User environment) 2000 -2001  National Board of BPVI    Columbus, OH   Technical Support Specialist (100+ User environment) 1998 \u2013 2000 United Parcel Service    Columbus, OH PC Technician (100+ User environment)   KEY SKILLS Communication: \u2022 Able to quickly resolve issues, strong communication skills, and able to maintain professional demeanour in stressful situations  IT: \u2022 Understand packet analysis and networking  \u2022 Familiar with incident response lifecycle  \u2022 Able to be part of on-call rotation  \u2022 Familiarity with Windows, and mobile operating systems  Cyber: \u2022 Previous Information Security experience  \u2022 Independent-thinker and self-starter \u2022 Team Player \u2022 Good follow-up skills and attention to detail  \u2022 Good customer service skills Department Interface Experience \u2022 Support Desk \u2022 Desktop and Infrastructure Support \u2022 Testing (Workstation/Server software/hardware configuration) \u2022 Account Management \u2022 Firewall Team \u2022 Security Operations Center (SOC) \u2022 Asset Management \u2022 Vulnerability Management \u2022 Security Engineering \u2022 Governance, Risk Management, and Compliance (GRC) \u2022 Information Security and Compliance Zone (Project Management) \u2022 Patch Management (SCCM) \u2022 Server Admins (Microsoft & Linux) \u2022 SharePoint \u2022 Data Backup/Storage \u2022 Third Party vendors Certifications: \u2022 ITIL v3, CompTIA A+, MCP, CompTIA Security+, CompTIA CySA+ (in progress) Other: \u2022 Microsoft Certified Professional \u2022 Microsoft Direct Access \u2022 Nessus Manager and Professional \u2022 McAfee Vulnerability Manager \u2022 SharePoint \n",
          "confidence": 1.0,
          "method": "full_text",
          "structured_data": null
        },
        "tax_term": {
          "value": "CONTRACT",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "source_by": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "designation": {
          "value": "Security Analyst",
          "confidence": 0.9,
          "method": "ner",
          "structured_data": null
        },
        "experience": {
          "value": "20+",
          "confidence": 0.9,
          "method": "regex_total_experience_summary",
          "structured_data": null
        }
      }
    },
    {
      "resume_path": "data\\input\\David Musia - Mainframe - TX.pdf",
      "confidence_score": 0.45500000000000007,
      "used_ocr": false,
      "extracted_fields": {
        "skills": {},
        "first_name": "David",
        "last_name": "Musia Denton",
        "primary_email": {
          "value": "davidmusia15@gmail.com",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "secondary_email": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "phone": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "city": {
          "value": "Denton",
          "confidence": 0.7,
          "method": "ner",
          "structured_data": null
        },
        "state": {
          "value": "TX",
          "confidence": 0.9,
          "method": "zip_database",
          "structured_data": null
        },
        "zip": {
          "value": "76210",
          "confidence": 0.7,
          "method": "regex",
          "structured_data": null
        },
        "work_authority": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "resume_link": {
          "value": "data\\input\\David Musia - Mainframe - TX.pdf",
          "confidence": 1.0,
          "method": "file_path",
          "structured_data": null
        },
        "raw_resume": {
          "value": "David Musia  \nDenton,  TX 76210        \ndavidmusia15@gmail.com  | https://www.linkedin.com/in/david -musia -bb1b04151/   \n \nOPERATOR and TECHNICIAN : \nCOMPUTER  OPERATOR  \n|\n MAINFRAME  OPERATOR  \n|\n BATCH  OPERATOR  \n|\n DATA CENTER  OPERATOR  \n \nPROFESSIONAL SUMMARY  \n \nMainframe, Batch,  Data Center and Computer Operator  with 20+ years\u2019 experience . Maintain ed 35+ mainframes consecutively. \nSuccessfully c ompleted and transitioned  with 3 corporate mergers.  Consistently identified  automation  opportunities  of \nmainframe processes to improve efficiencies.  Participated in interviewing, hiring, training and effectively managing personn el \nas needed by management.  Continually r eceive d accolades from customers and teammates for exceptional customer service.  \nMainframe industry experience includes Financial Services, Transportation , Card Services , Military, Oil, Medicare, Medicaid, \nSteel, Pharmaceutical, and U.S. Government . Strong decision -making skills and the ability to exercise composure and \nprofessionalism wh ile successfully managing all situations .   \n \nTECHNICAL EXPERTISE  \n \nAutomation Point  \nSRA  \nSM9  \nEON  \nTNG  \nCA-7 \nCA-11 \nSDSF  \nOPC  \nJOB MASTER  \nOPSMV S \nIMS \nCICS  \nDB2  \nMVS  \nJES2  \nJCS2  \nVirtual Tape  ATRM  \nHSC  \nSLS0  \nVM \nVSE  \nZEKE  \nEDSNET  \nTELLNET  \nSAM  \nWSF2  \nTSO  \nISPF  \nIMF  \nSYSVIEW  \nOMEGAMON  \nCAVIEW  \nNETVIEW  \nAPLMR  BOOLE and BABBAGE  \nJCL \nOffice Vision  \nCONTROL M  \nCONTROL R  \nVTAM  \nVPS  \nSAR  \nDOS  \nMS Offic e and Email  \nSkype  \nHP MyRoom  \nModify Commands  \nInfo Man  \nSTAR  \nVantive Digital Workflow / Service \nCenter  \n \nAREAS OF EXPERTISE  \n \nTeam Leader  \nManagement Support and Backup  \nMaintain 35+ Mainframe LPARS  \nIdentify  Automation  Opportunities  \nPerform IPL\u2019s  \nDisaster Recover Drills  \nDevelopment and Train New Employees  \nAnalyze, Identify,  Troubleshoot, Develop and Create Solutions  \nExceptional Customer Service  \nIdentify and Solve Hardware Issues  \nMonitor System Cycles  \nEmployee Interview and Hiring Decisions  ISO 9000, 9001 Certification  \nHigh Degree of Value on Accuracy and Detail  \nRisk, Controls and Compliance  \nSystems Migration  \nMonito r, Report, Document and Escalate  \nProject Administration  \nProblem Management  \nIncident Management  \nChange Management  \nQuality Management System  \nExce ptional  Planning  / Time Management  \nRoot Cause Analysis  \n \n \n \nEDUCATION  \n\n \nTexas State Technical College  \nComputer Science Technology and Programming  \nWaco , TX \n \nCAREER HIGHLIGHTS  \n \n\u2713 Identified numerous tasks to be automated to decrease operator intervention and increase streamlined operations.  \n\u2713 Relocated entire data center and trai ned Military personnel on operations.  \n\u2713 Facilitated training and development to offshore operators on outsource d mainframes.  \n\u2713 Monitored and supported 35+ mainframes.  \n\u2713 Analyze, troubleshoot and solve various system and customer mainframe issues.\n  \n\u2713 Create and proof QMS process documen ts.\n \n\u2713 Identify and troubleshoot operational issues, escalations and coordination of operational activities . \n\u2713 Ensur e the procedures o f change m anagement are followed for all change activity .\n \n\u2713 Monitor analyze and support  batch processing.\n  \n\u2713 Prioritize daily workload s to meet service level agreements.\n  \n \nWORK  EXPERIENCE  \n \nSenior Mainframe Operator           \nDXC Technology  (from EDS)          2000 \u2013 2019  \n Hewlett Packard Enterprises  (HPE) merged with DXC Technology 2017  \n Hewlett Packard (HP) merged with HPE 2015  \nElectronic Data Systems (EDS) merged with HP 2008  \n         \n \n \n",
          "confidence": 1.0,
          "method": "full_text",
          "structured_data": null
        },
        "tax_term": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "source_by": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "designation": {
          "value": "Project Administration",
          "confidence": 0.8,
          "method": "regex",
          "structured_data": null
        },
        "experience": {
          "value": "20+",
          "confidence": 0.9,
          "method": "regex_total_experience_summary",
          "structured_data": null
        }
      }
    },
    {
      "resume_path": "data\\input\\DEXTER K. NDENGABAGANIZI  - PM Program Manager - OH.pdf",
      "confidence_score": 0.555,
      "used_ocr": false,
      "extracted_fields": {
        "skills": {},
        "first_name": "PROFESSIONAL",
        "last_name": "EXPERIENCE C",
        "primary_email": {
          "value": "ndengaba@gmail.com",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "secondary_email": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "phone": {
          "value": "2693258811",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "city": {
          "value": "Pickerington",
          "confidence": 0.7,
          "method": "ner",
          "structured_data": null
        },
        "state": {
          "value": "OH",
          "confidence": 0.9,
          "method": "zip_database",
          "structured_data": null
        },
        "zip": {
          "value": "43147",
          "confidence": 0.7,
          "method": "regex",
          "structured_data": null
        },
        "work_authority": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "resume_link": {
          "value": "data\\input\\DEXTER K. NDENGABAGANIZI  - PM Program Manager - OH.pdf",
          "confidence": 1.0,
          "method": "file_path",
          "structured_data": null
        },
        "raw_resume": {
          "value": "DEXTER K. NDENGABAGANIZI  \n104 Thrush Circle  \uf06c  Pickerington, OH  43147  \uf06c (269) 325-8811  \uf06c ndengaba@gmail.com     \n \nEDUCATION  \nColumbia University : School of International and Public Affairs  New York, New York  \nGraduate Work Experience , International Finance and Economic Policy  Aug. 2017 \u2013 Jan. 2019  \n\uf0b7 Relevant Coursework : Finance, Public & Nonprofit Management,  Microeconomics, Macroeconomics, Statistics    \n \nMichigan State University: James M adison College  East Lansing, Michigan  \nBachelor of Arts in International Relations  May 2012\n \nPROFESSIONAL EXPERIENCE  \nCouncil on Foreign Relations  (CFR)  Washington, DC  \nAssistant Director, Washington Meetings Program  [May 2016 \u2013 Aug. 2017]  \nProgram Coordinator , Washington Meetings Program  [June 2015 \u2013 May 2016 ] \nProgram Associate, Washington Meetings Program  [Jan. 201 4 \u2013 June 2015 ] \nProgram Assistant , Washington Meetings Program  [Nov . 2012 \u2013 Jan. 201 4] \n\uf0b7 Conceptualize d, manage d, and coordinate d internal and external events ranging from roundtables to multi -day \n300+ daily attendee conferences with key policymakers across various stakeholder groups.  \n\uf0b7 Create d and m anage d relationships with  key domestic and international policymakers , subject matter experts, \nand business leaders to participate in CFR programming  and initiatives  for Council members.  \n\uf0b7 Research ed, monitor ed, and evaluate d international trends to  identify potential p rogramming for CFR \nmembers and the broader domestic and international political and policy community.  \n \nResults for Development Institute  Washington , DC  \nCenter for Health Market Innovations (CHMI) Intern  [May 2012 \u2013 Nov. 2012 ] \n\uf0b7 Assisted in the creation of s trategic plans to increase the visibility of dozens of health market innovations.  \n\uf0b7 Conducted needs synthesis used in the formulation of CHMI\u2019s Web and Facebook Outreach Strategy.  \n \nU.S. Embassy: Foreign Commercial Service  Singapore  \nForeign Commercial Service Intern  [Aug. 2011 \u2013 Jan. 2012 ] \n\uf0b7 Conduct ed trade promotion for 1,300+ American companies and products in Singapore . \n\uf0b7 Interviewed corporate and policy leaders, researched,  and produced  official U.S. Commercial Service Market \nResearch Report s used in U.S.-Singaporean business facilitation s. \n \nHouses of the Oireachtas: D\u00e1il \u00c9ireann (Irish National Parliament)  Dublin, Ireland  \nResearch Assistant , Fine Gael  [May 2011 \u2013 Aug. 2011 ] \n\uf0b7 Researched and produced reports on various European  economic policies , which were  used by Members of \nParliament in  nationally  televised  parliamentary address es.  \n \nADDITIONAL EDUCATION  \nUniversity of London  London, England [Summer 2009]  \nStudy Abroad Program - Economics of Law and Public Policy   \n \nUniversity of the Ryukyus  Okinawa, Japan [Sept. 2009 - Aug. 2010]  \nStudent Exchange Program \u2013 Study of Advanced Japanese and Japanese/Okinawan Culture  \n \nSKILLS  \nJapanese \u2013 Advanced -Low, Stata, R, Photoshop, Adobe Lightroom, Microsoft Access, Excel, Outlook, One Note  \n \nREPORTS AND PUBLICATIONS  \nSingapore: Aerospace Market  \u2013 U.S. Commercial Service: U.S. Department of Commerce (01/2012)  \nSingapore: Franchise Market  \u2013 U.S. Commercial Service: U.S. Department of Commerce  (12/2011)  \n",
          "confidence": 1.0,
          "method": "full_text",
          "structured_data": null
        },
        "tax_term": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "source_by": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "designation": {
          "value": "Program Coordinator",
          "confidence": 0.9,
          "method": "ner",
          "structured_data": null
        },
        "experience": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        }
      }
    },
    {
      "resume_path": "data\\input\\Enterprise Architect - LA - Vijay.pdf",
      "confidence_score": 0.405,
      "used_ocr": false,
      "extracted_fields": {
        "skills": {},
        "first_name": "Vijay",
        "last_name": "Kumar Kanamarlapudi",
        "primary_email": {
          "value": "vijay.kanamarlapudi@hotmail.com",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "secondary_email": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "phone": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "city": {
          "value": "",
          "confidence": 0.0,
          "method": "ner",
          "structured_data": null
        },
        "state": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "zip": {
          "value": "",
          "confidence": 0.0,
          "method": "regex",
          "structured_data": null
        },
        "work_authority": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "resume_link": {
          "value": "data\\input\\Enterprise Architect - LA - Vijay.pdf",
          "confidence": 1.0,
          "method": "file_path",
          "structured_data": null
        },
        "raw_resume": {
          "value": "Vijay Kumar Kanamarlapudi, TOGAF\u00ae, PMP\u00ae, CBAP\u00ae, ACP\u00ae, SAFe\u00ae4 POPM, SSM, PAHM, ITIL, SSBBP  1 \n VIJAY KUMAR KANAMARLAPUDI ,  \nTOGAF\u00ae, PMP\u00ae, CBAP\u00ae, PMI-ACP\u00ae, SAFe\u00ae 4 POPM, SSM , PAHM, ITIL\u00ae, SSBBP  \nEnterprise Business Architect / Project Manager/ Lead Business Analyst  \nvijay.kanamarlapudi@hotmail.com  | (860) 801 -0199  \n \nSummary  \nVijay has t hirteen  years of experience in healthcare as a  Business Consultant  with key expertise in \nBusiness Architecture, Project Management, Bu siness Analysis.  He has led teams in projects following \nAgile and Waterfall methodologies . \n \nCertifications  \n1. TOGAF\u00ae 9 Certified from The Open Group - 2019  \n2. Project Management Professional, PMP\u00ae  from Project Management Institute, PMI\u00ae - 2018  \n3. Certified Business Analysis Professional , CBAP\u00ae  from International Institute of Business \nAnalysis, IIBA\u00ae - 2014  \n4. Agile Certified Practitioner, PMI-ACP\u00ae  from Project Management Institute, PMI\u00ae -2017  \n5. Professional, Academy for Healthcare Management, PAHM  from America\u2019s Health Insurance \nPlans , AHIP - 2011 , 2017  \n6. SAFe \u00ae 4 Certified Product Owner/ Product Manager, SAFe \u00ae 4 POPM  from Scaled Agile \nFramework\u00ae , SAFe \u00ae \u2013 2017  \n7. SAFe \u00ae 4 Certified Scrum Master, SAFe \u00ae 4 SSM  from Scaled Agile Framework\u00ae, SAFe \u00ae- 2018  \n8. ITIL\u00ae  Foundation Certificate in IT Service Management from Axelos Global Best Practice - 2017  \n9. Six Sigma Black Belt Professional, SSBBP  from Management Strategy Institute - 2015  \n10. IBM Certified Database  Associate - DB2 Universal  Database V8.1 Family - DB2 Universal \nDatabase - 2007  \n \nKey Skills  \nEnterprise Business Architecture  Capability Modelling  \nValue Stream Mapping  Activity Mapping  \nBusiness Analysis  Planning and Monitoring  Project Management  \nRequirements  Management and Communication  Project Integration Management  \nElicitation  Project Risk Management  \nEnterprise Analysis  Project Scope Management  \nRequirements Analysis  Healthcare EDI Transactions  \nSolution Assessment and Validation  Waterfall Methodology  \nSCRUM Master  Agile Methodology  \nData Modeling  Project Quality Management  \nGap Analysis  Business Architecture  \nProcess Modelling  Use Case Modelling  \n \nTechnologies  \nVersion Controls: Clear Case, Visual SourceSafe, SharePoint  \nIBM Rational Tools: Requirements Composer, Requisite Pro, Clear Quest, Clear Case, Team Concert  \nSoftware: MS - Word, MS-Excel, Live m eeting, MS-Access, MS-Project, File -Aid, Cisco WebEx, TFS  \nProcess Modelling: Provision , Balsamic  \nLanguages: COBOL, JCL, SQL, XML  \nDatabases: IBM DB2, Oracle , SQL Server  \nVijay Kumar Kanamarlapudi, TOGAF\u00ae, PMP\u00ae, CBAP\u00ae, ACP\u00ae, SAFe\u00ae4 POPM, SSM, PAHM, ITIL, SSBBP  2 \n ERP: PeopleSoft  \nOther: Business Objects, nVision, Plan View, Cognos, Salesforce.com , Tableau , SSRS  \n \nEducation  \nMaster of Business Administration -MBA, May 2019 from Southeastern Louisiana University, Hammond, \nLA, USA  \nBach elor\u2019s in Electrical  and Electronics Engineering - B.Tech. , 2006 from Acharya Nagarjuna University, \nIndia  \n \nWork Experience  \nEnterprise Business Architect , Business Analyst Lead/ Senior Corporate Business Analyst  \u2022 11/2016- \nPresent  at Blue Cross and Blue Shield of Louisiana  \nPrincipal Business Analyst \u2022 09/2014 \u2013 10/2016 at Ultramatics Inc.  \nSenior Associate Consultant \u2022 04/2012 - 9/2014 at Infosys Ltd.  \nAssociate - Projects \u2022 09/2006 \u2013 04/2012 a t Cognizant Technology Solutions US Corp.  \n \nProfessional Experience  \n \nBlue Cross and Blue Shield of Louisiana , Baton Rouge, LA, US                             10/2018 - Present  \nROLE:  Enterprise Business Architect  \nSUMMARY:  \nWorked as a n Enterprise Business Architect analyzing enterprise level impacts on ideas and demands \nsubmitted by senior  management team and prepared business architecture for impacts at an enterprise \nlevel. Documented Business Capabilities; Value Streams; Processes th at are impacted for each idea and \nprepared Architecture Blue Prints for the Baseline and Target State Business Architecture.  \n \nRESPONSIBILITIES:  \n\u2022 Provide d strategic consultation to assigned line -of-business (LOB) or assigned functional area(s) of \nbusiness in  defining or designing business capabilities and processes, functions and organizational \nstructures.  \n\u2022 Participate d in divisional enterprise strategy development, including environmental analysis, \nopportunity identification, value cases and business innovati on portfolio development.  \n\u2022 Acted as an advocate for business needs while in parallel providing ongoing feedback on \ndevelopments and initiatives within IT  \n\u2022 Communicate d IT SLA metrics to respective areas of assigned business functions.  \n\u2022 Collaborate d, facilitat ed and consult ed with business stakeholders of the areas of responsibility in \nplanning, business case development and proposal of business initiatives in alignment with desired \nbusiness capabilities.  \n\u2022 Work ed with other peer business architects and EA architects to identify shared capabilities and \nprocesses and opportunities to create efficiencies across the enterprise and facilitate development \nof cross -functional solutions  \n\u2022 Provide d consultation to business a nalysts on an ongoing basis in communicating divisional \ncapability plans  \n\u2022 At the project execution level, collaborate d extensively with Business Analysts to ensure a tight \nalignment between the business capabilities and high -level use case scenarios identif ied in the \nSolution Blueprint document and corresponding detailed requirements identified in the Business \nRequirements Document (BRD) that support them.  \n\u2022 Participate d in enterprise architecture development, including business architecture, information \narchi tecture, application portfolio and technical architecture.  \n\u2022 Collaborate d with the enterprise program & project management office on reporting project status, \nissues, risks and benefits regarding areas of responsibility.  \nVijay Kumar Kanamarlapudi, TOGAF\u00ae, PMP\u00ae, CBAP\u00ae, ACP\u00ae, SAFe\u00ae4 POPM, SSM, PAHM, ITIL, SSBBP  3 \n \u2022 Explored  ways to apply new technology  to, and reuse existing technology for, business processes; \nresearch and provide information on technical trends and competitors' practices relevant to \nassigned LOB or assigned functional area(s) of business customers.  \n\u2022 Assess ed near -term needs, using struc tured interview processes, to establish business priorities; \nconsult with technical subject matter experts and develop alternative technical solutions; advise on \noptions, risks, costs vs. benefits, and impact on other business processes and system prioriti es. \n\u2022 Collaborate d with other IT functional areas to remain apprised of project status, and inform LOB \ncustomer management of progress; conversely, keep IT's technology and service managers aware of \nkey LOB customer issues, identifying and resolving potentia l problems and conflicts.  \n\u2022 Develop ed, motivate d and direct ed staff to create a team environment, and enable staff to fulfill the \nLOB customer operating objectives.  \n \nMY DELIVERABLES:  \nBusiness Architecture Blue print; Capability Mapping; Value Stream Mapping; Business Process Flow \nDiagram s; Business Need Analysis Document ; Data Requirements Specifications; User Interface \nSpecifications; Gap Analysis; Prototypes; Storyboarding; User Interface Screen Flow Diagram; Business \nRule Definitions . \n \nDELIVERABLES CONTRIBUTED TO:  \nNon -Functional Requirements Specification; Logical Data Model; Business Need Analysis Document; \nWork Breakdown Structure; Test Data Sheet; Test Plan; Test Scripts; Test  Strategy; Test Results \nSummary; Information Architecture; Project Deliverables List; Risk Register; Implementation \nPlan/Checkout activities ; Design Specifications; Technical Requirement Specifications ; SSO \nSpecifications.  \n \n \nBlue Cross and Blue Shi eld of Louisiana , Baton Rouge, LA, US                          10/2014 \u2013 10/2018  \nROLE:  Lead Business Analyst; Senior Corporate Business Analyst; Senior Enterprise Business \nAnalyst;  Scrum Master  \nSUMMARY:  \nWork ed as a program level business analyst overseeing the foundation to put together the operations \nof benefits administration of Medicare Advantage platform that Blue Cross Blue Shield of Louisiana is \nworking to put together.  Worked on various IT efforts to send data from BCBSLA to vendors and to \nreceive and sto re data from external vendors.  \n \nRESPONSIBILITIES:  \n\u2022 Worked closely with clients to identify business needs (requirements) and the costs and benefits of \nimplementing a proposed solution, as a project, in -order to produce an accurate business case for \nthe proj ect \n\u2022 Collaborated with business owners and subject matter experts to develop an understanding of \nbusiness processes and functions to aide in completing the standard project business case related to \na proposed project, analyzing business requirements and ide ntifying potential business solutions; \ncompletes appropriate documentation of business case, requirements and solutions.  \n\u2022 Participate d in divisional enterprise strategy development, including environmental analysis, \nopportunity identification, value cases and business innovation portfolio development.  \n\u2022 Performed project feasibility analysis, scopes project and helps client identify altern ative solutions; \nassist with the development of financial worksheets related to project costs and benefits  \n\u2022 Worked with clients to develop business process flows and to obtain a clear understanding of their \nbusiness models to identify potential pro cess improvements and efficiencies  \nVijay Kumar Kanamarlapudi, TOGAF\u00ae, PMP\u00ae, CBAP\u00ae, ACP\u00ae, SAFe\u00ae4 POPM, SSM, PAHM, ITIL, SSBBP  4 \n \u2022 Ensured adherence to BCBSLA project and program processes and lifecycles to ensure business and \nfunctional requirements are understood, agreed to, documented and can be traced to measurable \nproject success criteria.  \n\u2022 Analy zed business information and process flows, gather requirements and translate to Technical \nspecifications required for application programming and test strategy development.  \n\u2022 Assisted Project Manager in preparing an appropriate transition plan for projects prior to \nimplementation; assists with development and review of appropriate production documentation \nsuch as policies and procedures.  \n\u2022 Worked with clients and IT Quality Assurance to develop a test plan that can be executed via test \nscenarios and test cases  to ensure expected functionality, features, as well as traceability among \nbusiness requirements, technical specifications and testing are validated before releasing \napplication for User Acceptance Testing.  \n\u2022 Identified the functionality and features that should be tested and create test scenarios that validate \nperformance of the scenarios in the new application.  \n\u2022 Developed a test script that describes the details of the test scenarios  \n\u2022 Identified resources and functional areas that will be needed to perform testing  \n\u2022 Represented the Project Management Office in a positive, professional and enthusiastic manner \nwhen working with both internal and external clients to build and maintain the respect of the \nProject Management Office within the organization.  \n\u2022 Escalated  appropriate issues or problems to the project manager or Project Management Office \nManager, as appropriate, for discussion and evaluation  \n\u2022 Maintained professional demeanor with team members, business owners and fellow staff; provide \nconfidentiality for dis cussions regarding projects, position and salary information and other matters \nthat may be discussed in meetings  \n\u2022 Actively pursued personal leadership development to ensure professional growth; consistently \ndemonstrates a high level of quality results and m aintains expected productivity objectives.  \n\u2022 Served as Project Manager for selected effort when necessary.  \n\u2022 Adhered to organizational project management processes and lifecycles to ensure that project \nbusiness and functional requirements are understood, agree d to, can be traced to measurable \nproject success criteria and to drive quality among projects.  \n \nMY DELIVERABLES:  \nBusiness Process Flow Diagram; Data Requirements Specifications; User Interface Specifications; User \nStories; Gap Analysis; Deployment Communi cation Materials; End User Application Documentation; \nProduct Backlog; Issue Tracker; Risk Analysis; Prototypes; Storyboarding; User Interface Screen Flow \nDiagram; Business Rule Definitions; Requirements Traceability Matrix.  \n \nDELIVERABLES CONTRIBUTED TO:  \nNon-Functional Requirements Specification; Logical Data Model; Business Need Analysis Document; \nWork Breakdown Structure; Test Data Sheet; Test Plan; Test Scripts; Test Strategy; Test Results \nSummary; Information Architecture; Project Deliverables List; Ris k Register; Implementation \nPlan/Checkout activities.  \n \n \nCLIENT:  Aetna, Inc. , Hartford, CT, US                                                                     10/2013 - 10/2014               \nROLE:  Scrum Master; Lead Business Analyst  \nSUMMARY:  \nWorked as a Scrum  Master and Lead Business A nalyst for this project dealing not only with gathering \nand maintaining requirements but also guiding the team towards successful implementation of an \napplication with web -based  JAVA user interface functionalities in various  releases.  \n \nVijay Kumar Kanamarlapudi, TOGAF\u00ae, PMP\u00ae, CBAP\u00ae, ACP\u00ae, SAFe\u00ae4 POPM, SSM, PAHM, ITIL, SSBBP  5 \n RESPONSIBILITIES:  \n\u2022 Analysis of the existing business process and helping the product owners in determining the epics \naccording to the proposed business process  \n\u2022 Working with the product o wner to create and maintain product backlog  \n\u2022 Prepared user navigations flows and user interaction diagrams and requirement specifications for a \nweb based front end application.  \n\u2022 Documented business rules for storing the data entered by the user on the front JAVA user interface \n(web based UI) screens - into the back end DB2 tables.  \n\u2022 Planning the timelines for each of the sprints after categorizing the priority of the epics  \n\u2022 Facilitating productivity by removing impediments that obstructed the team\u2019s pursuit of its sprint \ngoals  \n\u2022 Ownership of requirements and also turning the requirements into working software  \n\u2022 Tracking and reporting the progress of the sprints and ensuring that the timelines are achieved \ncomfortably  \n\u2022 Conducting meetings for planning, daily scrum s tatus, scrum pre -planning, back log grooming, \nreview/demo and retrospective discussions  \n\u2022 Ensuring the application of standard processes in each and every stage of the sprints by enforcing \nthe rules of the agile process  \n\u2022 Documentation of the User stories, Use Case framework . \n\u2022 Preparation of Business Proce ss Flow Diagrams, User Interface Screen Flow Diagrams, User Interface \nMock Ups, Use Case Diagrams, Application Overview presentations  \n\u2022 Documentation of Deployment Communication Material and end user reference materials  \n\u2022 Ensuring that the scrum team works in a productive manner to deliver the goals of the sprints . \n \nMY DELIVERABLES:  \nBusiness Process Flow Diagram; Data Requirements Specifications; User Interface Specificatio ns; User \nStories; Gap Analysis; Deployment Communication Materials; End User Application Documentation; \nProduct Backlog; Issue Tracker; Risk Analysis ; Prototypes; Storyboarding ; User Interface Screen Flow \nDiagram ; Business Rule Definitions; Requirements Traceability Matrix.  \n \nDELIVERABLES CONTRIBUTED TO:  \nNon -Functional Requirements Specification; Logical Data Model; Business Need Analysis Document; \nWork Breakdown Structure; Test Data Sheet; Test Plan; Test Scripts; Test Strategy; Test Results \nSummary; Information Architec ture ; Project Deliverables List; Risk Register; Implementation \nPlan/Checkout activities.  \n \nCLIENT:  AETNA  INC., Hartford, CT, US                                                                   01/2013 \u2013 10/2013  \nROLE: Lead Business Analyst  \nSUMMARY : \nWorked as a Lead Business Analyst , managing the business analysts,  resolving i ssues to track to their \nclosure, ensuring  there are no scope -related concerns from stakeholders , guide and train  the Business \nAnalysts , and ensure  the requirements gathering, elicitation , and reviews are completed per plan.  \n \nRESPONSIBILITIES  \n\u2022 Leading a team of nine domain Business Analysts for all business analysis  activities  \n\u2022 Facilitated and lead requirement gathering sessions  \n\u2022 Problem s olving and issue resolution for scope related and business definition related topics  \n\u2022 Managing the requirement timelines and overcoming the challenges faced by individual domain \nlevel Business Analysts  \n\u2022 Coordinating all the tasks that need to be accomplished by all the Business Analysts in the project  \n\u2022 Busin ess analysis and requirement gathering  \nVijay Kumar Kanamarlapudi, TOGAF\u00ae, PMP\u00ae, CBAP\u00ae, ACP\u00ae, SAFe\u00ae4 POPM, SSM, PAHM, ITIL, SSBBP  6 \n \u2022 Conducting Joint Application Development (JAD) sessions  \n\u2022 Preparation of RTM \u2014Requirement Traceability Matrix  \n\u2022 Preparation of Functional Requirement Specification document  \n\u2022 Analysis of existing and nature of proposed proce ss \n\u2022 Developing process flow diagrams according to the proposed process and applications impacted  \n\u2022 Preparation of Use Case Narratives  \n\u2022 Reporting status of requirements progress for Level 1 management during project planning stage  \n\u2022 Support design, development, a nd testing t eams on clarifications for the requirements after \nrequirements phase  \n\u2022 Expertise in using various IBM Rational Tool : Rational Clear Case, Rational Clear Quest, Rational \nRequisite Pro  \n\u2022 Function Point Estimation  \n \nMY DELIVERABLES:  \nData Requirements Specification; Gap Analysis; User Interface Specification; Functional Requirements \nSpecification; Business Requirements Specification, Business Process Models; Use Case Model; Use Case \nNarratives; User Interface Screen Flow Diagram; Proto types ; Deployment Communication Material; Risk \nAnalysis; Issue Tracker; Decision Tables ; Coverage Matrix ; Requirements Traceability Matrix; Product \nSpecifications  \n \nDELIVERABLES CONTRIBUTED TO:  \nNon -Functional Requirements Specification; Logical Data Model; Source to Target Data Mapping \nDocuments; Business Need Analysis Document; Work Breakdown Structure; Test Data Sheet; Test Plan; \nTest Scripts; Test Strategy; Test Results Summary; Information Architecture; Project Delivera bles List; \nEstimation Work Book; Ri sk Register; Implementation Plan/ Checkout Activities.  \n \nCLIENT:  NORTHWESTERN MUTUAL LIFE INSURANCE , Milwaukee, WI, US         04/2012 - 01/2013  \nROLE:  Information Systems Business Consultant  \nSUMMARY : \nWorked as a business systems analyst in gathering requirements related to the proposed apart from \nunderstanding the nature of the current system. Played a key role in ensuring that the high -level design \ndeliverables aligned to the business needs enlisted by the business stakeholders.   \n \nRESPONSIBILITIES  \n\u2022 Analysis of existing system process functionality  \n\u2022 Analysis of the proposed business process flow and working on the proposed system process \nfunctionality based on the architecture guidelines  \n\u2022 Formulation of proposed process flows, use case diagrams and documentation of use cases.  \n\u2022 Documentation of the functional requirements for the proposed financial reports which vary across \nmultiple business areas  \n\u2022 Worked with the design teams to develop the technical specifications according to the reporting \nrequiremen ts \n\u2022 Helped the architectural and database design teams in migra ting the data from the old data base to \nthe new database  \n\u2022 Provided valuable contribution to the design of the new people soft accounting database (by \nproviding suggestions on the trees and nodes t o be created)  \n\u2022 Worked as an information system business consultant and helped to document the functional \nrequirements for the reports to be run from the new database  \n\u2022 Contributing in preparation of test strategy and test plan for the functionalities that are  expected \nfrom the new database  \nVijay Kumar Kanamarlapudi, TOGAF\u00ae, PMP\u00ae, CBAP\u00ae, ACP\u00ae, SAFe\u00ae4 POPM, SSM, PAHM, ITIL, SSBBP  7 \n \u2022 Preparation of high -level design documents with the help of the development team and conducted \nreview sessions with the business stakeholders  \n\u2022 Reporting the status of the functional specification and the high level design del iverables to the \nnecessary stakeholders  \n\u2022 Experience of working on nVision and Business Objects Reports  \n\u2022 Experience of working on reports based out of Cognos (for Business Intelligence Reporting)  \n \nMY DELIVERABLES:  \nReport Requirement Specifications; Data Requirements Specifications; High Level Design \nDocumentation; Business Process Models; Prototypes; Requirements Specifications; Deployment \nCommunication Material; End User Documentation; Gap Analysis; Issue Tracker; Risk Analysis; \nRequirements Traceability  Matrix  \n \nDELIVERABLES CONTRIBUTED TO:  \nNon -Functional Requirements Specification; Logical Data Model; Source to Target Data Mapping \nDocuments; Business Need Analysis Document; Work Breakdown Structure; Test Data Sheet; Test Plan; \nTest Scripts; Test Strategy ; Test Results Summary; Information Architecture; Project Delivera bles List; \nEstimation Work Book  \n \nCLIENT: AETNA INC., Hartford, CT, US                                                                 08 /2007 - 04/2012  \nROLE:  Business Analyst; Senior Business Analyst  \nSUMMARY:  \nWorked as a domain business analyst in all these projects and played a key role in analyzing the current \nprocess and working with the architecture team towards formulating a proposed process.  \nWorked wit h the appropriate stakeholders to deliver the requirements deliverables with good quality \nkeeping in mind the stringent timelines and compliance related constraints on the project.  \n \nRESPONSIBILITIES:  \n\u2022 Business Analysis and Requirement Gathering  \n\u2022 Preparation  of business process flow diagrams and use case model diagrams  \n\u2022 Conducting Joint Application Development (JAD) Sessions  \n\u2022 Preparation of RTM \u2014Requirement Traceability Matrix  \n\u2022 Preparation of Functional Requirement Specification Document  \n\u2022 Analysis of existing and nature of proposed process  \n\u2022 Developing process flow diagrams according to the proposed process and applications impacted  \n\u2022 Preparation of Use Case Specifications  \n\u2022 Reporting status of requirements progress for Level 1 management during project planning stage  \n\u2022 Support design, development, and testing teams on clarifications for the requirements after \nrequirements phase  \n\u2022 Expertise in using various IBM Rational Tools: Rational Clear Case, Rational Clear Quest, Rational \nRequisite Pro  \n\u2022 Worked on onsite/offshore model to  provide support to design, development and \nimplementation checkout activities.  \n \nMY DELIVERABLES:  \nRequirements Specification Document; Business Requirements Specification; Data Requirements \nSpecification; Prototypes; Issue Tracker; Use Case Narratives; Use  Case Models; Business Process \nModels; Risk Analysis; Gap Analysis; Data Scenarios; Decision Table; Business Rule Definitions; Data \nModels; Requirements Traceability Matrix; Function Point Estimation.  \n \nDELIVERABLES CONTRIBUTED TO:  \nVijay Kumar Kanamarlapudi, TOGAF\u00ae, PMP\u00ae, CBAP\u00ae, ACP\u00ae, SAFe\u00ae4 POPM, SSM, PAHM, ITIL, SSBBP  8 \n Non -Functional Requiremen ts Specification; Logical Data Model; Business Need Analysis Document; \nWork Breakdown Structure; Test Data Sheet; Test Plan; Test Scripts; Test Strategy; Test Results \nSummary; Information Architecture; Project Deliverables List, Risk Register  \n \nCLIENT:  AETNA INC. , Chennai, India                                                                         09/2006 \u2013 08/2007                  \nROLE:  Test Specialist  \nSUMMARY:  \nWorked as system integration tester, being the front runner in completing related testing, and made \nsure the deployed product is defect free. Identified design flaws and notified the project team of the \nimpacts and behavior of the system under flaws.   \n \nRESPONSIBILITIES  \n\u2022 Developed the various test artifacts, such as test plan, test scripts, test c ases, test execution logs, \ntest result summary  \n\u2022 Execution of application -oriented testing and recording the status in quality center  \n\u2022 Key resource with strong business knowledge in the corporate domain  \n\u2022 Performed complete analysis of the existing process and maintained proper repository for all the \nclarifications  \n\u2022 Conducted knowledge transfer session for explaining the design to the whole team  \n\u2022 Verification of coding for meeting the design in the given technology; performed the code review \naccording to the codin g standards  \n\u2022 Supported the team members for any technical challenges, as well as for the application challenges  \n\u2022 Well -versed in various tools used in the project such as IBM -DB2, Hummingbird Connectivity tool \nand monitoring jobs through Informatica  \n\u2022 Team management and effective team play  \n \nMY DELIVERABLES:  \nTest Data Sheet; Test Plans; Test Results Summary; Test Scripts; Test Strategy; SQA Testing Strategy and \nRequirements Summary; Coverage Matrix; Defect Tracker  \n \nDELIVERABLES CONTRIBUTED TO:  \nEstimation Wor kbook; Requirements Specification Document; Work Breakdown Structure; Use Case \nNarrative; Report Mockup  \n \n",
          "confidence": 1.0,
          "method": "full_text",
          "structured_data": null
        },
        "tax_term": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "source_by": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "designation": {
          "value": "Business Architect",
          "confidence": 0.9,
          "method": "ner",
          "structured_data": null
        },
        "experience": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        }
      }
    },
    {
      "resume_path": "data\\input\\Horace Royal - Resume.pdf",
      "confidence_score": 0.5700000000000001,
      "used_ocr": false,
      "extracted_fields": {
        "skills": {},
        "first_name": "Horace",
        "last_name": "Royal",
        "primary_email": {
          "value": "horaceroyaljr6_t3n@indeedemail.com",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "secondary_email": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "phone": {
          "value": "2402811264",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "city": {
          "value": "Washington",
          "confidence": 0.7,
          "method": "ner",
          "structured_data": null
        },
        "state": {
          "value": "DC",
          "confidence": 0.8,
          "method": "city_database",
          "structured_data": null
        },
        "zip": {
          "value": "",
          "confidence": 0.0,
          "method": "regex",
          "structured_data": null
        },
        "work_authority": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "resume_link": {
          "value": "data\\input\\Horace Royal - Resume.pdf",
          "confidence": 1.0,
          "method": "file_path",
          "structured_data": null
        },
        "raw_resume": {
          "value": "Horace Royal\nWashington, DC\nhoraceroyaljr6_t3n@indeedemail.com\n+1 240 281 1264\nWork Experience\nContract Closeout Specialist\nCTR Management\nNovember 2018 to Present\nContractor)\nEmployer: CTR Management\nClient: Department of Transportation (DOT)\n\u2022 Performs the full range of contract closeout activities pursuant to FAR 4.804, with a specialized focus on\nthe cognizant administration functions necessary to close flexibly-priced and Cost Accounting Standards\n(CAS) covered contracts.\n\u2022 Achieved closeout goals each month to efficiently and effectively eliminate the contract closeout\nbacklog.\n\u2022 Verify and coordinate closeout activities utilizing PRISM.\n\u2022 Supports the reconciliation of complex invoices.\n\u2022 Resolves issues related to de-obligated and excess funding.\n\u2022 Manages government contract billing/invoicing.\n\u2022 Closeout with a focus on Firm-Fixed Price (FFP), Labor Hour, and Time and Materials (T&M).\nContract Support Specialist\nCACI\nNovember 2018 to Present\nContractor)\nEmployer: CACI\nClient: Department of Health & Human Services (DHHS)\n\u2022 Performed closeouts of simplified acquisitions and various contract types (T&M, Labor, Cost Types).\n\u2022 Reviewed, prepared and submitted closeout documents. Input information into Contract\n\u2022 Management System. Retrieved information from various systems.\n\u2022 Processed Quick. Reviews and filled out all annual Prime Contractor Representations and\n\u2022 Worked jointly and coordinate with subsidiary company stakeholders to ensure all supporting policies\nand procedures are in-place to ensure compliance before making certification.\n\u2022 Collect, draft, review, maintain, and retained contract file documentation.\n\u2022 Draft acquisition/contract documentation for supported official(s) including requirements package\ndocumentation, Acquisition Strategy Panel (ASP) briefing slides, Acquisition Plan (AP), Business/Contract\nClearance form(s), Request for Proposal (RFP), and contract award document.\n\u2022 Pre-award functions include conducting market research, coordinating with stakeholders (e.g. Small\nBusiness Government Representative), publicizing contract actions, soliciting requirements, and drafting\nevaluation of offeror's proposal(s), and drafting awards.\n\u2022 Post-award functions include contract administration, contract actions (e.g. modification) and contract\ncloseout.\nContract Support Specialist\nPremier Management\nDecember 2016 to November 2018\nContractor)\nEmployer: Premier Management\nClient: Food drug Administration (FDA)\n\u2022 Performed all aspects of the Federal Government contract closeouts, some additional work in cradle-\nto-grave acquisitions.\n\u2022 Reconcile contract funding obligations and expenditures with final voucher amounts and determine\nde-obligated amounts.\n\u2022 Prepared contract closeout modifications using PRISM or like systems. Develops reports and prepares\nsupport documentation.\n\u2022 Performed audits of contract files ensuring adherence to policies, regulations, and identifying any\nduplication of efforts across contracts.\n\u2022 Reviewed documents for quality, clarity, and adherence to applicable regulations (FAR, agency\nregulations, etc.) and recommendation of revisions\n\u2022 Reconcile unbilled balances, excess funds, misaligned invoice payments, and related issues and\ncoordinate with the Finance Office to complete the reconciliation process for closure.\nContract Support Specialist\nCorporate Advance Auto\nDecember 2015 to March 2017\n\u2022 Managed complex contracts up to $100K for a variety of service contracts: Building & Facilities\nMaintenance, Lawn services/Grounds Maintenance, Janitorial Services,\n\u2022 Towing, and Police/Surveillance Services while performing in-depth technical market research and\nanalyzed past performance to prepare acquisition planning recommendations and cost analysis for\nDistrict Managers.\n\u2022 Drafted contracts and created, documented, and revised acquisition plans, policy procedures, and best\npractices on an ongoing basis.\n\u2022 Researched and analyzed federal contract laws and regulations.\n\u2022 Trained new personnel. Researched internally/externally to strategize and obtain the appropriate\nvendors, while performing market research to ensure vendors were given opportunity to bid. Negotiated\nwith vendors. Developed a price analysis.\nContract Support Specialist\nCavalier Consultants\nMarch 2015 to December 2015\nContractor)\nEmployer: Cavalier Consultants\nClient: Hillel Foundation\n\u2022 Researched the manufacturer of equipment or contractor.\n\u2022 Prepared provider monthly or annual service payments based on the terms of the maintenance\ncontract.\n\u2022 Negotiated bids for servicing of equipment and supply of labor.\n\u2022 Communicated with vendors as warranties were in the final phase of expiration dates for service\nof goods/ supplies Assisted Program Managers with technical issues with statement of objectives and\nperformance work statements.\n\u2022 Evaluated contract performance on previous contracts and meeting with the company representatives\nto discuss procurement needs, quality of items or services, current market prices, or delivery schedules.\nManaged several sales portfolio contract accounts totaled valued up to $250.000.00\nContract Support Specialist\nGate Gourmet\nJune 2012 to March 2015\n\u2022 Authorized and prepared with senior management staffing forecast schedules for annual budget for\ncredit cards, supplies, communications, travel and training new staff hires.\n\u2022 Developed and prepared inputs to briefings, planning documents, and policies for several airlines by\nconducting research and analysis of administrative information and data.\n\u2022 Provided sales financial analysis and support by designing graphs to present to Senior Management.\n\u2022 Completed and submitted all required documentation of existing contracts.\n\u2022 Acted as the contact person for execution of contracts.\n\u2022 Assisted in bidding sourcing teaming and subcontracting.\nContract Support Specialist\nDistrict Healthcare Inc\nJune 2006 to May 2012\n\u2022 Managed the evaluation process and for contract performance on previous contracts and organized\nkickoff- meetings with small and large businesses.\n\u2022 Created advertisements for bids, then reviewed and selected the bid most suitable to the timeframe\nand budget.\n\u2022 Researched the price of materials, labor cost, and overhead expenses to analyze the bids received.\n\u2022 Assessed whether contractors were adhering to contract parameters, laws, and regulations. Prepared\npurchase agreements, contracts, and leases to acquire the most cost-effective services and terms.\n\u2022 Compared prices, discounts, delivery dates, and handling charges. Negotiated prices and services.\nGrocery Manager\nShoppers Food Warehouse  - Clinton, MD\nMarch 1990 to January 2000\nI managed a store of 70 employees. I managed the shrink (theft,mis) lost of product. I also hired,trained\nemployees. I made the daily schedules. I ordered the products for the grocery department of dry goods.\nI did the monthly and quarterly/yearly inventory of the store. I stock the shelves as needed to keep\nproduct in store.\nEducation\nSTRAYER UNIVERSITY\n2015 to 2017\nAssociate in Business Management\nTemple Hills, MD\nSkills\n\u2022An organized professional with over 12 years of government contracting experience in both the\nfederal and private sectors. The focal-point of my expertise is in the realm of contract interpretation,\ncloseout, negotiation, and administration. In addition, I have extensive knowledge with the Federal\nAcquisitions Regulations (FAR) in which I have successfully executed complex multimillion-dollar\ncontracts for the federal government.\n\u2022Management Account Management, PSC ACQUISITIONS\n\u2022Contracting Writing System\n\u2022PD 2 Contracting Writing System\n\u2022SAM\n\u2022System for Award Management\n\u2022FPDS\n\u2022Federal Procurement Data System\n\u2022Comprizon, PD2 Contracting Writing System\n\u2022DCIS- Departmental Contracts Information System\n\u2022PRISM\n\u2022GLASS\n\u2022Contract Writing System for procurement\n\u2022DELPHI\n\u2022Finance Inventory (invoices, Billing data)\n\u2022Procurement\n\u2022Contracting\n\u2022Negotiation\n\u2022Outlook, windows 10, Microsoft,\n",
          "confidence": 1.0,
          "method": "full_text",
          "structured_data": null
        },
        "tax_term": {
          "value": "CONTRACT",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "source_by": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "designation": {
          "value": "Support Specialist",
          "confidence": 0.9,
          "method": "ner",
          "structured_data": null
        },
        "experience": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        }
      }
    },
    {
      "resume_path": "data\\input\\.Net - Data Lakes - Data Bricks - CA - Mehdi.doc",
      "confidence_score": 0.7289000000000001,
      "used_ocr": false,
      "extracted_fields": {
        "skills": {
          "domain_specific": [
            {
              "name": "engineering",
              "confidence": 0.48,
              "context": "full_text",
              "positions": [
                197,
                350,
                384,
                480,
                1340,
                1933,
                2040,
                3031,
                9997,
                17149,
                73524
              ],
              "experience_weight": 0.6000000000000001,
              "importance_score": 0.7
            }
          ],
          "cloud": [
            {
              "name": "azure",
              "confidence": 0.42,
              "context": "full_text",
              "positions": [
                556,
                1367,
                2630,
                2688,
                2761,
                2791,
                2810,
                2889,
                3169,
                3206,
                3350,
                3360,
                3413,
                3496,
                3526,
                3557,
                3686,
                3729,
                3751,
                3854,
                3871,
                3890,
                3906,
                3927,
                3941,
                3966,
                3979,
                4027,
                4101,
                4116,
                4356,
                4481,
                4506,
                4530,
                4556,
                4573,
                4589,
                4613,
                4637,
                4663,
                4680,
                4709,
                4728,
                4754,
                4774,
                4797,
                4822,
                4844,
                4861,
                4888,
                4908,
                4933,
                4949,
                5087,
                7272,
                10764,
                11565,
                13573,
                13888,
                13926,
                13955,
                14431,
                14524,
                14555,
                14605,
                14619,
                14632,
                14642,
                14663,
                14700,
                14723,
                14755,
                14785,
                14824,
                14870,
                14883,
                14907,
                14960,
                15012,
                15061,
                15091,
                18405,
                18525,
                20535,
                23618,
                23689,
                23741,
                23910,
                24021,
                24051,
                24066,
                24100,
                24170,
                24247,
                24280,
                24397,
                24522,
                24547,
                24571,
                24597,
                24614,
                24630,
                24687,
                24711,
                24737,
                24754,
                24783,
                24802,
                24828,
                24848,
                24871,
                24896,
                24918,
                24935,
                24962,
                24982,
                25007,
                25023,
                25161,
                25732,
                25896,
                26081,
                26273,
                26489,
                26551,
                26622,
                26674,
                26843,
                26954,
                26984,
                26999,
                27033,
                27103,
                27180,
                27213,
                27330,
                27455,
                27480,
                27504,
                27530,
                27547,
                27563,
                27620,
                27644,
                27670,
                27687,
                27716,
                27735,
                27761,
                27781,
                27804,
                27829,
                27851,
                27868,
                27895,
                27915,
                27940,
                27956,
                28094,
                28602,
                28862,
                31792,
                31863,
                31915,
                32084,
                32195,
                32225,
                32240,
                32274,
                32344,
                32421,
                32454,
                32571,
                32719,
                32744,
                32768,
                32794,
                32811,
                32827,
                32851,
                32875,
                32901,
                32918,
                32947,
                32966,
                32992,
                33012,
                33035,
                33060,
                33082,
                33099,
                33126,
                33146,
                33171,
                33187,
                33325,
                35975,
                38571,
                38683,
                40551,
                40852,
                44631,
                44932,
                47040,
                50469,
                51505,
                52454,
                67576,
                67981,
                68159,
                73867,
                74687
              ],
              "experience_weight": 0.4,
              "importance_score": 0.7
            },
            {
              "name": "aws",
              "confidence": 0.42,
              "context": "full_text",
              "positions": [
                562,
                1373,
                5173,
                5210,
                5370,
                5393,
                5745,
                5783,
                5803,
                5848,
                5908,
                7290,
                10384,
                10770,
                11553,
                13589,
                15177,
                15298,
                15549,
                15738,
                16147,
                16167,
                16239,
                16299,
                16377,
                18421,
                18517,
                18757,
                18780,
                18993,
                19059,
                19079,
                19124,
                19184,
                20545,
                31124,
                33734,
                33878,
                33901,
                34114,
                34152,
                34205,
                34250,
                34310,
                35947,
                38563,
                38675,
                45553,
                51517,
                67987,
                73880
              ],
              "experience_weight": 0.4,
              "importance_score": 0.7
            }
          ],
          "methodologies": [
            {
              "name": "devops",
              "confidence": 0.42,
              "context": "full_text",
              "positions": [
                1321,
                1610,
                3384,
                3563,
                7259,
                7278,
                10258,
                14625
              ],
              "experience_weight": 0.4,
              "importance_score": 0.7
            }
          ],
          "data_skills": [
            {
              "name": "hadoop",
              "confidence": 0.42,
              "context": "full_text",
              "positions": [
                2112,
                2369,
                3795,
                4379,
                5328,
                13499,
                14472,
                18700,
                24420,
                25259,
                27353,
                30754,
                32594,
                33850,
                35407,
                39704,
                42646,
                44022,
                47179,
                50193,
                52141,
                66761,
                67825,
                71267
              ],
              "experience_weight": 0.4,
              "importance_score": 0.9
            },
            {
              "name": "spark",
              "confidence": 0.42,
              "context": "full_text",
              "positions": [
                2119,
                2345,
                2463,
                2495,
                4362,
                5335,
                7959,
                11762,
                13506,
                13630,
                13636,
                14479,
                15491,
                17294,
                18165,
                18707,
                20817,
                21504,
                22916,
                24403,
                25266,
                25786,
                27336,
                28367,
                29944,
                30601,
                30761,
                32577,
                33857,
                35414,
                36882,
                37067,
                39698,
                42653,
                66755,
                67832,
                71274
              ],
              "experience_weight": 0.4,
              "importance_score": 0.9
            },
            {
              "name": "hive",
              "confidence": 0.42,
              "context": "full_text",
              "positions": [
                2173,
                2340,
                4315,
                4368,
                5341,
                8095,
                13674,
                14308,
                14501,
                15534,
                18231,
                18713,
                20953,
                24409,
                25343,
                27342,
                30828,
                31324,
                31743,
                32583,
                33863,
                35478,
                39732,
                42659,
                44038,
                47216,
                50209,
                66802,
                67871,
                71309
              ],
              "experience_weight": 0.4,
              "importance_score": 0.9
            }
          ],
          "programming": [
            {
              "name": "scala",
              "confidence": 0.42,
              "context": "full_text",
              "positions": [
                2277,
                2351,
                8128,
                8740,
                13825,
                16707,
                17022,
                18264,
                21010,
                21515,
                21754,
                22882,
                25491,
                29955,
                30194,
                34371,
                34756,
                35617,
                40086,
                43360,
                44478,
                47265,
                50735,
                66876,
                73326
              ],
              "experience_weight": 0.4,
              "importance_score": 1.0
            },
            {
              "name": "java",
              "confidence": 0.42,
              "context": "full_text",
              "positions": [
                2301,
                2357,
                5530,
                8966,
                15686,
                16565,
                25515,
                34395,
                35641,
                36302,
                36726,
                38025,
                40110,
                42153,
                43311,
                43384,
                43569,
                44502,
                47289,
                50574,
                50698,
                52854,
                53661,
                54363,
                71502,
                73134,
                73336
              ],
              "experience_weight": 0.4,
              "importance_score": 1.0
            },
            {
              "name": "sql",
              "confidence": 0.42,
              "context": "full_text",
              "positions": [
                3248,
                3947,
                3972,
                4536,
                6381,
                9143,
                9189,
                12786,
                24577,
                27510,
                32774,
                37839,
                44251,
                46908,
                47752,
                47787,
                48795,
                48903,
                49432,
                51059,
                52197,
                52830,
                53401,
                58636,
                59632,
                61117,
                61896,
                66720,
                66949,
                66991,
                69770,
                69975,
                70683,
                71095,
                71113,
                71143,
                71360,
                71366,
                71373,
                71677,
                72779
              ],
              "experience_weight": 0.4,
              "importance_score": 1.0
            }
          ]
        },
        "first_name": "Mehdi",
        "last_name": "Haghdad",
        "primary_email": {
          "value": "mhaghdad2014@gmail.com",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "secondary_email": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "phone": {
          "value": "9493931150",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "city": {
          "value": "Newport Beach",
          "confidence": 0.7,
          "method": "ner",
          "structured_data": null
        },
        "state": {
          "value": "CA",
          "confidence": 0.9,
          "method": "zip_database",
          "structured_data": null
        },
        "zip": {
          "value": "92612",
          "confidence": 0.7,
          "method": "regex",
          "structured_data": null
        },
        "work_authority": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "resume_link": {
          "value": "data\\input\\.Net - Data Lakes - Data Bricks - CA - Mehdi.doc",
          "confidence": 1.0,
          "method": "file_path",
          "structured_data": null
        },
        "raw_resume": {
          "value": "\nMehdi Haghdad\n62 Parkcrest\nNewport Beach, CA 92612\nCell phone: (949) 393-1150\nmhaghdad2014@gmail.com\n\n                                  EDUCATION\nPHD, UCLA/UNIVERSITY OF CALIFORNIA DAVIS     OBTAINED ON 6/2003\nPhD in Electrical and computer engineering (in Smart Antenna Systems for\nLow Earth Orbit (LEO) Satellites)\n\nThe Royal Institute Of Technology, Stockholm, Sweden\nM.S. Degree: Telecommunications Engineering.\nB.S. Degree: Electrical Engineering\nB.S. Degree: Computer Science.\n\n                                   SUMMARY\nI HAVE ALMOST 20 YEARS OF EXPERIENCE IN ADVANCE ENGINEERING, 10 YEARS OF\nEXPERIENCE IN BIG DATA, 8 YEARS IN CLOUD, MULTI-CLOUD, AZURE, AWS, GCP, 7\nYEARS IN ARTIFICIAL INTELLIGENCE, ML DL CNN. I HAVE WORKED FOR SOME OF THE\nMOST PRESTIGIOUS COMPANIES IN SILICON VALLEY AND OTHER LOCATIONS SUCH AS\nLOCKHEED MARTIN, SPACE SYSTEMS LORAL, MICROSOFT (4 TIMES AS A SENIOR\nSOLUTION ARCHITECT FOR DIFFERENT MULTI-BILLION DOLLAR CLIENTS), HEWLETT\nPACKARD, TEXAS INSTRUMENTS, OPTUM, ACXIOM, CITIBANK, UNITED HEALTHCARE,\nERICSSON, ABB ATOM, ADAPTEC BROAD LOGIC, DELL, ARGONAUT TECHNOLOGIES,\nBAUSCH & LOMB, DARPA, DEPARTMENT OF DEFENSE (DOD) ETC.\nI have been responsible for the development of many systems and\napplications from the designing board to the commercial release.\n\n                                  OBJECTIVE\nPRIMARILY LOOKING FOR CONSULTING AND CONTRACT WORK BUT ALSO OPEN TO\nPERMANENT POSITIONS.\nSenior solution architect Hands On!!, Senior developer, Architect, DevOps\nfor advance engineering in Mulit-Cloud (Azure, AWS and GCP) Big Data,\nArtificial Intelligence. Can also help to build team and lead projects from\nconceptual design to commercial release.\n\nSenior Big Data and Cloud Architect Consultant / BI  Solutions  Architect  /\nData Management and Cloud Lead/ DevOps\n\n\n\n                                   SKILLS\n    . 18 YEARS OF SOLID WORKING EXPERIENCE WITH A PHD FROM UNIVERSITY OF\n      CALIFORNIA UCLA/DAVIS\n    . 17 years of experience as Hands-On Solution Architect, Team Lead,\n      Specialist, Developer, doing Senior Big Data and Cloud Architect\n      Consultant / BI Solutions Architect / Data Management and Cloud Lead\n    . Cloud Architecture, Big Data Engineering, Machine Learning, Deep\n      Learning, Data Scientist, Business Intelligence (BI), Data Warehousing\n      System Engineering multi-tiered applications.\n    . 9+ years of experience in Big Data, Hadoop, Spark, MapR, Cloudera,\n      Hortonworks, Storm, Kafka Confluent, Hive, Impala, Flume, Sqoop,\n      MapReduce, Pig, HBase, NiFi, oozie, Tableau Power BI, Cloudera\n      visualization, QlikView Scala SBT (My Preference), Java Maven etc.\n    . Expert in Big Data: HDFS, Hive, Spark, Scala, Java, Python, Hadoop,\n      Sqoop, Map/Reduce, Hortonworks, Cloudera, MapR, NoSQL HBase,\n      Cassandra, Kafka Confluent, Storm, Spark Streaming, Zeppelin, Kibana,\n      Spark MLib, Mahout, R, NiFi, Falcon, Oozie, Kylin, Atlas, Drill, Solr,\n      ElasticSearch, Ambari,Ranger, Flume, Impala, Pig, HDInsight, SBT etc.\n    . 8 years of Azure cloud and Multi-Cloud Hands on!! Extensive full cycle\n      Azure Cloud experience with full Big Data, Machine Learning Deep\n      Learning, Azure Machine Learning Studio, Azure Power BI, and Azure\n      Search. Comprehensive architecture, development and deployments of\n      massive Azure infrastructures for prestigious companies including 4\n      times for Microsoft as senior solution architect. Full cycle Data Lake\n      Design, Data Engineering and Pipelines, Data Streaming, Data Fabric,\n      Big Data and Artificial Intelligence design and implementation on\n      massive scale. Used Azure Data Factory (ADF Gen1 and Gen2), Azure\n      Datalake Storage (ADLS Gen1 and Gen2), SQL/NoSQL on Cloud, HDInsight,\n      Databricks, Databricks Delta Lake, Infrastructure as Code, Cloud\n      Governance, Azure CLI, Azure PowerShell, Python, DevOps, Kubernetes,\n      Docker, AKS Azure Kubernetes, CI/CD(CICD, CI CD,CI-CD) tools, MLOps,\n      Machine Learning, Deep Learning, Azure Machine Learning Studio, Azure\n      Search, and Elasticsearch, Azure DevOps with automated build and\n      release CI/CD(CICD, CI CD,CI-CD) pipelines utilizing ARM templates,\n      Terraform, Jenkins, Ansible, Azure CLI, Automations, Runbooks, Build\n      Tools, Azure Data Lake Store, Azure Data Lake Analytics, HDInsight\n      (Managed Hadoop), Databricks, Stream Analytics, Machine Learning\n      Studio, Azure Databricks\u00b8 Azure Data Factory, Azure Data Flow, Azure\n      Data Warehouse, Azure Synapse, Azure SQL Data Warehouse (Azure SQL\n      DW), Azure Analysis Services, Event Hubs, Power BI, and Azure Data\n      Catalog, Atlas, Collibra Data Intelligence, Erwin Data Governance.\n      Azure security Azure Active Directory AAD, AD. AAD-DS. ESP, MFA, IR\n      Integration Runtime Gateway, Domain join, Apache Ranger, Ambari,\n      Advanced 4 pillar of security for HDInsight (perimeter VNET, Kerberos\n      AD ESP authentication, Hive policies Ranger and data encryption) Azure\n      Spark Hive HBase Hadoop Kafka Confluent Storm ML Services (R Server)\n      HDInsight clusters, Domain-joined HDInsight clusters, Azure Zeppelin\n      notebooks, Azure Jupyter notebooks, Azure SQL Data Warehouses, Azure\n      Databricks, Azure Data Lake, Azure Data Lake Factory, Azure Data Lake\n      Storage, Azure Data Lake Analytics, Azure Data Links, Azure\n      Integration Runtime (IR), Azure Data Gateway, Azure Kubernetes\n      Services, Azure Storage Blobs, Azure Active Directory, Azure Service\n      Principals, Azure Security Center, Azure Key Vaults, Azure Virtual\n      Network vnet, Azure Log Analytics, Azure Network Interfaces, Azure\n      Cosmos DB, Azure Cortana Intelligence Suite, Infrastructure as a\n      Service IaaS, Platform as a Service PaaS, Microsoft R Server, NLB, Key\n      phrase extraction Azure search, Unstructured text analytics, Event\n      hub, Streaming, Poly Base etc.\n    . 8 years of AWS Cloud Extensive full cycle Cloud AWS Redshift, RDS,\n      EMR, Kinesis, S3, Glue, DMS, Athena, EC2, Lambda, experience with full\n      Big Data, Amazon Elastic MapReduce (EMR), Hadoop, Spark, Hive, Pig,\n      Kafka Confluent, MSK, AWS Management Console, AWS CLI, Amazon EMR File\n      System (EMRFS); Comprehensive CDK and CloudFormation experience with\n      writing script in TypeScript, JavaScript, Python, Java, and C#.\n      Collaborative notebooks Apache Zeppelin, Jupyter, deep learning\n      frameworks like Apache MXNet, RDS Aurora MySQL PostgreSQL,\n      Elasticsearch and SOLR, Machine Learning and Deep Learning development\n      and deployment. AWS Compute E2C, FarGate, Lambda, VMware, AWS\n      Developer Tools, AWS Management Tools, Amazon Machine Learning, AWS\n      DeepLens, Amazon Deep Learning AIMs, Amazon TensorFlow on AWS and\n      other components etc.\n                    .\n    . 5 years of Google Cloud (GCP) Architected and implemented multiple\n      massive projects for different companies Hands On!! Including Big\n      Data, Artificial Intelligence, SOLR on Kubernetes indexing from\n      Dataproc directly. Used among others Cloud Dataproc, BigQuery, Cloud\n      Dataflow, Cloud Data Fusion, Cloud Dataprep, Data Catalog, Google\n      Kubernetes Engine (GKE), Kubernetes CLusters, Dataproc CLusters,\n      Container Registry, Deep Learning Containers, Cloud Bigtable, Cloud\n      SQL, Firebase Realtime Database. For artificial intelligence used AI\n      building blocks, Text-to-Speech, Speech-to-Text, AutoML, Vision AI,\n      Cloud Natural Language, Video AI, AI Platform, AI Hub and AI Platform\n      Deep Learning VM Image etc.\n    . 7 years of Machine Learning, Deep Learning and Artificial Intelligence\n      MLlib, TensorFlow, Keras, Weka Mahout, Multilayer perceptron\n      classifier (MLPC), the feedforward artificial neural network,\n      Convolutional Neural Network CNN, scikit-learn, Pandas,\n      Deeplearning4j, H2o, Sparkling Water ML, Caffe2, MxNet etc. Different\n      algorithms K-Means, Random Forest, Gradient Boosting algorithms (GBM,\n      XGBoost and CatBoost) etc.\n    . 9 years of search engines ELK Stack Elasticsearch, Logstash, Kibana,\n      Filebeat, SOLR from early versions until 8.4.2, SOLR on Kubernetes on\n      GCP!!, Lucene, Rsync, Tika. Also been involved with migration from\n      SOLR to Elasticsearch for at least three companies etc.\n    . Expert in DevOps using Azure DevOps, VSTS, AWS-CodePipeline,\n      Terraforms, Jenkins, Ansible, Git, Maven, Cloudera Navigator, Data\n      Lineage, Kubernetes, Docker.\n    . 5 years of Solid Kubernetes, Orchestration and Micro Services\n      experience Distributed Container based Architecture, Docker, Docker\n      CLI, Kubernetes, Kubernetes CLI, kubeflow, Kube-scheduler, Pods, Pods\n      deployments. Service Deployments, Ingress, Helm Charts, Helm Charts\n      CLI, YAML etc. Successfully installed and deployed for several\n      companies huge Kubernetes Cluster (the latest with more than 2000\n      nodes 16,000 CPUs deployed in 20 minutes) with both Data Pipeline,\n      Data Lake (Raw Data Landing Zone, Processing Zone, Consumption Zone)\n      for Machine Learning, Convolutional Neural Network CNN, Spark on\n      Kubernetes, HDFS on Kubernetes, Kafka Confluent on Kubernetes,\n      Elasticsearch Logstash Kibana ELK on Kubernetes, NiFi on Kubernetes,\n      Hive HBase Jupyter Zeppelin with Scala and Python on Kubernetes. Used\n      Vagrant Terraform HashiCorp for deployment with 2000 nodes, 16,000\n      CPUs with Peta Bytes and 150TB / day capacity etc.\n    . 3 years of IBM Cloud and IBM Cloud Private (ICP) Distributed Container\n      based Architecture, Docker, Docker CLI, Kubernetes, Kubernetes CLI,\n      Pods, Pods deployments. Service Deployments, Ingress, Helm Charts,\n      Helm Charts CLI. Successfully installed and deployed an entire IBM\n      Cloud Private ICP Cluster then implemented and deployed ELK\n      Elasticsearch, Logstash, Kibana, Filebeat, Kafka Confluent, Zookeeper,\n      Cassandra, Curator on ICP IBM Private Cloud, Kubernetes, Pods using\n      Helm Charts, Scala SBT.\n    . 16 years of hands on .NET development, architecture and management\n      experience in application, real time, instrumentation, web, front end,\n      back end, full stack, multiple products out there multiple awards\n    . 16 years of hands on Java development, architecture, front end, back\n      end, full stack\n    . 6 years of Android mobile development and architecture with multiple\n      apps in the app store\n    . 16 years of experience in SQL 7-2016, MySQL, Oracle, and other\n      databases T-SQL, SSIS, SSRS, SSAS, OLTP, OLAP, Multidimensional Cube,\n      MDX, PowerPivot, Tabular Model, SharePoint, PerformancePoint.\n    . Demonstrated experience and understanding of the best practices in all\n      aspects of data modeling, data warehousing (Inmon/Kimball approach).\n      Solid experience in Data Warehouse\n    . Strong knowledge and proven results in Data Warehouse and Data Mart\n      design including Dimensional Modeling (Star & Snowflake Schemas), ER\n      Modeling, 3 Normal Forms, Normalization and Demoralization, Logical\n      Model and Physical Model, Fact/Dimension/Hierarchy identifications.\n    . From Business Case to Data Visualization, I have designed and\n      developed solutions by combining Business Process with Information\n      Technology.\n    . Firmware embedded programming, ARM, PIC, DSP, FPGA, RTOS Linux etc.\n    . Significant management experience including 4 years as the VP of\n      engineering\n\n                                 EXPERIENCE\nMICROSOFT, NEW YORK, NEWPORT BEACH CALIFORNIA      3/2019-PRESENT\nThis was my 4th contract with Microsoft as a high level solution architect\nand expert. Please note I am always hands on, always in addition to doing\nthe architecture I do coding, do DevOps, and do my own developments and\nPOCs. This was a complex hybrid multi projects with Kubernetes, Big Data,\nMulti-cloud (Azue, AWS and GCP), MapR, Kafka, Confluent, SOLR,\nElasticsearch etc. It was for an important Microsoft client, a multibillion\ndollar company in New York with Petabytes of data, and 10 terabyte\nstreaming and data ingestion a day:\n\nI was involved with multiple Kubernetes projects, a hybrid of MapR and\nmulti-cloud system with a massive MapR cluster (with 1400 note 16000 CPUs)\nand Multi-Cloud project (Azure AWS and GCP) with cloud Migration, Kafka\nConfluent streaming using Kappa with massive amount of data in Petabytes\nwith real time streaming using Kappa architecture. This project among\nothers included a comprehensive Data Pipeline, multiple Data Lakes (Landing\nZone, Processing Zone and Consumption zone), high level of domain Join\nsecurity, Big Data Visualizations, Machine Learning, Deep Learning (CNN\nRNN). The architecture that we created is similar to Uber Gen-4\narchitecture with Hudi capable of processing tens of petabytes of streaming\ndata. Please note that due to confidentiality and sensitivity I cannot and\nwill not reveal specific technical details.\n\n    . I lead, architected and helped developing multiple Kubernetes projects\n      on multiple large Kubernetes clusters. Initially we created POCs on\n      AWS (EKS) and Azure (AKS) but finally decided to use the Google\n      Kubernetes Engine (GKE) in GCP for a number of reasons (more than\n      welcome to ask me why in the interview). We created multiple clusters\n      and deployed HDFS Spark in Kubernetes (up to 1000 pods), Kafka\n      Confluent in Kubernetes (created both but decided on using pure Kafka\n      up to 30 pods brokers in Kubernetes), Zookeeper in Kubernetes (up to\n      30 pods ZK), a comprehensive SOLR cluster in Kubernetes (50 SOLR\n      pods), Elastic Search in Kubernetes (50 pods Elasticsearch) and other\n      orchestrations. Please note that due to the StatefulSet nature, these\n      projects were fairly complex using headless services for HDFS, Kafka,\n      Zookeeper, SOLR, Elasticsearch etc however after deployments they were\n      very reliable, scalable. For example with one replica command change\n      in Helm Chart it would go from 5 node to 30 brokers Kafka in minutes.\n      Could index from inside the dataproc into the SOLR kubernetes in a\n      lightning speed. Used other component of the GCP among others Cloud\n      Dataproc, BigQuery, Cloud Dataflow, Cloud Data Fusion, Cloud Dataprep,\n      Data Catalog, Google Kubernetes Engine (GKE), Kubernetes CLusters,\n      Dataproc CLusters, Container Registry, CI/CD (CICD, CI CD,CI-CD)\n      pipelines, Deep Learning Containers, Cloud Bigtable, Cloud SQL,\n      Firebase Realtime Database. For artificial intelligence used AI\n      building blocks, Text-to-Speech, Speech-to-Text, AutoML, Vision AI,\n      Cloud Natural Language, Video AI, AI Platform, AI Hub and AI Platform\n      Deep Learning VM Image etc.\n    . I lead, architected and helped developing a massive MapR cluster with\n      1400 note 16000 CPUs. A comprehensive Data Pipeline with data\n      ingestions from variety of sources RDBMS, hardware logs, images,\n      streaming data, files, documents into the Data Lake. Using Confluent,\n      Kafka Confluent both inside Kubernetes and outside, Kappa architecture\n      with schema evolution, Avro, Parquet (similar to Uber Gen 4\n      architecture) we were able to ingest tens of terabytes of data a day\n      like a charm. The Big Data part used Hadoop, Spark, and Kafka\n      Confluent on mainly MapR but also interacting with Azure HDInsight,\n      AWS EMR, and Kubernetes Cluster. Used also, Spark, Spark Streaming,\n      Storm, Kafka Confluent, Hive, Pig, Impala, Flume, Sqoop, MapReduce,\n      Pig, HBase, NiFi, oozie, Tableau Power BI and, Oozie, QlikView etc.\n      Most of the code were written in Python, PySpark and Scala.\n    . I lead, architected and helped developing a comprehensive Azure\n      infrastructure working with ADF Azure Data Factory gen2, ADLS Azure\n      Data Lake gen2, Data Catalog, Databricks, Delta Lake, Delta tables,\n      Databricks Staging and Merge! Kubernetes, HDInsight, HDInsight\n      Monitoring 4.0, ESP. AD. AAD-DS. ESP, MFA, IR Integration Runtime\n      Gateway, CI/CD (CICD, CI CD,CI-CD) pipelines, Domain join, Apache\n      Ranger, Ambari, Advanced 4 pillar of security for HDInsight (perimeter\n      VNET, Kerberos AD ESP authentication, Hive policies Ranger and data\n      encryption), Advanced ADLS data lake structures, advance Machine\n      learning and CNN, ARM templates, Azure Runbook, notebooks Zeppelin,\n      Jupyter,, Hadoop, Spark, Kafka Confluent, Hive, Hbase. For AI used\n      Azure Machine Learning Service, Azure Machine Learning Studio. For\n      monitoring used Azure Monitor, Azure DevOps, Azure CLI, Azure\n      PowerShell, and Azure Automation. For security I used Azure Active\n      Directory, Azure Role Based Access Control, Azure Subscription\n      Management Azure RBAC, Multi-Factor Authentication, Azure Active\n      Directory Domain Services AAD-DS, Azure Policy, Azure Service\n      Principal, Azure Keys and Key Vault, Enterprise Security Package Azure\n      ESP. For Kubernetes and Micros Services I used Azure Kubernetes\n      Service (AKS), Service Fabric Mesh, Azure Container Instances and\n      Azure Container Registry etc.\n    . I lead, architected and helped developing a comprehensive AWS\n      infrastructure with: Redshift (also tested POC in Snowflake Data\n      Warehouse and Snowpipe), RDS, EMR, MSK (Kafka Confluent), S3 AWS\n      Compute E2C, Glue, DMS, Athena, EC2, RDS Aurora MySQL PostgreSQL,\n      Elasticsearch, Lambda projects with full Big Data, Amazon Elastic\n      MapReduce (EMR), CI/CD (CICD, CI CD,CI-CD) pipeline, CloudFormation\n      CFN,Spark, Kafka Confluent, Oozie, Sqoop, NiFi, Pig, Hive, Hbase, MSK,\n      AWS CLI, Amazon EMR File System (EMRFS). Comprehensive CDK and\n      CloudFormation experience with writing script in TypeScript,\n      JavaScript, Python, Java, and C#. Collaborative notebooks Zeppelin,\n      Jupyter. AWS networking used VPC, Private Subnet, Public Subnet,\n      Internet Gateway, Routing Security groups etc. Visualization and\n      analysis used QuickSight and CloudSearch. Also created a Kubernetes\n      cluster using Elastic Container Service for Kubernetes (EKS), App\n      Mesh, EC2 Container Service (ECS), FarGate. For AI used deep learning\n      frameworks like Apache MXNet Machine Learning and Deep Learning\n      development and deployment, VMware, AWS Developer Tools, AWS\n      Management Tools, Amazon Machine Learning, SageMaker, Alexa Skills\n      Kit, AWS DeepLens, Amazon Deep Learning AMIs, Amazon TensorFlow on AWS\n      and other components. For security used Identity and Access Management\n      (IAM), AWS Organizations, Multi-Factor Authentication. etc.\n    . I lead, architected and helped developing multiple  CDK and\n      CloudFormation projects with writing script in TypeScript, JavaScript,\n      Python and Java.\n    . I lead, architected and helped developing deep learning, machine\n      learning and Convolutional Neural Networks (CNN) using Python, R,\n      PySpark and Scala libraries like scikit-learn, Pandas, Deeplearning4j,\n      Sparkling Water ML, Caffe2, MxNet etc. Different algorithms were used\n      like K-Means, Random Forest, Gradient Boosting algorithms (GBM,\n      XGBoost, XGBoost and CatBoost). Also used GPUs especially with\n      Convolutional Neural Networks CNNs with TensorFlow, Keras.\n    . Development in, Scala, SBT, Eclipse, Python, Pyspark, R, Zeppeline,\n      Jupyter\n\nEricsson, Santa Clara, California 8/2018-3/2019\nDesigned and developed data engineering solutions as a senior Hands-On\nSolution Architect Consultant, Big Data, Data Engineer, Data Scientist,\nsenior developer, Data Warehousing, Spark, HDFS, Kafka Confluent, BI,\nKubernetes, IBM Cloud and IBM Cloud Private (ICP), advanced Machine\nLearning Deep Learning and Convolutional Neural Networks CNN. I am one of\nthe main Architects of GAIA (Ericsson Global Artificial Intelligence\nAccelerator) a $75,000,000 / year new department intended to use Artificial\nIntelligence ML, DL, CNNs to revolutionize cellular communication\nespecially for the G5. Working as senior Solution Architect Consultant,\nsenior developer, senior data engineer, senior data scientist. Implemented\n2000 node 16,000 CPUs Kubernetes cluster and implemented variety of Machine\nLearning, Deep Learning and Convolutional Neural Network CNN. The\nKubernetes Cluster that I created had Data Pipeline, Data Lake (Raw Data\nLanding Zone, Processing Zone, Consumption Zone) with the capacity of Peta\nBytes 150TB /Day Data for Machine Learning, Convolutional Neural Network\nCNN, Spark, HDFS, Kafka Confluent, Elasticsearch Logstash Kibana ELK, NiFi,\nHive, HBase, Jupyter Zeppelin with Scala and Python. Also implemented\nadvanced Machine Learning, Deep Learning, and Convolutional Neural Networks\nCNN and deployed massive POCs on Azure Cloud, GCP, AWS and OpenStack with\nimpressive results.\n\n    . Migrated and architected a Multi-Cloud solution using AWS and Azure\n      using resources such as Redshift, RDS, EMR, Kinesis, S3, Glue, DMS,\n      Athena, EC2, Lambda projects with full Big Data, CI/CD (CICD, CI CD,CI-\n      CD) pipelines, Amazon Elastic MapReduce (EMR), Hadoop, Spark, Hive,\n      Pig, Kafka Confluent, VPC, Subnet, Gateway, AWS Management Console,\n      AWS CLI, Amazon EMR File System (EMRFS), collaborative notebooks\n      Apache Zeppelin, Jupyter, deep learning frameworks like Apache MXNet,\n      Elasticsearch and SOLR, Machine Learning and Deep Learning development\n      and deployment. AWS Compute E2C, RDS Aurora MySQL PostgreSQL, FarGate,\n      Lambda, VMware, AWS Developer Tools, AWS Management Tools, Amazon\n      Machine Learning, AWS DeepLens, Amazon Deep Learning AIMs, Amazon\n      TensorFlow on AWS and other components. etc.\n\n   In a course of less than a year, I have architected and lead some of the\n      most sophisticated Big Data, Deep learning, Machine Learning, and\n      Convolutional Neural Network CNN in the nation for Ericsson Artificial\n      Intelligence Accelerator (Ericsson GAIA) in Santa Clara California.\n      For the first time the technologies were right to create a\n      comprehensive 2000 nodes 16,000 CPUs cluster on Kubernetes with all\n      necessary micro services with automatic orchestration with dynamic\n      deployment for a petabytes Artificial Intelligence system. Some of the\n      results were are impressive. Examples:\n\n\n    . Machine Learning, Deep Learning, Convolutional Neural Network CNN,\n      Anomaly detection on massive amount of real time streaming 5G wireless\n      data with 150TB /day live with high accuracy using the 2000 nodes\n      16,000 CPUs Kubernetes cluster. This would have taken years of\n      processing in the past and is now feasible in minutes on live data.\n    . High accuracy at detecting hack attack, security breach and data\n      breach on live data using CNNs\n    . High accuracy at predicting system availability and reliability and\n      predicting anomalies.\n\n\n***Please note that due to the sensitivity and proprietary nature of these\nprojects and since I am one of the main architect I cannot and will not\nreveal and will not go into too much details!!!\n\n\n    . Created massive Kubernetes clusters on OpenStak, Azure, GCP, AWS with\n      micro service Successfully installed and deployed clusters with more\n      than 2000 nodes 16,000 CPUs deployed in 20 minutes with both Data\n      Pipeline, Data Lake (Raw Data Landing Zone, Processing Zone,\n      Consumption Zone) for Machine Learning, Convolutional Neural Network\n      CNN, Spark on Kubernetes, HDFS on Kubernetes, Kafka Confluent on\n      Kubernetes, Elasticsearch Logstash Kibana ELK on Kubernetes, NiFi on\n      Kubernetes, Hive, HBase, kubeflow, Kube-scheduler, Jupyter Zeppelin\n      with Scala and Python on Kubernetes. Used Vagrant Terraform HashiCorp\n      for deployment with 2000 nodes, 16,000 CPUs with Peta Bytes and 150TB\n      / day capacity.\n    . I lead, architected and helped developing deep learning, machine\n      learning and Convolutional Neural Networks (CNN) systems for 5G\n      wireless data anomaly detection, system availability, Hack attack,\n      Security breach, data breach detection and protection. For peak\n      performance a distributed architecture were created using 16,000 CPUs\n      with amazing results using Spark, HDFS, Scala, SBT, MLlib, TensorFlow,\n      Keras. Some of the development were based on Multilayer perceptron\n      classifier (MLPC) which is a classifier based on the feedforward\n      artificial neural network. Also created other prototypes using Python,\n      R, PySpark and Scala libraries like scikit-learn, Pandas,\n      Deeplearning4j, Sparkling Water ML, Caffe2, MxNet etc. Different\n      algorithms were used like K-Means, Random Forest, Gradient Boosting\n      algorithms (GBM, XGBoost, XGBoost and CatBoost). Also used GPUs\n      especially with Convolutional Neural Networks CNNs with TensorFlow,\n      Keras.\n    . I used TensorFlow, Keras and create similar to YOLO type Convolutional\n      Neural Network on GPUs for detection of different type of anomalies on\n      wireless data. I used the conversion of data to pictures and run it\n      through the Neutral network with amazing results.\n    . I used Acumos AI platform for certain type of ML and CNN projects with\n      cascading Convolutional Neural Networks. I attempted to create an\n      Artificial Intelligence AI environment to facilitate cascading\n      Convolutional Neural Network using.\n    . Created Data Pipeline with real time, intermediate and permanent\n      repositories on Kubernetes. By using huge Kafka Confluent clusters\n      with huge partitions inside Kubernetes, the data was gathered in a\n      Round-robin fashion from variety of sources including real hardware,\n      routers, radios, etc. and was brought into the real time repositories.\n      Using Scala SBT Python Jupyter Zeppelin, Spark, Kafka Confluent, NiFi,\n      ELK, Kubeflow, Kube-scheduler etc. the capacity of the Data Pipeline\n      was 150TB /Day but could be easily extended by simple Kubernetes\n      Orchestration scripts.\n    . Created a Petabytes Data Lake with Raw Data Landing Zone, Processing\n      Zone and Consumption Zone. The Data Lake was specifically designed for\n      ease of use for Artificial Intelligence and therefore the Data\n      Scientist were able to directly access different type of data from\n      real time to intermediate to permanent Data from the Data Lake's\n      Consumption Zone for different type of anomaly detection. The\n      processing in the Data Lake was using Delta and Flip to ensure that\n      the data is accessible at any time even during the processing.\n    . Azure Cloud, I architected, led and did actual implementation of\n      massive Azure Cloud project with extensive full cycle Cloud Azure\n      experience covering the Data Ingestion, Data Transformation and Data\n      Consumption with Machine Learning Deep Learning. This was a massive\n      project on a large scale on Azure which covered a full range of areas\n      including but not limited to Big Data, Machine Learning Deep Learning,\n      Azure Machine Learning Studio, Azure Power BI, Azure Search, and\n      Elasticsearch on Azure development and deployment. Comprehensive\n      deployments of massive Azure infrastructures from inside the Visual\n      Studio 2017 using ARM Templates, Azure Runbook. Deployment of many\n      Azure projects for different companies and here are some of\n      practical!! Hands on!! Component that I have personally used, Azure\n      Spark Hive HBase Hadoop Kafka Confluent Storm ML Services (R Server)\n      HDInsight clusters, Domain-joined HDInsight clusters,, Azure Zeppelin\n      notebooks, Azure Jupyter notebooks, Azure SQL Data Warehouses, Azure\n      Databricks, Azure Data Lake, Azure Data Lake Factory CI/CD (CICD, CI\n      CD,CI-CD) pipelines, Azure Data Lake Storage, Azure Data Lake\n      Analytics, Azure Data Links, Azure Integration Runtime (IR), Azure\n      Data Gateway, Azure Kubernetes Services, Azure Storage Blobs, Azure\n      Active Directory, Azure Service Principals, Azure Security Center,\n      Azure Key Vaults, Azure Virtual Network vnet, Azure Log Analytics,\n      Azure Network Interfaces, Azure Cosmos DB, Azure Cortana Intelligence\n      Suite, Infrastructure as a Service IaaS, Platform as a Service PaaS,\n      Microsoft R Server, NLB, Key phrase extraction Azure search,\n      Unstructured text analytics, Event hub, Streaming, Poly Base etc..\n    . The Big Data part used Hadoop, Spark on mainly MapR but also Cloudera\n      and Hortonworks, Storm, Kafka Confluent, Hive, Pig, Impala, Flume,\n      Sqoop, MapReduce, Pig, HBase, NiFi, oozie, Tableau Power BI and\n      Cloudera visualization, QlikView etc.\n    . Development languages, Extensive Scala SBT (My Preference), Java\n      Maven, Eclipse Intellij (my preference), Python, R, Jupyter, PySpark,\n      Ruby and even some, Linux Shell Script, Shell Scripts\n\nMicrosoft, New York, California   3/2018-8/2018\nSenior Solution Architect Consultant, senior developer, Azure Big Data,\nETL, various databases, Data Warehousing, Spark, Databricks, HDInsight, BI,\nadvanced Machine Learning Deep Learning. I implemented, architected and a\nmassive Azure Cloud infrastructure with big data, machine learning, deep\nlearning, and Artificial Intelligence Neural Network\n\n   This was a project with Microsoft and I worked as Microsoft expert on\n      Azure in New York, other companies involved were Pragmatic Works and\n      Selective in New York. I was involved from the designing board all the\n      way to complete implementation and production release. Azure despite\n      simplicity has enormous amount of details. Many engineers that I\n      interviewed and some of whom that I worked with, may have known bits\n      and pieces but actually creating clusters and implementing systems on\n      Azure require substantial experience and know how, I have that.\n    . Azure Cloud, I architected, led and did actual implementation of\n      massive Azure Cloud project with extensive full cycle Cloud Azure\n      experience covering the Data Ingestion, Data Transformation and Data\n      Consumption with Machine Learning Deep Learning. This was a massive\n      project on a large scale on Azure which covered a full range of areas\n      including but not limited to Big Data, Machine Learning Deep Learning,\n      Azure Machine Learning Studio, Azure Power BI, Azure Search, and\n      Elasticsearch on Azure development and deployment. Comprehensive\n      deployments of massive Azure infrastructures from inside the Visual\n      Studio 2017 using ARM Templates, Azure Runbook. Deployment of many\n      Azure projects for different companies and here are some of\n      practical!! Hands on!! Component that I have personally used, Azure\n      Spark Hive HBase Hadoop Kafka Confluent Storm ML Services (R Server)\n      HDInsight clusters, Domain-joined HDInsight clusters, Azure Zeppelin\n      notebooks, Azure Jupyter notebooks, Azure SQL Data Warehouses, Azure\n      Databricks, Azure Data Lake, Azure Data Lake Factory, CI/CD (CICD, CI\n      CD,CI-CD) pipelines, Azure Data Lake Storage, Azure Data Lake\n      Analytics, Azure Data Links, Azure Integration Runtime (IR), Azure\n      Data Gateway, Azure Kubernetes Services, Azure Storage Blobs, Azure\n      Active Directory, Azure Service Principals, Azure Security Center,\n      Azure Key Vaults, Azure Virtual Network vnet, Azure Log Analytics,\n      Azure Network Interfaces, Azure Cosmos DB, Azure Cortana Intelligence\n      Suite, Infrastructure as a Service IaaS, Platform as a Service PaaS,\n      Microsoft R Server, NLB, Key phrase extraction Azure search,\n      Unstructured text analytics, Event hub, Streaming, Poly Base etc.\n\n\n\nOptum United Healthcare, Santa Ana (partially in Minneapolis), California\n8/2016-3/2018\nLead, Senior Solution Architect Consultant, senior developer, Big Data,\nData Engineer, Data Scientist, Data Warehousing, Spark, HDFS, Kafka, BI,\nKubernetes, IBM Cloud and IBM Cloud Private (ICP), advanced Machine\nLearning Deep Learning on the cutting edge of the Genome, Berkeley Amplab\nAdam Genomics, GATK. Elasticsearch SOLR. Also implemented and deployed a\nmassive Azure Cloud infrastructure with big data, machine learning, deep\nlearning, and artificial intelligence convolutional neural network CNN.\n\n   In a course of one year, I have architected and lead some of the most\n      sophisticated Big Data, Deep learning and Machine Learning, Azure\n      Cloud projects in the nation for Optum in Minneapolis and California.\n      I have had access and utilized thousands of servers, 12,000 CPUs,\n      enormous amount of memories and the results has been astonishing\n      beyond even my own and everybody's expectations. Examples:\n\n\n    . Genomic analysis for prediction of various cancers on 3000 known\n      samples from our genomic bank, 12,000 CPUs, enormous amount of\n      memories the processing time were reduced from 46.7 years to 22\n      minutes 47 seconds! with 99% prediction accuracy, this was recently\n      presented at a conference.\n    . 4 billion records with 53 pre and post processing queries, reduced\n      from days to under a minute!\n\n\n   Please note that due to the highly proprietary and sensitive nature of\n      these projects, I will not be able nor will I disclose the technical\n      details.\n\n\n    . I lead, architected and helped developing deep learning and machine\n      learning systems for genetic analysis and prediction system for\n      occurring of different type of cancers with more than 99% accuracy.\n      For peak performance a distributed architecture were architected using\n      12000 CPUs with amazing results. Spark, HDFS, Scala, SBT, MLlib,\n      TensorFlow, Keras. Some of the development were based on Multilayer\n      perceptron classifier (MLPC) which is a classifier based on the\n      feedforward artificial neural network. Also created other prototypes\n      using Python, R, PySpark and Scala libraries like scikit-learn,\n      Pandas, Deeplearning4j, Sparkling Water ML, Caffe2, MxNet etc.\n      Different algorithms were used like K-Means, Random Forest, Gradient\n      Boosting algorithms (GBM, XGBoost, XGBoost and CatBoost)\n    . For hardware processing of the Big Data, the Deep Learning and Machine\n      Learning different architectures were tested on IBM Neteeza, Teradata\n      and distributed architecture with 12000 CPUs and memory (Spark). The\n      result were absolutely clear, there is no comparison the distributed\n      architecture is far more superior and is the future!\n    . The Big Data part used Hadoop, Spark on mainly MapR but also Cloudera\n      and Hortonworks, Storm, KafKa, Hive, Pig, Impala, Flume, Sqoop,\n      MapReduce, Pig, HBase, NiFi, oozie, Tableau Power BI and Cloudera\n      visualization, QlikView etc.\n    . I lead, architected and helped developing a hybrid system of\n      Elasticsearch and HDFS using Logstash, Rsync and Kafka. The\n      Elasticsearch was sharded over 25 nodes but later deployed on AWS.\n      Three type of data were indexed and inputted into the Elasticsearch\n      (Kibana):\n    o The hardware and system logs for real time (Kafka) hardware and system\n      monitoring\n    o The patient and claim data from HDFS, Hive and HBase for search and\n      quick BI visualization in Kibana.\n    o Data Export files from MarkLogic\n    o For basic search used by different system via an API on top of the\n      Elasticsearch\n    . I lead, architected and helped developing a massive data ingest system\n      from different providers with a permanent and real time pipeline\n      (Kafka) using Delta and Flip methods for an ongoing uninterrupted data\n      ingest. The source data were RDBMS, Hive, HBase, MarkLogic, flat data\n      and log files etc.\n    . Azure Cloud, I architected, led and did actual implementation of\n      massive Azure Cloud project with extensive full cycle Cloud Azure\n      experience covering the Data Ingestion, Data Transformation and Data\n      Consumption with Machine Learning Deep Learning. This was a massive\n      project on a large scale on Azure which covered a full range of areas\n      including but not limited to Big Data, Machine Learning Deep Learning,\n      Azure Machine Learning Studio, Azure Power BI, Azure Search, and\n      Elasticsearch on Azure development and deployment. Comprehensive\n      deployments of massive Azure infrastructures from inside the Visual\n      Studio 2017 using ARM Templates, Azure Runbook. Deployment of many\n      Azure projects for different companies and here are some of\n      practical!! Hands on!! Component that I have personally used, Azure\n      Spark Hive HBase Hadoop Kafka Storm ML Services (R Server) HDInsight\n      clusters, Domain-joined HDInsight clusters, CI/CD (CICD, CI CD,CI-CD)\n      pipelines, Azure Zeppelin notebooks, Azure Jupyter notebooks, Azure\n      SQL Data Warehouses, Azure Databricks, Azure Data Lake, Azure Data\n      Lake Factory, Azure Data Lake Storage, Azure Data Lake Analytics,\n      Azure Data Links, Azure Integration Runtime (IR), Azure Data Gateway,\n      Azure Kubernetes Services, Azure Storage Blobs, Azure Active\n      Directory, Azure Service Principals, Azure Security Center, Azure Key\n      Vaults, Azure Virtual Network vnet, Azure Log Analytics, Azure Network\n      Interfaces, Azure Cosmos DB, Azure Cortana Intelligence Suite,\n      Infrastructure as a Service IaaS, Platform as a Service PaaS,\n      Microsoft R Server, NLB, Key phrase extraction Azure search,\n      Unstructured text analytics, Event hub, Streaming, Poly Base etc..\n    . Genome Analysis Toolkit 4 (GATK4) from Broad Institute, ADAM Genomics\n      Berkeley AMPLab, BAM, SAM, VCF genome variant. Worked also with mango,\n      gnocchi, deca, avocado, quinine, cannoli etc\n    . Using SOLR / Elasticsearch created a detail analytical graphical\n      dashboard in Kibana for Patient Data, Claim Data and Provider Data.\n    . I architected and led multiple AWS projects, Redshift, RDS, EMR,\n      Kinesis, S3, Glue, DMS, Athena, EC2, Lambda, with full Big Data,\n      Amazon Elastic MapReduce (EMR), Hadoop, Spark, Hive, Pig, Kafka, AWS\n      Management Console, AWS CLI, Amazon EMR File System (EMRFS),\n      collaborative notebooks Apache Zeppelin, Jupyter, deep learning\n      frameworks like Apache MXNet, Elasticsearch and SOLR, Machine Learning\n      and Deep Learning development and deployment. AWS Compute E2C,\n      FarGate, Lambda, VMware, AWS Developer Tools, CI/CD (CICD, CI CD,CI-\n      CD) pipelines, AWS Management Tools, Amazon Machine Learning, AWS\n      DeepLens, Amazon Deep Learning AIMs, Amazon TensorFlow on AWS and\n      other components. etc.\n    . Development languages, Extensive Scala SBT (My Preference), Java\n      Maven, Eclipse Intellij (my preference), Python, R, Jupyter, PySpark,\n      Ruby and even some, Linux Shell Script, Shell Scripts\n    . Successfully installed and deployed an entire IBM Cloud Private ICP\n      Cluster then implemented and deployed ELK Elasticsearch, Logstash,\n      Kibana, Filebeat, Kafka, Zookeeper, Cassandra, Curator on ICP IBM\n      Private Cloud, Kubernetes, Pods using Helm Charts, Scala SBT. IBM\n      Cloud and IBM Cloud Private (ICP) is a Distributed Container based\n      Architecture, Docker, Docker CLI, Kubernetes, Kubernetes CLI, Pods,\n      Pods deployments. Service Deployments, Ingress, Helm Charts, Helm\n      Charts CLI.\n\n\nOneStop, El Segundo, California   2/2016-8/2016\nSenior lead, Senior Solution Architect Consultant, senior developer, Big\nData, Data Warehousing, BI, SOLR, Lucene, Elasticsearch, Mahout, Weka\nMachine Learning Lead.\n   I was initially hired at OneStop because of similar experience I had from\n      Dell and Microsoft in Big Data, SOLR Elasticsearch and Machine\n      Learning, taxonomy etc.\n    . I lead, architected and helped developing a Big Data system with 256\n      nodes using Hadoop, Spark on Hortonworks and later migrated to\n      Cloudera, Storm, KafKa, Hive, Pig, Impala, Flume, Sqoop, MapReduce,\n      Pig, HBase, oozie, Tableau Power BI and Cloudera visualization,\n      QlikView\n    . Development languages, extensive Scala SBT (my preference), Java\n      Maven, Eclipse Intellij, Python, R, PySpark, Ruby, C#, Unix Shell\n      Scripts, Linux Shell Scripts\n    . I lead, architected and help developing the SOLR/Lucene Search system\n      which was later migrated to Elasticsearch with 15 nodes sharding. The\n      system was initially developed and maintained in house but later\n      deployed to AWS cloud and prototyped on Azure. The development and\n      test of the 15 node done on VirtualBox machines, physical machines\n      before deployment to the Cloud. The SOLR development was done in two\n      different phases, initially we did indexing directly on top of the\n      metadata extracted from various files with Apache Tika, Apache Flume\n      and scoop. I wrote a scheduler in Java that run delta indexing\n      periodically every few hours. We had customized faceting and then the\n      API would grab the top N results from the XML. The search worked\n      better than expected, the indexing was slow but the search was\n      extremely fast in fraction of a second. On the second phase we stored\n      all the raw documents in HDFS and create indexing and then use HBase\n      to store the index files in HDFS. Also an API was developed in JAVA\n      with a .NET wrapper with SOLR search calls into the SOLR engine.\n    . I lead, architected and help developing an advanced Machine Learning\n      system initially in Spark MLlib then Weka and ultimately a Mahout\n      Machine learning and recommendation system using both ItemSimilarity\n      and UserNeighborhood. I personally favored and created porotypes using\n      Spark MLlib, TensorFlow, Keras, Python libraries like scikit-learn,\n      Pandas but in this case Mahout worked very well. The .NET API would\n      record every time a product was clicked or purchased. The data was\n      recorded in the database and then the metadata was created and the\n      mahout would create a scoring table (0-10) for product and region. The\n      .NET API would select top N highest score and would present it as\n      recommendations.\n    . I lead, architected and helped developing a gigantic amount of data\n      extraction, data warehousing, Big Data. The data was gathered in\n      access of tens of terabytes from more than 40 top of the lines brands\n      Ecommerce sites partnered and operated by OneStop like FRYE, Juicy\n      couture, NYDJ, PAIGE, Splendid, Coffee Beans, Jones New York, Hudson\n      and many more. Used SSIS ETL for SQL to port data to the Data\n      Warehouse and then used Sqoop for extracting from RDBMS to the HDFS,\n      used Flume for extracting from logs files, FTP NAS files to the HDFS,\n      used Apache Tika and Java for extracting metadata from various files\n      into the HDFS, used Nutch for web crawling and for extraction metadata\n      into the HDFS, used SAPI, CMU Sphinx, Kaldi for customer service voice\n      to text conversion into the HDFS\n    . I architected and implemented a real time and streaming component for\n      the Cloudera visualization using Apache Strom and Apache Kafka.\n    . I lead, architected and help developing an elaborate real time\n      visualization using Tableau and Cloudera visualization for the big\n      data portion.\n    . The Big data prototype was deployed both on AWS and Azure. For a\n      number reasons the final decision for the cloud deployment was made\n      for deployment into the AWS not Azure.\n    . I lead and oversaw the conversion of part of the SOLR search project\n      to Elasticsearch and benchmarked the performance. Although I liked\n      working with JASON for various reason SOLR was preferred initially.\n    . I lead, architected and help developing Kibana 4.5 visualization on\n      top of both SOLR and Elasticsearch.\n    . Wrote and oversaw a development of combination of batch files, python\n      and Ruby scripts for SOLR/Lucene and Big Data deployment and\n      configurations. I have to add that I started the conversion of batch\n      file to Python but there were simply not enough time.\n    . Did extensive prototyping and benchmarking and helped evaluating the\n      performance of the big data on Massively Parallel Processing (MPP) and\n      other  Data Warehouse Appliances such as IBM Netezza, Teradata, APS\n      (PDW), Oracle Exadata\n\n\nCanadian Tire, California / Toronto     8/2015-2/2016\nSenior Big Data, DW and BI Lead Solution Architect Consultant.\n    . Led multiple large scale Big Data, Enterprise Data Warehouse EDW and\n      Business Intelligence BI projects on Teradata utilizing Spark, Hadoop,\n      Hortonworks, Cloudera, Hive, Impala, Flume, Sqoop, Map Reduce, Pig,\n      HDInsight, HBase, oozie, and facilitating the real-time data analysis\n      by the data scientist.\n    . Led multiple EDW projects, prototyped and evaluated their performance\n      on the Massively Parallel Processing (MPP), other Data Warehouse\n      Appliances such as IBM Netezza, Teradata, APS (PDW), Oracle Exadata\n    . Development languages, extensive Scala SBT (my preference), Java\n      Maven, Eclipse Intellij, Python, R, PySpark, Ruby, C#, Unix Shell\n      Script, Linux Shell Scripts\n    . Leading the team, I designed architected and implemented the migrating\n      from legacy information warehouse to a modern high performance Big\n      Data and Data Warehouse running on multiple DW appliances. Drafted a\n      BI/DW prioritized implementation roadmap working with the business and\n      finance department.\n    . Leading the team we migrated and deployed 5 projects to Azure Cloud. I\n      was personally involved in the full cycle of vendor selection,\n      requirement gathering, design, development and the deployment of these\n      projects. The migration included different aspects of the projects\n      from front, backend, and integration. We went through thorough\n      research before selecting the Azure cloud for this project and also\n      utilized cutting edge utilities to perform the migration and\n      deployment.\n    . Drafted a BI/DW prioritized implementation roadmap while taking input\n      from internal divisional service plans, business and IT strategy\n      documentation, as well as corporate BI Strategy and the Financial\n      Planning and Reporting System\n    . Designed Enterprise Information Management (EIM) solutions for retail\n      operation. Led technical teams and designed various BI solutions\n      including loyalty programs, card management, POS data management,\n      customer behavioral analysis, store dashboards, finance, ecommerce,\n      cyber security analytic.\n    . Defined the data governance strategy, designed security patterns,\n      implemented data standards and procedures across the enterprise;\n      drafted business specific methodology to establish business\n      stakeholder-driven data stewardship through MDM\n    . Conducted BI maturity assessment of the organization. Architected\n      DW&BI Program Structure, defined the role of DW&BI Program Steering\n      Committee, it's mission, objectives, roles and responsibilities,\n      monitored regular improvements to help manage risks, evaluate trends,\n      and develop capacity and capability to achieve the Program mission\n\nNovaWurks/DARPA, Los Alamitos, California    11/2014-8/2015\nSenior Big Data, DW and BI Lead Solution Architect Consultant, Java Android\nconsultant\nWorked as senior Big Data Solution Architect, team leader and core\ndeveloper on PHOENIX project, an advanced satellite system for DARPA\n(Defense Advanced Research Projects Agency), a network of small satellites\ndue to launch to orbit in 2015. Due to the sensitivity cannot go into too\nmuch details!\n    . Led several Big Data projects on massive amount of transmitted and\n      logged data from the satellite network to the ground station. These\n      projects were developed utilizing Cloudera, Hadoop, Spark, Hive,\n      Impala, Flume, Sqoop, Storm, Pig, HDInsight, HBase, oozie. Due to the\n      real time nature of the project Apache Storm and Apache Kafka was used\n      for handling of the streaming and the real time data feed.\n    . I led the team, designed, architected and implemented an elaborate\n      Data Warehouse and Data Mart using Dimensional Modelling Star Schema\n      for satellite data aggregation, data storage, data log, real time\n      operation status data and other needs.\n    . Utilized the Cloudera Visualizations, Dashboards, and Reports to\n      monitor the operation of the satellites and any warning issues due to\n      any errors, miss functions or failures. Other visualization tools were\n      also created using Java and Android.\n    . Development languages, extensive Scala SBT (my preference), Java\n      Maven, Eclipse Intellij, Python, R, PySpark, Ruby, C#, Shell Scripts\n    . Led the team developed multiple real time Android Apps and middleware\n      using Android Studio and Eclipse, Android SDK and Java, RESTful APIs,\n      Retrofit, GSON, JSON, Regex, JGroups IP Multicast, Apache Thrift,\n      Python. Also used the following technologies and systems, Xilinx FPGA,\n      Verilog, TI DSP, ARM\u00ae Cortex\u00ae-A9 Cores: i.MX 6 Series Multicore\n      Processors etc.\n\nParamit, Morgan Hill, California  7/2013-11/2014\nSenior Big Data, DW and BI Lead Solution Architect, .NET Architect\nConsultant.\n    . Led multiple large scale Big Data, Enterprise Data Warehouse EDW and\n      Business Intelligence BI projects utilizing, Hadoop, Cloudera, Hive,\n      Impala, Flume, Sqoop, Map Reduce, Pig, HDInsight, HBase, oozie, and\n      facilitating real-time data analysis by data scientist.\n    . Leading the team, I designed architected and implemented the migration\n      from legacy normalized SQL, FoxPro, medical device manufacturing, ERP,\n      MRP, CRM, sales, finance and other information warehouses to a\n      consolidated modern high performance Big Data Warehouses running on\n      multiple DW appliances.\n    . Development languages, extensive Scala SBT (my preference), Java\n      Maven, Eclipse Intellij, Python, R, PySpark, Ruby, C#, Shell Scripts\n    . Leading the team we migrated and deployed multiple projects to Azure\n      Cloud. I was personally involved in the full cycle of vendor\n      selection, requirement gathering, design, development and the\n      deployment of these projects. The migration included different aspects\n      of the projects from front, backend, and integration. We went through\n      thorough research before selecting the Azure cloud for this project\n      and also utilized cutting edge utilities to perform the migration and\n      deployment.\n    . Using a combination of WPF C# application GUI and the Cloudera\n      Visualizations, Dashboards, and Reports created advanced data\n      visualization and data entry tools for ERP, MRP, CRM, sales, finance\n      and other departments.\n    . I lead, architected and help developing a SOLR/Lucene Search for the\n      huge amount of ERP, MRP and CRM. The SOLR project was later converted\n      to Elasticsearch. The Elasticsearch /Lucene system was architected\n      with 5 nodes sharding. It was developed and tested on 5 node\n      VirtualBox machines and then deployed to AWS cloud. Created an API in\n      C# .NET for calls to the search engine. Also a GUI was developed in C#\n      .NET for search calls to the Elasticsearch.\n    . Developed a customized SOLR indexing scheduler in C# which would run\n      periodically to do the delta indexing.\n    . Drafted a BI/DW prioritized implementation roadmap while taking input\n      from internal divisional service plans, business and IT strategy\n      documentation, as well as corporate BI Strategy and the Financial\n      Planning and Reporting System\n    . Designed Enterprise Information Management (EIM) solutions for the\n      manufacturing process, customer support and retail operation. Led\n      technical teams and designed various BI solutions including medical\n      device manufacturing tracking process, component reliability analysis,\n      vendor analysis, customer behavioral analysis, finance, ecommerce,\n      cyber security analytic.\n    . Conducted BI maturity assessment of the organization. Architected\n      DW&BI Program Structure, defined the role of DW&BI Program Steering\n      Committee, it's mission, objectives, roles and responsibilities,\n      monitored regular improvements to help manage risks, evaluate trends,\n      and develop capacity and capability to achieve the Program mission\n    . Led the team and developed multiple applications including medical\n      device, ERP, MRP applications with big data architecture. Used NET\n      4.5, C#, WPF, WCF, WF, MVVM Light, Telrik, MVC 4 Razor Entity\n      Framework 6.0 TFS, SQL 2012.\n\nMicrosoft, Redmond WA  2/2013-7/2013\nSenior Big Data, DW and BI Lead Solution Architect, .NET Architect\nConsultant.\n    . Led multiple Azure Cloud Big Data, NoSQL Riak, MongoDB SIP Trunk VOIP\n      projects doing analysis on massive amount of voice to text converted\n      data utilizing Hadoop/HDInsight, PDW, Map/Reduce jobs, Hive, and\n      Sqoop.\n    . Development languages, extensive C++, Scala SBT (my preference), Java\n      Maven, Eclipse Intellij, Python, R, PySpark, Ruby, C#, Shell Scripts\n    . Created real time multithreaded C# code using C++ Dubango Library,\n      SIP, TCP, UDP, RTP the VOIP telephony voice was recorded and using\n      SAPI converted to text. The text was then stored into key value and\n      document tables using Riak and MangoDB. The voice data gathered from\n      Cisco/IPCC telephone systems. Integrated with Cisco Verint for VOIP\n      call recording, quality monitoring (QM), and speech analytics.\n    . Microsoft SQL Server Parallel Data Warehouse (SQL Server PDW) was\n      chosen as the main appliance for the Big Data processing due to its\n      Massively Parallel Processing (MPP) architecture designed for Big Data\n      Processing.\n    . Microsoft Power BI in conjunction with a .NET application is used for\n      data visualization.\n    . Led the design and development of the Workforce Management (WFM) data\n      warehouse and BI solution to optimize adherence and attendance in the\n      contact center. The predictive analytic component accurately forecasts\n      the number of CSRs needed in the call center to fulfill the services.\n    . Led the design and development of an efficient BI auditing framework\n      that collects the data from packages being executed and used in data\n      flows, row counters, versioning, and error handling. The framework is\n      crucial for monitoring, timing, troubleshooting, and auditing. Also,\n      developed Stored Procedures, Views, and Functions for the framework to\n      automate logging the information and error handling in the packages.\n    . Led the design and development of ETL processes and data mapping using\n      SQL server, Master Data Services (MDS), SSIS to extract data from\n      Lagan ECM and division data sources including SQL server and oracle\n      databases, flat files, and excel sheets. The data, then, is\n      transformed and loaded into a data warehouse for reporting.\n    . Led the design and development of data quality ETL packages to correct\n      and cleanse the data and enhance the quality of consolidated data.\n      Wrote hundreds lines of .NET C# code, embedded in the packages, to\n      create a rules engine that loads business rules and apply them to the\n      data efficiently. In addition, the data quality issues are mapped for\n      reporting purposes.\n    . Led the design and development of a SQL Server SSAS Analysis cube\n      utilizing star schema with complex MDX calculated measures, named sets\n      and KPIs to present an analytical view for the data and data quality\n      with multiple dimensions.\n    . Led the design and development of map application and report using\n      ASP.NET/C# web application. The application loads the data from the\n      data warehouse, combines it with geographical information, and\n      displays the data on a map. The application communicates through\n      restful mapping services and uses client side scripts (JavaScript and\n      AJAX) to improve performance and user experience.\n\nDell, Austin, TX 6/2012-2/2013\nSenior Big Data, DW and BI Lead Solution Architect, .NET Architect\nConsultant.\n    . Led a Big Data project on gigantic amount of taxonomy data and\n      customer portfolio using Hadoop,  Cloudera, Hive, Map Reduce, Pig,\n      HDInsight, and facilitating real-time data which was both analyzed and\n      also in real time restructured the Dell website on the demographic\n      portfolio of the customers.\n    . I architected, worked and help developing the SOLR/Lucene Search\n      deployed to Azure. The indexing was done directly on top of the\n      metadata extracted from various files with customized Java code and\n      Apache Tika. Used customized faceting to overwrite the default search\n      criteria.\n    . Development languages, extensive Java Maven, Eclipse Intellij, Python,\n      R, Scala SBT, PySpark, Ruby, C#, Unix Shell Scripts\n    . Developed a customized SOLR indexing scheduler in C# which would run\n      periodically to do delta indexing.\n    . Wrote variation of batch files, python for SOLR/Lucene deployment and\n      configurations\n    . Leading the team, we designed architected and implemented the\n      migrating from legacy normalized SQL taxonomy data, customer portfolio\n      data and other data to a modern high performance Big Data Warehouses\n      running on multiple DW appliances.\n    . Defined the data governance strategy, designed security patterns,\n      implemented data standards and procedures across the enterprise;\n      drafted business specific methodology to establish business\n      stakeholder-driven data stewardship through MDM\n    . Led multiple EDW projects, prototyped and evaluated the performance on\n      Azure cloud, AWS Amazon Cloud, Massively Parallel Processing (MPP)\n      Data Warehouse Appliance\n    . I wrote complicated taxonomy algorithm in C# to load, sort the\n      taxonomy data into huge multidimensional trees on the memory which\n      made the data processing supper fast.\n    . Created Taxonomy data visualization using the Cloudera Visualizations,\n      Dashboards, and Reports to monitor customer profile, demography and\n      other useful data. Other visualization tools were also created using\n      C#.\n    . Created data quality ETL packages to correct and cleanse the taxonomy\n      data and enhance the quality of consolidated data. The consolidated\n      taxonomy data then were segmented using Hadoop and Cloudera.\n    . Led the design and development of a SQL Server SSAS Analysis cube\n      utilizing star schema with complex MDX calculated measures, named sets\n      and KPIs to present an analytical view for the data and data quality\n      with multiple dimensions.\n    . Leading the team we migrated and deployed multiple projects to Azure\n      Cloud. I was involved in the full cycle of vendor selection,\n      requirement gathering, design, development and the deployment of these\n      projects. The migration included different aspects of the projects\n      from front, backend, and integration.\n    . In conjunction with the Big Data I was involved in multiple projects\n      using variety of technologies including MVC 4 Razor, WPF, WF, WCF,\n      TPL, LINQ, SQL 2012, jQuery, Android, Java, J2EE, JRE, Ajax,\n      AngularJS, ExtJS, Entity Framework 5.0,.NET 4.5\n\nBEW / General Electric / 3 Gorges China, San Ramon, California      6/2011-\n6/2012\nTeam Leader, .NET Architect, Hands on Developer Consultant.\n    . Worked as a system architect, core developer on a sophisticated\n      control system for generators and wind turbines lead the software\n      (WPF), hardware (Xilinx FPGA & TI DSP 6000) and firmware (C++\n      Verilog/VHDL) teams.\n    . The high level software controlled a network of generators via TCP/IP.\n      The WPF C# project was architected using MVVM light, Entity Framework,\n      LINQ, WCF Services SQL etc. The Silverlight ASP .NET project was\n      architected using MVVM light, Entity Framework, LINQ, WCF RIA Services\n      Domain Service/Context. Developed equivalent Android application for\n      reading the generator's parameters like RPM, temperature, sensor\n      Voltages etc. Used Java programming and the Android Software\n      Development Kit, Eclipse using the Android Development Tools (ADT)\n      Plugin. Also worked on the firmware and FPGA DSP TMS320C6713\n      TMS320F28335 EMIF, I2C, MCBS, GPIO, RTC UART, Anybus CANbus, DM9000,\n      second level bootloader, EEPROM, code composer 3.3 etc FPGA Xilinx\n      Spartan 6, Xilinx ISE Design Suite 13.2, Verilog and VHDL.\n\nTexas Instruments, Dallas Texas   9/2010-6/2011\nTeam Leader, .NET Architect, Hands on Developer Consultant.\nWorked as the main architect, team leader, and core developer on a\nscientific highly multithreaded WPF C# application for emulation and design\nof advanced communication chips using scientific algorithms. I also worked\non an Android application for the PLL, Java programming using the Android\nSoftware Development Kit, Eclipse using the Android Development Tools (ADT)\nPlugin. The WPF application was architected using propriety MVVM\narchitecture. Utilizing advanced 3D objects the application was similar to\nOrCad and AutoCad. A smaller prototype version was also developed in\nSilverlight.\n\nMultibeam Corporation / Tokyo Electron (TEL), Santa Clara, California\n7/2008-9/2010\nTeam Leader, Architect, Hands on Developer Consultant.\n    . Worked as the project lead, helped designing, architecting, and\n      implementing a revolutionary complex electron beam based instrument\n      for the next generations of semiconductor fabs. Advanced analog\n      digital boards, Embedded Linux, Xilinx & Altera FPGA, Quartus, NIOS,\n      ARM9, ARM11, C, C# .NET, WPF (MVVM), WCF etc. DSP TMS320C67x GPIO, RTC\n      UART, Modbus, DM9000, second level bootloader, EEPROM. Altera FPGA,\n      Stratix, Cyclone Series, Quartus II Nios II.\n\nDepartment of Defense Contract (DOD), Washington DC      3/2007-7/2008\nTeam Leader, Architect, Hands on Developer Consultant.\n    . Architected, developed and led a highly sophisticated\n      hardware/firmware/software system. Due to the classified nature of the\n      project, I can only provide the following generic information: The\n      project involved advanced radio scanners, signal generators using GPS,\n      WCDMA, CDMA, GSM and other systems and protocols. The software\n      application controlling the instruments was a multi-tiered application\n      written in C#, .NET, Visual C++, MFC, CLR, Embedded Linux. It utilized\n      a very advanced multi-threading architecture with sophisticated\n      synchronization, message handling, logging system, serialization etc.\n      Specialized algorithms were devised to speed up the real-time\n      performance of hardware/software. Again because of the defense-related\n      nature of this project I cannot reveal any more details.\n\nPatton Design, Irvine, California 7/2000-3/2007\nVice President.\nWorked as the vice president of software and hardware. I led and developed\nthe software/hardware for a $140,000 instrument medical device - FDA.\nPlease check the website of Patton Design and Busch & Lomb to see this\naward winning instrument for cataract surgery. I designed, architected, led\nthe team and developed the software and also directed the hardware and\nfirmware developments. The software included a sophisticated multithreading\narchitecture, RS232 and TCP/IP communications, managed wrapper for firmware\ncalls, video streaming, voice recognition, database hierarchy encryption\netc. In addition to leading the team and acting as the vice president, I\npersonally wrote the complex core components in C# .NET. Due to the large\nscale of the medical device - FDA projects with hundreds of screens many of\nthe .NET C# libraries and objects had to be used. We also used legacy\nunmanaged code inside the managed code (wrappers). DirecX, DirectShow,\nWindows Communication foundation WCF, Windows Presentation Foundation WPF,\nWF, Silverlight, WCSF, SCSF, Enterprise Library, animation, video, audio\netc were also used.\nIn addition to the main control application I wrote and oversaw the\nfirmware in C++ Embedded Linux, C++ Round Robin, CodeWarrior. I also\noversaw and participated in the hardware development using OrCad 10.\n\n* Patton Design / Cameron Health: Developed the software and participated\nin the hardware design of the heart pacemaker medical device - FDA and the\ncontroller called Q-TECH Programmer. medical device - FDA The heart\npacemaker is transplanted in the heart and controlled by the wireless\ncontroller via Bluetooth. Due to the FDA regulations I could not use the\n.NET framework but had to use Embedded Visual C++ 4.0 and MFC for windows\nCE. More than 140 screens! Very sophisticated programming involving memory\nmanagements, DirectX, DirectShow etc.\n\n* Patton Design / GoVideo: Worked as the Vice President/architect/team\nleader on a joint project between, GoVideo, Patton Design, Daewoo and MTK\nin Taiwan. I led the Patton Design team developed a TiVo style DVD/VCR\ncombo with hard drive recording capability. I was the vice president and\nthe team coordinator between the 4 companies overseeing hardware, software\nand Firmware (Embedded Linux), several patents were filed. The System was\npresented at the CES show in Las Vegas in 2007 and received tremendous\npositive recognitions.\n\n* Contract with usCalibration Inc.: Architected, developed and led a\nsophisticated web based application using C# .NET and Visual Studio 2005,\nSQL Server 2005 and SSRS. I wrote the core part of the application. The\napplication was successfully launched in 2006 for Calibration systems with\nadvanced security systems. Tens of thousands of lines of code with advanced\nnavigation systems with several pending patents.\nHewlett Packard (HP), Cupertino, California  4/2000-7/2000\nTeam Leader, Architect, Hands on Developer Consultant.\nWorked as senior developer/ technical lead on an advanced server client\nbased communication system for server diagnostics. The system was designed\nusing TCP/IP and SNMP protocols for monitoring hardware sensors like\nthermocouples, voltage and current monitoring sensors and other hardware\nsensors installed on HP servers. By reading these sensors, HP was able to\nremotely do detailed hardware/software diagnostics of the HP servers around\nthe globe.\nWorked on hardware, software and the overall system architecture. The\nsoftware had a server and client component and was written in visual C++,\nCOM (ATL), DCOM, ASP, Visual J++, XML, SNMP, MIB, SQL and InstallShield.\n\nBroad Logic, Milpitas, California 7/99-4/2000\nTeam Leader, Architect, Hands on Developer Consultant.\nI was brought to BroadLogic, Inc. by Paul Rudnick because of my expertise\nin satellite communication systems and my experience from Space Systems\nLoral and CyberStar. Prior to this, I had worked closely with Adaptec and\nBroadlogic on the development of the satellite receiver hardware while\nstill a senior manager at Space Systems Loral.\nI worked on the design and implementation of the next generation of two way\nsatellite Express PC transceiver cards, a high speed two way satellite\ncommunication system. I designed, simulated, researched, architected and\nled the project for the development of an advanced two way satellite\ncommunication system (satellite Express PC transceiver cards). Audio, video\ntransfer and high speed internet access over satellite. Using, frequency,\ntime, phase multiplexing. TDMA, CDMA, GMSK, Conditional Access. TCP/IP,\nUDP, DVB, SNMP, MIB and proprietary protocols. Using OQPSK modulation\nimplementation on the Texas Instrument DSP Chip. I have written several\ndocuments related to this system.\n\nHewlett Packard (HP), Mayfield, California   3/99-7/99\nTeam Leader, Architect, Hands on Developer Consultant.\nWorked as the senior architect, technical lead and senior developer on the\nHP Ecommerce site which later became the foundation of the HP website for\nPC and servers. The web application was developed in Visual InterDev 6.0\nusing Active Server Pages (ASP), Microsoft E-Commerce, SQL 7.0, XML, Visual\nC++ 6.0 and Visual Basic 6.0, Visual J++ 6.0, COM (ATL), DCOM, JavaScript\nand VB Script. The web server was Microsoft Internet Information Server\n(IIS), Microsoft site server 3.0, with Microsoft E-Commerce edition 3.0 and\nFrontPage extension running under the NT Server I have written several\ndocuments related to this application.\n\nHewlett Packard (HP), Cupertino, California  5/98-3/99\nTeam Leader, Architect, Hands on Developer Consultant.\nWorked as the senior architect, technical lead and senior developer on the\nHP servers configuration software which later became a major component and\nthe foundation of the HP website for PC and servers configuration. Stand\nalone and the web application in was developed in Visual InterDev 6.0 using\nActive Server Pages (ASP), Microsoft E-Commerce, SQL 6.5, Visual C++ 6.0\nand Visual Basic 6.0, COM (ATL), DCOM, JavaScript and VB Script. The web\nserver was Microsoft Internet Information Server (IIS), Microsoft site\nserver and FrontPage extension running under the NT Server. I have written\nseveral documents related to this application.\n\nSpace Systems Loral (CyberStar), Mountain View, California    11/96- 5/98\nTeam Leader, Architect, Hands on Developer Consultant.\nI was brought into Space Systems Loral from Lockheed Martin by Bob Lapin to\nhelp starting the CyberStar division at the Space Systems Loral. By the\ntime I left the CyberStar in 1998 to finish my PhD in Satellite\nCommunication, the CyberStar division had grown to more than 100 employees.\nI personally interviewed majority of those people.\nI was one of the main architects of the CyberStar project and oversaw the\ndesign, development and implementation of different aspects of hardware,\nsoftware, firmware and the satellite communication at CyberStar.\nI first established a complete satellite communication link both uplink and\ndownlink, using 3rd party modulators, demodulators, encoders, decoders,\ncryptography modules, conditional access, transmitter, receivers,\namplifiers, dampers, data aggregator, data parsers etc. Very soon we were\nable to transmit and receive from and to the satellite. We were primarily\nusing MIB and DVB protocols initially but I was one the first who managed\nin 1998 to implement TCP/IP and high speed internet access over satellite\nusing an ACK table!! (patents)\nTo develop the integrated transceiver hardware we started working with\nAdaptec and I personally was directly involved in the design and\nimplementation of the satellite receiver card hardware using OrCad. This\nlater led to the creation of BroadLogic from Adaptec. I was later hired by\nBroadLogic to continue the improvement of the two way satellite receiver /\ntransmitter.\n\nThis project was personally very important to me and made me understand and\nexperience the satellite communication in a very comprehensive way both\ntheoretically and practically. It helped me to get a PhD in Low Earth Orbit\nSatellite Communication from the University of California, one of very few\nwho did. I travelled extensively in both US and in Europe and came in\ncontact with some amazing people from NASA, Lockheed Martin, BroadLogic,\nEuropean Space Agency etc I wrote many documents in satellite communication\nduring this period for Space Systems Loral.\n\nLockheed Martin, Milpitas, California   6/96- 11/96\nTeam Leader, Architect, Hands on Developer Consultant.\nLockheed Martin at the time in 1996 had the most sophisticated high\nresolution CCDs (Farichild) in the world which were in use in a number of\nsensitive military applications, advanced high resolution digital satellite\nimaging, and few civilian applications.\nDue to the classified nature of some of these projects I cannot in detail\ndescribe what I did. However I was involved in the design and development\nof some of these advanced and sensitive projects. I worked as a senior\nengineer, designing and developing, systems, hardware, firmware and\nsoftware.\n* Hardware: We used OrCad for designing analog and digital circuits,\nfilters, amplifiers, Data collectors from CCDs, interfaces etc.\n*Firmware was written in C++ , flat file Round Robin, on Freescale HC and\nARM family CPUs.\n*Software: developed 32-bits, real time applications in Visual C++ 4.2\nusing MFC and SDKs under Windows 95 for control and testing of an advanced\ndigital camera with high resolution CCD. The GUI software is designed for\ndriving the special digital camera through parallel communication and\ntesting of IPS, ADP, CCD and different part of the system. The tests\nincluded advanced image processing and image quality tests. The project\ninvolved both 16-bits and 32-bits DLLs and VXDs (device drivers), Thunking\nand also conversion from and between 16-bits and 32-bits.\n\n\n                               OTHER PROJECTS\n\nERICSSON (ELLEMTEL), STOCKHOLM, SWEDEN\nTeam Leader, Architect, Hands on Developer.\nDeveloped and designed hardware and a control system for the new generation\nof AXE telephone systems, based on the FUTUREBUS+ bus technology, Using the\nVHDL programming language. I wrote the VHDL program on the SUN platform\n(SUN OS version 3.0).\nI documented the application in a detailed technical white paper entitled\n\"Verification Methods for Hardware Construction\". This paper was released\nto all programmers and hardware engineers at Ericsson and KTH. A copy is\navailable for your review.\n\nABB Atom AB, Vasteras, Sweden\nTeam Leader, Architect, Hands on Developer Consultant.\nElectrical and Computer Lab--section SLC3:\nDeveloped a series of utility programs / application in Quick BASIC\n(version 5.0) used for calibration of computer operated measurement\nequipment in the nuclear power plant reactors. Programs were run on the HP\n9000/300, and Intel 286 platforms.\n\nTECHNOLOGY\n    . Data Warehouse, Data Mart, OLAP, OLTP Databases, Teradata, Netezza,\n      Oracle, Parallel Data Warehouse (PDW), SQL Server, MDM, MDS, Data\n      Quality (DQ), Spark, Hadoop, Hortonworks, Cloudera, Apache KafKa,\n      Hive, Impala, Flume, Sqoop, Map/Reduce, Pig, HDInsight, HBase, Storm,\n      oozie, Python, Scala, HDFS, StreamInsight, PolyBase, Microsoft SSIS,\n      SSAS, SSRS, ETL, BI, MDX, PL/SQL, TSQL, ERwin, Enterprise Architecture\n      (EA),  SQL Servere 2000/2005/2008/2012/2014, Power Query, Power Map,\n      PowerPivot, Power View, IBM Cognos, SPSS, InfoSphere DataStage,\n      Informatica PowerCenter, SAP BusinessObjects (BO), SAP HANA, Crystal\n      Reports, Hyperion, MicroStrategy, SharePoint 2007/2010/2013, Nintex,\n      SharePoint Social, Collaboration, Record Management, Search, Web\n      forms, InfoPath, Branding, CSOM, JSOM, PerformancePoint, Clustering,\n      Failover, Web Analytics, Google Visualization, .NET 1.1 to 4.5, C#,\n      WCF, Restful Services, WPF/Silverlight, WF, VB .NET, ASP .NET,\n      ADO.NET, LINQ, MVC, MVVM, MVP, AJAX, Visual Studio, Dashboard\n      Designer, SharePoint Designer, Visio, TFS, Cloud, Azure, PaaS, SaaS,\n      IaaS, HTML 5.0, DHTML, XML, XSL, WSDL, XSD, JSON, COM, DCOM, MFC, C,\n      Visual C++, Visual Basic, PowerShell scripts, and SDKs, DocXpress, BI\n      Documentation, Nintex, SharePoint Social, Collaboration, Record\n      Management, Search, Web forms, Branding.\n\n                                   SKILLS\n    . BIG DATA, HADOOP, SPARK, CLOUDERA, HORTONWORKS, STORM, KAFKA, HIVE,\n      IMPALA, FLUME, SQOOP, MAPREDUCE, PIG, HDINSIGHT, HBASE, OOZIE, TABLEAU\n      POWER BI AND CLOUDERA VISUALIZATION\n    . Cloud Azure, AWS, , HDInsight, Cortana Intelligence Suite, Data\n      Factory, Data Gateway, Infrastructure as a Service IaaS, Platform as a\n      Service PaaS, Microsoft R Server, NLB, Key phrase extraction Azure\n      search, Unstructured text analytics, Event hub, Streaming, Poly Base\n    . Search engines Elasticsearch, SOLR, Lucene, Kibana. Logstash, Rsync,\n      Tika\n    . Machine Learning and recommendation engines MLlib, TensorFlow, Keras,\n      Weka Mahout, Multilayer perceptron classifier (MLPC), the feedforward\n      artificial neural network, scikit-learn, Pandas, Deeplearning4j,\n      Sparkling Water ML, Caffe2, MxNet etc. Different algorithms K-Means,\n      Random Forest, Gradient Boosting algorithms (GBM, XGBoost, XGBoost and\n      CatBoost)\n    . BI Framework: Strategy and Implementation Plans, Enterprise Metrics,\n      Integration Points, Gap Analysis, BI Portfolio, Performance Management\n      (PM), Analytic and PM Technologies, Defining Business and Decision\n      Process, Building Metadata and Services Centers, Establishing\n      Enterprise Information Management (EIM) Committees, Defining The Role\n      of DW and BI Program Steering Committee, It's Mission, Objectives,\n      Roles and Responsibilities, DAMA DMBOK\n    . Architecture and Data Modeling: Initial Conceptual Solution, Solution\n      Blueprints, Technology Impact Analysis (TIA), Gap Analysis, Technology\n      Roadmap, Dimensional modelling, ER Modelling, Start Schema, Snowflake,\n      Fact, Dimension, Hierarchy, Inmon/ Kimball/ Imhoff, Data Marts, EDW,\n      ERWin 9.5/8.0/7.x, DeZign, Microsoft Visio, Enterprise Architecture\n      (EA), Service Oriented Architecture (SOA), UML, Zachman, TOGAF, Star &\n      Snowflake Schemas, 3 Normal Forms, Normalization and Demoralization,\n      Logical Model and Physical Model, Fact/Dimension/Hierarchy\n      identifications, Data Warehouse Development Lifecycle, Data Mapping,\n      Data Dictionaries\n    . Data Governance: IBM InfoShere MDM, Informatica MDM, MDS, DQS,\n      Profisee Maestro, SAS MDM\n    . Integration and ETL: SSIS/SSRS/SSAS, SQL Server\n      2014/2012/2008R2/2008/2005, Informatica PowerCenter, DataStage,\n      Cognos, ETL Mapping design, Data Profiling, Data Validation, Data\n      Migration, Data Cleansing, Data Structure, Data Quality Services\n      (DQS), BIDS, SQL Data Tools (SSDT), Auditing Framework, Execution\n      Plans, ETL Parallel Processing, Error Handling, Custom Scripting, IBM\n      Cognos, InfoSphere DataStage, Informatica PowerCenter, SAP\n      BusinessObjects (BO)\n    . Data Warehousing and Analysis: OLAP/Cube/MDX/DAX, Dimensional\n      Modelling, Tabular Modelling, KPIs, KPPIs, Data Analysis, SPSS,\n      Predictive Analysis, Data Mining, Machine Learning, SAP HANA,\n      Statistical Analysis, SAS, SAS VA (Visual Analytics), R, XLSTAT,\n      Sentiment analysis, Speech analytics, Teradata. Netezza, Cloudera,\n      PDW, Aginity, Master Data Services (MDS), Master Data Management\n      (MDM), Data Quality (DQ), Analysis of Change (AOC), Metric Engine.\n    . Reporting: Predefined Reports, Ad-hoc Reporting, Analytical Reports,\n      Custom Reporting with .NET/ Report Viewer, SQL Server Reporting\n      Services (SSRS), SharePoint 2013/2010/2007/2003, PerformancePoint,\n      PowerPivot, Power View, Crystal Reports, Hyperion, MicroStrategy,\n      Cognos Report Studio, Framework, Workspace Advanced, DMR, TM1\n    . Data Visualization: Power Map, PowerPivot, Power View, SharePoint,\n      Liferay, PerformancePoint, Google Visualization, Esri's GIS\n      (geographic information systems) , mapping, SAP Lumira, QlikView,\n      Tableau, Data Mapping\n    . Database: MS-SQL, Oracle, Oracle SQL Developer, TSQL, MDX, DMX,\n      PL/SQL, Stored Procedure, View, Function, Erwin Data Modeler, DB2,\n      PowerDesigner, MongoDB, Access, Excel, FoxPro, Informix, NoSQL, Big-\n      data, Hadoop, Spark, HBase, HDInsight, PDW, PolyBase, Hive, HQL,\n      Map/Reduce, HFS, Alert\n    . Programing Languages: SQL, T-SQL, PL/SQL, C#, WCF, Restful Services,\n      WPF/Silverlight, WF, VB .NET, ASP .NET, ADO.NET, LINQ, MVC, MVVM, MVP,\n      AJAX, HTML 5.0, DHTML, XML, XSL, WSDL, XSD, JSON, Java Script,\n      PowerShell, COM, DCOM, VB Script, UNIX Shell Scripting\n    . Others: Agile, Extreme Programing, RUP, Use Cases, SDLC, TCP/IP, CVS,\n      Microsoft Team Foundation Server (TFS), Tortoise SVN, SQL*Plus, TOAD,\n      WinSQL, SilverLight, LightSwitch, Kerberos, Single Sign-On, Datazen,\n      One-Key.\n    . Architecture and Design: Enterprise Architecture (EA), Service\n      Oriented Architecture (SOA), Enterprise Service Bus (ESB), Top-Down/\n      Bottom-Up Design, Structured Design, Object Oriented Design, Multi-\n      tiered and Multi-threaded architecture, Rational Rose, UModel,\n      Patterns: Model/View/ViewModel (MVVM), MVC, MVP, Visio, UML, Zachman,\n      TOGAF, Federal Enterprise Architecture, Gartner Methodology\n    . Business Optimization: Asset management, Information Technology\n      Infrastructure Library (ITIL), customer satisfaction, call center\n      management, service request enhancement, AODA compliance, fraud\n      detection, CRM and ERP optimization, improving marketing\n      effectiveness, portfolio optimization, governance, risk management,\n      compliance, healthcare patient records management, electronic medical\n      records (EMR), optimizing routes and schedules for logistics planning,\n      insurance risk assessment, optimizing manufacturing production.\n    . Integration: Windows API, Biztalk, SOA, WCF, SSIS\n    . Data Access: ADO.NET, LINQ, Entity Framework, Microsoft Enterprise\n      Library, OLE DB, Oracle Data Provider, MS OLAP, SQL Master Data\n      Services (MDS), StreamInsight\n    . Software Development\n    . Methodologies: Test driven programming, Agile software development,\n      Extreme Programming (XP) Microsoft .NET Framework (from 1.0 to 4.0),\n      C#, Visual Basic .NET, VB .NET, ADO .NET, WinFX including Windows\n      communication foundation (WCF), windows workflow (WF), windows\n      presentation foundation (WPF), XAML, XML, HTML, HTML5, Java J2EE,\n      Spring Framework, JavaScript, AJAX, RESTful services, Payment Card\n      Industry (PCI), Image Processing\n    . Visual C++ (MFC, SDKs, COM, DCOM, ATL ActiveXs), VB, C++, Perl, VHDL,\n      Verilog, Shell, Skill, Ocean, SystemC\n    . Scala SBT, Java Maven\n    . Version control tools: Source Safe, Team Foundation Version Control,\n      (TFVC), Subversion Tortoise SVN\n    . Code metrics: Simian, RSM\n    . Type/ industry: financial, banking, biomedical, pharmaceutical,\n      engineering, telecommunication, semiconductor, logistics, health,\n      scientific, e-commerce, instrumental\n    . Internet Development: ASP .NET, MVC, Sliverlight, HTML, DHTML, Web\n      services for marketing and financial applications, AJAX, ASP,\n      JavaScript and VB Script, XML, Microsoft Internet Information Server\n      (IIS), Microsoft E-Commerce, PHP, Webload\n    . Cloud Computing\n    . Windows Azure, Amazon AWS EC2\n    . SharePoint: SharePoint 2013/2010/2007/2003, Multi-machine SharePoint\n      Farm Architecture, Setup, Configuration, Load Balancing, Clustering,\n      Backup Plans, Web Part and module development, Collaboration, Social,\n      Search, Web Content Management, Enterprise Content Management, App\n      Management, PerformancePoint and PowerPivot, PowerView, Application\n      Federation, Secure Store Application, Business Connectivity, Usage\n      Reports, SharePoint Designer, Dashboard Designer, PerformancePoint,\n      dashboard, charts, KPI, Scorecards, reports, filters, Excel Services,\n      PowerPivot Services, Web Analytics, Static Analysis, Hit Counters,\n      Custom Development, PowerShell, SharePoint API, Object Model, web\n      parts web services, workflows, Content Management, site\n      collections/structure\n    . Mobile Development\n    . Android, iOS, Windows\n    . Operating System Used: Windows, UNIX, Windows Azure, Linux, Android,\n      iOS, Windows Mobile, MS-DOS\n    . Hardware and Simulation: Matlab, Cadence Spectre, Spice, Eldo, ANSYS\n    . Algorithms: Genetic algorithm, simulated annealing based algorithms,\n      heuristic search, binary search, quick sort\n    . Automation and Scripting: VB, Perl, Unix Shell\n\n                                 LANGUAGES:\nENGLISH, SWEDISH, PERSIAN AND NORWEGIAN.\n",
          "confidence": 1.0,
          "method": "full_text",
          "structured_data": null
        },
        "tax_term": {
          "value": "CONTRACT",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "source_by": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "designation": {
          "value": "Cloud Architect",
          "confidence": 0.9,
          "method": "ner",
          "structured_data": null
        },
        "experience": {
          "value": "20",
          "confidence": 0.9,
          "method": "regex_total_experience_summary",
          "structured_data": null
        }
      }
    },
    {
      "resume_path": "data\\input\\Akin - PM  - OH.doc",
      "confidence_score": 0.56,
      "used_ocr": false,
      "extracted_fields": {
        "skills": {
          "domain_specific": [
            {
              "name": "retail",
              "confidence": 0.5,
              "context": "education_section",
              "positions": [
                37,
                6876
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "crm",
              "confidence": 0.5,
              "context": "education_section",
              "positions": [
                986,
                1576,
                11082,
                13258,
                14160
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "infrastructure",
              "confidence": 0.5,
              "context": "education_section",
              "positions": [
                1798,
                13786,
                14520
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "design",
              "confidence": 0.5,
              "context": "education_section",
              "positions": [
                5390,
                11961,
                14012,
                14131
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "compliance",
              "confidence": 0.5,
              "context": "education_section",
              "positions": [
                7214,
                10697
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "security",
              "confidence": 0.5,
              "context": "education_section",
              "positions": [
                7646,
                9617,
                12290,
                12970,
                13226
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "energy",
              "confidence": 0.49999999999999994,
              "context": "education_section",
              "positions": [
                4078
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "government",
              "confidence": 0.49999999999999994,
              "context": "education_section",
              "positions": [
                10180
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "ecommerce",
              "confidence": 0.49999999999999994,
              "context": "education_section",
              "positions": [
                11045
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "audit",
              "confidence": 0.49999999999999994,
              "context": "education_section",
              "positions": [
                12456
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "manufacturing",
              "confidence": 0.49999999999999994,
              "context": "education_section",
              "positions": [
                14179
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "education",
              "confidence": 0.49999999999999994,
              "context": "education_section",
              "positions": [
                14738
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "finance",
              "confidence": 0.49999999999999994,
              "context": "education_section",
              "positions": [
                14845
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "soft_skills": [
            {
              "name": "planning",
              "confidence": 0.5,
              "context": "education_section",
              "positions": [
                658,
                947,
                1041,
                10518,
                13326,
                13619,
                13673
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "analysis",
              "confidence": 0.5,
              "context": "education_section",
              "positions": [
                1063,
                1185,
                3379,
                4926,
                5149,
                8841,
                8872,
                12561
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "clarity",
              "confidence": 0.5,
              "context": "education_section",
              "positions": [
                1231,
                3906
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "innovation",
              "confidence": 0.5,
              "context": "education_section",
              "positions": [
                14023,
                14070
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "collaboration",
              "confidence": 0.49999999999999994,
              "context": "education_section",
              "positions": [
                1250
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "initiative",
              "confidence": 0.49999999999999994,
              "context": "education_section",
              "positions": [
                2995
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "documentation",
              "confidence": 0.49999999999999994,
              "context": "education_section",
              "positions": [
                3601
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "consulting",
              "confidence": 0.49999999999999994,
              "context": "education_section",
              "positions": [
                5109
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "methodologies": [
            {
              "name": "agile",
              "confidence": 0.5,
              "context": "education_section",
              "positions": [
                1550,
                4708,
                5543,
                8473,
                11270,
                11509
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "scrum",
              "confidence": 0.5,
              "context": "education_section",
              "positions": [
                1556,
                4259,
                4841,
                8327,
                11032,
                11260,
                11330,
                11519,
                13767
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "waterfall",
              "confidence": 0.5,
              "context": "education_section",
              "positions": [
                1562,
                4698,
                5549,
                8479
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "lean",
              "confidence": 0.5,
              "context": "education_section",
              "positions": [
                13851,
                14007
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "pmi",
              "confidence": 0.49999999999999994,
              "context": "education_section",
              "positions": [
                1523
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "rad",
              "confidence": 0.49999999999999994,
              "context": "education_section",
              "positions": [
                1542
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "rup",
              "confidence": 0.49999999999999994,
              "context": "education_section",
              "positions": [
                1572
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "safe",
              "confidence": 0.49999999999999994,
              "context": "education_section",
              "positions": [
                9698
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "sdlc": [
            {
              "name": "postman",
              "confidence": 0.49999999999999994,
              "context": "education_section",
              "positions": [
                1598
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "cloud": [
            {
              "name": "azure",
              "confidence": 0.49999999999999994,
              "context": "education_section",
              "positions": [
                1645
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "business_skills": [
            {
              "name": "brd",
              "confidence": 0.5,
              "context": "education_section",
              "positions": [
                5324,
                8585
              ],
              "experience_weight": 0.0,
              "importance_score": 0.8
            },
            {
              "name": "reporting",
              "confidence": 0.5,
              "context": "education_section",
              "positions": [
                12378,
                12929
              ],
              "experience_weight": 0.0,
              "importance_score": 0.8
            },
            {
              "name": "srs",
              "confidence": 0.49999999999999994,
              "context": "education_section",
              "positions": [
                5289
              ],
              "experience_weight": 0.0,
              "importance_score": 0.8
            }
          ],
          "networking_equipment": [
            {
              "name": "authentication",
              "confidence": 0.5,
              "context": "education_section",
              "positions": [
                7619,
                7728
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "authorization",
              "confidence": 0.49999999999999994,
              "context": "education_section",
              "positions": [
                7122
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "frameworks": [
            {
              "name": "rest",
              "confidence": 0.49999999999999994,
              "context": "education_section",
              "positions": [
                11470
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ]
        },
        "first_name": "Akin",
        "last_name": "Saheeb",
        "primary_email": {
          "value": "asaheeb@Gmail.com",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "secondary_email": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "phone": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "city": {
          "value": "Westerville",
          "confidence": 0.7,
          "method": "ner",
          "structured_data": null
        },
        "state": {
          "value": "OH",
          "confidence": 0.9,
          "method": "zip_database",
          "structured_data": null
        },
        "zip": {
          "value": "43081",
          "confidence": 0.7,
          "method": "regex",
          "structured_data": null
        },
        "work_authority": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "resume_link": {
          "value": "data\\input\\Akin - PM  - OH.doc",
          "confidence": 1.0,
          "method": "file_path",
          "structured_data": null
        },
        "raw_resume": {
          "value": "\n                              Akin  Saheeb, MBA\n                           Westerville, Ohio 43081\n                              asaheeb@Gmail.com\n\n\n\nSummary of Qualifications:\n|Experienced Project Manager offering Retail and Financial Services domain|\n|expertise                                                                |\n|Extensive experience managing PAYMENTS projects                          |\n|Card Present Payment transaction                                         |\n|Card Not Present Payment transaction                                     |\n|Centralized Payments settlement                                          |\n|Apple Pay implementation                                                 |\n|Mobile wallet                                                            |\n|Integration of loyalty and offer products with payment solutions - Pay,  |\n|Earn, Redeem, Save                                                       |\n|Created and managed project plan for additional functionality request to |\n|enable client to process an 'above site' payment that would allow user to|\n|settle their bill when dining in the restaurant by using their Mobile App|\n|or Mobile Internet site to check out with 'Pay at the Table' capabilities|\n|Project planning and execution, scope definition, change control, risk   |\n|mitigation, stakeholder engagement, and project close out                |\n|Budget management of up to $42M and teams of 25+                         |\n\nTechnical Skills Summary:\n . Budget / Financial Management including Business Case completion,\n   Business Process Reengineering, Budget Planning, Enterprise Content\n   Management, CRM Systems, Cost Control, Schedule Management, Strategic\n   Planning, Earned Value Analysis, Quality Control, Process Improvement,\n   Risk Management, Scope Management, Change Management, Client Relations,\n   Business Analysis, Third Party Provider Risk Management,\n . Clarity, Confluence - Collaboration Software, Jira - Workflow Management,\n   PlanView, OPPM, Microsoft Project, Project Workbench, MS Project\n   Portfolio Management, SAP, SharePoint, PeopleSoft Financial module,\n   Microsoft Visio, Access, PowerPoint, Microsoft Office Suite, Team\n   Foundation Server. Visual Studio\n . PMBOK - PMI Processes, SDLC, RAD, JAD, Agile/Scrum, Waterfall, RUP, CRM,\n   EVA, and Six Sigma.\n . Postman Collection tool - from API point-of-view. Azure DeveOps,\n   Microsoft Office 365\n                             Experience Profile\n\nWendy's, Dublin, Ohio\n                                               10/2019 - Present\nConsultant - Payment Solutions Lead Project Manager\n                     Enterprise IT & Infrastructure Solutions\n . Canada Card Present is payment solutions that spans multiple solutions;\n   On-Line Strategies Services Payments, LLC, GK Software, Moneris, GiveX ,\n   and Wendy's Payment Solutions.\n    Wendy's is seeking to implement an in-store payment processing solution\n   for their Canadian and US locations that will apply hardware-based Point-\n   to-Point Encryption (P2PE) of cardholder data (CHD) within the Ingenico\n   Lane 3000 PIN Encryption Devices (PED) that are integrated with their NCR\n   Aloha POS registers and Customer Self Order (CSO) kiosks.\n o Credit Card Processing\n o On-line Debit Card Processing\n o Gift Card Processing\n o Point-to-Point Encryption Services\n o End-of-Day Processing\n o Tokenization Services\n o Device Management Services\n o Store-to-Host Payment Processing Services\n o Network Connectivity\n\n\n . Canada Card Not Present\n . Centralized Payments Settlement\n\n\nFiserv, Dublin, Ohio\n                                               07/2018 - 08/2019\nConsultant - Biller Solutions Client Implementations Sr. Project Manager\n . Electronic Billing and Payment Solutions.\n    o Electronic Bill drives paperless\n    o Mobile Wallet\n    o Remittance solution to receive and post payment details faster\n    o Team Size 10\n . Bill Discovery and Alert also referred to as Challenger is an ePayments\n   initiative that spans multiple solutions; Fiserv Output Solutions,\n   ePayments, and Billler Solutions. The solution requires Fiserv to get pre-\n   loaded biller data to setup biller as payee. This project focuses on\n   developing an end to end process that will be followed to setup each\n   Biller, across the Biller and Payments Group (BPG).\n . Designed business modelling, requirements modelling, and system analysis\n   to map out the desire workflow to understand it more completely.\n . Engaged with customers and technical staff to create business and\n   technical requirements and other deliverables on projects.\n . Developed and maintained documentation for assigned projects.\n . Partnered with various IT departments to achieve alignment on the\n   supportability and architectural standards related to the technology\n   being implemented.\n . Communicated project status internally and managed communication between\n   the project team and stakeholders. Utilized Clarity to maintain project\n   risks, action items/issues, milestones, and tracked team members' time.\n\nBill Discovery and Alert Projects:\n . PennyMac Loan Services\n . NISSAN/Infiniti\n . TXU Energy\n . AEP (American Electric Power)\n . Frontier Communications\n . AT&T\n . Southwest Gas\n . Jacksonville Electric Authority (JEA)\n\nOCP, San Mateo, CA\n                                               08/2017 - 04/2018\nConsultant - Remote Project Manager/Scrum Master - Software-as-a-Service\nProject\n . Team Size 5 - 7\n . Streamlined systems, and resolved issues for increased profitability, on-\n   time project delivery, and optimized customer satisfaction.\n . Tracking multiple projects with multiple budgets.\n . Effectively managing globally distributed model of delivering solution.\n . Developed business processes and system solutions for various domains\n   from conceptualization to implementation.\n . Managed project using Waterfall/Agile-hybrid framework.\n\nFirst Data Corporation, Hagerstown, MD\n07/2016 - 06/2017\n(Digital Commerce Solutions)\nConsultant - Sr. Project Manager/Scrum Master\nAdvisory Services: Key tasks include client interaction, needs assessment\nanalysis, process improvements, and the ability to execute on key projects\nwithin anticipated schedules.\n . In addition to client facing Project Management responsibilities, I have\n   provided consulting services on Digital Commerce, analysis of Mobile\n   Payment, and Reward programs.  Designed and reviewed of various documents\n   including the Software Requirement Specifications\n   (SRS), Business requirements document (BRD), Use Case Specifications,\n   Functional Specifications (FSD), Systems Design Specification (SDS),\n   Requirement Traceability Matrix (RTM) and testing documents.\n . Created and managed project plan for client solutioning project using --\n   Agile-Waterfall Hybrid methodology, as part of (Digital Commerce\n   Solutions).\n . Developed the project charter, integrated project plan, resource plan,\n   contingency plan and related project management artifacts. Developed\n   communication plans for (day-to-day discussions, team meetings) and\n   written (status reports, change requests) form.\n . Defined and tracked project milestones, maintained and reported on\n   overall integrated delivery plan.\n . Coordinated and facilitated post deployment reviews for continual release\n   improvements.  Managed deployment of concurrent releases; communicated\n   release processes, timelines, requirements, and strategies to\n   stakeholders and project resources.\n . Managed change management requests to ensure release contents were\n   compliant.\n . Maintained communications with QA to ensure problem records were resolved\n   and closed prior to release launch. Created and distributed release\n   processes to cross functional teams.\n\nPay Now APIs Project\n . Budget-450K, Team size- 10+\n . Worked collaboratively with customers, as well as internal and external\n   stakeholders.\n . Created and managed project plan for additional functionality request to\n   enable client to process an 'above site' payment that would allow user to\n   settle their bill when dining in the restaurant by using their Mobile App\n   or Mobile Internet site to check out with 'Pay at the Table'\n   capabilities.\n\nCiti Retail Services Token Service Provider and Integration with Samsung\n . Budget- 1M, Team size- 9-13\n . Track First Data Integration with TCH for Cryptogram Verification\n   associated with Token.\n . Manage connectivity with TCH which include Cryptogram Validation,\n   authorization Advice, and Detokenization.\n\nMasterCard Introduction of New 2 Series BIN Project- (Compliance Project)\n . Acquirers will need to upgrade their systems to be compatible with 2-\n   Series BINs.\n . All merchants will need to be compliant with the 2-Series migration as of\n   January 1st. 2017.\n\nPetro Universal Commerce Enablement Project\n . Budget-2.5M, Team size- 25+\n . Mobile Application solutions developed to integrate loyalty and offer\n   products with payment solutions - Pay, Earn, Redeem, Save.\n . Managed mobile device authentication - enhance the security of merchant,\n   commerce offering providing multiple layers of mobile device\n   authentication.\n\nValuelink TEST MIDs Project (Operational Efficiencies Project)\n . Budget-100K, Team size- 9+\n . The objective of this concept/idea is to create a series of TEST MIDs as\n   a result of anticipated growth; to be used exclusively in a sandbox/test\n   environment.\n . Planned and managed requirements to allow quick boarding of potentially\n   new or standard Valuelink merchants on the uComm gateway and in some\n   cases the merchants are not existing Valuelink clients or are Valuelink\n   standard clients.\n\nDiscover Financial Services, New Albany, OH                         02/2015\n- 12/2015\n(Common Payment Platform)\nConsultant - Sr. Program Manager / Scrum Master: Wallet Service Provider -\nApple Pay\n . Budget-5M, Team size- 25+\n . Created and managed project plan for Apple Pay Implementation project\n   using Agile - Waterfall Hybrid methodology, as part of Common Payment\n   Platform (CPP). Created Business Requirement Document (BRD), Functional\n   Requirement Specification (FRS) document, User Requirement Specification\n   (URS) and Change Request (CR) document for system application\n   development.\n . Collaborated with multiple functional areas to complete required\n   deliverables.\n . Performed contract Gap Analysis.\n . Performed Requirement Analysis and developed Use Cases, Activity Diagrams\n   using Visio diagraming. Worked with a wide variety of internal staff,\n   including senior level management, to ensure process flows and policies\n   are properly defined and documented according to company guidelines.\n . Coordinated and facilitated post deployment reviews for continual release\n   improvements.  Managed deployment of concurrent releases; communicated\n   release processes, timelines, requirements, and strategies to\n   stakeholders and project resources.\n . Maintained communications with QA to ensure problem records were resolved\n   and closed prior to release launch. Created and distributed release\n   processes to cross functional teams.\n\n\n3-D Secure Integration Project (ProtectBuy)\n . ProtectBuy is an added layer of security to Discover card holder that\n   helps make online shopping experience more safe and secure.\n . Defined boarding process and procedures\n . Defined engagement process\n . ACS Vendor Certification; MPI Vendor Certification; Issuing Participant\n   Certification\n . Acquirer/Merchant Certification; Hosted Vendor - OBO (On-behalf-Of);\n   Defined Support Service Model\n . Anti-Money Laundering Project\n . Performed a variety of AML/KYC money laundering activities in support of\n   Bank Secrecy Act (BSA) and /or Anti-Money Laundering regulations designed\n   to prevent financial-related crimes against the government while\n   decreasing client reputational risk.\n . Review cases for adherence to EDD procedures and ensure that\n   recommendations are accurate based on all information available.\n\nCitigroup, Cincinnati, OH\n03/2014 - 09/2014\n(Digital ATM)\nConsultant - Program Manager: ATM Windows 7 Upgrade\n . Budget-42M, Team size- 50+\n . Managed all forecasting and demand planning activities for E01 and E02\n   project size and effort estimates in excess of $41 Million dollars for\n   global initiatives.\n . Managed Windows 7 32-bit software solution that is in compliance with\n   Citi CATE in 23 countries.\n . Managed regional project managers.\n . Utilized HP Quality Center and TFS to manage release plan.\n . Managed Global release to support ATM Windows 7.\n . Performed integration change control.\n . Managed acceptance criteria for regional consumptions.\n\nOne Call Professionals, Columbus, OH\n02/2013 - 11/2013\nConsultant - Scrum Master: eCommerce (Global Distribution System) CRM\n . Worked on one workstreams of a large project to manage GDS - e-commerce\n   (Global Distribution System) implementations.\n . Coached team in optimal time utilization through concepts of scrum and\n   agile management methodologies.\n . Led sprint reviews and daily scrum meetings to touch base with whole team\n   and ensure that all members were performing satisfactorily.\n . Trained team members as well as the rest of the company staff in\n   employing agile and scrum practices to improve work flow.\n . Managed system integrations. Planned the release to ensure that sprints\n   did deliver increments with the right quality.\n\nJPMorgan Chase, Columbus, OH                                  02/2012 -\n10/2012\nConsultant - IT Risk Project Manager:  (Data Management & Analytics)\n . Managed multiple cross-functional projects with globally dispersed teams\n   to manage the risks, issues and change request to minimize its impact to\n   project scope, schedule and cost on the design and build of Integrated\n   Consumer Data Warehouse (ICDW); Enterprise Data Warehouse (EDW); and\n   Info1.\n . Communicated the project status, risks and issues on regular basis to all\n   stakeholders.\n . Collaborated with multiple functional areas to complete required\n   deliverables. Provided day to day direction to team members. Managed\n   Enterprise Security Manager (ESM) remediation review for Info1 Data\n   Management.\n . Managed Guardium Logs Reporting on EDW (Enterprise Data Warehouse)\n   servers, to ensure that there is an audit trail of the changes being\n   performed by the DBA team with elevated access.\n . Designed Business Impact Analysis process for ICDW (Integrated Customer\n   Data Warehouse), Information One, and EDW staffs.\n . Managed ICDW (Integrated Customer Data Warehouse) Control Self-Assessment\n   to ensure that data back-up and retention requirements are defined and\n   agreed upon by key business stakeholders.\n . Managed third Party Provider Oversight-Lifecycle to ensure that there is\n   a clear escalation/reporting process for issues arising (e.g. security\n   breaches, Service Level Agreement problems).\n\nStaffmark, Cincinnati, OH                                     01/2011\n-10/2011\nConsultant - IT Project Manager\n . Provided management with accurate, independent and timely assessments in\n   the area of risk, internal control weakness, information systems\n   security, ERP Systems (PeopleSoft -CRM), database technologies, disaster\n   recovery and Business continuity planning etc.\n . Managed resources during the entire project with strict deadlines and\n   budget constraint.\n . Acted as a point of contact to all stakeholders by providing regular\n   project status updates to avoid communication breakdown and surprises.\n   Facilitated project meetings, change control, resource planning and\n   leveling, risk management and contingency planning.\n\n\n\n\n\nVeyance Technologies, Inc, Marysville, OH\n                                  04/2007 - 08/2008\nConsultant - IT Project Manager /Scrum Master\n . Plant infrastructure upgrade and SAP Implementation\n . Create and execute Lean and Operational Excellence organizational\n   strategies, manage business program initiatives, and identify and\n   implement Best Practices.\n . Led and managed the Lean Design for Innovation Committee for the\n   development of an \"Innovation Methodology Playbook\"\n . Led cross-functional team to design and implement SAP ERP/CRM system for\n   the Manufacturing business using ASAP approach.\n . Worked with and managed off-shore developer teams.\n . Designed a process for managing new data Center applications and\n   initiatives, from concept to implementation.\n\nJPMorgan Chase, Columbus, OH                                  1991 - 2006\nIT Project Manager\n . Managed several medium to large development projects focusing mostly on\n   call center infrastructure, data center applications and enterprise\n   telecommunication offerings with a project team of between 8 and 20, and\n   duration of between 3 months and 12 months: with the aid of MS Project\n   Portfolio Management.\n\n|Education & Certifications                                                 |\n\n\n . Master of Business Administration, Information Technology Project\n   Management, Minor - Finance, Ashland University, Ashland, OH\n\n\n . Bachelor of Science, in Business Administration, Franklin University,\n   Columbus, OH\n . Information Technology Project Management Certificate (MBA, Ashland\n   University) [pic][pic]\n-----------------------\n                           Program/Project Manager\n\n\n",
          "confidence": 1.0,
          "method": "full_text",
          "structured_data": null
        },
        "tax_term": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "source_by": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "designation": {
          "value": "Project Manager",
          "confidence": 0.9,
          "method": "ner",
          "structured_data": null
        },
        "experience": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        }
      }
    },
    {
      "resume_path": "data\\input\\Aleem Ahmed - resume.doc",
      "confidence_score": 0.635,
      "used_ocr": false,
      "extracted_fields": {
        "skills": {
          "methodologies": [
            {
              "name": "xp",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                58,
                5614,
                6651
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "safe",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                103
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "rad",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                801
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "agile",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                4556,
                5443,
                5479
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "tdd",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                4967
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "scrum",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                5449
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "rup",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                7027
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "devops": [
            {
              "name": "svn",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                77
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "git",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                81,
                3001
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "openshift",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                285
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "bitbucket",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                3013
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "cloud": [
            {
              "name": "jenkins",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                152,
                2606,
                3005,
                3828,
                4403,
                5239,
                5572
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "docker",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                160,
                256,
                2375
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "kubernetes",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                187
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "aws",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                270,
                2189,
                2262,
                2312,
                3033
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "ec2",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                274
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "batch",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                2616,
                3960
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "functions",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                5106
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "programming": [
            {
              "name": "chef",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                167
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "puppet",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                172,
                3108
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "gradle",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                222
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "java",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                304,
                549,
                720,
                934,
                1025,
                2569,
                2888,
                3178,
                3882,
                4238,
                4493,
                5497,
                5715,
                6551,
                6742,
                6829,
                7800
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "sql",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                314,
                5078,
                7923,
                7927
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "xml",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                318,
                335,
                354,
                2935,
                3495,
                4309,
                4888,
                5529,
                6193,
                6866,
                6924,
                7539,
                7832
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "xslt",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                344,
                6221,
                6936,
                7836
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "html",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                455,
                1702,
                1789,
                3857,
                6961,
                7899
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "python",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                582,
                3058
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "css",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                1707,
                1809
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "javascript",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                1711,
                1794,
                2954,
                3862,
                4323,
                7888
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "json",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                2970
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "perl",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                3053
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            }
          ],
          "frameworks": [
            {
              "name": "ansible",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                179
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "soap",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                349,
                3499,
                4313,
                4869,
                5524,
                6054,
                6600
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "spring",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                400,
                692,
                1035,
                1300,
                1531,
                1591,
                1756,
                2898,
                2909,
                3586,
                4263,
                4274,
                5022,
                5545,
                6449,
                6561
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "testng",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                2771,
                4115
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "junit",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                2988
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "jquery",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                4334
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "databases": [
            {
              "name": "oracle",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                589,
                606,
                635,
                660,
                758,
                1056,
                2645,
                2994,
                3222,
                3284,
                3437,
                3989,
                4221,
                4352,
                4363,
                5068,
                5580,
                7948
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "domain_specific": [
            {
              "name": "utilities",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                596,
                3229,
                3291
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "insurance",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                891,
                4440
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "design",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                968,
                1161,
                1209,
                1724,
                4844,
                5849,
                6843,
                7078,
                7179
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "architecture",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                1349,
                4589,
                4695,
                4747,
                5960,
                7377
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "infrastructure",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                2087
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "energy",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                3135
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "security",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                3593,
                4860
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "education",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                8005
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "soft_skills": [
            {
              "name": "analysis",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                1152
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "leadership",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                5356
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "presentation",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                6257,
                6905,
                6994
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "consulting",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                6676
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "sdlc": [
            {
              "name": "refactoring",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                1216
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "specialized_domains": [
            {
              "name": "containerization",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                2382
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ]
        },
        "first_name": "Aleem",
        "last_name": "Ahmed",
        "primary_email": {
          "value": "ahmed.8663167@gmail.com",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "secondary_email": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "phone": {
          "value": "6089091931",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "city": {
          "value": "",
          "confidence": 0.0,
          "method": "ner",
          "structured_data": null
        },
        "state": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "zip": {
          "value": "",
          "confidence": 0.0,
          "method": "regex",
          "structured_data": null
        },
        "work_authority": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "resume_link": {
          "value": "data\\input\\Aleem Ahmed - resume.doc",
          "confidence": 1.0,
          "method": "file_path",
          "structured_data": null
        },
        "raw_resume": {
          "value": "\n\nAleem Ahmed\n\n\n(608) 909-1931\n\n\nahmed.8663167@gmail.com\n\n----------------------------------------------------------------------------\n             --------------------------------------------------\n\nProfessionally 14+ experience  in  Java  development  experience  of  Web/N-\nTier/Client Server/Distributed technologies (SOA,  Restful/SOAP,  WSDL,  Web\nServices, JMS, Spring, Hibernate,  EJBs,  Servlets,  JSP,  XML,  JavaScript,\nStruts, WebSphere Application Server, Rational  Application  Developer(RAD),\nWebLogic Application Server) and Client Server technologies (AWT, Swing)  on\nvarious platforms  like  Windows  2k/NT/XP/7/8,  Linux  and  Unix.  He  also\nexperienced in working on DevOps process and tools (Code review, unit test\nautomation, Build & Release automation, Environment, Service,  Incident  and\nChange  Management)  .He  has  developed  software  systems   for   Utility,\nInsurance and  Telecom  domains  with  responsibilities  in  all  phases  of\nsoftware development life cycle  including  Analysis,  Design,  Development,\nTesting, Implementation, Support and Training.\n\n\n .  Strong  experience  in  AWS Cloud  platform  and  its  features  which\n   includes EC2, VPC, EBS, Cloud Watch,  Cloud  Trail,  CloudFormation  AWS\n   Config, Autoscaling, Cloud Front, IAM, S3. Drives Enterprise Application\n   technical blueprints, business requirements & specifications, integration\n   of heterogeneous environments, API implementations. He is an  independent\n   worker  with   strong   problem   solving,   analytical,   interpersonal,\n   communication, presentation and testing skills.\n\n\n . Expertise in developing  inter-operable  Web  Services  and  its  related\n   technologies like SOAP, WSDL, UDDI, XML related technologies/tools such\n   as JAXB, XSD,  XML with  good  understanding of JAX-WS,  JAX-RS,  JAX-\n   RPC inter-operable issue.\n\n\n . Development experience using Java/J2EE ( JSF, JSP, Servlets,  JDBC,  EJB,\n   MDB, JTA, JPA, JNDI, JMS, IBM MQ, Web Services (Axis, JAX-RPC, JAX-WS),\n   MVC Struts frameworks, Hibernate , Spring  ,  Log4j,  JUnit4,  Maven  and\n   ANT).\n\n\nTechnical Skills:\n|Operating Systems   |Linux, Unix, Solaris, Windows /NT/2000/2003/XP      |\n|Version Control     |SVN, GIT, CVS, Visual Source Safe, PVCS, Clear Case,|\n|                    |Bit bucket                                          |\n|Automation/Build    |Jenkins, Docker, Chef, Puppet, Ansible, Kubernetes, |\n|Tools               |Harvest, ANT, Maven, Gradle, Artifactory            |\n|Virtualization      |Docker, Amazon AWS/EC2, VMWare, OpenShift           |\n|Language            |Java, J2EE, SQL                                     |\n|XML Technologies    |XML, WSDL, XSLT, SOAP, XML Schema, Web services.    |\n|Web Technologies    |J2EE, Spring, Hibernate, JSP, JNDI, LDAP, Servlets, |\n|                    |JMS, JDBC, Swing, HTML                              |\n|Web App & Web       |WebLogic, WebSphere, Tomcat, JBoss, JIRA, Service   |\n|Servers             |Now                                                 |\n|Scripting Languages |Java Script, Unix shell scripting, Python           |\n|Oracle Utilities    |Oracle Customer Care & Billing, Oracle MDM, OUAF    |\n|Database            |Oracle 10g/9i/11g                                   |\n|App Framework       |Spring MVC, Struts, Hibernate, Java Persistence API |\n|Data Integration    |Oracle Fusion, MQ, WMB 7.0                          |\n|IDE Tools           |Eclipse, RAD 8.5/7.0/6.1, JBuilder, JDeveloper,     |\n|                    |IntelliJ Idea                                       |\n\n\nProfessional Experience:\nAmerican Family Insurance, Madison WI                               (July\n2015 - Feb 2020)\nSr. Java Developer\nResponsibilities:\n o Design, develop, and maintain web-based applications  using  Java,  J2EE,\n   Spring, Hibernate, and Oracle\n o  Involved  actively  in  the  complete  software  development  lifecycle,\n   including performance analysis, design, development, and testing\n o Participate in design refactoring and enhancement of critical component\n o Developed the application using spring  framework  that  leverages  model\n   view layer architecture, also configured Dependency Injection.\n o Extensively used Hibernate in data access  layer  to  access  and  update\n   information in the database.\n o Involved in the Development of Spring Framework Controllers\n o Developing application using Spring core module and worked  on  Messaging\n   service.\n o Developed user interface using JSP, JSP Tag libraries  JSTL,  HTML,  CSS,\n   JavaScript.\n o Design highly usable GUIs using Spring Framework, JSPs, Struts MVC, HTML,\n   JavaScript, and CSS\n o Worked  on  Single  Page  Applications  using  AngularJS  which  includes\n   Routing, Directives, and Templates.\n o Used JMS for the asynchronous exchange  of  critical  business  data  and\n   events amongJ2EE components and legacy system.\n o Implemented a fully automated CI build and deployment infrastructure  and\n   processes for multiple projects.\n o Involved in performing application deployment to AWS Elastic  Bean  Stack\n   environment.\n o Configured & deployed applications on AWS for a multitude of applications\n   utilizing the AWS stack, cloud formation.\n o Standalone system through SCCM and Docker containerization.\n o Performed  source  control  systems  to  automate  for  Build/deployment,\n   Software Configuration/Continuous Integration/Continuous Delivery/Release\n   Management related tasks in Java EE Environments using Maven and Jenkins\n o Batch/Shell scripting\n o Used Oracle as a transactional database for customer payments.\n o Participated in integration testing and unit testing.\n o Implemented TestNG framework for system integration testing.\n o Performed troubleshooting and fixed production defects.\n\n\n\n\nEnvironment: JAVA, J2ee, Spring MVC,  Spring  Core,  Hibernate,  JSP,  XML,\nAngularJS, CSS3, JavaScript, AJAX, JSON, RESTful  APIs,Junit,  Oracle,  Git,\nJenkins,  Bitbucket,  Maven,  CVS,  AWS,  Shell  scripting,  Perl,   Python,\nGuidewire Applications, Apache Tomcat, JBoss, Puppet, CA Harvest, JIRA.\n\nDTE Energy, Detroit, MI                                           (May 2013\n- May 2015)\nSenior Java Developer\nResponsibilities:\n o Worked on Oracle Utilities Customer Care and Billing Module.\n o Worked on Oracle Utilities Application Framework (OUAF) for usage data\n   management for bill print module.\n o Integrated meter data management with billing module using Oracle Fusion\n   ESB.\n o Implemented web services using JAX-WS, XML, SOAP, and WSDL.\n o Produced and consumed web services from external systems.\n o Implemented Spring security for payment gateway.\n o Performed source control systems to automate for Build/deployment,\n   Software Configuration/Continuous Integration/Continuous Delivery/Release\n   Management related tasks in JavaEE Environments using Maven and Jenkins\n o Developed UI using HTML, JavaScript, CSS3, and Java Server Pages (JSPs).\n o Business Logic was implemented using EJBs and DAOs.\n o Batch/Shell scripting\n o Used Oracle as a transactional database for customer payments.\n o Participated in integration testing and unit testing.\n o Implemented TestNG framework for system integration testing.\n o Performed troubleshooting and fixed defects.\n\nEnvironment: Oracle CCB 2.3.1, Java, J2EE,  JSP,  MVC  Struts,  Spring  MVC,\nSpring Core, Hibernate, Web Services, XML, SOAP, WSDL,  JavaScript,  jQuery,\nHTML5,  CSS3,  Oracle  11g,  Oracle  Fusion  ESB,  WebLogic  Server,  Maven,\nJenkins, ClearCase, UNIX, Windows 8.\n\n\n\nAAA Insurance, Dearborn, MI                            (October 2010 -\nApril 2013)\nSenior Java Developer\nResponsibilities:\n o Developed SOA services using Agile methodology.\n o Designed the architecture for middleware integration with external\n   systems based upon business requirements using J2EE architecture\n   framework with special emphasis on SOA architecture and web services.\n o Implemented web services using JAX-WS and JAX-RS.\n o Developed SOA design using WS-Security, SOAP/HTTP, WSDL, and XML/XSD\n   (schemas).\n o Utilized elements of extreme programming approach (especially TDD, pair\n   programming, and code walkthrough's).\n o Developed Spring and Hibernate applications.\n o Developed Oracle PL/SQL packages, procedures, and functions.\n o Worked with messaging queue applications.\n o Developed web application using Struts.\n o Extensively used Maven for build and Jenkins for continuous integration.\n o Performed code reviews and suggested best coding practices.\n o Provided technical leadership for development team to implement the\n   system.\n o Developed applications using Agile Scrum methodology.\n\nEnvironment: Agile Methodology, Java, J2EE, SOA, Web  Services,  SOAP,  XML,\nWSDL, Struts, Spring, Hibernate, JMS, Maven, Jenkins, Oracle 11g, Linux  and\nWindows 2003/XP/7, Data Power, WebSphere Application Server 7.\n\nSyniverse, Tampa, FL                                       (January 2008 -\nSeptember 2010)\nSenior Java Developer\nResponsibilities:\n o Worked on Direct Operator Billing SOA project and implemented web\n   services using JAX-WS.\n o Performed design, development, and testing of the web services\n   application.\n o Developed web services using service-oriented architecture (SOA).\n o Created and updated WSDLs for web services (produce and consume both).\n o Used SOAP protocol to access the web service - WSDL schemas and JAX-RPC\n   for remote.\n o Procedure calls of the third-party property leads (SOA).\n o Used XML technologies like XPath, XSLT, and JAXB for data exchange and\n   presentation.\n o Developed middleware components like Message Driven Beans for JMS/MQ\n   integration.\n o Built service-oriented platform using web services for client requests.\n o Developed code using spring and hibernate frameworks to access database.\n o Used CVS for source code maintenance.\n\nEnvironment: Java/J2EE, Spring, Hibernate, JPA, Web  Service  (XPath,  SOAP,\nWSDL, JAXB, JAX-RPC), MDB, SAML, JBoss, Windows 2003/XP, UNIX, Solaris.\n\nMindTree Consulting Pvt Ltd, Bangalore, India                   (June 2004\n- December 2007)\nSenior Java Developer\nResponsibilities:\n o Analyzed, designed, and developed application based on Java/J2EE and\n   design patterns.\n o Used XML technologies for data exchange and presentation using XML, XSL,\n   and XSLT.\n o Developed various HTML, DHTML, and JSP pages for the presentation layer.\n o Implemented RUP methodology and designed application using UML design\n   patterns.\n o Visualized and designed use cases, sequence diagrams, and class diagrams.\n o Used J2EE design patterns and developed EJB and DAO components.\n o Used Struts framework for the development of J2EE components.\n o Created servlets for abstraction between the client and server layers\n   (using MVC Architecture).\n o Developed DAO (Data Access Objects) and called them from EJB.\n o Designed and developed the system business logic using Stateless Session\n   Beans.\n o Used XML templates to present the user data in different formats.\n o Created JAR, WAR and EAR files.\n o Deployed EAR file on WebLogic Application Server.\n o Used ClearCase for version control.\n o Created build scripts using ANT.\n o Provided support and maintenance.\n\n\nEnvironment: Java, J2EE, JDBC,  Servlets,  JSP,  EJB,  XML,  XSLT,  WebLogic\nApplication  Server  8.1/6.1,  Struts,  UML,  JavaScript,  HTML,   JBuilder,\nClearCase, SQL, SQL Server 2000/2005,  Oracle  10g/9i,  UNIX,  Sun  Solaris,\nRedhat Linux, Windows 2000.\n\n\n\nEducation: Master's Degree in Computer Applications Osmania University,\nIndia\n\n",
          "confidence": 1.0,
          "method": "full_text",
          "structured_data": null
        },
        "tax_term": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "source_by": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "designation": {
          "value": "Application Developer",
          "confidence": 0.8,
          "method": "regex",
          "structured_data": null
        },
        "experience": {
          "value": "14+",
          "confidence": 0.9,
          "method": "regex_total_experience_summary",
          "structured_data": null
        }
      }
    },
    {
      "resume_path": "data\\input\\Alexander Mai - SAS - VC.doc",
      "confidence_score": 0.66,
      "used_ocr": false,
      "extracted_fields": {
        "skills": {
          "programming": [
            {
              "name": "sas",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                25,
                59,
                2239,
                3803,
                4402,
                6017,
                6684
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "sql",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                2247,
                3807,
                4425,
                6051,
                6688
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "xml",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                2395,
                2411
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            }
          ],
          "domain_specific": [
            {
              "name": "medical",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                95,
                435,
                2329
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "design",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                174
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "insurance",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                297,
                580,
                5239
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "policy",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                3520
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "clinical",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                3679
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "government",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                4824
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "finance",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                5031
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "construction",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                5922
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "security",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                6215
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "sdlc": [
            {
              "name": "surveys",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                423
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "soft_skills": [
            {
              "name": "research",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                728
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "analysis",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                879,
                1021,
                1211,
                2486,
                2594,
                3294,
                5882
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "documentation",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                1637,
                6904
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "organization",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                3380,
                6408
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "consulting",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                3974
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "presentation",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                6891
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "business_skills": [
            {
              "name": "reporting",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                922,
                1702,
                3217,
                3527,
                3853,
                4013,
                6523
              ],
              "experience_weight": 0.0,
              "importance_score": 0.8
            }
          ],
          "networking_equipment": [
            {
              "name": "accounting",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                3988,
                5020
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "cloud": [
            {
              "name": "shield",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                6262
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "data_skills": [
            {
              "name": "excel",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                6812
              ],
              "experience_weight": 0.0,
              "importance_score": 0.9
            }
          ]
        },
        "first_name": "Alexander",
        "last_name": "T. Mai",
        "primary_email": {
          "value": "Alextmai@gmail.com",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "secondary_email": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "phone": {
          "value": "7742326198",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "city": {
          "value": "MA",
          "confidence": 0.7,
          "method": "ner",
          "structured_data": null
        },
        "state": {
          "value": "LA",
          "confidence": 0.8,
          "method": "city_database",
          "structured_data": null
        },
        "zip": {
          "value": "02445",
          "confidence": 0.7,
          "method": "regex",
          "structured_data": null
        },
        "work_authority": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "resume_link": {
          "value": "data\\input\\Alexander Mai - SAS - VC.doc",
          "confidence": 1.0,
          "method": "file_path",
          "structured_data": null
        },
        "raw_resume": {
          "value": "\n                              Alexander T. Mai\n\nAddress     7 University Road                                       Cell\n  (774) 232-6198\n                  Brookline, MA 02445\nEmail     Alextmai@gmail.com\n\nEducation\nNortheastern                                                      University\n                       Boston, MA\n    . Graduated with Bachelor of Science Degree in  Business  Administration\n                                        May 2007\n    . Majored in Dual Concentration: Management  Information  Systems  (MIS)\n      and Finance\n    . Completed six months co-op programs at Zoran Corporation and Raytheon\n    . Honors: Dean's List, Balfour Academy's Scholarship\n\n\nSkills\n    . Applications: Advanced SAS, SQL, R,  Stata,  XML  Mapper,  SSIS,  SAP,\n      ArcGIS, TeamSite, Cognos\n    .  Certifications:  Microsoft  Word  Certified,   SAS   Certified   Base\n      Programmer for SAS 9 Credential\n\nExperience\nHarvard                            Medical                            School\n       Boston,  MA                        Statistical  Programmer   &   Data\nAnalyst                                                                 June\n2016 - Present\n    . Design and develop statistical and data management programs associated\n      with grant funded projects on disability and health insurance.\n    . Perform in multiple types of large cross-sectional and longitudinal\n      data files. Some data stem from population-based surveys (e.g.,\n      Medical Expenditure Panel Survey, Health and Retirement Study). Other\n      data include health care and prescription drug claims from private and\n      public insurance plans (e.g., Medicare and/or Medicaid).\n    . Develop high quality data sets to estimate statistical models and\n      conduct hypothesis testing.\n    . Interpret research protocols and create programs and algorithms that\n      are then discussed with the PI.\n    . Work independently and make decisions about how to implement analysis\n      and provide well written detailed reporting of results, including\n      determination of proper summary statistics, report formats and other\n      analysis considerations.\n    . Responsible for successful completion of these tasks within competing\n      project timelines, and must ensure integrity of data collection, data\n      review, data compilation, and analysis techniques.\n\nSenior                             Whole                              Health\n                Cambridge,    MA                           HEDIS     Manager\n                                                       October  2014  -  May\n2016\n    . Responsible for all aspects of the HEDIS rates production process,\n      including working with internal data sources, external data sources,\n      supplemental data sources and vendor partners to produce and submit\n      the company's HEDIS rates on an annual basis.\n    . Supported the integration of hybrid data, administrative data\n      refreshes and supporting HEDIS documentation in order to meet\n      mandatory NCQA and CMS STARS data reporting requirements.\n    . Analyzed the HEDIS results, compares them to nationally identified\n      benchmarks, and advises the director on opportunities and strategies\n      to continue improving the rates.\n    . Participated on teams working to understand the barriers to strong\n      performance, and identifying opportunities to resolve the barriers in\n      meeting all requirements of the HEDIS program, CAHPS and HOS as\n      needed.\n    . Worked closely with relevant departments to assure understanding of\n      the financial implications of the HEDIS and Star Rating programs.\n    . Utilized SAS and SQL programming to automate and transform multiple\n      reports, reconcile and transfer medical records by file types to\n      different locations, and validate XML files using XML Mapper\n\nCommonwealth of Massachusetts - Center for Health Information  and  Analysis\n(CHIA)                 Boston, MA                        Programmer  Analyst\n                                                            February 2013  -\nOctober 2014\n    . Conducted statistical programming and analysis on All Payer Claims\n      Database (APCD), Acute Hospital Case Mix Databases, Medicaid\n      Management Information System (MMIS), Centers for Medicare and\n      Medicaid Services (CMS), Census, and other data resources to support\n      the analytic and information needs of key audiences.\n    . Worked with the Quality Assurance team to evaluate and standardize\n      APCD by applying statistical analyses related to health care service\n      utilization, health care costs, and quality measurement.\n    . Supported the analytics for the state's consumer health care quality\n      and cost website by calculating cost metrics for providers in the\n      Commonwealth and annual Cost Trends reporting.\n    . Consulted with Subject Matter Experts (SME) to determine appropriate\n      analysis plans and data extraction code specifications, coordinates\n      projects involving organization of data, text and graphics, designing\n      study methodologies; and assisting with the development of new areas\n      of data collecting and policy reporting for the agency.\n    . Applied HEDIS measures to analyze health care quality and utilization\n      data to identify best practices, care delivery trends and clinical\n      practice variation within hospital, provider group and individual\n      physician sites of care\n    . Trained other analysts on SAS, SQL, ArcGIS and helped troubleshoot\n      production reporting issues, identified root causes and implement\n      necessary coding updates\n\nCVS                                                                 Caremark\nWoonsocket,  RI                        Manager  &   Senior   Consulting   of\nAccounting and  Financial  Reporting                            May  2012  -\nJanuary 2013\n    . Responsible for the creation and delivery of multiple Medicare Part D\n      reports that summarize results, identify trends, and develop forecasts\n      for: Beneficiary Premiums, Drug Utilization, Incurred but not Recorded\n      Claims, Risk Scores, Actuarial Metrics and Member Eligibility.\n    . Utilized large databases management in an AIX environment, and applied\n      basic Unix language, SAS Data Step and Proc SQL to turn million of\n      records of raw data from multiple tables/files into summary reports.\n    . Automated majority of the reports used by the accountants to book\n      journal entries. Other reports will be delivering to external business\n      clients as part of their monthly financial package.\n    . Managed raw data from both external and internal by performing data\n      reconciliation and reports variance. External are the government\n      (Center for Medicare and Medicaid Services) and third party vendors.\n      Internal are the Enterprise Data Warehouse and monthly data extracts\n      from AS400.\n    . Interacted effectively with members of accounting, finance, actuarial,\n      forecasting, billing, account management, and Medicare operations\n      groups. In addition, reviewed and assisted junior team members in\n      their workload.\n\nMIB  Solutions,  Risk  Management  Solutions  for  the  Insurance   Industry\n                           Braintree,   MA                          Database\nProgrammer                                                           Analyst\n          December 2010 - April 2012\n    . Performed logic and syntax checks, and standardizes data submitted\n      from various sources. Appropriately documented procedures in support\n      of projects as assigned. Managed interns and worked on multiple\n      projects at the same time.\n    . Streamlined previously written computer code and used or modified to\n      meet the current project objectives. Participated in skills growth\n      path to promote achievement of corporate goals as recommended by the\n      Department Manager.\n    . Interacted with Database Programmer Analysts, Actuaries, and\n      Developers to develop software applications for data analysis.\n      Participated in the evaluation, construction and transition into\n      production of software.\n    . Migrated software applications done in SAS and the legacy environment\n      to SQL Server based applications. Complied with all applicable\n      corporate and departmental software policies, standards, and practices\n      including all standards relating to Security and Disaster Recovery.\n\nBlue       Cross       Blue       Shield        of        MA        (BCBSMA)\n                                   Boston, MA\nData                  Analyst                  &                  Programmer\n                                July 2007 - February 2010\n    . Managed production databases and web/online  content  development  and\n      organization.\n    . Designed/developed, programmed, maintained and  published  operational\n      reports.\n    . Delivered production ready reporting with graphs, charts  and  dynamic\n      drill through capabilities.\n    .  Performed  adhoc  and  customer   reports   for   Informatics/analyst\n      community.\n    .  Developed  tools  using  SAS,  SQL,  and  Cognos,  to  minimize  data\n      manipulation required by informatics analysts.\n    . Created value by integrating current business Excel, Access  processes\n      into DW production process.\n    . Trained interns  and  prepared  presentation,  documentation  and  key\n      management reports.\n\n\n\n\n",
          "confidence": 1.0,
          "method": "full_text",
          "structured_data": null
        },
        "tax_term": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "source_by": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "designation": {
          "value": "Data Analyst",
          "confidence": 0.9,
          "method": "ner",
          "structured_data": null
        },
        "experience": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        }
      }
    },
    {
      "resume_path": "data\\input\\Data Architect - VA - Mehadi.doc",
      "confidence_score": 0.8150000000000001,
      "used_ocr": false,
      "extracted_fields": {
        "skills": {
          "domain_specific": [
            {
              "name": "architecture",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                5,
                5561,
                8164,
                9693,
                9771,
                9897,
                10328,
                11062
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "design",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                41,
                736,
                1870,
                8008,
                8141,
                9746,
                10305,
                11039
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "governance",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                85,
                2669,
                2908
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "mining",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                191,
                2149
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "government",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                378
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "banking",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                419,
                6059,
                6423
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "engineering",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                668,
                6254,
                8375,
                10539,
                11273,
                11608,
                14096,
                14162
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "audit",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                926,
                3124,
                4717,
                5579
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "security",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                5658,
                6809,
                15214
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "retail",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                6407
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "marketing",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                6456
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "defense",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                7828,
                7969,
                8049,
                10560,
                10620,
                13115
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "education",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                9554,
                9857,
                15703
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "military",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                10694,
                10796
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "logistics",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                11668,
                13260
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "soft_skills": [
            {
              "name": "analysis",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                109,
                748,
                2120,
                2279,
                3590,
                4029,
                4046,
                4492,
                4833,
                6745,
                7079,
                8761,
                12227,
                14876
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "documentation",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                1273
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "presentation",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                1287,
                11530
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "research",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                6475,
                7427,
                7473,
                7505,
                8177,
                10341,
                11075
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "business_skills": [
            {
              "name": "reporting",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                118,
                1039,
                5721
              ],
              "experience_weight": 0.0,
              "importance_score": 0.8
            }
          ],
          "data_skills": [
            {
              "name": "etl",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                154,
                757,
                1877,
                2202,
                5787,
                7491,
                7692,
                7707,
                7758,
                11514
              ],
              "experience_weight": 0.0,
              "importance_score": 0.9
            },
            {
              "name": "erwin",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                496,
                4640,
                8475,
                10826,
                14434
              ],
              "experience_weight": 0.0,
              "importance_score": 0.9
            },
            {
              "name": "informatica",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                761,
                1076,
                2832
              ],
              "experience_weight": 0.0,
              "importance_score": 0.9
            },
            {
              "name": "databricks",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                1094,
                2348
              ],
              "experience_weight": 0.0,
              "importance_score": 0.9
            },
            {
              "name": "hadoop",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                1168
              ],
              "experience_weight": 0.0,
              "importance_score": 0.9
            },
            {
              "name": "hive",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                1175
              ],
              "experience_weight": 0.0,
              "importance_score": 0.9
            },
            {
              "name": "excel",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                1316,
                6378,
                14911
              ],
              "experience_weight": 0.0,
              "importance_score": 0.9
            }
          ],
          "programming": [
            {
              "name": "xml",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                531,
                9998
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "sql",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                794,
                801,
                805,
                819,
                1248,
                1393,
                2042,
                6367,
                11774,
                12325,
                12338,
                14806,
                14819,
                14891,
                15384
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "python",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                1027
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "crystal",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                1148
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "sas",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                1191
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "java",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                1404
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "perl",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                14828
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "vba",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                14929
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            }
          ],
          "databases": [
            {
              "name": "oracle",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                956,
                1234,
                2857
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "sybase",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                1241
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "db2",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                1269
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "cloud": [
            {
              "name": "azure",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                1088,
                1105,
                2327
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "aws",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                1164
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "translate",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                9306,
                12587
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "functions",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                9449,
                12730,
                15348
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "batch",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                12301,
                14303,
                14779
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "sdlc": [
            {
              "name": "optimization",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                12512,
                15010
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ]
        },
        "first_name": "Mehadi",
        "last_name": "Hassan",
        "primary_email": {
          "value": "Mehadimh@yahoo.com",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "secondary_email": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "phone": {
          "value": "7039661154",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "city": {
          "value": "Nokesville",
          "confidence": 0.9,
          "method": "address_pattern",
          "structured_data": null
        },
        "state": {
          "value": "VA",
          "confidence": 0.9,
          "method": "address_pattern",
          "structured_data": null
        },
        "zip": {
          "value": "20181",
          "confidence": 0.9,
          "method": "address_pattern",
          "structured_data": null
        },
        "work_authority": {
          "value": "With Excellent Credit And Clean Background Industry",
          "confidence": 0.8,
          "method": "regex",
          "structured_data": null
        },
        "resume_link": {
          "value": "data\\input\\Data Architect - VA - Mehadi.doc",
          "confidence": 1.0,
          "method": "file_path",
          "structured_data": null
        },
        "raw_resume": {
          "value": "\nMehadi Hassan\n11800 Belmont Farm Ln, Nokesville, VA 20181\nMehadimh@yahoo.com\n703-966-1154\n\n\nSYNOPSIS:\n\nA senior level Information Technology professional with years of experience\nin leading and hands-on experience in all phases of database solutions\nrelated projects for government and non-government organizations.\n\nCORE COMPETENCIES:\nData Architecture, Data modeling, Database design and development, Data\nManagement, Data Governance, Complex Data Analysis,\nReporting/Visualization, Data Ingest (ETL) for Datamart, Data profiling,\nData mining, Data Quality, Metadata Management, Master Data Management,\nRequirements gathering, Quality Assurance/Testing\n\nWORK STATUS:\nUS CITIZEN with Excellent Credit and Clean background\n\nINDUSTRY:\nGovernment (DOD, DHS-TSA, HHS, DOE), Financial/Banking/Mortgage/Credit etc.\n\nTECHNCAL SKILLS:\n\nModeling/Methodologies: ER studio, Erwin, Canonical data model (CDM), UML,\nXML, National Information Exchange Model (NIEM), Decision Modeling, Object\nModeling, Ontology, Star Schema, Snow Flake, OLAP, OLTP, Forward/Reverse\nEngineering, Natural Language Processing, Conceptual/Logical/Physical\nDesign\n\nData Analysis/ETL: Informatica (AXON, IDQ, Glossary), PL/SQL, No SQL, SQL\nPlus, TOAD, SQL Developer, IBM Information Server Suite (Information Server\nAnalyzer, Web sphere, Quality Stage, Data Stage, Audit Stage, Business\nGlossary), Oracle Data Quality for Data Integration (Profiling, Time\nSeries, Quality), Python, R\n\nBI/Reporting/Visualization/Cloud: Tableu, Informatica, Azure Databricks,\nAzure Data lakes, POWER BI, Business Objects,  Crystal Reports, AWS,\nHadoop, HIVE, Map Reduce, SAS BI\n\nOperating Systems: Windows, UNIX.\n\nDBMS: Oracle, Sybase, SQL Server, MS Access, DB2.\n\nDocumentation & Presentation: PowerPoint, Word, Excel, Visio, MS Project.\n\n Other: Clear Case, Clear Quest, Control M (scheduler), JIRA, SQL Loader ,\nJava, C++\n\n\nBUSINESS EXPERIENCE:\n\nLeidos/Accelerated Information Management\n                                  11/07 - Present\nClients: DHS-TSA, HHS\n\nLead Data Architect\n\nTask highlights:\n    . Meet clients, stake holders and subject matter experts on regular\n      basis to gather requirements, collect information, understand/clarify\n      business rules, present logical and physical data models, get approval\n      on proposed data models, present sample reports etc.\n    . Construct conceptual/logical/physical data models, compose data\n      dictionary, design ETL and create source target mapping.\n    . Collect, archive, cleanse and validate metadata from various sources\n      to compose master data and various metadata.\n    . Wrote complex PL/SQL scripts such as stored procedures, triggers,\n      transformation logics for data analysis and validation, data\n      mining/cleansing, data load, data conversion/migration, ETL and report\n      validation.\n    . Performed statistical, qualitative, and quantitative analysis on large\n      data sets using tools such as AZURE Data Lakes and Databricks, Tableu\n    . Developed complex Adhoc and canned reports for internal customers,\n      external stakeholders/requests such as press, congressional inquiry,\n      GAO etc.\n    . Provided expert support such as naming standards for database\n      tables/entities, columns/attributes, business glossaries; followed\n      standards such as NIEM.\n    . Compose data governance proposal, roadmap, and policies by meeting\n      with stake holders and learning known issues in various data movements\n      within the enterprise.\n    . Use tools such as Informatica AXON, IDQ and Oracle Data Quality to\n      stand up and implement data governance, data quality, data catalog,\n      data lineage and data profiling.\n    . Analyze data anomalies, develop data standards, compose business\n      glossary, develop, and implement data cleansing rules for data\n      integrity.\n    . Develop data audit rules to measure improvement of data quality as an\n      on-going/continues process (In-band) in important stages of data\n      ingest.\n    . Create data quality plan, data quality baseline metrics for legacy\n      data and data profiling process plans.\n\n\n\nProlink\n                          09/06 - 10/07\nServed Fannie Mae\n\nLead data Analyst\nWorked with the data stewards' team on the data management program under\nthe corporate data strategy to implement data quality solutions at Fannie\nMae.\n\nTask Highlights:\n    . Perform data analysis on multiple systems related to guarantees,\n      Mortgage based securities, foreclosures, risks, originations and\n      create as-is lineage, find problems with data flows both in physical\n      and logical designs and provide \"To be Lineage\" which may be able to\n      correct existing data flow issues.\n    . Profile data on multiple systems by entity and by attribute and\n      analyze overall health of the data in production systems.\n    . Created various reports, statistical analysis, pattern analysis, risk,\n      and benefit matrices on adhoc basis.\n    . Monitor data based on the business rules and find exceptions and turn\n      in reports to the business owners to take measures to correct data.\n    . Create trend graphs and matrices to monitor overall data accuracy.\n    . Collected business glossary/metadata and populated/updated metadata\n      repository.\n    . Managed metadata assets such as data dictionary, data models, naming\n      standards, business glossary and data structure analysis and provided\n      technical support and data handling recommendations.\n    . Created Conceptual/logical/physical models for meta data repository\n      using Erwin modeling tool.\n    . Provided expert guidance on automating Data Quality, data audit,\n      performance matrix collection jobs based on business rules and typical\n      data quality dimensions.\n    . Perform impact analysis on targeted data changes and system changes on\n      upstream and downstream systems.\n    . Provided expert support such as naming standards for database\n      tables/entities, columns/attributes, business glossaries; followed\n      standards such as NIEM.\n    . Create document of the data quality monitoring processes for business\n      users for all data quality profiling or monitoring in production.\n    . Performed some data stewardship duties with enterprise data\n      stewardship team and successfully performed duties such as data\n      quality requirements gathering, designing data improvement solutions,\n      analyzing data, profile, monitor and correct enterprise data,\n      databases, data warehouses, data flows, data lineage, models, and\n      metadata.\n    . Provided support on database model/architecture, data audit controls,\n      data quality controls, metadata management, archival requirements,\n      security controls, cross reference or data lineage, database and\n      reporting performance requirements, export transformation and load\n      (ETL) and specific file handling procedures for various data movement.\n\n\n\n\nChevy Chase bank\n                     07/05 - 08/06\n\n\n\n\nSenior Data Analyst\nThe project involved working with the data management team at CHEVY CHASE\nBANK to build a data warehouse, Business Intelligence and metadata\nrepository for Banking, Mortgage, Credit cards\n\nTask Highlight:\n . Interview SMEs (subject matter experts) during the process of designing\n   the EDW to understand multiple systems, data and processes while\n   designing or re-engineering a system.\n . Profile, analyze and monitor financial data to certify overall accuracy\n   of the data using PL/SQL and MS Excel\n . Analyze General ledger, retail, business banking, mortgage, Home equity\n   and marketing data and research existing reports to provide\n   recommendations to improve data quality in terms of prevention,\n   validation and performance on a EDW for a local bank.\n . Create data a dictionary to provide overviews of the diagrams and data\n   entity, attributes and relationships.\n . Performed analysis and collected matrices on the database activity,\n   growth, security, and performance of the databases within the enterprise.\n . Customize meta-model definitions and user properties to further enhance\n   the data dictionary.\n . Managed metadata assets such as data dictionary, data models, naming\n   standards, business glossary and data structure analysis and provided\n   technical support and data handling recommendations.\n . Creating, changing and maintaining jobs on data stage to migrate data\n   from different sources to the enterprise data warehouse and data marts.\n . Identify stewardship/ownership of the data and processes.\n . Analyzed existing systems and data flows to change or enhance data\n   movements.\n . Research sources of data; identify targets, and research existing ETL\n   processes; research and review final data ownership provides\n   recommendations to improve data quality.\n . Identify existing systems and processes where system allows unwanted\n   data.\n . Designed and developed ETL jobs using ETL tools.\n . Created and edited mapping documents for ETL processes\n\n\n\nComputer Sciences Corp\n       01/05 - 06/05\nServed Department of Defense\n\n\nSenior Computer Scientist\nPerformed duties as a senior software developer in the Enterprise Data\nWarehouse Project for Department of Defense TCAIMS project\n\nTask Highlights:\n . Design and develop EDW for Department of Defense.\n . Create and run daily, weekly, bi-weekly queries to maintain quality data.\n . Identity and design CDA (common data architecture), research sources of\n   the data, shared entities and attributes and provide recommendations.\n . Interview SMEs (subject matter experts) to understand multiple systems,\n   data, and processes while designing or re-engineering a system.\n . Re-engineer fact and dimension tables for the data warehouse IQ\n   databases.\n . Used Erwin and Power Designer for data modeling and used normalization\n   techniques to normalize data.\n . Efficiently performed stewardship duties such as analyzing data, profile,\n   monitor and correct enterprise data, databases, data warehouses, data\n   flows, data lineage, models, and metadata.\n . Perform data analysis on multiple systems and create as-is lineage, find\n   problems with data flows both in physical and logical designs and provide\n   \"To be Lineage\" which may be able to correct existing data flow issues.\n . Profile data on multiple systems by entity and by attribute and analyze\n   overall health of the data in production systems.\n . Monitor data based on the business rules and find exceptions and turn in\n   reports to the business owners to take measures to correct data.\n . Write stored procedures, triggers, and data migration scripts.\n . Backup/restore databases.\n . Translate transaction logs, performed synchronization, dumped the\n   transaction logs and altered databases when required.\n . Created stored procedures/functions, triggers, and queries to perform\n   data modification, data migration and data audits.\n\nDepartment of Education/ Anteon Corp\n                                 09/04 - 12/04\n\nLead Systems Analyst/Data Modeler\nPlayed lead role in the Federal Student Aid (FSA) developing Common data\narchitecture and metadata repository.\n\nTask Highlights:\n . Design a CDA (Common data architecture) conceptual logical model for FSA\n   (Federal Student Aid) under Department of Education using Popkin's SAEM\n   and System Architecture and MS Visio tools.\n . Collect data entities and attributes from multiple documents, systems,\n   XML repository and SMEs to during the process of designing the CDA.\n . Identify improper/wrong data collection processes and provide\n   recommendations on best practices to achieve quality data.\n . Create data a dictionary to provide overviews of the diagrams and data\n   entity, attributes, and relationships.\n . Identity and design CDA (common data architecture), research sources of\n   the data, shared entities and attributes and provide recommendations.\n . Interview SMEs (subject matter experts) to understand multiple systems,\n   data and processes while designing or re-engineering a system.\n\nDefense Systems Inc /WFInet\n   12/03 - 08/04  Client: Department of Defense (DOD)\n\nSenior Data Warehouse Architect\nPlayed mixed roles in the MSC (Military Sealift Command) Enterprise data\nWarehouse project.\n\nTask Highlights:\n . Designed, developed EDW for Military sealift command.\n . Used Erwin and Power Designer for data modeling and used normalization\n   techniques to normalize data.\n . Create data dictionary to provide overviews of the diagrams and data\n   entity, attributes, and relationships.\n . Identity and design CDA (common data architecture), research sources of\n   the data, shared entities and attributes and provide recommendations.\n . Interview SMEs (subject matter experts) to understand multiple systems,\n   data, and processes while designing or re-engineering a system.\n . Maintain standards and cross reference data with SMEs.\n . Customize meta model definitions and user properties to further enhance\n   the data dictionary.\n . Administer production and development databases and maintained\n   collection, ETL, storage and presentation of the quality data for the\n   AWRDS systems.\n . Ran a DQE (data quality engineering) shop to present quality inventory,\n   financial and logistics data/reports to the Army war reserve support\n   command during war and peace time.\n . Write queries in SQL to generate reports.\n . Write reports using Cognos powerplay and impromptu.\n . Identify stewardship/ownership of the data and processes.\n . Creating, changing and maintaining jobs on data stage to migrate data\n   from different sources to the enterprise data warehouse and data marts.\n . Re-engineer fact and dimension tables for the data warehouse IQ\n   databases.\n . Write stored procedures, triggers, and data migration scripts.\n . Generating reports using Cognos tools for data analysis in drillthru.\n . Write data migration scripts in data stage jobs and batch programs by\n   using SQL, Transact SQL.\n . Created a helpdesk problem tracker database to be used by EDW developers\n   and users to track problems and solutions.\n . Involve in memory management, I/O CPU utilization, query optimization and\n   other performance related issues.\n . Backup/restore databases.\n . Translate transaction logs, performed synchronization, dumped the\n   transaction logs and altered databases when required.\n . Created stored procedures/functions, triggers, and queries to perform\n   data modification, data migration and data audits.\n . Perform all the other DBA related duties on regular basis and took full\n   responsibility of the databases.\n . Developed intranet information website for MSC EDW (not in production\n   yet) which will be source and feedback site for EDW developers and EDW\n   users.\n\n\nStanley Associates\n01/98 - 11/03\n Department of Defense\n\n\nSenior Data Analyst/ Database Engineer\nWorked in the AWRDS/ABS project for Army War Reserve support command.  The\nAWRDS/ABS systems provide logistics information to the army in war and\npeace time.\n\nTask Highlights:\n . Designed/developed AWRDSA2k data warehouse and ABS data-mart (read only)\n   to be used by soldiers during war and peacetime.\n . Designed, supported, and administered MWB (maintenance workbench) data\n   mart for Army war reserve while deployed to Kuwait (Arifjan FSC) during\n   Operation Iraqi Freedom.\n . Administered production and development databases and maintained user\n   access both site and off-site.\n . Rebuild databases from Gold(master) shell databases and consolidated\n   databases for sites, copied and restored database on need-to-need basis\n   during development, testing and implementation phases of new AWRDS/ABS\n   modules.\n . Managed data in multiple databases\n . Provide various database support for different application development\n   and operations and maintenance projects.\n . Provide data quality engineering support.\n . Gather DB requirements\n . Ran a DQE (data quality engineering) shop to present quality data/reports\n   to the Army war reserve support command during war and peace time.\n . Wrote queries and created batch executables to be run daily, weekly, BI-\n   weekly to monitor the validity and the quality of the data in AWRDS/ABS\n   databases.\n . Used Erwin and power designer for data modeling and used normalization\n   techniques to normalize data.\n . Interviewed SMEs to identify processes, sources of data and maintain data\n   strategy.\n . Wrote stored procedures, triggers, and data migration scripts during\n   development/module changes/updates on AWRDS, ABS, MWB and other RDBMS.\n . Wrote data migration scripts and batch programs by using MS SQL, Transact\n   SQL, Unix/Perl and DOS scripts as required.\n . Performed data analysis using SQL, ACCESS, Advanced Excel Macros with\n   VBA, and Cognos Powerplay.\n . Involved in memory management, I/O CPU utilization, query optimization\n   and other performance related issues.\n . Translated transaction logs, performed synchronization, dumped the\n   transaction logs and altered databases when required.\n . Installed new update patches and MS security patches both on server and\n   client machines.\n . Used PCAnywhere to access, troubleshoot remote databases.\n . Created stored procedures/functions, triggers, queries, passthru SQL\n   scripts to perform data modification, data migration and data audits.\n . Always maintained the history of data/database change requests by\n   customers both locally and on the network PVCS tracker and version\n   manager.\n . Performed all the other DBA related duties on regular basis and took full\n   responsibility of the databases.\n\n\n\n\n\nEDUCATION:\n\nMS, Computer Information Systems, Strayer University\nBS, Computer Science, West Virginia University\n\n\n\n",
          "confidence": 1.0,
          "method": "full_text",
          "structured_data": null
        },
        "tax_term": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "source_by": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "designation": {
          "value": "Lead Data Architect",
          "confidence": 0.9,
          "method": "ner",
          "structured_data": null
        },
        "experience": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        }
      }
    },
    {
      "resume_path": "data\\input\\Enterprise Architect - NJ - Madan.doc",
      "confidence_score": 0.8084693877551021,
      "used_ocr": false,
      "extracted_fields": {
        "skills": {
          "methodologies": [
            {
              "name": "xp",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                32
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "rad",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                282,
                11203
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "tdd",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                4078,
                5505,
                7027
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "agile",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                5175
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "databases": [
            {
              "name": "oracle",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                84,
                204,
                224,
                11865,
                12842,
                13318,
                19852,
                20110,
                20533,
                20754
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "db2",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                115
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "programming": [
            {
              "name": "sql",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                104,
                111,
                19780
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "java",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                129,
                184,
                429,
                551,
                3362,
                4473,
                4974,
                5853,
                7593,
                7677,
                9447,
                9987,
                10044,
                11069,
                11157,
                13188,
                13233,
                13451,
                14371,
                15663,
                15855,
                16690,
                16847,
                18776,
                19334,
                19655,
                19698,
                20470,
                20690,
                20717
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "html",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                195,
                16459,
                16494,
                16579,
                18837,
                19784,
                20681,
                20761
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "xml",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                200,
                443,
                7749,
                10116,
                11180,
                13367,
                15717,
                16415,
                16562,
                16731,
                17162,
                18820,
                19759,
                20686,
                20766
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "xslt",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                461,
                15725,
                16423,
                16536,
                16739,
                18794,
                19767
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "puppet",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                710
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "json",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                2838,
                4536,
                5916,
                7744,
                10111
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "move",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                6664
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "javascript",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                13330,
                13379,
                15706,
                15743,
                16623,
                16711,
                16744,
                18885,
                19588,
                19711,
                19789
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            }
          ],
          "devops": [
            {
              "name": "svn",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                315
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "git",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                319,
                4552,
                5932,
                7772,
                10139
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "mesos",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                679,
                2881
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "openshift",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                9906
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "frameworks": [
            {
              "name": "spring",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                338,
                350,
                369,
                2785,
                3110,
                3264,
                3489,
                3816,
                4217,
                4259,
                4483,
                4871,
                5105,
                5311,
                5706,
                5748,
                5863,
                6833,
                7310,
                7352,
                7614,
                7687,
                9189,
                9762,
                9804,
                10054,
                12769,
                13260
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "soap",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                447,
                2815,
                4513,
                5893,
                7721,
                10088
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "rest",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                452,
                2797,
                3848,
                4495,
                4742,
                5126,
                5343,
                5875,
                6865,
                7187,
                7703,
                9475,
                9636,
                10070
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "junit",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                18941
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "domain_specific": [
            {
              "name": "design",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                405,
                1102,
                1180,
                1203,
                1518,
                1543,
                3633,
                3783,
                3882,
                4850,
                5204,
                5278,
                5377,
                6049,
                6060,
                6194,
                6726,
                6800,
                6899,
                7633,
                9350,
                9414,
                10832,
                11036,
                12299,
                12391,
                12419,
                16193,
                18387,
                18525,
                19948
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "infrastructure",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                653,
                7525,
                9963
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "architecture",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                847,
                901,
                1163,
                1223,
                1501,
                1554,
                14769,
                18614,
                20303
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "compliance",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                1152,
                1397
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "publishing",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                3376,
                4988
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "security",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                3529,
                3589,
                12055,
                18871,
                19221
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "policy",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                8043,
                8219,
                8451,
                8542,
                8590,
                8807,
                8941
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "sales",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                11488
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "clinical",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                14447
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "banking",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                15903
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "marketing",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                18125
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "engineering",
              "confidence": 0.5,
              "context": "education_section",
              "positions": [
                120,
                162,
                199
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "cloud": [
            {
              "name": "kubernetes",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                668,
                2870,
                4441,
                4593
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "docker",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                685,
                2887,
                3148
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "aws",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                692,
                2847,
                3155
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "jenkins",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                702,
                4186,
                4361,
                5613,
                7135
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "batch",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                10696
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "soft_skills": [
            {
              "name": "initiative",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                982,
                1120,
                2009,
                2305,
                2609
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "planning",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                2119,
                3640
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "analysis",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                2190,
                12290,
                12306,
                16184,
                16200,
                18378,
                18394
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "innovation",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                2222,
                2286
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "documentation",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                16902
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "data_skills": [
            {
              "name": "kafka",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                2858
              ],
              "experience_weight": 0.0,
              "importance_score": 0.9
            }
          ],
          "business_skills": [
            {
              "name": "brd",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                10937
              ],
              "experience_weight": 0.0,
              "importance_score": 0.8
            },
            {
              "name": "reporting",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                13778,
                16947
              ],
              "experience_weight": 0.0,
              "importance_score": 0.8
            }
          ],
          "networking_equipment": [
            {
              "name": "authentication",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                12064,
                12199
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ]
        },
        "first_name": "Madan",
        "last_name": "Vemurie",
        "primary_email": {
          "value": "madanvemurie78@gmail.com",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "secondary_email": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "phone": {
          "value": "8563694341",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "city": {
          "value": "Jersey",
          "confidence": 0.7,
          "method": "ner",
          "structured_data": null
        },
        "state": {
          "value": "OH",
          "confidence": 0.8,
          "method": "city_database",
          "structured_data": null
        },
        "zip": {
          "value": "",
          "confidence": 0.0,
          "method": "regex",
          "structured_data": null
        },
        "work_authority": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "resume_link": {
          "value": "data\\input\\Enterprise Architect - NJ - Madan.doc",
          "confidence": 1.0,
          "method": "file_path",
          "structured_data": null
        },
        "raw_resume": {
          "value": "\n\n\n                                Madan Vemurie\n                              Phone: 8563694341\n                     Email Id:  madanvemurie78@gmail.com\n        LinkedIn: http://www.linkedin.com/pub/madan-vemurie/8/441/599\n\n\nObjective:\n\n      Highly industrious and self-motivated, creative individual seeking a\n      challenging position in Managing and Designing/Delivering client\n      solutions that best utilizes my past expertise in Technical Sales and\n      Management. Experienced technical product manager, technical project\n      lead, and business analyst with over 16 years of experience in the\n      health and fitness, excellent communication skills and a track record\n      of successful product releases working in cross - functional teams.\n\nSummary:\n\n    . 16 plus years of experience in  application  design,  development  and\n      testing\n    . Extensive experience on Spring Boot Microservice design and  hands  on\n      development.\n    . Worked on NodeJS Microservice design and development.\n    . Working as an Enterprise Architect in recent assignments\n    . Worked as an Architect with BPM Architecture.\n    . Worked on Swagger API Design model and WS02 for publishing the API's.\n    . Worked as a Release Manager and Operations Manager during  my  Comcast\n      Project.\n    . Strong Command on Weblogic 11.03, Websphere 8,  Tomcat,  Jboss  7  and\n      other Application Servers, Open Shift\n    .  Extensive  experience   in   developing   applications   using   J2EE\n      technologies such as JPA, Servlets, EJB,  Spring  Boot,  Struts,  JMS,\n      JDBC, Hibernate.\n    . Experience in REST and SOAP Web services.\n    . Experience with  Mockito  unit  testing  framework  and  Cucumber  for\n      automation testing and Integration testing.\n    . Experience in MVC (Model View Controller) architecture by using Struts\n      Frameworks, Spring Framework, Spring Integration, Spring Data,  Spring\n      Boot, Service Oriented Architecture, Design Patterns and UML.\n    . Extensive knowledge of XML, XPATH, XSLT, SOAP, WSDL,  SOA,  REST  API,\n      JASON, Jersey.\n    . Expert in BPM 10.2 and BPM 5.7.\n    . Experience in Pivotal Cloud Foundry installation of micro services and\n      testing them.\n    . Experience in AWS EC2 Cloud set up for micro services.\n    . Expert in Spring Framework, Spring Integration,  Spring  Data,  Spring\n      Boot.\n    . Expert knowledge in Core Java and Multi Threading.\n    . Experience in Unix Shell Scripting.\n    . Knowledge in all phases of Software Development Life Cycle (SDLC)  and\n      AGILE methodology.\n    . Experience in SVN, GIT Repository, CVS, ClearCase, MS Visual Source.\n    . Experience in UML.\n    . Team player with very good communication  skills  and  inter  personal\n      skills.\nEducation:\n    . Masters of Science, Computer and Information Sciences Jan 03'-Dec 04'\n      North Illinois University, Dekalb, IL USA\n    . Bachelors of Engineering, Electronics and Communication Engineering\n      Dr. Ambedkar Institute of Engineering, Bangalore, India\n\n\n\n\nTechnical Skills:\n\n|Operating     |:|Windows 95 / 98 / 2000 / NT/XP/ 7/8, Sun Solaris8.0,     |\n|System        | |Linux, Red Hat                                           |\n|Database      |:|Oracle 11, MS-Access, SQL, PL/SQL, DB2.                  |\n|Languages     |:|JAVA, C, C++, J2EE, EJB, Applets, Servlets, JDBC, AWT,   |\n|              | |UML, Swings, Java Beans, HTML, XML,                      |\n|Oracle        |:|BPM 10g, JBpm, Oracle DB 10g, Weblogic 11.03             |\n|Products      | |                                                         |\n|IDE/Version   |:| Eclipse, RAD, IntelliJ, MS Visio, MS VSS, CVS, SVN, GIT.|\n|Controller    | |                                                         |\n|Web           |:|Spring Boot, Spring Integration, Spring Data, Struts,    |\n|Applications  | |MVC, Design Patterns, EJB, JSP, Java Servlets, XML, SOAP,|\n|              | |REST API, XSLT, XPATH, JDBC, SOA, Hibernate.             |\n|Micro Services|:|Sprint Boot, Node Js                                     |\n|Web Servers   |:|BEA Weblogic, Java Web Server, Apache TomCat, Websphere  |\n|              | |V4, IIS.                                                 |\n|Cloud         |:|Pivotal Cloud Foundry PaaS(Platform as a Service),       |\n|Infrastructure| |Kubernetes, Mesos, Docker, AWS                           |\n|CI/CD         |:|Jenkins, Puppet                                          |\n\n\n\n\n\nProfessional Experience:\n\n\n\n      Client Royal Caribbean Cruise Line\n                                         Manager (Enterprise Architect), PA\n                                                          Jan 2019-Till Date\n\n\n      Leading the project Architecture and project delivery in the Advisory\n      and Architecture group for Royal Caribbean Cruise on Employee track. I\n      am leading the initiative to improve Employee efficiency and\n      experience and ease of doing business on cruise.\n\n\n      EMPLOYEE Track\n      Working on design of 3 major initiative\n     1. Scheduling and ILO compliance Architecture and Design.\n        I am working on design and complete Architecture which will be used\n        by delivery and executives to bring in and integrate with out of\n        box tool.\n        This will automate all the crew members Scheduling, Time Keeping\n        and Compliance rules engine on the ships and help to ease the\n        employee experience\n     2. Stateroom Attendant App Architecture and Design.\n        I am working on a design and Architecture of an app which will be\n        developed by RCCL team. This app will be used to assign to assign\n        guest rooms and areas of the attendants for cleaning. It provides\n        them with checklist to ensure that all areas are covered. And once\n        attendant completes the work these are send to floor supervisors\n        for inspection.\n        Supervisor has the ability update the status of inspection and may\n        be reject it and reassign with comments.\n     3. JDE HCM replacement with SAP HCM initiative and working on cut over\n        strategy of all the Ship and Shore products which are dependent on\n        HCM. And planning the gradual systems cut over with data synching\n        strategy, risk analysis and roll back strategy.\n\n\n             Innovation and Emerging Technology Track\n       1.  Started working with Innovation Team on initiative for Kids\n          tracking on ship during voyage. This is very important for kids\n          safety and kids will wear a WOW band and they can use the band\n          which has a Bluetooth beepers which can be tracked by Auruba/Cisco\n          listeners on ship.\n          The location is compiled and tracked on guest app.\n       2. Mustering 2.0 was an initiative to remove the guest pain point by\n          removing the safety mustering at one time and giving them\n          provisions to view videos on app or stateroom TV.\n\n\n      Environment: UML, draw.io, Spring Boot, REST Web Services, SOAP Web\n      Services, GSON/JSON, XSD, AWS, VMware, Kafka, TIBCO, Kubernetes, Mesos\n      Docker.\n\n\n\n    > Prudential (Cognizant Technology Solutions)\n                                                                       Sr.\n      Manager/Delivery Executive\n                  Nov 2017- Dec 2018\n      Working as Technical Delivery and Sr. Architect with hands on\n      experience for a project to convert Web Method Based Services to\n      Spring Boot services and deploying on Docker AWS.\n      My team has delivered a very fast pacing project where we have\n      converted 100 plus Documentum Services to Spring Boot Micro Services.\n      Introduced Swagger model to write swagger Jason file and convert it to\n      Java Code and Publishing Swagger Jason file on WS02 for testing and\n      viewing and discovering.\n      This whole project is converted to spring boot Micro Services with all\n      the security, common feature and adding Apigee API Gateway based\n      security with a very small team.\n      . Involved in Design, Planning, and Co-ordination with offshore team\n        and helped in Coding of the application common features in Service\n        layer environment.\n      . Involved in Design and coding involving core Spring Technologies.\n    . Involved in REST Web services development and design\n      . Trained the team with Mockito unit testing framework and Cucumber\n        for automation testing and integration testing and implement the\n        testing frame work with 85% of test coverage.\n      . Experience in TDD environment and created unit test cases before\n        writing the business logic using Mockito. Used Sonar with Jenkins\n        for code coverage.\n      . Used Spring Boot framework for development and spring annotations.\n      . Created Custom Annotations and used Lombok Annotations for POJO's.\n      . CI/CD builds with Jenkins\n      . Worked with Dev Ops team for Dockerization of services and\n        deploying on Kubernetes.\n\n\n      Environment: UML, Core Java, J2EE, Spring Boot, REST Web Services,\n      SOAP Web Services, GSON/JSON, XSD, Tomcat, GIT, STS IDE, Swagger IO,\n      APIGEE, UNIX, CI/CD, Kubernetes\n\n\n\n\n    > TD Ameritrade (Cognizant Technology Solutions)\n                                                                       Sr.\n      Architect/Sr. Manager\n                        May 2017- Nov 2017\n\n\n      Working as API Designer and Development lead for a Rest API Pilot\n      project in BOS Service layer team in TD Ameritrade. Worked on a Pilot\n      project to initiate API design standards and Spring framework in the\n      project. Introduced Swagger model to write swagger Jason file and\n      convert it to Java Code and Publishing Swagger Jason file on WS02 for\n      testing and viewing and discovering.\n      This was a Pilot project to set up the Spring Framework for REST API\n      development and introduce the team with Agile Principals.\n      . Involved in Design, Coding of the application in Service layer\n        environment.\n      . Involved in Design and coding involving core Spring Technologies.\n    . Involved in REST Web services development and design\n      . Experience with Mockito unit testing framework and Cucumber for\n        automation testing and integration testing.\n      . Experience in TDD environment and created unit test cases before\n        writing the business logic using Mockito. Used Sonar with Jenkins\n        for code coverage.\n      . Created and updated stories worked on stories on Jira tracker.\n      . Used Spring Boot framework for development and spring annotations.\n      . Created Custom Annotations and used Lombok Annotations for POJO's.\n      . Environment: UML, Core Java, J2EE, Spring Boot, REST Web Services,\n        SOAP Web Services, GSON/JSON, XSD, Tomcat, GIT, STS , Swagger IO,\n        WS02,UNIX\n\n\n\n    > FDC (Cognizant Technology Solutions)\n      Sr. Architect/Sr. Manager\n                           Aug 2016- April 2017\n\n\n      Solution design and Design work for integration with Fraud monitoring\n      Solution with a third Party Machine Learning and Monitoring Response\n      System Feedzai. Design micro services which are currently in\n      Production.\n      Also integration with another Smart Phone device Fraud alert system\n      InAuth Service.\n      The client which this products are used are mainly from Exxon Mobile,\n      Chickfilae, Tacobell.\n\n\n      This new integration is designed completely as an independent micro\n      services which are deployed separately and have their own purpose.\n      These services are also currently on PCF (Pivotal Cloud Foundry) PaaS\n      as a test services and intention was to move completely to PCF in\n      future.\n   Responsibilities: -\n      . Involved in Design, Coding of the application in Service layer\n        environment.\n      . Involved in Design and coding involving core Spring Technologies.\n    . Involved in REST Web services development and design\n      . Experience with Mockito unit testing framework and Cucumber for\n        automation testing and integration testing.\n      . Experience in TDD environment and created unit test cases before\n        writing the business logic using Mockito. Used Sonar with Jenkins\n        for code coverage.\n      . Developing micro API's and REST web services for the businesses and\n        vendors to use.\n      . Created and updated stories worked on stories on Jira tracker.\n      . Used Spring Boot framework for development and spring annotations.\n      . Created Custom Annotations and used Lombok Annotations for POJO's.\n      . Worked on POC for Pivotal Cloud Foundry PaaS as for deployments of\n        applications on Cloud Infrastructure.\n      . Worked on POC for API Gateway Apigee on PCF.\n      . Used JPA (Java Persistence API) Spring Data for DB design and\n        development.\n\n\n   Environment: UML, Core Java, J2EE, Spring Boot, JPA, REST Web Services,\n   SOAP Web Services, GSON/JSON, XML, XSL, XSD, Web Sphere, GIT, STS,\n   Mocito, Apigee , UNIX\n\n\n\n    > United Health Care, Horsham, PA\n\n\n         Sr. Architect/Software Engineer\n                       Mar 2015- June 2016\n\n\n      Digital Service Medicare and Retirement\n\n\n            Solution Summary document for a new project was developed.\n      Provider Web is a web site residing within the SHIP Comprehensive\n      Policy Administration System (COMPAS) with self-service features for\n      providers.  When it was built 11 years ago, no enterprise-wide\n      provider portal existed, so it was built into the Policy\n      Administration system.  Today, there are better options, specifically,\n      leveraging the UHC-wide Provider Portal, also known as Optum Provider\n      Touch Point (PTP). Our recommendation is to align to the enterprise\n      standard by retiring the Policy Administration system Provider Web in\n      favor of leveraging Optum PTP.\n           In addition, the Policy Administration system has an outstanding\n      policy exception (PEX) with UHC Corporate IT for accessing the United\n      Claim Processing System (UCPS) system directly in order to maintain\n      information in the UCPS Provider Database. To resolve the outstanding\n      PEX issue, the Policy Administration system will integrate with ACES\n      provider maintenance services in order to pass information between\n      UCPS and the Policy Administration system.\n\n\n           Hands on a development of micro API Services and Restful Web\n      Service, which provides services and to business and other teams for\n      looking up claims, Provider Demographics, plans, eligibilities,\n      preferences, fulfillments. Used Spring Boot framework with extensive\n      unit testing using mockito framework, automation testing using\n      Cucumber and integration testing.\n\n\n\n\n\n\n   Responsibilities: -\n      . Involved in Design, Coding of the application in SOA environment.\n      . Involved in Design and coding involving core Java technologies.\n    . Worked on REST Web services.\n      . Worked on writing unit test cases with Mockito framework and\n        Cucumber for automation testing and integration testing.\n      . Developing micro API's and REST web services for the businesses and\n        vendors to use.\n      . Created and updated stories worked on stories on Pivotal tracker.\n      . Used Spring Boot framework for development and spring annotations.\n      . Created Custom Annotations and used Lombok Annotations for POJO's.\n      . Worked on POC for Openshift automatic deployments of applications\n        on Cloud Infrastructure.\n      . Used JPA (Java Persistence API) for DB queries\n\n\n   Environment: UML, Core Java, J2EE, Spring Boot, JPA, REST Web Services,\n   SOAP Web Services, GSON/JSON, XML, XSL, XSD, Web Sphere, GIT, IntelliJ ,\n   UNIX\n\n\n\n    Bank Of America, Newark\n    Sr. Architect/ Technical Lead\n                     Jan 2013-Feb 2015\n\n\n        I am currently working as a Sr. Architect and Technical Lead in Bank\n   of America in Card Technologies and Common Services team. I work closely\n   with Business team and other teams on quarterly releases. I have worked\n   on Fraud Activity Verification project where Business is moving the Fraud\n   Verification service in house. CTCS team is mainly an Orchestration Layer\n   between different teams and we provide Wed Services, which are consumed\n   by different teams.\n   I have also worked on Batch processes project, which is used mainly for\n   our Mailers and Agreement printing services for customers.\n\n\n   Responsibilities: -\n      . Involved in Design, Coding of the application in SOA environment.\n      . Involved in Business requirements meeting to collect BRD\n        requirements.\n      . Involved in Integration testing with all the teams in SOA\n        environment.\n      . Involved in Design and coding involving core Java technologies.\n      . Worked on JBPM 6 projects for Card fraud project.\n\n\n   Environment: UML ,Core Java, J2EE, Web Services, XML, XSL, XSD, Web\n   Sphere, RAD 7.5, JBpm 6, RTC, UNIX.\n\n\n\nComcast, Mount Laurel, NJ\n Sr. Architect/ Technical Lead\n                               Jan 2007-Dec 2012\n\n\n  I have started working in Comcast from Jan 2007 and from very first week\nthe work was exciting. I have worked on lot of Major project development\nand releases for Commercial Work Bench Portal. Sales team to create and\nProvision Business customers with Comcast Line of Businesses products use\nthis product. I worked on all the major Product release in Commercial\nWorkbench both in designing and development phase. We have also done lot of\nupgrades to our projects from Weblogic 8.1 to Weblogic 10.01. Also last\nyear we have done a major upgrade from Aqua-logic BPM 5.7.3 to Oracle BPM\n10.\nWorked as a Release Manager, Enterprise Architect and Technical Lead for\nall development release.\nSome of my responsibilities in the application:-\n    . Integration with LDAP server for security authentication for  SSO  and\n      also for SalesForce integration. Also for one of our  external  client\n      we have used Public Key and Private Key authentication for getting the\n      Login Tokens.\n    . Involved  in  Requirements  gathering,  Requirement  analysis,  Design\n      analysis, Integration and deployment for enhancements and changes.\n    . Involved High Level design documents and Detail design  documents  for\n      our projects in every monthly release.\n    . Performance evaluation with the performance team before every release.\n    . Involved in  coding  and  Integration  testing  with  clients  of  the\n      application in SOA environment. We have different vendor  clients  who\n      are consuming out  services  like  Salesforce,  IBM  CPQ  (Evolution),\n      Century 2.1.\n    . Application uses Spring frame work and Struts framework  for  existing\n      services and we have Oracle BPM 10g as a middleware.\n    . Integration our application with external  Business  Class  Voice  and\n      Business Class Trunking provisioning system.\n    . Created UI page for Trunking product using GWT\n    . Integration with Enterprise services  like  billing  systems,  account\n      services, offer management services, order management services.\n    . Used multi threading and core Java for parallel processing.\n\n\n\n   Environment: UML, Java, J2EE, BPM 10.3, BPM 5.7, Spring, Struts, BEA\n   Weblogic 10.03, BEA Weblogic 10.1 Portal, Oracle 10G2, JavaScript, GWT,\n   DWR, AJAX, Web Services , XML, XSL, XSD, JavaScript, Hibernate,  Apache\n   Tomcat,  UNIX.\n\n\n\n > Pfizer Inc. New London, CT\n      Senior Java Developer/Architect\n                       April 2006- Dec 2006\n   1.   Data Standard Catalog         Team Lead\n   Working on a Data Standards Catalog project and this application is used\n   all over the world Pfizer end users and study teams to post issues on\n   Core data standards and efficacy standards. This application is\n   interfaced with an extra view product for reporting the issues and\n   tracking issues. This application is deployed in Weblogic 8.1.\n\n\n       .  Directly involved with business team to maintain the whole\n         application. First point of contact for application support and\n         upgrades.\n       .  Involved in coding, interfacing and testing with the extra view\n         team.\n       .  Written stored procedures and packages for the versioning.\n       .  Documenting and maintaining the requirements and the application\n         documents.\n       . Maintaining the production and staging application server boxes\n         and directly co-ordination with the middle tier team for new\n         releases.\n\n\n   2. Dictionary extract project\n      Developed a java application using Live Link API, which is used to\n   upload files from the clinical trials servers every day to the Pfizer\n   Live Link servers, which are used by the business and end user for\n   review.\n      . Involved in coding, interfacing and testing of the project.\n      . Written stored procedures and packages.\n      . Documenting and maintaining the requirements and the application\n        documents.\n      . Implemented Service oriented Architecture for web service\n        communications with the different Pfizer clients.\n      . Maintaining the production and staging application server boxes and\n        directly co-ordination with the middle tier team for new releases.\n\n\n   3. WHO- Drug Dictionary and TMS servers front end web interface\n      Developed web front end using JSP and Struts frame work for a front\n      end interface were a business person can come and login using Pfizer\n      NT account username and password and upload the spreadsheets or\n      download the spread sheet for the dictionary terms.\n\n\n       . Involved in designing, coding, interfacing and testing of the\n         project.\n       .  Documenting and maintaining the requirements and the application\n         documents.\n       .   Used Adobe photo shop for look and feel images.\n       . Maintaining the production and staging application server boxes\n         and directly co-ordination with the middle tier team for new\n         releases.\n       . Used Hibernate for mapping.\n   Environment: Core Java,Rational Rose, UML, WingNut, J2EE, Struts,\n   JavaScript, XML, XSL, XSLT, VB scripting, JavaScript, Hibernate,  Apache\n   Tomcat,  Adobe Photoshop, UNIX, CVS for version controlling.\n\n\n\n\n   FUNDTech, Norcross, GA\n   Senior Java Developer\n                              Nov 2005- April 2006\n   Working on a Banking project and our main job is Maintenance and\n   production support. So we are dealing with small failures and code\n   Enhancements. The project is both web based and swing based so I am\n   dealing with both the sides issue on client side.\n\n\n    .  Involved in  Requirements  gathering,  Requirement  analysis,  Design\n      analysis, Integration and deployment for enhancements and changes.\n    . Involved in  coding  and  Integration  testing  with  clients  of  the\n      application\n    . The application uses the Swing and Servlets. The views are  programmed\n      using XML and XSLT pages which are converted into HTML. And  use  AJAX\n      for changing the Html page without reloading the page.\n    .  Used XSLT for transforming the XML content into HTML\n    . Used Starteam for version control.\n    . Used JavaScript as the scripting language for the View.\n\n\n      Environment: Core Java, J2EE, Struts, JMS, JavaScript, JBuilder,  XML,\n      XSL,  XSLT,   JavaScript,   Apache  Tomcat,  UNIX,  SOA,   Star   Team\n      Enterprise.\n\n\n   Cingular Wireless, Alpharetta, GA\n                                               Senior Analysts(Java\n   Developer)\n      Dec 2004- Nov 2005\n\n\n   1. EDD (Enterprise Data Documentation), EDD Resend Web  Application,  EDD\n      Reporting Tool. In Production from August 2005\n      Designed and developed along with my team members  an  automated  tool\n      which can receive customer payment information from clients queue in a\n      bean format and convert it in XML string and put it in the local queue\n      and store it LDAP database.  After  that  sending  a  notification  of\n      payment to the customer via SMS, Email or Post Card according  to  the\n      preference.\n      Designed and  developed  a  web  application  which  is  used  by  CSR\n      (Customer Service Representatives) to resend the payments  information\n      a customer for past two years. This web  application  takes  customers\n      information and displays the records of all the previous payments  and\n      then the customer can choose one or more  than  one  record.  And  the\n      message is again sent via SMS, Email or Post Card which ever  customer\n      prefers.\n      Developed  and  developed  web  application  which  is  used  by   the\n      production support team and the  administration  to  view  the  totals\n      records of each client and the resend records. It will also  tell  how\n      many records have been successfully sent and how many failed.\n\n\n   2. Corporate Email System\n      Designed and developed a web application which is going to be used  by\n      Cingular agents or Marketing people to send emails to  individuals  or\n      group of people or to a Category of people. The agents  can  create  a\n      new template or use the  existing  template  to  send  emails  to  the\n      clients.\n\n\n   Responsibilities:\n\n\n    . Involved  in  Requirements  gathering,  Requirement  analysis,  Design\n      analysis, Integration and deployment\n    . Involved in  coding  and  Integration  testing  with  clients  of  the\n      application\n    .  Responsible  for  the  design  and  development  of  the  application\n      framework\n    . Designed and Developed UI's using MVC architecture\n    . The application uses the STRUTS framework. The  views  are  programmed\n      using JSP pages with the struts tag library. Model is a combination of\n      EJB, DAO and Java classes.\n    . Used XSLT for transforming the XML content into HTML and used SOA  for\n      web service security.\n    . Used JavaScript for much functionality of the web  pages.  Used  JUnit\n      for unit testing of the system and Log4J for logging\n    . Used EJB and DAO a middleware in designing and developing a three-tier\n      distributed application\n    . Used web services with session bean for Synchronous communication with\n      the client and even used a vendor tool for web service security.\n    . Used Hiberate as a mapping tool\n    . All the queries are written and tested on TOAD for this application.\n    . The Java Message Service  (JMS)  API  is  used  to  allow  application\n      components to create, send, receive, and read messages\n    . Installed Web Logic for the application is UNIX environment.\n    . Used ClearCase for version control.\n    . Used AJAX to modify the page asynchronously.\n    . Used JavaScript as the scripting language for the View.\n\n\n   Environment: Core Java, J2EE, Struts, JSP, EJB, Servlets, JMS, JDBC, Java\n   (JDK 1.4), JavaScript, JBuilder, Rational Rose, Hibernate DAO, XML,  XSL,\n   XSLT, AJAX, PL/SQL, HTML, JavaScript, WebLogic 8.1 SP4, Web  Sphere  4.1,\n   JBOSS, Apache Tomcat, Oracle 9i/8i, UNIX (HP)\n\n\n > Sonata Software Ltd., Bangalore, India     Software Developer\n   Nov'01 - Jan'02\n\n      For the design and simulation  of  the  Electronic  Control  Units  at\n   Continental Teves during their R&D phase, Sonata proposed and developed a\n   web-based system with  a  central  Oracle  database.  Web  interface  and\n   interface  to  other  existing  systems  were  provided.   Offline   data\n   manipulation for field engineers and  overseas  database  synchronization\n   was also implemented. MVC architecture was used with JSP,  Servlets,  and\n   JavaBeans to build the interface for the database.\n   Responsibility:\n    . Designed and developed session and entity beans\n    . Developed Java Servlets used for the server end\n    . Installed and maintained Oracle Server\n    .  Involved  in  unit  testing,  module  testing,  product  testing  and\n      development of test cases.\n    . Involved in the web interface development using HTML, XML,  Java  AWT,\n      Swing\n   Environment: Java, JSP, C++, C#, Servlets, JDBC, JavaBeans, Oracle, HTML,\n   XML, Jawa AWT, Swing Windows NT\n\nReferences:  Available  or  please  check  out  my  LinkedIn   profile   for\nreferences.\n\n\n\n\n",
          "confidence": 1.0,
          "method": "full_text",
          "structured_data": null
        },
        "tax_term": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "source_by": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "designation": {
          "value": "product manager",
          "confidence": 0.9,
          "method": "ner",
          "structured_data": null
        },
        "experience": {
          "value": "16",
          "confidence": 0.9,
          "method": "regex_total_experience_summary",
          "structured_data": null
        }
      }
    },
    {
      "resume_path": "data\\input\\Finance Manager - CA - Stepfan.doc",
      "confidence_score": 0.7613548387096775,
      "used_ocr": false,
      "extracted_fields": {
        "skills": {
          "soft_skills": [
            {
              "name": "analysis",
              "confidence": 0.84,
              "context": "project_section",
              "positions": [
                362,
                675,
                1031,
                1147,
                3946,
                4032,
                4162,
                6039,
                7026,
                8503,
                8702,
                10247,
                11695,
                12260,
                12506,
                12630,
                12802,
                12979,
                13259,
                14032,
                14041,
                14495,
                14952,
                19476
              ],
              "experience_weight": 0.2,
              "importance_score": 0.7
            },
            {
              "name": "prioritization",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                419
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "leadership",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                2259,
                19702
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "planning",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                2782,
                7249,
                7418,
                7867,
                8693,
                10432,
                10766,
                12377
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "accountability",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                3089,
                5461
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "research",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                3381,
                4153,
                6021,
                7901,
                8322
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "documentation",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                6725,
                14777,
                17151
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "training",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                7587
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "collaboration",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                9041
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "organization",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                9145
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "influence",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                12394
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "cloud": [
            {
              "name": "functions",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                653,
                1156,
                4419,
                7184
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "comprehend",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                8191
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "business_skills": [
            {
              "name": "reporting",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                1207,
                3791,
                4583,
                5516,
                5741,
                5827,
                6242,
                7052,
                7229,
                7273,
                7459,
                8986,
                12643,
                13285
              ],
              "experience_weight": 0.0,
              "importance_score": 0.8
            }
          ],
          "domain_specific": [
            {
              "name": "finance",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                1318,
                2351,
                2474,
                4099,
                4449,
                7563,
                19737
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "infrastructure",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                4000,
                4245
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "contracts",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                4081,
                6810,
                9286
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "compliance",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                5480,
                13752,
                14605,
                17004
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "legal",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                5735,
                8864,
                13103
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "policy",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                7952
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "manufacturing",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                8764,
                10675,
                14170,
                14223,
                14457
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "sales",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                8888,
                9986,
                12059,
                12254,
                12350,
                12830,
                13648
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "engineering",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                9488,
                10359
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "construction",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                13055,
                13081
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "government",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                15736,
                17042,
                17322
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "security",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                18713
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "networking_equipment": [
            {
              "name": "accounting",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                4107,
                4373,
                4408,
                7941,
                8275,
                8339,
                11321,
                13966,
                14021,
                14514,
                14574,
                14631
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "programming": [
            {
              "name": "assembly",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                10374
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            }
          ],
          "frameworks": [
            {
              "name": "express",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                16455,
                16602,
                17121
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "databases": [
            {
              "name": "oracle",
              "confidence": 0.84,
              "context": "project_section",
              "positions": [
                19512
              ],
              "experience_weight": 0.2,
              "importance_score": 0.7
            }
          ],
          "data_skills": [
            {
              "name": "excel",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                19626
              ],
              "experience_weight": 0.0,
              "importance_score": 0.9
            }
          ]
        },
        "first_name": "Stepfan",
        "last_name": "Jiles",
        "primary_email": {
          "value": "Stepfanjiles@hotmail.com",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "secondary_email": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "phone": {
          "value": "9016908956",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "city": {
          "value": "San Ramon",
          "confidence": 0.7,
          "method": "ner",
          "structured_data": null
        },
        "state": {
          "value": "CA",
          "confidence": 0.9,
          "method": "zip_database",
          "structured_data": null
        },
        "zip": {
          "value": "94583",
          "confidence": 0.7,
          "method": "regex",
          "structured_data": null
        },
        "work_authority": {
          "value": "Meetings",
          "confidence": 0.8,
          "method": "regex",
          "structured_data": null
        },
        "resume_link": {
          "value": "data\\input\\Finance Manager - CA - Stepfan.doc",
          "confidence": 1.0,
          "method": "file_path",
          "structured_data": null
        },
        "raw_resume": {
          "value": "\n                                Stepfan Jiles\n                         2218 Canyon Village Circle\n                             San Ramon, CA 94583\n                          Stepfanjiles@hotmail.com\n                                (901)690-8956\n\nOBJECTIVE\nTo obtain a position with a company that seeks an assertive, energetic\nfinance professional resource. The ideal position will allow me to utilize\nmy leadership and strong financial planning & analysis skills to have a\npositive impact on profitability and productivity.\n\nEDUCATION\nChamplain College\n       Burlington, VT\nCost Accountant Certification (CCA)\n                      July, 2018\nFlorida Institute of Technology\n\n   Melbourne, FL\nExecutive MBA Finance & Accounting\n\n   June, 2011\nBryan College\n\n                      Dayton, TN\nBachelors of Science in Business Administration\n\nMay, 1998\nMajor: Management\nWentworth Military Academy\n\nLexington, MO\nLiberal Arts Associate Degree\n\n              May, 1993\nMajor: Accounting\n\nEXPERIENCE\nPacific, Gas & Electric (PG&E) Utilities\nInsight Global Inc.- Finance Consultant\n                                                           January 2020 -\nPresent\nSr. Project Control's Analyst\n                                           San Ramon, Ca\nSchedule Development and Analysis:\n . In coordination with Project Manager develop project schedule and cash\n   flow/forecast plans with functional department input.\n . Attend Project and Job kickoff and walk down meetings.\n . Create, maintain and update schedules.\n . Document, monitor and communicate project milestones and risks with\n   appropriate stakeholders.\n . Participate in project status meetings, collect progress data and revise\n   project plan as needed.\n . Monitor financial progress and maintain Project Manager's order group.\n   Develop, maintain and control project schedule plans using appropriate\n   software (for ex: SAP Project System, SAP Work Management System, and MS\n   Project).\nCost Plan Development and Analysis:\n . Use the cost and schedule plan to determine if project objectives are\n   achievable.  Create, maintain, and update monthly forecast budgets.\n   Prepare various cost reports and maintain forecast accuracy. Prepare\n   project performance analysis, cost, and schedule status reports. Identify\n   cost and schedule variances from objectives and recommend corrective\n   action.\n . Assess and report on project performance using established industry\n   standards.\n . Ensure that responses to project budget, and accruals' cost requests\n   reflect accurate and current project cost information and stakeholders\n   are in concurrence.\n . Provide regular communication on project cost, schedule and risk status\n   to project team members, stakeholders and public.\n . Interface with design and planning resources and software systems.\n . Review and assist in the preparation of Advance Authorizations, Job\n   Estimates and Re-Authorizations and assist PM in routing for approval.\n . Prepare journal entries and coordinate or process goods receipts in\n   current software system.\n . Ensure that responses to budget and cost requests reflect correct project\n   cost information and stakeholders are in concurrence.\n . Input data into various programs and prepare various cost and forecasting\n   reports.\nProject Documentation & Reporting:\n . Verify that project cost and schedule milestones were/were not attained\n   and provide input to identify future process or business improvements and\n   work with responsible parties to implement.\n . Manage orders from inception through completion and all required\n   documentation is entered in current software system (Ex: EDRS).\n . Resolve all open items, ensure compliance requirements are met and\n   settlement rules are entered, and close out order.\n . Maintain scope change, contingency release, change order, and journal\n   entry logs.\n . Maintain written and electronic project documentation and records for\n   required aspects of the project: Maintain project files in accordance\n   with established guidelines and requirements (Ex: utilizing the\n   electronic document management system or EDMS).\n . Document change order requests, project status, key issues, risks and\n   resolution, priority changes and approvals.\n . Provide Project Manager with monthly report of project costs and/or\n   schedule information including variance analysis according to an agreed\n   upon level of detail and prioritization.\n . Provide Project Team members with current status report containing\n   schedule and cost information. Comply with Utility Operations Policies,\n   Standards and Guidelines.\n . Assist PM with Post Job Critique.\nCentral Data Management Functions - Quality and Analysis.\n . Prepare various monthly and weekly project management reports and report\n   performance reports to management group including scorecards.\n . Ensure quality of reports for the department, to report out to\n   stakeholders.\n . Validate weekly and monthly data sets.\n . May provide support to project manager(s) as necessary.\n . Fulfill ad-hoc requests for cost-related data analysis.\n . Participate on (or lead) various process improvement initiatives within\n   Project Management.\nCentral Financial Analysis Functions - Project Budget & Cycle Forecast\nMonitoring/Reporting\n . Prepare various monthly and weekly project performance reports - budget\n   and cycle forecast reports; and finance status reports, utilizing the\n   financial tracking software (e.g. SAP/BW system and EPM).\n . Coordinate and work with all stakeholders, on creating new reports, that\n   best measure and support good business decisions, to meet the Year End\n   Annual Budget target.\n . Coordinate and summarize project cycle forecast variance explanations for\n   the department, to report out to stakeholders.\n . Ensure that all responses to budget, quarterly cycle requests reflect\n   accurate and current project cost information and stakeholders are in\n   concurrence.\n . Post and update all the project performance; budget and cycle forecast;\n   and status reports on the Project Management website.\nFinancial and Performance Management\n . Monitor, analyze and report out on the Department's Forecasting\n   Performance, with weekly performance reports, utilizing the financial\n   tracking tools (e.g. SAP/BW).\n . Analyze financial results on a monthly basis, providing explanations of\n   significant cost drivers to PM Leadership - reports such as the Green-Red\n   Scorecard and Lessons Learned Reports.\n\nCordoba Corp.- Finance Consultant\nBART/ Santa Clara Valley Transit Authority Phase II Extension Project\n        December 2017 - January 2020\nProject Finance Consultant\n                          San Jose, Ca\n . Lead the tracking of the capital needs of the BART/ Santa Clara Valley\n   Transit Authority Phase II Extension Project, including project\n   commitments, project spend, project budgets, and overhead.\n . Maintain and update annual GAAP and cash operating forecast models.\n . Support the strategic planning process for the Project Controls'\n   division.\n . Support the Budget process and monitor the capital needs of the BART/\n   Santa Clara Valley Transit Authority Phase II Extension Project.\n . Lead and track KPI's and financial metrics to inform senior leader\n   decision making, measure divisional performance, and drive\n   accountability.\n . Lead Project FP&A to roll-up and map task and sub-task division models to\n   corporate consolidated models.\n . Prepare monthly project scope reviews and track actuals to budget;\n   present and communicate to senior management.\n . Perform various other financial analyses and industry related research,\n   tracking and compiling industry trends and competitive analyses, to\n   inform senior management strategic decision-making; effectively\n   communicate findings to team/colleagues through presentations, memos, and\n   other deliverables.\n . Manage and assist in the preparation of annual project business reviews\n   and variance analyses.\n . Manage sub contract level ultimate's, tracking against actual activity\n   from third party reporting.\n . Track invoice rights and availabilities for project, supporting\n   comprehensive agreement strategy.\n . Support Program Management Team (PMT) Strategy and Analysis in the setup\n   of the processes and procedures infrastructure for ongoing deal analysis\n   for new third party venders and ongoing contracts.\n . Perform finance/accounting, cost economics, customer and market research\n   analysis. Analyzing, interpreting and presenting data related to\n   transportations and infrastructure market operations.\n\n\nCalifornia High Speed Rail Project\n        April, 2017 - November 2017\nSr. Financial Analyst\n              Sacramento, Ca\n.     Accounting duties - Performs complex accounting functions for High\nSpeed Rail Finance and Budget\n       department, internal office departments, and program. Analyzes and\nassigns transaction codes in accordance\n       with state reporting and local management requirements. Reviews and\nreconciles account and fund balances;\n        reviews documents and claims for accuracy, completeness, and\nuniformity to rules, regulations, and laws;\n       coordinates, prepares and reviews year-end closing process and\ndocuments. Performs periodic review of\n       financial reports and verifies accuracy and project solvency.\n.     Budget Monitoring - Support the development of budgets for internal\nand external customer; support the\n       creation of new budget coding; provides information and assists\nstaff and administrators in budget preparation,\n       implementation, and control; prepares, balances, compiles, and\nenters budget data. Support the preparation of\n       quarterly revisions and billings; in accordance with Federal\nRailroad Administration (FRA) guidelines, monitors\n       and evaluates federal and state fund budgets and cash flow for\nappropriateness to ensure solvency and\n       accountability for compliance with state law.\n.     Financial Reporting - Prepares complex financial reports as mandated\nby the administration, California High\n       Speed Rail Authority, and other agencies; assists internal and\nexternal business customers in compiling data and\n       interpreting legal reporting requirements and regulations. Monitors\ntimelines to meet strict deadlines in\n       reporting regulations; reviews grant letters, funding and\nentitlement reports; reviews and verifies statistical and\n       financial information including independent reports and long-term\ndebt documents.\n.     Research and Data Analysis - Researches and analyzes financial data\nfor internal and external clients; creates,\n       organizes, and maintains files using database and spreadsheet\nprograms; retrieves and organizes data into\n       required reporting formats; collects, retrieves and organizes data\nto identify financial discrepancies and resolve\n       client inquiries; recommends solutions to clients and staff ensuring\nthat corrections or changes are implemented\n       properly.\n.     Communications and Technical Support - Maintains communication with\ninternal and external clients regarding\n       financial matters; provides information and technical support in the\ndevelopment and revision of policies and\n       regulations; assists in the development and documentation of office\nprocess updates and revisions to procedures;\n      reviews proposed contracts for adherence to rules and regulations.\nSupport procedures for monitoring grants.\n\nCBRE                                                         August, 2015 -\nJan., 2017\nSr. Project Analyst/Sr. Financial Analyst\n San Francisco, Ca\n.      Manages a broad range of complex financial analysis and/or financial\nreporting activities to measure\n        profitability for a region, line of business or large, complex\nclient. Includes the management of such functions as\n        budgeting forecasting, financial reporting, strategic planning and\nmanagement reporting processes and work\n        product.\n.      Manages one or all of the following: capital budgeting process,\ncapital forecasting process, capital strategic\n        planning process, and standard management reporting. Establishes\noverall departmental priorities and ensures\n        that all deadlines are met.\n.      Supervises finance staff including training. Reviews and approves\ncertain HFM monthly journal entries.\n        Conducts other special financial and business studies and other\nduties as assigned by management.\n.     Produces a variety of routine and ad hoc financial reports, packages\nand pro forma analyses for senior\n       management and planning unit. Participates in the research,\ndevelopment and preparation of accounting policy\n       and procedures, as required. May act as consultant to management on\nfinancial policies, procedures, and\n       applications. Manages special projects and prepares presentations\nfor senior management. Performs other duties\n       as assigned.\n.    Ability to comprehend, analyze, and interpret complex financial\ninformation and transactions and accounting\n      principles. Ability to independently research complex accounting\ntransactions/issues. Ability to problem-solve,\n      both independently and working as a team. Perform advanced analytical\nand quantitative skills. Draws upon the\n      analysis of others and makes recommendations that have a direct\nimpact on the company.\n\nSiemens, Inc.\nNovember, 2013 - May, 2015\nCommercial Program Manager\nSacramento, Ca\n.     Directly lead the financial planning & analysis of the U.S.\noperations lite rail and locomotive bogie\n       manufacturing team, supporting my technical bogies' program manager\nin relation to all commercial and legal\n       issues. Performing sales and expense forecasting and budgeting for\nall Bogie projects.\n.     Perform weekly & monthly program reporting/project status\ndiscussions/milestone reviews. Collaboration on the\n       project completion report and summarizing the lessons learned with\nfeedback to the organization executive\n       management staff. Collaborating with internal & external customers\nin negotiations and also formulating and\n       implementing contracts in the program.\n.     Analyzing and assessing complex, possibly international suppliers,\ncontract agreements'. Managing claim and\n       change order management: Asserting own claims and warding off\nunjustified engineering claims.\n.     Maintain contractual changes with regard to the scope of delivery and\nservices, prices, deadlines or other\n       contractual agreements, as well as forecast opportunity and risk\nmanagement: Identifying and financially\n       assessing opportunities and risks, defining and implementing\nsuitable measures for reducing risks or realizing\n       opportunities and taking precautions for remaining risks.\n.     Drawing up the order receipts, concurrent and final costing\ncalculations for quotes and bids for one off sales\n       request. Performing asset management, correcting assignment and\nmonitoring of cost. Drawing up invoices and\n       tracking claims. Maintaining internal project controlling\n(deadlines, costs and quality).\nFinancial Analyst III\n.     Performs complex financial and operational analysis to support short\nterm and long range strategic plans and\n       operating budgets. Act as a key partner to the Engineering, UT\nAssembly Production & Scheduling cost centers\n       for financial planning and controlling activities. I am the\nHyperion/Essbase/Smartview lead for this company.\n.     Conducts work order variance analyses on actual expenditures to\nbudget estimates. Evaluates data, prepares\n       forecasts, and analyzes trends in operations, manufacturing, general\nbusiness conditions and other areas.\n.     Verifies labor cost and workload planning by comparing plan vs.\nactual costs. Identifies voucher-\n       related or other transactions mistakenly applied to the projects\nthat need to be removed.  Coordinates the\n       correction through the EZ Suite feeder system or as a journal entry.\n.    Works closely with executive management to assist in hourly rate\ninternal calculations. Provide timely and\n       accurate invoices and billing information to selected outside\ncustomers. Monitor projects for Project Status EAC\n       vs. actual related issues. Ensures appropriate action is taken to\nupdate statuses and prepares required accounting\n       entries. I am the site Hyperion, Essbase, Smartview lead support for\nthe OBA department.\n.    Responds to various ad hoc requests as well as requests for additional\ninformation from customers both Internal\n       and External.  Coordinates the development of additional cost\ncontrol reports. Perform monthly budgeting,\n       forecasting, ETC/EAC monitoring, revenue recognition and variance\nanalysis.\n.    Assists Department Managers in the timely and accurate preparation of\nannual capital budgets and mid-month\n      forecasting. (ERP JD Edwards, Hyperion, Essbase and Smart View\nretrieve software).\n\nUnited Technology Corporation\nAugust, 2012 - July, 2013\nFinancial Analyst II\nFairfield, CA\n.     Develop and maintain financial models and metrics to measure\n      performance for company sales mix that\n       accurately predict business performance and highlight key issues for\n      senior management. Complete key\n       financial reports including but not limited to: Monthly performance\n      recap, weekly sales analysis, monthly gross\n       margin and other key metrics. Provide ad hoc analytical support for\n      Sales & Inventory Operations Planning\n       (SIOP) to influence decision making with financial perspective.\n.     Support monthly A/P & A/R invoicing for SBU and direct labor\n      analysis. Manage and prepare monthly account\n        commission journal entry and accrual, process check request and\n      maintain account analysis for reporting.\n        Manage and update a percentage of our prepaid and accrual sub-\n      ledger. Reconcile all general ledger accounts\n        and prepare/update general ledger account analysis.\n.      Build and maintain sales and cost schedules to track actual vs.\nbudget variance on a MTD and YTD basics.\n        Support budgeting and forecasting processes by performing trends\nanalysis via moving average schedule and\n        models.  Audits Capital Projects (e.g. construction applications,\nconstruction invoices, legal services invoices,\n        etc.) for the purpose of ensuring proper account codes,\nmathematical correctness and availability of\n        funds. Perform revenue recognition analysis for monthly sale\nreporting under GAAP. Maintain\n        lease register for all lease agreements and assets on site.\n.      Provide financial support to Program Managers and Pricing Analyst by\nmonitoring program financial\n        performance and developing cost effective strategies to meet budget\ncost performance. Establish and\n        maintain cost of goods sold schedule to determine estimate at\ncompletion for sales order programs.  Databases &\n        software being utilized daily Hyperion Financial Management(HFM),\nSAP, eSOX compliance module, Auto\n        time, SAP BO/BW, SmartView, Microsoft office.\n\nParamount Staffing\nMenlo- Nike Golf                                               January,\n2011- April, 2012\nCost Accountant/Financial Analyst\n      Memphis, TN\n.     Support off-site corporate Controller in all accounting duties, i.e.\n      month end closings, accruals, cost accounting\n       analysis, analysis of actual against budget & forecast time\n      period(s) revenues and cost of materials, standard cost\n       vs. actual cost variance of manufacturing production cost. Analyzes\n      financial and manufacturing cost data on\n       local level to produce relevant decision making tools and provide\n      recommendation to senior management to\n       make sound business decisions. Support the on- site General Manager\n      & Site Production Manager of the golf\n       manufacturing operations in financial analysis, inventory accounting\n      and financial decision making as directed.\n.     Ensure accounting transactions are in compliance with Nike Golf\n      accounting policies and US GAAP.  Assist in\n       the maintenance of complete and accurate standard operating\n      procedures and SOX related internal control\n       documentation.  Assist in the maintenance of domestic and/or\n      international ledgers including the preparation of\n       A/P related journal entries and reconciliations.  Decision support\n      analysis of financial transactions to determine\n       present and future financial performance. Perform bookkeeping to G/L\n      for month end closing using SAP, HFM,\n       Smartview.\n\nRobert Half & Associates\n                                                                      Nov,\n2005 - Jan, 2014\nContract Financial Consultant\n                                                 Memphis, TN / San\nFrancisco, Ca\n.    Analyzes transaction data for specific account codes for the purpose\nof identifying potential budget variances,\n      compiling statistical information, developing procedures, and\nconforming to established financial practices and\n      regulatory requirements. Maintains spreadsheets for the purpose of\nensuring cash balances are managed on a\n      weekly basis, reconciling monthly expenditures and revenues and\ngeneral ledger cash balances.\n.    Responds to inquiries from a wide variety of internal and external\nsources (e.g. staff, supplier companies,\n      government agencies, etc.) for the purpose of providing information,\ndirection and/or appropriate referrals.\n      Review vendor activity to ensure all monthly invoices have been\nreceived and processed. Review contract\n      obligations to ensure all contract invoices have been received and\nprocessed. Perform the operating cash account\n      reconciliation and related journal entries. Perform various detailed\nmonth-end allocations for fringe benefits,\n      shared costs and indirect cost allocations. Record the release from\nrestrictions revenue journal entry.  Perform\n      various account reconciliations and work to resolve any\ndiscrepancies. Other duties / special projects as assigned.\n\nFedEx Trade Networks Transport & Brokerage, Inc.\n                                               April, 2008 - Jan, 2012\nSr. Express Clearance Operations (ECO) Import Agent\n                                                         Memphis, TN\n.    Serves as the primary link between FedEx Trade Networks Transport &\n      Brokerage, Inc. and FedEx Express for\n      account and shipment specific issues. Sorts and assigns departmental\n      incoming work and monitor daily workflow\n      to ensure regulatory timeframes are met. Failure to meet these\n      timeframes could cost the company and our\n      customer dollars in potential liquidated damages and additional duty.\n      Refer potential major problems to the\n      manager.\n.    Maintains uniform business practices and procedures to ensure\n      compliance with U.S. customs and other\n      government agency (DEA, FDA, DOD, F&WA) laws and regulations and\n      according to FedEx Express\n      expectations. Analyzes documentation provided to determine that\n      harmonized tariff number and whether the\n      shipment qualifies for special tariff treatment, NAFTA, reduced duty\n      rates and ensures all government agency\n      regulations are followed. Also, ensures that the required information\n      is submitted or transmitted electronically.\n.    Completes the classification of imported merchandise by determining\n      the proper tariff classification and associate\n      duty rate, and calculating the entered value using t/b and FedEx\n      systems.\n\nMemphis Marriott East\n                                                                  June,\n2003 - Sept, 2005\nNight Auditor Supervisor/Night Manager.\n\nMemphis, TN\n.     Responsible for reconciling all hotel cashier transactions;\nreviewing, organizing and compiling management\n        reports on a timely basis and ensuring the accuracy of guest\nbillings and hotel ledger transactions.  I was also\n        responsible for effectively communicating concerns and/or related\nissues to all levels of management and\n        performing guest service agent duties as required, including check-\nin, check-out, switchboard operation and\n        reservations.\n.     Oversaw the overall operation of a 14 floor 319 room hotel property\nduring the evening and/or night shift to\n        ensure guest satisfaction and safety. I had full hotel authority in\nthe absence of the General Manager during the\n        night shift. Responsible for following up and making decisions\naffecting guest and team member issues.\n        Maintain direct supervision over all night shift team members.\n.    Supervised and trained night Guest Services Agents to ensure guests\n      are satisfied with their stay and reports are\n      accurately completed. Immediately address any security issues found\n      on property. Ensuring the property is well\n      maintained and free from any safety hazards. Consistently walking\n      through all departments to ensure that all\n      staff members are in proper uniform and present in work areas.\n.\nCOMPUTER SKILLS\nAs a Leader, I have fostered a team approach on consistently executing a\nsolid partnership by developing strong relationships with internal and\nexternal clients with an objective of meeting all clients' financial\nexpectations from a simplified approach. Excellent critical thinking,\nanalytical and spreadsheet modeling skills. Demonstrate the ability to\ndocument relevant facts and information to support testing and conclusions\nso other reviewers can follow the auditor's logic and methodology.\nProficient in Ariba, SAP analysis for office (AO), Primavera P6, Oracle,\nSAP, SAP BW/BO experience, Hyperion (HFM retrieve), Smart View, JD Edwards,\nPower BI, QuickBooks, advance Microsoft Excel (Pivot tables, V-lookup,\nSUMIFS, AVERAGEIFS, COUNTIFS).\n\nADDITIONAL SKILLS\nMy leadership in the past 10 years in Finance has allowed me to acquire\noutstanding qualities throughout the process of leading the delivery of\nfinancial solutions and services to meet the client's financial budgets. As\na Leader, it's imperative that I am effective at collaborating with all\nlevels of individuals to ensure the delivery of a customer centric\nenvironment.\n\n\n",
          "confidence": 1.0,
          "method": "full_text",
          "structured_data": null
        },
        "tax_term": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "source_by": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "designation": {
          "value": "Project Manager",
          "confidence": 0.9,
          "method": "ner",
          "structured_data": null
        },
        "experience": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        }
      }
    }
  ]
}