{
  "timestamp": "2025-06-15T06:21:26.286063",
  "summary": {
    "total_processed": 56,
    "successful_extractions": 56,
    "failed_extractions": 0,
    "success_rate": 100.0,
    "ocr_usage_percentage": 0.0,
    "avg_extraction_time": 71.51542419195175
  },
  "field_analysis": {
    "empty_fields": {
      "primary_email": 6,
      "secondary_email": 56,
      "phone": 18,
      "city": 20,
      "state": 20,
      "zip": 25,
      "work_authority": 49,
      "tax_term": 42,
      "source_by": 56,
      "experience": 29
    },
    "field_confidence": {
      "primary_email": [
        0.0,
        0.9,
        0.9,
        0.0,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.0,
        0.9,
        0.9,
        0.0,
        0.0,
        0.9,
        0.9,
        0.0,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9
      ],
      "secondary_email": [
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0
      ],
      "phone": [
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.9,
        0.0,
        0.9,
        0.9,
        0.0,
        0.9,
        0.0,
        0.9,
        0.9,
        0.0,
        0.9,
        0.9,
        0.0,
        0.9,
        0.9,
        0.0,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.0,
        0.9,
        0.9,
        0.0,
        0.0,
        0.9,
        0.9,
        0.0,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.0,
        0.9,
        0.0,
        0.9
      ],
      "city": [
        0.0,
        0.0,
        0.7,
        0.0,
        0.0,
        0.7,
        0.9,
        0.7,
        0.7,
        0.9,
        0.7,
        0.7,
        0.0,
        0.7,
        0.7,
        0.0,
        0.7,
        0.7,
        0.7,
        0.0,
        0.7,
        0.7,
        0.0,
        0.7,
        0.9,
        0.7,
        0.7,
        0.7,
        0.7,
        0.0,
        0.7,
        0.7,
        0.0,
        0.7,
        0.0,
        0.7,
        0.0,
        0.0,
        0.7,
        0.0,
        0.0,
        0.7,
        0.0,
        0.0,
        0.7,
        0.0,
        0.9,
        0.9,
        0.7,
        0.9,
        0.9,
        0.7,
        0.7,
        0.0,
        0.7,
        0.0
      ],
      "state": [
        0.0,
        0.0,
        0.9,
        0.0,
        0.0,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.0,
        0.8,
        0.9,
        0.0,
        0.8,
        0.9,
        0.9,
        0.0,
        0.8,
        0.9,
        0.0,
        0.8,
        0.9,
        0.8,
        0.9,
        0.9,
        0.9,
        0.0,
        0.8,
        0.9,
        0.0,
        0.8,
        0.0,
        0.8,
        0.0,
        0.0,
        0.8,
        0.0,
        0.0,
        0.8,
        0.0,
        0.0,
        0.8,
        0.0,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.0,
        0.9,
        0.0
      ],
      "zip": [
        0.0,
        0.0,
        0.7,
        0.0,
        0.0,
        0.7,
        0.9,
        0.7,
        0.7,
        0.9,
        0.7,
        0.7,
        0.0,
        0.0,
        0.7,
        0.0,
        0.0,
        0.7,
        0.7,
        0.0,
        0.7,
        0.7,
        0.0,
        0.7,
        0.9,
        0.0,
        0.7,
        0.7,
        0.7,
        0.0,
        0.0,
        0.7,
        0.0,
        0.0,
        0.0,
        0.7,
        0.0,
        0.0,
        0.7,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.9,
        0.9,
        0.7,
        0.9,
        0.9,
        0.7,
        0.7,
        0.7,
        0.7,
        0.7
      ],
      "work_authority": [
        0.0,
        0.0,
        0.8,
        0.0,
        0.0,
        0.8,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.8,
        0.0,
        0.8,
        0.8,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.8,
        0.9,
        0.0,
        0.8,
        0.9,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0
      ],
      "resume_link": [
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0
      ],
      "raw_resume": [
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0
      ],
      "tax_term": [
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.9,
        0.0,
        0.0,
        0.9,
        0.0,
        0.0,
        0.0,
        0.9,
        0.9,
        0.0,
        0.9,
        0.9,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.9,
        0.0,
        0.0,
        0.9,
        0.0,
        0.9,
        0.0,
        0.0,
        0.9,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.9,
        0.0,
        0.9,
        0.9,
        0.0,
        0.9,
        0.0,
        0.0,
        0.0,
        0.0
      ],
      "source_by": [
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0
      ],
      "designation": [
        0.9,
        0.8,
        0.8,
        0.9,
        0.8,
        0.8,
        0.9,
        0.8,
        0.9,
        0.9,
        0.8,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.8,
        0.9,
        0.9,
        0.8,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.8,
        0.9,
        0.9,
        0.8,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.8,
        0.8,
        0.8,
        0.8,
        0.8,
        0.8,
        0.8,
        0.8,
        0.9,
        0.8,
        0.8,
        0.9,
        0.8,
        0.9,
        0.8,
        0.9
      ],
      "experience": [
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.0,
        0.9,
        0.9,
        0.0,
        0.0,
        0.0,
        0.9,
        0.0,
        0.0,
        0.9,
        0.0,
        0.9,
        0.0,
        0.0,
        0.9,
        0.0,
        0.0,
        0.9,
        0.0,
        0.0,
        0.0,
        0.0,
        0.9,
        0.0,
        0.0,
        0.9,
        0.0,
        0.9,
        0.0,
        0.0,
        0.9,
        0.0,
        0.9,
        0.0,
        0.9,
        0.9,
        0.0,
        0.9,
        0.9,
        0.0,
        0.0,
        0.9,
        0.0,
        0.0,
        0.0,
        0.9,
        0.0,
        0.9
      ]
    }
  },
  "skills_analysis": {
    "categories": {
      "technical_skills": {
        "count": 0,
        "skills": []
      },
      "business_skills": {
        "count": 0,
        "skills": []
      },
      "creative_skills": {
        "count": 0,
        "skills": []
      },
      "communication_skills": {
        "count": 0,
        "skills": []
      },
      "industry_skills": {
        "count": 0,
        "skills": []
      },
      "soft_skills": {
        "count": 0,
        "skills": []
      },
      "other_skills": {
        "count": 0,
        "skills": []
      }
    },
    "total_skills": 0,
    "unique_skills": 0
  },
  "resume_details": [
    {
      "resume_path": "data\\input\\.Net - TX - Hassan.pdf",
      "confidence_score": 0.333,
      "used_ocr": false,
      "extracted_fields": {
        "skills": {
          "soft_skills": [
            {
              "name": "analysis",
              "confidence": 0.42,
              "context": "full_text",
              "positions": [
                294,
                1048,
                9011
              ],
              "experience_weight": 0.4,
              "importance_score": 0.7
            }
          ],
          "domain_specific": [
            {
              "name": "design",
              "confidence": 0.42,
              "context": "full_text",
              "positions": [
                303,
                1057,
                4522,
                5423,
                5940,
                6404,
                9020
              ],
              "experience_weight": 0.4,
              "importance_score": 0.7
            }
          ]
        },
        "first_name": "Hassan",
        "last_name": "Mwase",
        "primary_email": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "secondary_email": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "phone": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "city": {
          "value": "",
          "confidence": 0.0,
          "method": "ner",
          "structured_data": null
        },
        "state": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "zip": {
          "value": "",
          "confidence": 0.0,
          "method": "regex",
          "structured_data": null
        },
        "work_authority": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "resume_link": {
          "value": "data\\input\\.Net - TX - Hassan.pdf",
          "confidence": 1.0,
          "method": "file_path",
          "structured_data": null
        },
        "raw_resume": {
          "value": "  \nHassan Mwase  \n \nProudly presented by Hassan Mwase  IT professional with 1 6+ years of experience as a software developer/solution architect  on multiple \nlarge complex  project s and production support for a variety of clients  including most recent \nexperience developing Web applications . Proficient in analysis, design, development, testing, \nintegration and troubleshooting skills.  Result oriented back -end and front -end developer with \nexceptional analytical skills, team oriented, dependable and very flexible. Experience with \nWaterfall and JIRA Agile methodologies.  \n \nTechnical Skills  \n\uf0b7 Experienced Software development of Web applications using  Azure DevOps, Cloud, Git, \nASP.NET Web Applicati on, ASP.NET Web Forms , ASP.NET MVC,  C#, .NET \nFramework, Kendo UI , JavaScript,  Razor,  Angular  JS, CSS 3, jQuery , bootstrap, Ajax, \ndependency injection,  JSON, LINQ, HTML5,  Microsoft Visual Studio enterprise 201 7, \nActive Server  Pages extended files,  Eclipse , Test Driven Development (TDD ), Selenium \nFramework, Web Services, Postman, Visual Studio code , DevExpress, SQL Server \nManagement Studio . \n\uf0b7 Proficient in analysis, design, development, testing, integration and troubleshooting skills.  \n\uf0b7 Experience with Waterfal l and JIRA Agile methodologies .  \n\uf0b7 Result oriented back -end and front -end developer with exceptional analytical skills, team \noriented, dependable and very flexible.  \n\uf0b7 Familiarity with REST/ HTML/CSS 3.  \n\uf0b7 Excellent relational database experience using Oracle and  SQL Server  including writing \nPL/SQL, SQL stored procedures, functions, triggers and views.  \n\uf0b7 Experience developing using Oracle Database,  Oracle Forms, Oracle Reports,  SQL*Plus, \nPL/SQL, UNIX, Pro*C . \n\uf0b7 Knowledge of Java/J2EE architecture/technologies,  WebSphere , Test Driven \nDevelopment TDD  and Oracle ADF , ODBC  \n\uf0b7 Writing unit tests with NUnit and Moq  \n\uf0b7 Familiar with Regular Expression and Pattern Matching.  \n\uf0b7 SQL Server \u2013 Microsoft SQL Server Management Studio v.18.8 , Visual Studio 2017, \nMicrosoft SQL Server To ols for Visual Studio, GraphiViz 2.38 \u2013 This is used in the PMEF \nNEDAT and new NSIGHT application as an API for creating bubble visualization images.  \nMicrosoft Internet Information Services  (IIS) version 10  web server . OleDB API \nUncovered errors in timeshe et contracts and Provided guidance in resolving major issues \nfrom past years. Applied fix to application code.  \n \n \nProfessional Experience  \n \nSecret Security Clearance  11/2020 to 11/2030                        May. 2020  \u2013 Present  \nSenior Software Programmer/Analyst Consultant /Agile developer  \nTools : .NET, ASP.NET C#, Web Forms .Net Framework ver.4.6, JavaScript, CSS3,  jQuery, \nbootstrap v4.0.0 , JSON, LINQ, HTML5, Visual Basic 201 7, Microsoft Visual Studio enterprise,  \nHassan Mwase           Page 2 \nProudly presented by Hassan Mwase  Developer, Web Services, Web Forms, Databases: SQL Server  and Oracle , Microsoft team TSF  \ndevelopment environment  \n\uf0b7 Develop web applications using C#,  ASP.NET and JavaScript . \n\uf0b7 As a senior developer, I develop, enhance  and Support critical federal Government  \nprojects.  \n\uf0b7 Develop and maintain multiple web applications written in ASP.NET, C#, JavaScript,  \nand Web Forms.  \n\uf0b7 Participate in project meetings with other technical staff, business owners and subject \nmatter experts.  \n\uf0b7 Troubleshooting and debugging Defects  \n\uf0b7 Write automated UI test script using ServiceNow ATF  Framework . \n \n \nFarmers Insurance  \u2013 USA                                                                      Sept. 2018  \u2013 Aug. 2020  \nSenior Programmer/Analyst Consultant /Agile developer  \nTools : .NET, ASP.NET C#, MVC, Web Forms  .Net Framework ver.4.6, JavaScript,  CSS3,  jQuery, \nbootstrap  v4.0.0, JSON, LINQ, HTML5, Visual Basic 201 7,  Microsoft Visual Studio enterprise,  \nDeveloper, Selenium Framework, Web Services, Web Forms, Databases: SQL Server  and DB2 ,  \nMicrosoft team TSF  development environment  \n\uf0b7 Develop web applications using C#, Azure DevOps, Git, ASP.NET MVC , JavaScript, \njQuery, Razor MVC . \n\uf0b7 Develop and maintain multiple customer web applications written in C# , JavaScript and \nWeb Forms.  \n\uf0b7 Participate in project meetings with other technical staff, business owners and subject \nmatter experts.  \n\uf0b7 Troubleshooting and debugging Defects  \n\uf0b7 Write automated UI test script and fix defects using Selenium Framework . \n\uf0b7 Write unit tests with NUnit and Moq  \n \nBoeing  Company  \u2013 USA                                                                              Feb. 2018  \u2013 Sept.2018  \nSenior Programmer/Analyst Consultant /Agile developer  \nTools : .NET, ASP.NET MVC ,C#, MVC,  Dependency Injection DI,  .Net Framework ver.4.6, \nJavaScript, Razor,  Angular  JS,  CSS3, jQuery, bootstrap , JSON, LINQ, HTML5 ,  Microsoft Visual Studio \nenterprise 201 7, PL/SQL, SQL D eveloper, Databases: Oracle 12C and SQL Server  , JIRA  Agile , Microsoft \nteam TSF development environment  \n\uf0b7 Develop se rver side C# pages, create entities in the relation object mapping to modernize \nBoeing data ERP application  using  ASP.NET MVC framework .  \n\uf0b7 Manage Agile task board .  \n\uf0b7 Design and develop back -end Database stored procedures, functions, triggers and views . \n\uf0b7 Code C# interfaces, Classes, view models, API controllers  and data services web resources.  \n\uf0b7 Develop server side XHML5 web pages leveraging angular JS  and jQuery. \n\uf0b7 Code C# Dependency Injection processes \u2013 DI Manager interfaces, DI Manager classes, \nDependency  Injection configuration and API Controllers .   \nHassan Mwase           Page 3 \nProudly presented by Hassan Mwase   \n \nCliffs Natural resources  - Cleveland , OH                                                    July 2016  \u2013 Dec. 2017  \nSenior Programmer/Analyst Consultant  \nTool s: .NET, C#, MVC,  .Net Framework ver.4.6, JavaScript, Angular JS, CSS, JSON, LINQ, HTML5,  \nMicrosoft Visual Studio enterprise 2015,  PL/SQL, Postman , SQL Developer , Toad,  Databases: Oracle  \n12C and SQL Server   \n\uf0b7 Re-designed the Oracle ERP application and convert oracle  forms and reports  web application \nusing  ASP.NET MVC framework . \n\uf0b7 Create d Technical specs based on user desired functionality changes.  \n\uf0b7 Design and develop back -end Database stored procedures, functions, triggers and views . \n\uf0b7 Coded  C# interfaces, Classes, view models, JavaScript controllers  and data services web \nresources.   \n\uf0b7 Develop front end web content XHML 5 web pages  leveraging angular  JS. \n\uf0b7 Code system enhancements, defect fixes  and production support .   \n\uf0b7 Commit and publish finished web application solutions.  \n\uf0b7 Support project and business users to Create  Unit Test, integration, and UAT test scenarios and \nscripts, as well as, performing system and integration testing.   \n\uf0b7 Design, build and implement client and server side applications  as needed , as well as \nsupporting and enhancing custom applications.  \n \nGap Inc. - Columbus , OH                                               July 2015 \u2013 Nov. 2015 \nSenior Programmer/Analyst Consultant /Agile developer  \nTools : Oracle 11G, PL/SQL, SQL Developer, Toad, JDeveloper,  Oracle 11G Web logic environment, \nEclipse, JavaScript,  Ruby , Eclipse, WebSphere, Tomcat,  JIRA Agile development environment.  \n\uf0b7 Onsite team lead working with product managers and business users to re-design the on -\nline order fulfillment process from batch processing to on -demand and transform oracle \ninterface based application to Web UI .  \n\uf0b7 Technical lead for on -site/offshore developers and QA.  \n\uf0b7 Convert legacy forms application to Web  UI using Java Script. \n\uf0b7 Coded supporting stored  procedure s, functions, packages  and views . \n\uf0b7 Assisted to test conver sion of  character based SSH screens for RF devices to a stateless \nHTML user interface, update to the tech stack from Oracle Forms to Java6/HTML5, \ndesigned to  extend the life of a legacy Warehouse Management System(RWMS)   \n\uf0b7 Create  test cases , run automated system tests and validate  output of Ruby automated testing \nfor process online customer orders.  \n\uf0b7 Support warehouse order processing and shipment message queues (MQ) .  \n\uf0b7 Create d Unit Test , integration, and UAT test scenarios and  scripts, as well as, performing \nsystem and integration testing. Write integration testing cases.   \nCliffs Natural Resources - Cleveland , OH  June 2011  \u2013 April 2015  \nSenior Programmer  Consultant  \nTools : Oracle 11G, SQL Server, Oracle Forms, Oracle Forms, .NET, C#, MVC,.Net Framework, \nJavaScript, HTML5,  Microsoft Visual Studio enterprise 2015,  PL/SQL, SQL Developer, Toad  \n\uf0b7 Support business functions through the development, implementation, and maintenance \nof applications systems.  \nHassan Mwase           Page 4 \nProudly presented by Hassan Mwase  \uf0b7 Responsible for Maintenance and support of Company ERP application, support various \nenterprise applications enhancements and code extensions, build and apply fixes.  \n\uf0b7 Maintained and coded database objects such as procedures, functions, packages and \ninterfac e that support ERP applications, ASP.NET MVC, .Net Framework and Object \nBusiness Intelligence  Enterprise Edition (OBIEE).  \n\uf0b7 Coded PL/SQL subprograms that were called by the front -end application written in C#  \nand ORM Entity framework .  \n\uf0b7 Worked with applicatio n managers to g ather requirements and build solutions based off of \nuser requirements and build enhancements . \n\uf0b7 Proactively assess opportunities, risks, challenges for technology and business, define \nscope, plan and schedule of custom enhancements.  \n\uf0b7 Consistent ly exceeded project requirements and expectations on -time delivery.  \n \nWalt Disney World  - Orlando , FL Feb.2008  \u2013 Aug.2010  \nLead  Senior Programmer/Analyst Consultant  \nTools:  Oracle 10G, Oracle Retail supply chain, Oracle Retail Merchandizing, Oracle Database, Oracle \nForms, Oracle Forms, PL/SQL, S QL Developer, Toad  \n\uf0b7 As project Stream lead, managed  full lifecycle implementation effort. Engaged client managers to \nreview project plans, timelines and deliverables.  \n\uf0b7 SME lead Oracle WMS/RMS implementation/integration contractor and client developer  team . \n\uf0b7 Responsible for analysis, design and final delivery of functioning and technical integrated multiple \nfacility warehouse. Setup and configure environments.  \n\uf0b7 Designed, wrote code  and tested application enhancements, mod ifications, and interfaces. \nSupported post go -live including system integration stabilization, inventory stock on hand \nimbalance resolution between RWMS13.x and RMS13.x issue resolution and knowledge transfer \nto Disney IT personnel.  \nCato Corporation, 8100 Denmark Road, Charlotte, NC   Dec. 2007 \u2013 Aug. 2008  \nProject  Lead - Senior Programmer/Analyst Consultant  \nLead technical consultant, to upgrade  RMS . Plan and recom mend most efficient upgrade strategy of all \ncurrent Oracle retail application modules  to be web -based.   \nTesco Central Europe Prague , Czech Republic  Aug.2007 \u2013 Oct.2007  \nLead Senior Programmer/Analyst Consultant  \nLead functional and technical RWMS Project lead consultant,  build and deploy Oracle Retail \nWarehouse Management System for both Prevov Fresh DC and Ambient DC . Review foundation and \ntransactional data to ensure successful build, test and implementation process  of Slovakian DC in \nPresov  \nIBM - Circuit City Richmond , VA Aug.2006 \u2013 July 2007  \nSenior Analyst Consultant  \nTools: RWMS 10.3 , UNIX, Pro*C, Oracle Database, SQL*Plus, PL/SQL, Oracle Reports, Oracle Forms, \nOracle, RIB  \nLead technical analyst to assist RMS11 implementation and integration . Worked on Purchase \nOrder , Cost and Deals team .  \n \nGAP Inc.  \u2013 Columbus , OH 2005 \u2013 2006  \nSenior Analyst Consultant  \nHassan Mwase           Page 5 \nProudly presented by Hassan Mwase  Tools: RWMS 10.3 , UNIX, Pro*C, Oracle Database, SQL*Plus, PL/SQL, Oracle Reports, Oracle Forms, \nOracle, RIB  \nDevelopment team lead developer on the WMS system - a highly customized version of Oracle \nRetail WMS.   \n \nRetek /Oracle  \u2013 Minneapoli s, MN  2003 \u2013 2005 \nSenior Developer  \nTools: RWMS 10.3 , UNIX, Pro*C, Oracle Database, SQL*Plus, PL/SQL, Oracle Reports, Oracle Forms, \nOracle, RIB  \n\uf0b7 Technical lead/supervisor of Retek WMS Developer and customer product support.  \n\uf0b7 Developed RWMS 10.3 /RMS 10.3 and RIB hospital application support interface. On -site DC \noperation s support and technical lead for RWMS/RIB  at the Navy Exchange Norfolk \nVirginia   .  \n \n \n \nEducation  \n \nBachelor of Commerce  \nUniversity of Calgary, Calgary, Alberta  \n \nComputer Programming  \nYork University, Toronto, Ontario  \n \nCertification  \nOracle Certified Professional \u2013 Certified Oracle DBA  \n \n \n",
          "confidence": 1.0,
          "method": "full_text",
          "structured_data": null
        },
        "tax_term": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "source_by": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "designation": {
          "value": "software developer",
          "confidence": 0.9,
          "method": "ner",
          "structured_data": null
        },
        "experience": {
          "value": "6+",
          "confidence": 0.9,
          "method": "regex_total_experience_summary",
          "structured_data": null
        }
      }
    },
    {
      "resume_path": "data\\input\\Affzal_SrDeveloperResumeS.pdf",
      "confidence_score": 0.48500000000000004,
      "used_ocr": false,
      "extracted_fields": {
        "skills": {},
        "first_name": "AFFZAL",
        "last_name": "ABDUL",
        "primary_email": {
          "value": "Affzal.sql@gmail.com",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "secondary_email": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "phone": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "city": {
          "value": "",
          "confidence": 0.0,
          "method": "ner",
          "structured_data": null
        },
        "state": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "zip": {
          "value": "",
          "confidence": 0.0,
          "method": "regex",
          "structured_data": null
        },
        "work_authority": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "resume_link": {
          "value": "data\\input\\Affzal_SrDeveloperResumeS.pdf",
          "confidence": 1.0,
          "method": "file_path",
          "structured_data": null
        },
        "raw_resume": {
          "value": " \nAFFZAL ABDUL  \nDeveloper  Sr. \nEmail:Affzal.sql@gmail.com  \nM: 361 -330-0666  \n \nPROFESSIONAL SUMMARY  \n \n\u2022 Over 9 years  of experience ranging software development, testing, assurance, and implementation and working in \ncomplex engagements.  Prominently has experience in data warehousing and Business Intelligence technologies using \nMS SQL Server 2000/2005/2008R2/2012/2014 &  2016 on Azure.  \n\u2022 Solid 8 years in depth experience  in MS SQL Server database development and administration  in enterprise \nenvironment including server installation, configuration, upgrade, maintenance, performance tuning , optimization, \nbackup/restore, recovery, migration, monitoring, security planning and trouble -shooting.  \n\u2022 Good understanding in database and data warehousing concepts (OLTP & OLAP)  and also data analysis  along with \nvisualizations using Tableau  and Sisense . \n\u2022 Experience in bo th building and utilizing DevOps  pipelines for the projects which started brand new and legacy systems .  \n\u2022 Leverage strong communication and interpersonal skills to cultivate strong working relationships with clients, vendors \nand offshore partners.  \n\u2022 Worked in dependent and as a team with strong critical thinking, time  management  and proven handling of \nresponsibilities and projects.  \n\u2022 Experience in Information Technology field as a n SQL server Developer with strong expertise in  SQL server \ndevelopment, Physical/Logical design, designing Stored -Procedures/T -SQL coding, Troubleshooting, Backups and \nRestore of databases,  SSIS/SSRS, OLTP and OLAP.  \n\u2022 Experience in creating visualizations on the data and created cosmetic clear dashboards  on top of the analyzed data  in \nboth Tableau and PowerBI . \n\u2022 Transformed data from one source  to other destinations  using tools like SSIS , Informatica  PowerCenter . \n\u2022 Experience in Optimizing  Code and Improving Efficiency in databases including  Re-indexing, Updating Statistics, \nRecompiling Stored Procedures  and performing other maintenance tasks and DBCC Commands.  \n\u2022 Good understanding of Microsoft Reporting  Service  (SSRS) with Report authoring , Report management , Report \nformatting, Report distribution, Report delivery and Report security.  \n\u2022 Extensive time spent on POC for Sisense  to be used in data visualization purposes and thereby  have to stick with using \ntableau for visualization purposes of the business data.  \n\u2022 Experience in enhancing and deploying the SSIS Packages from development server to production server.  Migrated  \nDTS packages into SSIS packages using upgrade advisor tool.  \n\u2022 Used Database Monitoring tools like profiler, performance monitor  and Event viewer.  Good understanding in SQL user \nlogins, database user accounts, server roles, database roles and its permissions . \n\u2022 Experience in integrating the SSRS reports into web applications using Report viewer control, URL methods and \ncreating data driven subscriptions to different users using Report Manager.  \n\u2022 Expertise in PowerShell  Scripting / Python  and making REST  API calls to pull data and also load it to databases.   \n\u2022 Expert ex perience in handling huge volume of data and transforming and cleansing of data and raw data.  Involved in \nhuge data migrations, transfers using BCP, SSIS and Informatica PowerCenter  from SQL Analysis server, Teradata, \nOracle . \n\u2022 Experience in Agile methodology in the SDLC process ( both scrum & Kanban).  \n \n \nEducation Background  \n \n\u2022 Bachelor of Engineering (Information Technology) - Osmania University, India. (2005 -2009)  \n\u2022 Master\u2019s in Information Technology and Management \u2013 IIT, Chicago. (2010 -2012)  \n \n \nTECHNICA L SKILLS  \n \nSQL Server Tools   : Enterprise Manager, Profiler, Query Analyser, Memory Optimization Advisor  \nExport & Import (SSIS), SSRS . \nRDBMS   : MS SQL Server 2005/2008 , 2012 , 2014 , 2016.MS  Access, Oracle  11g, Teradata . \nETL   : SSIS, Informatica  Power Center (9.5 & 10.2) . \nProgramming Languages  : SQL, T -SQL, PL-SQL, Core Java, C #, UNIX , PowerShell  and Vbs  \nInternet Technologies  : IIS, XML, HTML.  \nProductivity Applications  : MS Office Suite along with Outlook.  JIRA . \n \nOperating System  : Windows95/98, Windows 2000/NT/XP , 2003 , 7, MS DOS.  \nReporting Tools   : Crystal Reports 11, SSRS , ARMS reporting , Sisense, and Tableau . \nDesign Documentation              :           UML, MS Visio  \nOther  Tools    :            Tidal , Control -M, VSS, TFS, GIT,  SVN.  \n \n \nPROFESSIONAL EXPERIENCE  \n \nAnthem Inc. , Norfolk , VA                                                                         Mar 2017\u2013 Till date  \nDeveloper  Sr. \n \nAnthem Inc . is an American  health insurance company  founded in the 1940s, prior to 2014 known as  WellPoint, Inc. It is \nthe largest for -profit  managed health care  company in the  Blue Cross and Blue Shield Association . It covers 7 .7 million \nseniors, people with disabilities, low -income families and other state and federally sponsored beneficiaries, and federal \nemployees in 26 states, making it the nation\u2019s largest provider of health care for public programs . \n \nResponsibilities:  \n \n\u2022 Expertise in Provider  dealing with legacy system to load files from states into the FACETS application.  Expertise in \nProvider business domai n area and the flow of the FACETS  application. in-depth knowledge of both providers in \nFACETS.  \n\u2022 Apply the Business rules on the data coming from multiple operating markets. Maintain the business rules in a table for \nvisibility to all required processes.  \n\u2022 Actively worked in gathering requirements with SA and PO to convert them to technical documents for the deve lopment \nto initiate in Jira.  \n\u2022 Used JIRA, SVN Tortoise  & Bit bucket  for code repository and logged / tracked the activities in the team. Also CTU tool \nfor code deployments.  \n\u2022 Used  Splunk  for parsing complex files and  Splunk language ( SPQL ) to make the searches and created dashboards on \nthe data. Also configured notifications as requested by the business.  \n\u2022 Did POC  for DevOps tools and deeper research on both Sisense  and Tableau  tools for utilizing them as data \nvisualization purposes across the provider department.  \n\u2022 Have hands on experience in pulling data from JIRA using REST calls thru PowerShell scripting and load them to SQL \ndb. Also did dashboards on the loaded data and provi ded support for the created dashboards in tableau . \n\u2022 Used Python  to massage the data in REST calls using JIRA libraries and loading them to database. Built dashboards \non top the loaded data for project management team.  \n\u2022 Checked the configuration and performance of the existing databases including data file allocation, index, fill factor, \nfragmentation and the impact on system performance. Analyzed potential problems (response delay, locking, server \ndown time, etc.) to avo id and optimize.  \n\u2022 Actively involved  with production support ( 24*7) and Development environment.  \n\u2022 Used Premise  to trace the slow running queries and tried to Optimize SQL queries for improved performance and \navailability. Also used precise tool for reviewin g the performance.  \n\u2022 Actively involved in code reviews and mentoring the developers. And also involved in development activities.  \n\u2022 Excellent Team Building, Project Management, Analytical, Interpersonal & Communication Skills.  \n\u2022 Extensively worked and did resear ch on Redgate  suite tool belt and implemented them across the whole team.  \n\u2022 Have hands on experience in pulling data from JIRA using REST API calls thru PowerShell scripting and load them to \nSQL db. Also did dashboards on the loaded data and provided support for the created dashboards in tableau.  \n\u2022 Participated in Informatica  DVO  trainings and started extensive use of the tool as part of the DevOps roadmap.  Also \ndid POC  on icedq  and attended trainings.  \n\u2022 Built SQL pipelines using Redgate tool belt  in bamboo and deeper knowledge of branching strategy.  \n\u2022 Used both SonarQube  and Veracode  in the DevOps pipelines using Bamboo . \n\u2022 Built the automation framework for reducing the regression test cycle efforts.  \n\u2022 Used Informatica  PowerCenter  and SSIS  to both move and modify data from different environments and optimized \nworkflows.  \n\u2022 Proficient in SSIS ETL packages  & Informatica workflow  designing and development for various complex solutions \nincluding files processing, Incremental loads, historical data maintena nce. Extensively used Configurations, Logging, \nDebugging using breakpoints, Check points, Transactions, Error and Event Handling . \n\u2022 Assisted team in adding new data concepts and developing new data model development methodologies, principles, \nstandards and g overnance considering process optimization aspects.  \n\u2022 Worked on Tidal to schedule the jobs needed and configure them to receive the notifications on demand basis.  \n \n\u2022 Actively in Cloud  CoE trainings and completed AWS Cloud practitioner course.  \n\u2022 Creation of POC and doing architecture POC for upcoming AWS  engagements.  \n \n \n \nEnvironment : MS SQL Server 2016, 2014/2012/2008/2005/2000(SSMS), Visual Studio 2010/2012 ,2017 , TFS, VSS, \nReporting Services (SSRS), Integration Services (SSIS) , Informatica  PowerCenter , T-SQL,  PL-SQL,  .Net, Redgate tools, \nPowerShell Scripting,  python, Excel , Sisense, Tableau.  \n \n \n \n \nAccordant, A CVS Caremark Company, Greensboro, NC                                                                May 2015 \u2013 Mar 2017  \nSr. SQL  DBA  /Developer  \n \nAccordant Health Services , a CVS Caremark company, is a recognized leader in delivering disease management and case \nmanagement services for people with rare chronic conditions.  It has been providing value -added services on behalf of our \ncontracted clients such as health plans, emplo yers, and third -party  administrators (TPAs) for more than 14 years.  \n \nResponsibilities:  \n\u2022 Installation of MS SQL 2012  enterprise edition  on production, Test and Development environments . \n\u2022 Configured many alerts for sql server ongoing health checks and disk space alerts.  \n\u2022 Took part in design and implementation of Log shipping on few instances for the development and testing teams.  \n\u2022 Involved in Business requirement gathering, Technical Design Documents, Business use cases and Data mapping.  \n\u2022 Extensively worked on SSIS , designed and created mappings using various SSIS transformations like  \nOLEDB Command, Conditional Split, Lookup, Aggregator, Multicast, lookup and fuzzy logic.  \n\u2022 Developed SQL scripts to Insert/Update and Delete data in production Environment. Develop a nd implement policies \nand standards for preserving the integrity and security of data.  \n\u2022 Configure SSIS Package for run time Parameters and Configuration file.  Set the Standards for ETL development for the \nteam to follow on naming standards and best practice s for the meta -data, event handling and logging  \n\u2022 Strong expertise in writing stored procedures and performance optimization for the poor running queries. Good T -SQL \nProgramming skills.  \n\u2022 Worked with the development team to implement data strategies, build data flows and develop data models.  \n\u2022 Used Reporting Services (SSRS) to schedule reports to be generated on predetermined time.  \n\u2022 Generated on -demand and scheduled reports for business analysis or management decision using SQL Server \nReportin g Services (SSRS).  \n\u2022 Used Team Foundation Server and Visual Source Safe as a version control tool.  Migrated objects and solutions from \nvisual source safe to TFS. \n\u2022 Experience in deploying created reports in various sources like Web browser, XML and PDF.  \n\u2022 Used S QL Server Profiler to trace the slow running queries and tried to Optimize SQL queries for improved performance \nand availability.  Implement table partitioning  to avoid table level locking issues when different operating company\u2019s \ndata process is initiated at the same time by locking at partition level.  \n\u2022 Good understanding of Microsoft Reporting Service (SSRS) with Report authoring, Report management, Report \nformatting, Report distribution, Report delivery and Report security.  \n\u2022 Extensive experience in Capacity  planning, Performance Tuning, Disaster Recovery, Troubleshooting procedures . \nActively took part in disaster recovery planning and handled the real time issues.  \n\u2022 Also worked on ARMS  reporting application where I have hands -on experiencing in formatting reports, scheduling and \ndelivery of reports.  \n\u2022 Has good exposure to deploy the solutions in cloud platform and also storage usage.  \n\u2022 Involved in implementing business rules on client files  and membership files from the client and mock the data to \nrequired frequencies and also developed mechanism to alert when frequencies are not met on the fields in \nimplementation.  \n\u2022 Has hands -on experience in deploying the ssis packages to both file and serv er levels.  \n \n \nEnvironment : MS SQL Server 2014/2012/2008/2005/2000(SSMS), Visual Studio 2010/2012, TFS, VSS, Reporting \nServices (SSRS), Integration Services (SSIS), T -SQL, .Net, Excel.  \n \n \n \nFirstView Financial LLC, Atlanta, GA                                                         July 2011 \u2013 May 2015  \nSr. SQL DBA  /Developer  \n \nFirstView Financial LLC, Atlanta, GA provides prepaid debit card solutions enabling clients to enjoy all the benefits of \noperating a fully custom prepaid d ebit card program.  \n                                                                                                                                             \nResponsibilities  \n\u2022 Installation of MS SQL 2008 R2 servers on production, Test and Development boxes . \n\u2022 Developed ETL solutions using SQL Server Integration Services (SSIS) to import data based on requirements for \neasy/less maintenance and easy upgrade.  \n\u2022 Employed condition -based  notifications to let the user know the status of the Agent job s. \n\u2022 Developed complex SSIS packages  using proper control flow tasks  and data flow  transformations as per business \nrequirements.  \n\u2022 Migrated data from EXCEL and Flat files using SSIS packages to load data to the relational database in SQL Server \n2008R2 supporti ng BI solutions.  \n\u2022 Implemented Jaro -Winkler distance algorithm to calculate the similarity between strings in Sql server.  \n\u2022 Developed custom vb scripting for use in building custom functionality in job alerts.  \n\u2022 Developed dynamic SQL and dynamic store procedures, views, indexes, CTEs, cursors for business needs.  \n\u2022 Developed test scripts and environment of the developed SSIS solutions and the loaded data.  \n\u2022 Developed complex SQL scripts based on the business requirements in SQL Server databases.  \n\u2022 Implemented Disaster Recovery plans using Database Mirroring and Log Shipping.  Participated  in disaster recovery \ndrills and played important role in DR restoration.  \n\u2022 Controlling day to day activities of Production database and troubleshooting the issues.  \n\u2022 Researched and implemented SQL Server Auditing 2008 on SQL Servers for Auditing needs.  \n\u2022 Supported SSAS cubes as part of production support and monitored the loading process.  \n\u2022 Involved in requirement gathering, technical documentation Phases, supporting testing . \n\u2022 Created ETL jobs to load and clean gigabytes of data and also handled partitioning of the tables in ETL as a part of \nperformance tuning while handling huge volume of data.  \n\u2022 Provided production support and resolved production tickets.  \n\u2022 Handled  user requested issues through Service Tickets.  \n\u2022 Involved in Change Control Management is the discipline of systematically identifying and controlling change requests \nto a project from both internal and external sources.  \n\u2022 Migrated SQL Server 2005 database to  MS SQL Server 2008  \n\u2022 Designed and implemented comprehensive Backup plan and disaster recovery strategies Implemented.  \n\u2022 Created database objects like tables, views, indexes, stored -procedures, triggers, cursors   \n\u2022 Successfully implemented Database Mirroring in  SQL Server 2008  \n\u2022 Successfully Configured Snapshot, Transactional and Transactional with updatable subscription in Replication.  \n\u2022 Created Maintenance Plans for production servers (Full, Differential and Transactional Backups).  \n\u2022 Monitored and modified Performan ce using execution plans and Index tuning.  \n\u2022 Installation of 32 bit and 64 Oracle 10g client and applied DST Patches for Oracle Linked servers  \n\u2022 Co-coordinating with the programmer analyst for optimizing query, writing stored procedures.  \n\u2022 Conducting Root Cause Analysis of application availability and narrow down to issues related to coding practices, \ndatabase bottlenecks, or network latency  \n\u2022 Creating logins, groups, users, roles, database devices, databases, mirroring  devices, checking for database \nconsiste ncy, fixing DBCC errors, monitoring  error logs, database space allocations, transaction log space allocations , \nfine tuning SQL performance.  \n\u2022 Resolving Locking and Blocking issues by using various SQL  server internal commands  \n \nEnvironment  \nMS SQL Server 2008R2 Enterprise Edition, Oracle, SSMS, SSIS, SSRS , TOAD, SQL Plus, OLAP , OLTP, T -SQL, MS Excel, \nMS Access, Unix, MS Visual Studio.Net, VB, XP professional,2003, Crystal Reports XI.  \n",
          "confidence": 1.0,
          "method": "full_text",
          "structured_data": null
        },
        "tax_term": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "source_by": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "designation": {
          "value": "Developer",
          "confidence": 0.8,
          "method": "regex",
          "structured_data": null
        },
        "experience": {
          "value": "9",
          "confidence": 0.9,
          "method": "regex_total_experience_summary",
          "structured_data": null
        }
      }
    },
    {
      "resume_path": "data\\input\\Data Entry - NY - Lauren.pdf",
      "confidence_score": 0.645,
      "used_ocr": false,
      "extracted_fields": {
        "skills": {},
        "first_name": "Lauren",
        "last_name": "Dubin",
        "primary_email": {
          "value": "laurendbrnx2005@aol.com",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "secondary_email": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "phone": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "city": {
          "value": "Bronx",
          "confidence": 0.7,
          "method": "ner",
          "structured_data": null
        },
        "state": {
          "value": "NY",
          "confidence": 0.9,
          "method": "zip_database",
          "structured_data": null
        },
        "zip": {
          "value": "10468",
          "confidence": 0.7,
          "method": "regex",
          "structured_data": null
        },
        "work_authority": {
          "value": "S",
          "confidence": 0.8,
          "method": "regex",
          "structured_data": null
        },
        "resume_link": {
          "value": "data\\input\\Data Entry - NY - Lauren.pdf",
          "confidence": 1.0,
          "method": "file_path",
          "structured_data": null
        },
        "raw_resume": {
          "value": "Lauren Dubin  \n2835 WEBB AVENUE APT. 8G  \nBRONX, NY 10468  \nCELL:  917 -392-2569  \nPHONE/FAX (718) 549 -3568  \nlaurendbrnx2005@aol.com  \n \nPROFESSIONAL SUMMARY  \n \nA skille d and dedicated Administrative, Data Entry, and Customer Service professional with  10 years experience \ncoordinating, planning, and supporting daily operational functions. Excellent written and oral communication skills. \nExperience handling sensitive and confidential business. Superior organizational and multi -tasking skills. Ability to \nwork independently with minimal supervision, as well as in team environment. Highly motivated individual with \nprofessional demeanor.  \n \nSKILLS  \n \nData Entry, Typing (80+ WPM), Transcription, Medical Scribe, fast long -hand, familiarity with ICD -10 medical \ncoding, insurance, pre -certifications/authorizations, Windows 10, Microsoft Office 365 which includes Word  2019 , \nExcel  2019 , PowerPoint  2019 , Access  2019 , Outlook , and Publisher, Eudora, EPIC, NextGen, Athena, Mozilla \nFirefox & Thunderb ird, Adobe applications , Mac computers, electronic tablets, QuarkXPress, PC DOCS, medical \nterminology, Electronic Medical Records, most transcription systems including internet applications, and voice \nrecognition software such as PowerScribe, Legal word pr ocessing centers, recep tion including Call Centers, clerical, \nscanning, data entry, customer service, computer tech support/Help Desk (problem solving & troubleshooting  \nabilities) , maintaining filing and database systems . \n \nPROFESSIONAL EXPERIENCE  \n \nMetropol itan Transportation Authority     August 2017 -PRESENT  \nBronx, NY \nData Entry/Office assistant  \n\u2022 Comparing data entered with source documents.  \n\u2022 Correcting errors when appropriate.  \n\u2022 Maintaining required records of all completed work and compiling weekly reports.  \n\u2022 Assist in maintaining filing system by organizing, purging, maintaining and retrieving files.  \n\u2022 Clerical duties as assigned  \n \nAcacia Network, Bronx, NY       December 2016 \u2013 June 2017  \nMedical Scribe  \n\u2022 Anticipate physician needs to facilitate the flow of clinics.  \n\u2022 Accurately and thoroughly record patient medical history an d physical exam, procedures and treatments \nperformed by healthcare professionals, including nurses and physician assistants.  \n\u2022 Patient education and explanations of risks and benefits, physician -dictated diagnoses, prescriptions and \ninstructions for patient or family members for self -care and follow -up. \n\u2022 Prepare referral letters as directed by the physician.  \n\u2022 Collect, organize and catalog data for physician quality reporting system and other quality im provement \nefforts and format for submission.  \n\u2022 Attend training s on diverse subjects such as information technology, legal, HIPAA and regulatory \ncompliance, billing and coding.  Quickly assimilate new knowledge into process and procedures.  \n\u2022 Proofread and edit all the physician\u2019s medical documents for accuracy, spelling , punctuation and grammar.  \n\u2022 Utilizing NextGen E lectronic Medical Record system.  \n\u2022 Use of Microsoft Word 2016, Outlook 2016  \n \nTunstall Medical Alert \u2013 Long Island, NY      March 2016 \u2013 July 2016  \nData En try Operator, Customer Service Representative | Temporary As signm ent \n\u2022 Entering alphanumeric information into computer system in preparation for medical billing  \n\u2022 Assisting with authorizations for installation of medical alert equipment  \n\u2022 Electronic and paper filing \n\u2022 Customer service duties  \n\u2022 Clerical duties as assigned  \n \nMill Basin Radiology Services \u2013 New York, NY     October 2013 \u2013 November 2017  \nRemote Medical Transcriptionist /Medical data entry |Part-time and  Per Diem ) \n\u2022 Home based transcription utilizing Word 20 13 of time sensitive material in formatted medical radiology  \nreports, mainly sonograms on an as needed basis, often with deadlines  \n\u2022 Entry of medical data in radiology and medical procedures into Microsoft Word  \n\u2022 Use of Windows 10, medical terminology, Office  2013 including Outlook  \n \nPatient Care Associates \u2013 Mamaronec k, NY    November 2012 \u2013 June 2013  \nMedical Data Entry Operator and Transcriptionist, Office Assistant  \n\u2022 Entering of medical data of radiology and cardiology formats  for acute care facilities involvi ng deadlines and \ntime sensitive material, record keeping of same, projects and spreadsheets recording productivity  \n\u2022 Use of medical terminology, internet incorporated transcription and dictaphone applications  \n\u2022 Reception and clerical support as needed  \n \nMill B asin Radiology Services \u2013 New York, NY     November 2011 \u2013 October 2012  \nSenior In -House Medica l Transcriptionist, Secretary  \n\u2022 Entry of medical data in radiology and medical procedures into Microsoft Word 2003, often with deadlines , \ntime sensitive material fo rmatted  into medical radio logy reports, mainly sonograms  \n\u2022 Filing \n\u2022 Record keeping   \n\u2022 Problem solving and troubleshooting  \n\u2022 Reception  and clerical support as needed    \n\u2022 Use of medical terminology,  Windows XP, Mic rosoft Word 2003 and Excel 2003  \n \nWeill Cornell Medical  College \u2013 New York, NY     May 2009 \u2013 September 2011  \nMedical Administrative Assistant  \n\u2022 Performed administrative support services for the Director and Faculty of the Myeloproliferative Diseases \nprogram in the Department of Medicine/ Division of Hematology an d Medical Oncology.  Maintained \ncomputerized calendar   \n\u2022 Coord inated scheduling appointment process for long -term scheduling monthly, weekly, and daily views   \n\u2022 Coordinated all necessary paperwork for daily meetings and events  \n\u2022 Dealt with press and broadcast media to set up arrangements for the Director's public appear ances    \n\u2022 Handled travel arrangements including local, domestic, and foreign travel  \n\u2022 Handled confidential information and situations on a daily basis   \n\u2022 Screened all phone calls   \n\u2022 Assisted in prepa ration and typing of research grants and contract application s and protocol submissions  \n\u2022 Data entry of clinical material as dictated into Epic electronic medical record  \n\u2022 Performed other job -related duties as required. Use of medical terminology, Windows XP,  Microsoft Office \n2007, Eudora, Mozilla Firefox & Thunderbird , Adobe, Outlook, and Epic  \n \nJCW Enterprises Typing Service \u2013 New York, NY    November 2005 \u2013 December 2010  \nRemote Medical Transcriptionist  \n\u2022 Home based typing and transcription, work was on a contr act basis and commission and consisted of \nmedical transcripti on and data entry  of radiology and surgical reports  \n\u2022 Use of Windows XP, Microsoft Office 2003: Word and Excel, Centricity and internet applications; RTAS \ndictaphone  \n \nEDUCATION  \n \nThe Rhodes School \u2013 New York, NY  \nHigh School Diploma  \n \nMandl School \u2013 The Colleg e of Allied Health \u2013 New York, NY  \nMedical Secretary Science Certificate  \n",
          "confidence": 1.0,
          "method": "full_text",
          "structured_data": null
        },
        "tax_term": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "source_by": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "designation": {
          "value": "Data",
          "confidence": 0.8,
          "method": "regex",
          "structured_data": null
        },
        "experience": {
          "value": "10",
          "confidence": 0.9,
          "method": "regex_total_experience_summary",
          "structured_data": null
        }
      }
    },
    {
      "resume_path": "data\\input\\David Buchberger - Security - OH.pdf",
      "confidence_score": 0.675,
      "used_ocr": false,
      "extracted_fields": {
        "skills": {
          "methodologies": [
            {
              "name": "itil",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                0
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "domain_specific": [
            {
              "name": "security",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                30
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "security_tools": [
            {
              "name": "nessus",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                127
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "mcafee",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                159
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ]
        },
        "first_name": "David",
        "last_name": "Buchberger",
        "primary_email": {
          "value": "david_buchberger@columbus.rr.com",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "secondary_email": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "phone": {
          "value": "6142029370",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "city": {
          "value": "Sugar Grove",
          "confidence": 0.9,
          "method": "address_pattern",
          "structured_data": null
        },
        "state": {
          "value": "OH",
          "confidence": 0.9,
          "method": "address_pattern",
          "structured_data": null
        },
        "zip": {
          "value": "43155",
          "confidence": 0.9,
          "method": "address_pattern",
          "structured_data": null
        },
        "work_authority": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "resume_link": {
          "value": "data\\input\\David Buchberger - Security - OH.pdf",
          "confidence": 1.0,
          "method": "file_path",
          "structured_data": null
        },
        "raw_resume": {
          "value": "Resume \u2013 David Buchberger \n 1 RR1 Box 592, Sugar Grove, OH 43155 - (614) 202-9370 -  david_buchberger@columbus.rr.com  PROFILE  Security professional and USAF veteran with 20+ years\u2019 IT experience (including at Battelle and State of Ohio), with 4+ years in security operations, seeks position that requires familiarity with security tools, ability to communicate with all levels of the organization, and experience with the implementation of cyber strategies from inception to execution.   EDUCATION  Embry Riddle Aeronautical University                                                                               B.S.  EMPLOYMENT HISTORY  August 2018 \u2013  February 2020 Infinite Contract State of Ohio \u2013 Office of Information Security and Privacy   Tier 1 Cyber Security Analyst \u2022 Respond to assigned tickets using ServiceNow \u2022 Remediate phishing attacks using Microsoft Defender Advanced Threat Protection, Office 365, and Symantec Management Center \u2022 Use Symantec Web Analytics to allow approved access to Web page URLS \u2022 Assist Tier 2 and Tier 3 with tickets as needed \u2022 Use McAffe ePO to detect malware on servers and desktops \u2022 Use Tanium to troubleshoot infected systems  2015 \u2013 2018 Battelle      Columbus, OH   Network Systems Analyst II (Security Operations Center/Vulnerability Management) \u2022 Scan Network with Nessus Manager and Professional \u2022 Create and monitor Scan schedule \u2022 Build Nessus Agent Groups \u2022 Perform credentialed, Non-credentialed and Agent scans on servers and workstations \u2022 Perform credentialed and Non-Credentialed WAS and DMZ scans \u2022 Perform internal and external Discovery scans \u2022 Work with clients to resolve Nessus plugin scan requirements \u2022 Prioritize network vulnerability threat with Kenna Security \u2022 Monitor Kenna Security Connector tunnel Nessus file upload integrity \u2022 Assign and track ServiceNow vulnerability remediation requests \u2022 Provide vulnerability remediation assistance to server Admins \u2022 Maintain Access Management CMDB \u2022 Create reports for upper management \u2022 Create detailed instructional documentation \u2022 SOC exposure to SIEM tools i.e. Carbon Black (Response and Defense), FireEye,   2013 \u2013 2015 Battelle      Columbus, OH   Network Systems Analyst II (Internal Network) \u2022 Maintain 2FA (RSA Token, Terminal Server and VMware) NIST FISMA and HIPAA environment \u2022 Verify VEEAM backup integrity, create reports and post to SharePoint for client review \u2022 Troubleshoot, install, and remove third party applications \u2022 Troubleshoot network connectivity issues \u2022 Troubleshoot printer issues \u2022 Install updates \u2022 Remediate vulnerabilities \u2022 Familiarity with Microsoft Group Policy objects as it applies to workstations \u2022 Experience with Microsoft Direct Access \u2022 Submit Firewall and Switch RFC \u2022 Assist remote Users \u2022 Create monthly internal network vulnerability report  2006 \u2013 2013 Battelle      Columbus, OH   Sr. Comm/Equip Spec III (20,000+ User environment)  \nResume \u2013 David Buchberger \n 2 2002 \u2013 2006  Arlington Computer Products (Discover Card)   Columbus, OH   PC Technician (3,500+ User environment) 2001 \u2013 2002  TEKsystems (Columbia Gas)    Columbus, OH   PC Technician (1,000+ User environment) 2000 -2001  National Board of BPVI    Columbus, OH   Technical Support Specialist (100+ User environment) 1998 \u2013 2000 United Parcel Service    Columbus, OH PC Technician (100+ User environment)   KEY SKILLS Communication: \u2022 Able to quickly resolve issues, strong communication skills, and able to maintain professional demeanour in stressful situations  IT: \u2022 Understand packet analysis and networking  \u2022 Familiar with incident response lifecycle  \u2022 Able to be part of on-call rotation  \u2022 Familiarity with Windows, and mobile operating systems  Cyber: \u2022 Previous Information Security experience  \u2022 Independent-thinker and self-starter \u2022 Team Player \u2022 Good follow-up skills and attention to detail  \u2022 Good customer service skills Department Interface Experience \u2022 Support Desk \u2022 Desktop and Infrastructure Support \u2022 Testing (Workstation/Server software/hardware configuration) \u2022 Account Management \u2022 Firewall Team \u2022 Security Operations Center (SOC) \u2022 Asset Management \u2022 Vulnerability Management \u2022 Security Engineering \u2022 Governance, Risk Management, and Compliance (GRC) \u2022 Information Security and Compliance Zone (Project Management) \u2022 Patch Management (SCCM) \u2022 Server Admins (Microsoft & Linux) \u2022 SharePoint \u2022 Data Backup/Storage \u2022 Third Party vendors Certifications: \u2022 ITIL v3, CompTIA A+, MCP, CompTIA Security+, CompTIA CySA+ (in progress) Other: \u2022 Microsoft Certified Professional \u2022 Microsoft Direct Access \u2022 Nessus Manager and Professional \u2022 McAfee Vulnerability Manager \u2022 SharePoint \n",
          "confidence": 1.0,
          "method": "full_text",
          "structured_data": null
        },
        "tax_term": {
          "value": "CONTRACT",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "source_by": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "designation": {
          "value": "Security Analyst",
          "confidence": 0.9,
          "method": "ner",
          "structured_data": null
        },
        "experience": {
          "value": "20+",
          "confidence": 0.9,
          "method": "regex_total_experience_summary",
          "structured_data": null
        }
      }
    },
    {
      "resume_path": "data\\input\\David Musia - Mainframe - TX.pdf",
      "confidence_score": 0.45500000000000007,
      "used_ocr": false,
      "extracted_fields": {
        "skills": {},
        "first_name": "David",
        "last_name": "Musia Denton",
        "primary_email": {
          "value": "davidmusia15@gmail.com",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "secondary_email": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "phone": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "city": {
          "value": "Denton",
          "confidence": 0.7,
          "method": "ner",
          "structured_data": null
        },
        "state": {
          "value": "TX",
          "confidence": 0.9,
          "method": "zip_database",
          "structured_data": null
        },
        "zip": {
          "value": "76210",
          "confidence": 0.7,
          "method": "regex",
          "structured_data": null
        },
        "work_authority": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "resume_link": {
          "value": "data\\input\\David Musia - Mainframe - TX.pdf",
          "confidence": 1.0,
          "method": "file_path",
          "structured_data": null
        },
        "raw_resume": {
          "value": "David Musia  \nDenton,  TX 76210        \ndavidmusia15@gmail.com  | https://www.linkedin.com/in/david -musia -bb1b04151/   \n \nOPERATOR and TECHNICIAN : \nCOMPUTER  OPERATOR  \n|\n MAINFRAME  OPERATOR  \n|\n BATCH  OPERATOR  \n|\n DATA CENTER  OPERATOR  \n \nPROFESSIONAL SUMMARY  \n \nMainframe, Batch,  Data Center and Computer Operator  with 20+ years\u2019 experience . Maintain ed 35+ mainframes consecutively. \nSuccessfully c ompleted and transitioned  with 3 corporate mergers.  Consistently identified  automation  opportunities  of \nmainframe processes to improve efficiencies.  Participated in interviewing, hiring, training and effectively managing personn el \nas needed by management.  Continually r eceive d accolades from customers and teammates for exceptional customer service.  \nMainframe industry experience includes Financial Services, Transportation , Card Services , Military, Oil, Medicare, Medicaid, \nSteel, Pharmaceutical, and U.S. Government . Strong decision -making skills and the ability to exercise composure and \nprofessionalism wh ile successfully managing all situations .   \n \nTECHNICAL EXPERTISE  \n \nAutomation Point  \nSRA  \nSM9  \nEON  \nTNG  \nCA-7 \nCA-11 \nSDSF  \nOPC  \nJOB MASTER  \nOPSMV S \nIMS \nCICS  \nDB2  \nMVS  \nJES2  \nJCS2  \nVirtual Tape  ATRM  \nHSC  \nSLS0  \nVM \nVSE  \nZEKE  \nEDSNET  \nTELLNET  \nSAM  \nWSF2  \nTSO  \nISPF  \nIMF  \nSYSVIEW  \nOMEGAMON  \nCAVIEW  \nNETVIEW  \nAPLMR  BOOLE and BABBAGE  \nJCL \nOffice Vision  \nCONTROL M  \nCONTROL R  \nVTAM  \nVPS  \nSAR  \nDOS  \nMS Offic e and Email  \nSkype  \nHP MyRoom  \nModify Commands  \nInfo Man  \nSTAR  \nVantive Digital Workflow / Service \nCenter  \n \nAREAS OF EXPERTISE  \n \nTeam Leader  \nManagement Support and Backup  \nMaintain 35+ Mainframe LPARS  \nIdentify  Automation  Opportunities  \nPerform IPL\u2019s  \nDisaster Recover Drills  \nDevelopment and Train New Employees  \nAnalyze, Identify,  Troubleshoot, Develop and Create Solutions  \nExceptional Customer Service  \nIdentify and Solve Hardware Issues  \nMonitor System Cycles  \nEmployee Interview and Hiring Decisions  ISO 9000, 9001 Certification  \nHigh Degree of Value on Accuracy and Detail  \nRisk, Controls and Compliance  \nSystems Migration  \nMonito r, Report, Document and Escalate  \nProject Administration  \nProblem Management  \nIncident Management  \nChange Management  \nQuality Management System  \nExce ptional  Planning  / Time Management  \nRoot Cause Analysis  \n \n \n \nEDUCATION  \n\n \nTexas State Technical College  \nComputer Science Technology and Programming  \nWaco , TX \n \nCAREER HIGHLIGHTS  \n \n\u2713 Identified numerous tasks to be automated to decrease operator intervention and increase streamlined operations.  \n\u2713 Relocated entire data center and trai ned Military personnel on operations.  \n\u2713 Facilitated training and development to offshore operators on outsource d mainframes.  \n\u2713 Monitored and supported 35+ mainframes.  \n\u2713 Analyze, troubleshoot and solve various system and customer mainframe issues.\n  \n\u2713 Create and proof QMS process documen ts.\n \n\u2713 Identify and troubleshoot operational issues, escalations and coordination of operational activities . \n\u2713 Ensur e the procedures o f change m anagement are followed for all change activity .\n \n\u2713 Monitor analyze and support  batch processing.\n  \n\u2713 Prioritize daily workload s to meet service level agreements.\n  \n \nWORK  EXPERIENCE  \n \nSenior Mainframe Operator           \nDXC Technology  (from EDS)          2000 \u2013 2019  \n Hewlett Packard Enterprises  (HPE) merged with DXC Technology 2017  \n Hewlett Packard (HP) merged with HPE 2015  \nElectronic Data Systems (EDS) merged with HP 2008  \n         \n \n \n",
          "confidence": 1.0,
          "method": "full_text",
          "structured_data": null
        },
        "tax_term": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "source_by": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "designation": {
          "value": "Project Administration",
          "confidence": 0.8,
          "method": "regex",
          "structured_data": null
        },
        "experience": {
          "value": "20+",
          "confidence": 0.9,
          "method": "regex_total_experience_summary",
          "structured_data": null
        }
      }
    },
    {
      "resume_path": "data\\input\\DEXTER K. NDENGABAGANIZI  - PM Program Manager - OH.pdf",
      "confidence_score": 0.555,
      "used_ocr": false,
      "extracted_fields": {
        "skills": {},
        "first_name": "PROFESSIONAL",
        "last_name": "EXPERIENCE C",
        "primary_email": {
          "value": "ndengaba@gmail.com",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "secondary_email": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "phone": {
          "value": "2693258811",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "city": {
          "value": "Pickerington",
          "confidence": 0.7,
          "method": "ner",
          "structured_data": null
        },
        "state": {
          "value": "OH",
          "confidence": 0.9,
          "method": "zip_database",
          "structured_data": null
        },
        "zip": {
          "value": "43147",
          "confidence": 0.7,
          "method": "regex",
          "structured_data": null
        },
        "work_authority": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "resume_link": {
          "value": "data\\input\\DEXTER K. NDENGABAGANIZI  - PM Program Manager - OH.pdf",
          "confidence": 1.0,
          "method": "file_path",
          "structured_data": null
        },
        "raw_resume": {
          "value": "DEXTER K. NDENGABAGANIZI  \n104 Thrush Circle  \uf06c  Pickerington, OH  43147  \uf06c (269) 325-8811  \uf06c ndengaba@gmail.com     \n \nEDUCATION  \nColumbia University : School of International and Public Affairs  New York, New York  \nGraduate Work Experience , International Finance and Economic Policy  Aug. 2017 \u2013 Jan. 2019  \n\uf0b7 Relevant Coursework : Finance, Public & Nonprofit Management,  Microeconomics, Macroeconomics, Statistics    \n \nMichigan State University: James M adison College  East Lansing, Michigan  \nBachelor of Arts in International Relations  May 2012\n \nPROFESSIONAL EXPERIENCE  \nCouncil on Foreign Relations  (CFR)  Washington, DC  \nAssistant Director, Washington Meetings Program  [May 2016 \u2013 Aug. 2017]  \nProgram Coordinator , Washington Meetings Program  [June 2015 \u2013 May 2016 ] \nProgram Associate, Washington Meetings Program  [Jan. 201 4 \u2013 June 2015 ] \nProgram Assistant , Washington Meetings Program  [Nov . 2012 \u2013 Jan. 201 4] \n\uf0b7 Conceptualize d, manage d, and coordinate d internal and external events ranging from roundtables to multi -day \n300+ daily attendee conferences with key policymakers across various stakeholder groups.  \n\uf0b7 Create d and m anage d relationships with  key domestic and international policymakers , subject matter experts, \nand business leaders to participate in CFR programming  and initiatives  for Council members.  \n\uf0b7 Research ed, monitor ed, and evaluate d international trends to  identify potential p rogramming for CFR \nmembers and the broader domestic and international political and policy community.  \n \nResults for Development Institute  Washington , DC  \nCenter for Health Market Innovations (CHMI) Intern  [May 2012 \u2013 Nov. 2012 ] \n\uf0b7 Assisted in the creation of s trategic plans to increase the visibility of dozens of health market innovations.  \n\uf0b7 Conducted needs synthesis used in the formulation of CHMI\u2019s Web and Facebook Outreach Strategy.  \n \nU.S. Embassy: Foreign Commercial Service  Singapore  \nForeign Commercial Service Intern  [Aug. 2011 \u2013 Jan. 2012 ] \n\uf0b7 Conduct ed trade promotion for 1,300+ American companies and products in Singapore . \n\uf0b7 Interviewed corporate and policy leaders, researched,  and produced  official U.S. Commercial Service Market \nResearch Report s used in U.S.-Singaporean business facilitation s. \n \nHouses of the Oireachtas: D\u00e1il \u00c9ireann (Irish National Parliament)  Dublin, Ireland  \nResearch Assistant , Fine Gael  [May 2011 \u2013 Aug. 2011 ] \n\uf0b7 Researched and produced reports on various European  economic policies , which were  used by Members of \nParliament in  nationally  televised  parliamentary address es.  \n \nADDITIONAL EDUCATION  \nUniversity of London  London, England [Summer 2009]  \nStudy Abroad Program - Economics of Law and Public Policy   \n \nUniversity of the Ryukyus  Okinawa, Japan [Sept. 2009 - Aug. 2010]  \nStudent Exchange Program \u2013 Study of Advanced Japanese and Japanese/Okinawan Culture  \n \nSKILLS  \nJapanese \u2013 Advanced -Low, Stata, R, Photoshop, Adobe Lightroom, Microsoft Access, Excel, Outlook, One Note  \n \nREPORTS AND PUBLICATIONS  \nSingapore: Aerospace Market  \u2013 U.S. Commercial Service: U.S. Department of Commerce (01/2012)  \nSingapore: Franchise Market  \u2013 U.S. Commercial Service: U.S. Department of Commerce  (12/2011)  \n",
          "confidence": 1.0,
          "method": "full_text",
          "structured_data": null
        },
        "tax_term": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "source_by": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "designation": {
          "value": "Program Coordinator",
          "confidence": 0.9,
          "method": "ner",
          "structured_data": null
        },
        "experience": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        }
      }
    },
    {
      "resume_path": "data\\input\\Enterprise Architect - LA - Vijay.pdf",
      "confidence_score": 0.405,
      "used_ocr": false,
      "extracted_fields": {
        "skills": {},
        "first_name": "Vijay",
        "last_name": "Kumar Kanamarlapudi",
        "primary_email": {
          "value": "vijay.kanamarlapudi@hotmail.com",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "secondary_email": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "phone": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "city": {
          "value": "",
          "confidence": 0.0,
          "method": "ner",
          "structured_data": null
        },
        "state": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "zip": {
          "value": "",
          "confidence": 0.0,
          "method": "regex",
          "structured_data": null
        },
        "work_authority": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "resume_link": {
          "value": "data\\input\\Enterprise Architect - LA - Vijay.pdf",
          "confidence": 1.0,
          "method": "file_path",
          "structured_data": null
        },
        "raw_resume": {
          "value": "Vijay Kumar Kanamarlapudi, TOGAF\u00ae, PMP\u00ae, CBAP\u00ae, ACP\u00ae, SAFe\u00ae4 POPM, SSM, PAHM, ITIL, SSBBP  1 \n VIJAY KUMAR KANAMARLAPUDI ,  \nTOGAF\u00ae, PMP\u00ae, CBAP\u00ae, PMI-ACP\u00ae, SAFe\u00ae 4 POPM, SSM , PAHM, ITIL\u00ae, SSBBP  \nEnterprise Business Architect / Project Manager/ Lead Business Analyst  \nvijay.kanamarlapudi@hotmail.com  | (860) 801 -0199  \n \nSummary  \nVijay has t hirteen  years of experience in healthcare as a  Business Consultant  with key expertise in \nBusiness Architecture, Project Management, Bu siness Analysis.  He has led teams in projects following \nAgile and Waterfall methodologies . \n \nCertifications  \n1. TOGAF\u00ae 9 Certified from The Open Group - 2019  \n2. Project Management Professional, PMP\u00ae  from Project Management Institute, PMI\u00ae - 2018  \n3. Certified Business Analysis Professional , CBAP\u00ae  from International Institute of Business \nAnalysis, IIBA\u00ae - 2014  \n4. Agile Certified Practitioner, PMI-ACP\u00ae  from Project Management Institute, PMI\u00ae -2017  \n5. Professional, Academy for Healthcare Management, PAHM  from America\u2019s Health Insurance \nPlans , AHIP - 2011 , 2017  \n6. SAFe \u00ae 4 Certified Product Owner/ Product Manager, SAFe \u00ae 4 POPM  from Scaled Agile \nFramework\u00ae , SAFe \u00ae \u2013 2017  \n7. SAFe \u00ae 4 Certified Scrum Master, SAFe \u00ae 4 SSM  from Scaled Agile Framework\u00ae, SAFe \u00ae- 2018  \n8. ITIL\u00ae  Foundation Certificate in IT Service Management from Axelos Global Best Practice - 2017  \n9. Six Sigma Black Belt Professional, SSBBP  from Management Strategy Institute - 2015  \n10. IBM Certified Database  Associate - DB2 Universal  Database V8.1 Family - DB2 Universal \nDatabase - 2007  \n \nKey Skills  \nEnterprise Business Architecture  Capability Modelling  \nValue Stream Mapping  Activity Mapping  \nBusiness Analysis  Planning and Monitoring  Project Management  \nRequirements  Management and Communication  Project Integration Management  \nElicitation  Project Risk Management  \nEnterprise Analysis  Project Scope Management  \nRequirements Analysis  Healthcare EDI Transactions  \nSolution Assessment and Validation  Waterfall Methodology  \nSCRUM Master  Agile Methodology  \nData Modeling  Project Quality Management  \nGap Analysis  Business Architecture  \nProcess Modelling  Use Case Modelling  \n \nTechnologies  \nVersion Controls: Clear Case, Visual SourceSafe, SharePoint  \nIBM Rational Tools: Requirements Composer, Requisite Pro, Clear Quest, Clear Case, Team Concert  \nSoftware: MS - Word, MS-Excel, Live m eeting, MS-Access, MS-Project, File -Aid, Cisco WebEx, TFS  \nProcess Modelling: Provision , Balsamic  \nLanguages: COBOL, JCL, SQL, XML  \nDatabases: IBM DB2, Oracle , SQL Server  \nVijay Kumar Kanamarlapudi, TOGAF\u00ae, PMP\u00ae, CBAP\u00ae, ACP\u00ae, SAFe\u00ae4 POPM, SSM, PAHM, ITIL, SSBBP  2 \n ERP: PeopleSoft  \nOther: Business Objects, nVision, Plan View, Cognos, Salesforce.com , Tableau , SSRS  \n \nEducation  \nMaster of Business Administration -MBA, May 2019 from Southeastern Louisiana University, Hammond, \nLA, USA  \nBach elor\u2019s in Electrical  and Electronics Engineering - B.Tech. , 2006 from Acharya Nagarjuna University, \nIndia  \n \nWork Experience  \nEnterprise Business Architect , Business Analyst Lead/ Senior Corporate Business Analyst  \u2022 11/2016- \nPresent  at Blue Cross and Blue Shield of Louisiana  \nPrincipal Business Analyst \u2022 09/2014 \u2013 10/2016 at Ultramatics Inc.  \nSenior Associate Consultant \u2022 04/2012 - 9/2014 at Infosys Ltd.  \nAssociate - Projects \u2022 09/2006 \u2013 04/2012 a t Cognizant Technology Solutions US Corp.  \n \nProfessional Experience  \n \nBlue Cross and Blue Shield of Louisiana , Baton Rouge, LA, US                             10/2018 - Present  \nROLE:  Enterprise Business Architect  \nSUMMARY:  \nWorked as a n Enterprise Business Architect analyzing enterprise level impacts on ideas and demands \nsubmitted by senior  management team and prepared business architecture for impacts at an enterprise \nlevel. Documented Business Capabilities; Value Streams; Processes th at are impacted for each idea and \nprepared Architecture Blue Prints for the Baseline and Target State Business Architecture.  \n \nRESPONSIBILITIES:  \n\u2022 Provide d strategic consultation to assigned line -of-business (LOB) or assigned functional area(s) of \nbusiness in  defining or designing business capabilities and processes, functions and organizational \nstructures.  \n\u2022 Participate d in divisional enterprise strategy development, including environmental analysis, \nopportunity identification, value cases and business innovati on portfolio development.  \n\u2022 Acted as an advocate for business needs while in parallel providing ongoing feedback on \ndevelopments and initiatives within IT  \n\u2022 Communicate d IT SLA metrics to respective areas of assigned business functions.  \n\u2022 Collaborate d, facilitat ed and consult ed with business stakeholders of the areas of responsibility in \nplanning, business case development and proposal of business initiatives in alignment with desired \nbusiness capabilities.  \n\u2022 Work ed with other peer business architects and EA architects to identify shared capabilities and \nprocesses and opportunities to create efficiencies across the enterprise and facilitate development \nof cross -functional solutions  \n\u2022 Provide d consultation to business a nalysts on an ongoing basis in communicating divisional \ncapability plans  \n\u2022 At the project execution level, collaborate d extensively with Business Analysts to ensure a tight \nalignment between the business capabilities and high -level use case scenarios identif ied in the \nSolution Blueprint document and corresponding detailed requirements identified in the Business \nRequirements Document (BRD) that support them.  \n\u2022 Participate d in enterprise architecture development, including business architecture, information \narchi tecture, application portfolio and technical architecture.  \n\u2022 Collaborate d with the enterprise program & project management office on reporting project status, \nissues, risks and benefits regarding areas of responsibility.  \nVijay Kumar Kanamarlapudi, TOGAF\u00ae, PMP\u00ae, CBAP\u00ae, ACP\u00ae, SAFe\u00ae4 POPM, SSM, PAHM, ITIL, SSBBP  3 \n \u2022 Explored  ways to apply new technology  to, and reuse existing technology for, business processes; \nresearch and provide information on technical trends and competitors' practices relevant to \nassigned LOB or assigned functional area(s) of business customers.  \n\u2022 Assess ed near -term needs, using struc tured interview processes, to establish business priorities; \nconsult with technical subject matter experts and develop alternative technical solutions; advise on \noptions, risks, costs vs. benefits, and impact on other business processes and system prioriti es. \n\u2022 Collaborate d with other IT functional areas to remain apprised of project status, and inform LOB \ncustomer management of progress; conversely, keep IT's technology and service managers aware of \nkey LOB customer issues, identifying and resolving potentia l problems and conflicts.  \n\u2022 Develop ed, motivate d and direct ed staff to create a team environment, and enable staff to fulfill the \nLOB customer operating objectives.  \n \nMY DELIVERABLES:  \nBusiness Architecture Blue print; Capability Mapping; Value Stream Mapping; Business Process Flow \nDiagram s; Business Need Analysis Document ; Data Requirements Specifications; User Interface \nSpecifications; Gap Analysis; Prototypes; Storyboarding; User Interface Screen Flow Diagram; Business \nRule Definitions . \n \nDELIVERABLES CONTRIBUTED TO:  \nNon -Functional Requirements Specification; Logical Data Model; Business Need Analysis Document; \nWork Breakdown Structure; Test Data Sheet; Test Plan; Test Scripts; Test  Strategy; Test Results \nSummary; Information Architecture; Project Deliverables List; Risk Register; Implementation \nPlan/Checkout activities ; Design Specifications; Technical Requirement Specifications ; SSO \nSpecifications.  \n \n \nBlue Cross and Blue Shi eld of Louisiana , Baton Rouge, LA, US                          10/2014 \u2013 10/2018  \nROLE:  Lead Business Analyst; Senior Corporate Business Analyst; Senior Enterprise Business \nAnalyst;  Scrum Master  \nSUMMARY:  \nWork ed as a program level business analyst overseeing the foundation to put together the operations \nof benefits administration of Medicare Advantage platform that Blue Cross Blue Shield of Louisiana is \nworking to put together.  Worked on various IT efforts to send data from BCBSLA to vendors and to \nreceive and sto re data from external vendors.  \n \nRESPONSIBILITIES:  \n\u2022 Worked closely with clients to identify business needs (requirements) and the costs and benefits of \nimplementing a proposed solution, as a project, in -order to produce an accurate business case for \nthe proj ect \n\u2022 Collaborated with business owners and subject matter experts to develop an understanding of \nbusiness processes and functions to aide in completing the standard project business case related to \na proposed project, analyzing business requirements and ide ntifying potential business solutions; \ncompletes appropriate documentation of business case, requirements and solutions.  \n\u2022 Participate d in divisional enterprise strategy development, including environmental analysis, \nopportunity identification, value cases and business innovation portfolio development.  \n\u2022 Performed project feasibility analysis, scopes project and helps client identify altern ative solutions; \nassist with the development of financial worksheets related to project costs and benefits  \n\u2022 Worked with clients to develop business process flows and to obtain a clear understanding of their \nbusiness models to identify potential pro cess improvements and efficiencies  \nVijay Kumar Kanamarlapudi, TOGAF\u00ae, PMP\u00ae, CBAP\u00ae, ACP\u00ae, SAFe\u00ae4 POPM, SSM, PAHM, ITIL, SSBBP  4 \n \u2022 Ensured adherence to BCBSLA project and program processes and lifecycles to ensure business and \nfunctional requirements are understood, agreed to, documented and can be traced to measurable \nproject success criteria.  \n\u2022 Analy zed business information and process flows, gather requirements and translate to Technical \nspecifications required for application programming and test strategy development.  \n\u2022 Assisted Project Manager in preparing an appropriate transition plan for projects prior to \nimplementation; assists with development and review of appropriate production documentation \nsuch as policies and procedures.  \n\u2022 Worked with clients and IT Quality Assurance to develop a test plan that can be executed via test \nscenarios and test cases  to ensure expected functionality, features, as well as traceability among \nbusiness requirements, technical specifications and testing are validated before releasing \napplication for User Acceptance Testing.  \n\u2022 Identified the functionality and features that should be tested and create test scenarios that validate \nperformance of the scenarios in the new application.  \n\u2022 Developed a test script that describes the details of the test scenarios  \n\u2022 Identified resources and functional areas that will be needed to perform testing  \n\u2022 Represented the Project Management Office in a positive, professional and enthusiastic manner \nwhen working with both internal and external clients to build and maintain the respect of the \nProject Management Office within the organization.  \n\u2022 Escalated  appropriate issues or problems to the project manager or Project Management Office \nManager, as appropriate, for discussion and evaluation  \n\u2022 Maintained professional demeanor with team members, business owners and fellow staff; provide \nconfidentiality for dis cussions regarding projects, position and salary information and other matters \nthat may be discussed in meetings  \n\u2022 Actively pursued personal leadership development to ensure professional growth; consistently \ndemonstrates a high level of quality results and m aintains expected productivity objectives.  \n\u2022 Served as Project Manager for selected effort when necessary.  \n\u2022 Adhered to organizational project management processes and lifecycles to ensure that project \nbusiness and functional requirements are understood, agree d to, can be traced to measurable \nproject success criteria and to drive quality among projects.  \n \nMY DELIVERABLES:  \nBusiness Process Flow Diagram; Data Requirements Specifications; User Interface Specifications; User \nStories; Gap Analysis; Deployment Communi cation Materials; End User Application Documentation; \nProduct Backlog; Issue Tracker; Risk Analysis; Prototypes; Storyboarding; User Interface Screen Flow \nDiagram; Business Rule Definitions; Requirements Traceability Matrix.  \n \nDELIVERABLES CONTRIBUTED TO:  \nNon-Functional Requirements Specification; Logical Data Model; Business Need Analysis Document; \nWork Breakdown Structure; Test Data Sheet; Test Plan; Test Scripts; Test Strategy; Test Results \nSummary; Information Architecture; Project Deliverables List; Ris k Register; Implementation \nPlan/Checkout activities.  \n \n \nCLIENT:  Aetna, Inc. , Hartford, CT, US                                                                     10/2013 - 10/2014               \nROLE:  Scrum Master; Lead Business Analyst  \nSUMMARY:  \nWorked as a Scrum  Master and Lead Business A nalyst for this project dealing not only with gathering \nand maintaining requirements but also guiding the team towards successful implementation of an \napplication with web -based  JAVA user interface functionalities in various  releases.  \n \nVijay Kumar Kanamarlapudi, TOGAF\u00ae, PMP\u00ae, CBAP\u00ae, ACP\u00ae, SAFe\u00ae4 POPM, SSM, PAHM, ITIL, SSBBP  5 \n RESPONSIBILITIES:  \n\u2022 Analysis of the existing business process and helping the product owners in determining the epics \naccording to the proposed business process  \n\u2022 Working with the product o wner to create and maintain product backlog  \n\u2022 Prepared user navigations flows and user interaction diagrams and requirement specifications for a \nweb based front end application.  \n\u2022 Documented business rules for storing the data entered by the user on the front JAVA user interface \n(web based UI) screens - into the back end DB2 tables.  \n\u2022 Planning the timelines for each of the sprints after categorizing the priority of the epics  \n\u2022 Facilitating productivity by removing impediments that obstructed the team\u2019s pursuit of its sprint \ngoals  \n\u2022 Ownership of requirements and also turning the requirements into working software  \n\u2022 Tracking and reporting the progress of the sprints and ensuring that the timelines are achieved \ncomfortably  \n\u2022 Conducting meetings for planning, daily scrum s tatus, scrum pre -planning, back log grooming, \nreview/demo and retrospective discussions  \n\u2022 Ensuring the application of standard processes in each and every stage of the sprints by enforcing \nthe rules of the agile process  \n\u2022 Documentation of the User stories, Use Case framework . \n\u2022 Preparation of Business Proce ss Flow Diagrams, User Interface Screen Flow Diagrams, User Interface \nMock Ups, Use Case Diagrams, Application Overview presentations  \n\u2022 Documentation of Deployment Communication Material and end user reference materials  \n\u2022 Ensuring that the scrum team works in a productive manner to deliver the goals of the sprints . \n \nMY DELIVERABLES:  \nBusiness Process Flow Diagram; Data Requirements Specifications; User Interface Specificatio ns; User \nStories; Gap Analysis; Deployment Communication Materials; End User Application Documentation; \nProduct Backlog; Issue Tracker; Risk Analysis ; Prototypes; Storyboarding ; User Interface Screen Flow \nDiagram ; Business Rule Definitions; Requirements Traceability Matrix.  \n \nDELIVERABLES CONTRIBUTED TO:  \nNon -Functional Requirements Specification; Logical Data Model; Business Need Analysis Document; \nWork Breakdown Structure; Test Data Sheet; Test Plan; Test Scripts; Test Strategy; Test Results \nSummary; Information Architec ture ; Project Deliverables List; Risk Register; Implementation \nPlan/Checkout activities.  \n \nCLIENT:  AETNA  INC., Hartford, CT, US                                                                   01/2013 \u2013 10/2013  \nROLE: Lead Business Analyst  \nSUMMARY : \nWorked as a Lead Business Analyst , managing the business analysts,  resolving i ssues to track to their \nclosure, ensuring  there are no scope -related concerns from stakeholders , guide and train  the Business \nAnalysts , and ensure  the requirements gathering, elicitation , and reviews are completed per plan.  \n \nRESPONSIBILITIES  \n\u2022 Leading a team of nine domain Business Analysts for all business analysis  activities  \n\u2022 Facilitated and lead requirement gathering sessions  \n\u2022 Problem s olving and issue resolution for scope related and business definition related topics  \n\u2022 Managing the requirement timelines and overcoming the challenges faced by individual domain \nlevel Business Analysts  \n\u2022 Coordinating all the tasks that need to be accomplished by all the Business Analysts in the project  \n\u2022 Busin ess analysis and requirement gathering  \nVijay Kumar Kanamarlapudi, TOGAF\u00ae, PMP\u00ae, CBAP\u00ae, ACP\u00ae, SAFe\u00ae4 POPM, SSM, PAHM, ITIL, SSBBP  6 \n \u2022 Conducting Joint Application Development (JAD) sessions  \n\u2022 Preparation of RTM \u2014Requirement Traceability Matrix  \n\u2022 Preparation of Functional Requirement Specification document  \n\u2022 Analysis of existing and nature of proposed proce ss \n\u2022 Developing process flow diagrams according to the proposed process and applications impacted  \n\u2022 Preparation of Use Case Narratives  \n\u2022 Reporting status of requirements progress for Level 1 management during project planning stage  \n\u2022 Support design, development, a nd testing t eams on clarifications for the requirements after \nrequirements phase  \n\u2022 Expertise in using various IBM Rational Tool : Rational Clear Case, Rational Clear Quest, Rational \nRequisite Pro  \n\u2022 Function Point Estimation  \n \nMY DELIVERABLES:  \nData Requirements Specification; Gap Analysis; User Interface Specification; Functional Requirements \nSpecification; Business Requirements Specification, Business Process Models; Use Case Model; Use Case \nNarratives; User Interface Screen Flow Diagram; Proto types ; Deployment Communication Material; Risk \nAnalysis; Issue Tracker; Decision Tables ; Coverage Matrix ; Requirements Traceability Matrix; Product \nSpecifications  \n \nDELIVERABLES CONTRIBUTED TO:  \nNon -Functional Requirements Specification; Logical Data Model; Source to Target Data Mapping \nDocuments; Business Need Analysis Document; Work Breakdown Structure; Test Data Sheet; Test Plan; \nTest Scripts; Test Strategy; Test Results Summary; Information Architecture; Project Delivera bles List; \nEstimation Work Book; Ri sk Register; Implementation Plan/ Checkout Activities.  \n \nCLIENT:  NORTHWESTERN MUTUAL LIFE INSURANCE , Milwaukee, WI, US         04/2012 - 01/2013  \nROLE:  Information Systems Business Consultant  \nSUMMARY : \nWorked as a business systems analyst in gathering requirements related to the proposed apart from \nunderstanding the nature of the current system. Played a key role in ensuring that the high -level design \ndeliverables aligned to the business needs enlisted by the business stakeholders.   \n \nRESPONSIBILITIES  \n\u2022 Analysis of existing system process functionality  \n\u2022 Analysis of the proposed business process flow and working on the proposed system process \nfunctionality based on the architecture guidelines  \n\u2022 Formulation of proposed process flows, use case diagrams and documentation of use cases.  \n\u2022 Documentation of the functional requirements for the proposed financial reports which vary across \nmultiple business areas  \n\u2022 Worked with the design teams to develop the technical specifications according to the reporting \nrequiremen ts \n\u2022 Helped the architectural and database design teams in migra ting the data from the old data base to \nthe new database  \n\u2022 Provided valuable contribution to the design of the new people soft accounting database (by \nproviding suggestions on the trees and nodes t o be created)  \n\u2022 Worked as an information system business consultant and helped to document the functional \nrequirements for the reports to be run from the new database  \n\u2022 Contributing in preparation of test strategy and test plan for the functionalities that are  expected \nfrom the new database  \nVijay Kumar Kanamarlapudi, TOGAF\u00ae, PMP\u00ae, CBAP\u00ae, ACP\u00ae, SAFe\u00ae4 POPM, SSM, PAHM, ITIL, SSBBP  7 \n \u2022 Preparation of high -level design documents with the help of the development team and conducted \nreview sessions with the business stakeholders  \n\u2022 Reporting the status of the functional specification and the high level design del iverables to the \nnecessary stakeholders  \n\u2022 Experience of working on nVision and Business Objects Reports  \n\u2022 Experience of working on reports based out of Cognos (for Business Intelligence Reporting)  \n \nMY DELIVERABLES:  \nReport Requirement Specifications; Data Requirements Specifications; High Level Design \nDocumentation; Business Process Models; Prototypes; Requirements Specifications; Deployment \nCommunication Material; End User Documentation; Gap Analysis; Issue Tracker; Risk Analysis; \nRequirements Traceability  Matrix  \n \nDELIVERABLES CONTRIBUTED TO:  \nNon -Functional Requirements Specification; Logical Data Model; Source to Target Data Mapping \nDocuments; Business Need Analysis Document; Work Breakdown Structure; Test Data Sheet; Test Plan; \nTest Scripts; Test Strategy ; Test Results Summary; Information Architecture; Project Delivera bles List; \nEstimation Work Book  \n \nCLIENT: AETNA INC., Hartford, CT, US                                                                 08 /2007 - 04/2012  \nROLE:  Business Analyst; Senior Business Analyst  \nSUMMARY:  \nWorked as a domain business analyst in all these projects and played a key role in analyzing the current \nprocess and working with the architecture team towards formulating a proposed process.  \nWorked wit h the appropriate stakeholders to deliver the requirements deliverables with good quality \nkeeping in mind the stringent timelines and compliance related constraints on the project.  \n \nRESPONSIBILITIES:  \n\u2022 Business Analysis and Requirement Gathering  \n\u2022 Preparation  of business process flow diagrams and use case model diagrams  \n\u2022 Conducting Joint Application Development (JAD) Sessions  \n\u2022 Preparation of RTM \u2014Requirement Traceability Matrix  \n\u2022 Preparation of Functional Requirement Specification Document  \n\u2022 Analysis of existing and nature of proposed process  \n\u2022 Developing process flow diagrams according to the proposed process and applications impacted  \n\u2022 Preparation of Use Case Specifications  \n\u2022 Reporting status of requirements progress for Level 1 management during project planning stage  \n\u2022 Support design, development, and testing teams on clarifications for the requirements after \nrequirements phase  \n\u2022 Expertise in using various IBM Rational Tools: Rational Clear Case, Rational Clear Quest, Rational \nRequisite Pro  \n\u2022 Worked on onsite/offshore model to  provide support to design, development and \nimplementation checkout activities.  \n \nMY DELIVERABLES:  \nRequirements Specification Document; Business Requirements Specification; Data Requirements \nSpecification; Prototypes; Issue Tracker; Use Case Narratives; Use  Case Models; Business Process \nModels; Risk Analysis; Gap Analysis; Data Scenarios; Decision Table; Business Rule Definitions; Data \nModels; Requirements Traceability Matrix; Function Point Estimation.  \n \nDELIVERABLES CONTRIBUTED TO:  \nVijay Kumar Kanamarlapudi, TOGAF\u00ae, PMP\u00ae, CBAP\u00ae, ACP\u00ae, SAFe\u00ae4 POPM, SSM, PAHM, ITIL, SSBBP  8 \n Non -Functional Requiremen ts Specification; Logical Data Model; Business Need Analysis Document; \nWork Breakdown Structure; Test Data Sheet; Test Plan; Test Scripts; Test Strategy; Test Results \nSummary; Information Architecture; Project Deliverables List, Risk Register  \n \nCLIENT:  AETNA INC. , Chennai, India                                                                         09/2006 \u2013 08/2007                  \nROLE:  Test Specialist  \nSUMMARY:  \nWorked as system integration tester, being the front runner in completing related testing, and made \nsure the deployed product is defect free. Identified design flaws and notified the project team of the \nimpacts and behavior of the system under flaws.   \n \nRESPONSIBILITIES  \n\u2022 Developed the various test artifacts, such as test plan, test scripts, test c ases, test execution logs, \ntest result summary  \n\u2022 Execution of application -oriented testing and recording the status in quality center  \n\u2022 Key resource with strong business knowledge in the corporate domain  \n\u2022 Performed complete analysis of the existing process and maintained proper repository for all the \nclarifications  \n\u2022 Conducted knowledge transfer session for explaining the design to the whole team  \n\u2022 Verification of coding for meeting the design in the given technology; performed the code review \naccording to the codin g standards  \n\u2022 Supported the team members for any technical challenges, as well as for the application challenges  \n\u2022 Well -versed in various tools used in the project such as IBM -DB2, Hummingbird Connectivity tool \nand monitoring jobs through Informatica  \n\u2022 Team management and effective team play  \n \nMY DELIVERABLES:  \nTest Data Sheet; Test Plans; Test Results Summary; Test Scripts; Test Strategy; SQA Testing Strategy and \nRequirements Summary; Coverage Matrix; Defect Tracker  \n \nDELIVERABLES CONTRIBUTED TO:  \nEstimation Wor kbook; Requirements Specification Document; Work Breakdown Structure; Use Case \nNarrative; Report Mockup  \n \n",
          "confidence": 1.0,
          "method": "full_text",
          "structured_data": null
        },
        "tax_term": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "source_by": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "designation": {
          "value": "Business Architect",
          "confidence": 0.9,
          "method": "ner",
          "structured_data": null
        },
        "experience": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        }
      }
    },
    {
      "resume_path": "data\\input\\Horace Royal - Resume.pdf",
      "confidence_score": 0.5700000000000001,
      "used_ocr": false,
      "extracted_fields": {
        "skills": {},
        "first_name": "Horace",
        "last_name": "Royal",
        "primary_email": {
          "value": "horaceroyaljr6_t3n@indeedemail.com",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "secondary_email": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "phone": {
          "value": "2402811264",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "city": {
          "value": "Washington",
          "confidence": 0.7,
          "method": "ner",
          "structured_data": null
        },
        "state": {
          "value": "DC",
          "confidence": 0.8,
          "method": "city_database",
          "structured_data": null
        },
        "zip": {
          "value": "",
          "confidence": 0.0,
          "method": "regex",
          "structured_data": null
        },
        "work_authority": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "resume_link": {
          "value": "data\\input\\Horace Royal - Resume.pdf",
          "confidence": 1.0,
          "method": "file_path",
          "structured_data": null
        },
        "raw_resume": {
          "value": "Horace Royal\nWashington, DC\nhoraceroyaljr6_t3n@indeedemail.com\n+1 240 281 1264\nWork Experience\nContract Closeout Specialist\nCTR Management\nNovember 2018 to Present\nContractor)\nEmployer: CTR Management\nClient: Department of Transportation (DOT)\n\u2022 Performs the full range of contract closeout activities pursuant to FAR 4.804, with a specialized focus on\nthe cognizant administration functions necessary to close flexibly-priced and Cost Accounting Standards\n(CAS) covered contracts.\n\u2022 Achieved closeout goals each month to efficiently and effectively eliminate the contract closeout\nbacklog.\n\u2022 Verify and coordinate closeout activities utilizing PRISM.\n\u2022 Supports the reconciliation of complex invoices.\n\u2022 Resolves issues related to de-obligated and excess funding.\n\u2022 Manages government contract billing/invoicing.\n\u2022 Closeout with a focus on Firm-Fixed Price (FFP), Labor Hour, and Time and Materials (T&M).\nContract Support Specialist\nCACI\nNovember 2018 to Present\nContractor)\nEmployer: CACI\nClient: Department of Health & Human Services (DHHS)\n\u2022 Performed closeouts of simplified acquisitions and various contract types (T&M, Labor, Cost Types).\n\u2022 Reviewed, prepared and submitted closeout documents. Input information into Contract\n\u2022 Management System. Retrieved information from various systems.\n\u2022 Processed Quick. Reviews and filled out all annual Prime Contractor Representations and\n\u2022 Worked jointly and coordinate with subsidiary company stakeholders to ensure all supporting policies\nand procedures are in-place to ensure compliance before making certification.\n\u2022 Collect, draft, review, maintain, and retained contract file documentation.\n\u2022 Draft acquisition/contract documentation for supported official(s) including requirements package\ndocumentation, Acquisition Strategy Panel (ASP) briefing slides, Acquisition Plan (AP), Business/Contract\nClearance form(s), Request for Proposal (RFP), and contract award document.\n\u2022 Pre-award functions include conducting market research, coordinating with stakeholders (e.g. Small\nBusiness Government Representative), publicizing contract actions, soliciting requirements, and drafting\nevaluation of offeror's proposal(s), and drafting awards.\n\u2022 Post-award functions include contract administration, contract actions (e.g. modification) and contract\ncloseout.\nContract Support Specialist\nPremier Management\nDecember 2016 to November 2018\nContractor)\nEmployer: Premier Management\nClient: Food drug Administration (FDA)\n\u2022 Performed all aspects of the Federal Government contract closeouts, some additional work in cradle-\nto-grave acquisitions.\n\u2022 Reconcile contract funding obligations and expenditures with final voucher amounts and determine\nde-obligated amounts.\n\u2022 Prepared contract closeout modifications using PRISM or like systems. Develops reports and prepares\nsupport documentation.\n\u2022 Performed audits of contract files ensuring adherence to policies, regulations, and identifying any\nduplication of efforts across contracts.\n\u2022 Reviewed documents for quality, clarity, and adherence to applicable regulations (FAR, agency\nregulations, etc.) and recommendation of revisions\n\u2022 Reconcile unbilled balances, excess funds, misaligned invoice payments, and related issues and\ncoordinate with the Finance Office to complete the reconciliation process for closure.\nContract Support Specialist\nCorporate Advance Auto\nDecember 2015 to March 2017\n\u2022 Managed complex contracts up to $100K for a variety of service contracts: Building & Facilities\nMaintenance, Lawn services/Grounds Maintenance, Janitorial Services,\n\u2022 Towing, and Police/Surveillance Services while performing in-depth technical market research and\nanalyzed past performance to prepare acquisition planning recommendations and cost analysis for\nDistrict Managers.\n\u2022 Drafted contracts and created, documented, and revised acquisition plans, policy procedures, and best\npractices on an ongoing basis.\n\u2022 Researched and analyzed federal contract laws and regulations.\n\u2022 Trained new personnel. Researched internally/externally to strategize and obtain the appropriate\nvendors, while performing market research to ensure vendors were given opportunity to bid. Negotiated\nwith vendors. Developed a price analysis.\nContract Support Specialist\nCavalier Consultants\nMarch 2015 to December 2015\nContractor)\nEmployer: Cavalier Consultants\nClient: Hillel Foundation\n\u2022 Researched the manufacturer of equipment or contractor.\n\u2022 Prepared provider monthly or annual service payments based on the terms of the maintenance\ncontract.\n\u2022 Negotiated bids for servicing of equipment and supply of labor.\n\u2022 Communicated with vendors as warranties were in the final phase of expiration dates for service\nof goods/ supplies Assisted Program Managers with technical issues with statement of objectives and\nperformance work statements.\n\u2022 Evaluated contract performance on previous contracts and meeting with the company representatives\nto discuss procurement needs, quality of items or services, current market prices, or delivery schedules.\nManaged several sales portfolio contract accounts totaled valued up to $250.000.00\nContract Support Specialist\nGate Gourmet\nJune 2012 to March 2015\n\u2022 Authorized and prepared with senior management staffing forecast schedules for annual budget for\ncredit cards, supplies, communications, travel and training new staff hires.\n\u2022 Developed and prepared inputs to briefings, planning documents, and policies for several airlines by\nconducting research and analysis of administrative information and data.\n\u2022 Provided sales financial analysis and support by designing graphs to present to Senior Management.\n\u2022 Completed and submitted all required documentation of existing contracts.\n\u2022 Acted as the contact person for execution of contracts.\n\u2022 Assisted in bidding sourcing teaming and subcontracting.\nContract Support Specialist\nDistrict Healthcare Inc\nJune 2006 to May 2012\n\u2022 Managed the evaluation process and for contract performance on previous contracts and organized\nkickoff- meetings with small and large businesses.\n\u2022 Created advertisements for bids, then reviewed and selected the bid most suitable to the timeframe\nand budget.\n\u2022 Researched the price of materials, labor cost, and overhead expenses to analyze the bids received.\n\u2022 Assessed whether contractors were adhering to contract parameters, laws, and regulations. Prepared\npurchase agreements, contracts, and leases to acquire the most cost-effective services and terms.\n\u2022 Compared prices, discounts, delivery dates, and handling charges. Negotiated prices and services.\nGrocery Manager\nShoppers Food Warehouse  - Clinton, MD\nMarch 1990 to January 2000\nI managed a store of 70 employees. I managed the shrink (theft,mis) lost of product. I also hired,trained\nemployees. I made the daily schedules. I ordered the products for the grocery department of dry goods.\nI did the monthly and quarterly/yearly inventory of the store. I stock the shelves as needed to keep\nproduct in store.\nEducation\nSTRAYER UNIVERSITY\n2015 to 2017\nAssociate in Business Management\nTemple Hills, MD\nSkills\n\u2022An organized professional with over 12 years of government contracting experience in both the\nfederal and private sectors. The focal-point of my expertise is in the realm of contract interpretation,\ncloseout, negotiation, and administration. In addition, I have extensive knowledge with the Federal\nAcquisitions Regulations (FAR) in which I have successfully executed complex multimillion-dollar\ncontracts for the federal government.\n\u2022Management Account Management, PSC ACQUISITIONS\n\u2022Contracting Writing System\n\u2022PD 2 Contracting Writing System\n\u2022SAM\n\u2022System for Award Management\n\u2022FPDS\n\u2022Federal Procurement Data System\n\u2022Comprizon, PD2 Contracting Writing System\n\u2022DCIS- Departmental Contracts Information System\n\u2022PRISM\n\u2022GLASS\n\u2022Contract Writing System for procurement\n\u2022DELPHI\n\u2022Finance Inventory (invoices, Billing data)\n\u2022Procurement\n\u2022Contracting\n\u2022Negotiation\n\u2022Outlook, windows 10, Microsoft,\n",
          "confidence": 1.0,
          "method": "full_text",
          "structured_data": null
        },
        "tax_term": {
          "value": "CONTRACT",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "source_by": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "designation": {
          "value": "Support Specialist",
          "confidence": 0.9,
          "method": "ner",
          "structured_data": null
        },
        "experience": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        }
      }
    },
    {
      "resume_path": "data\\input\\.Net - Data Lakes - Data Bricks - CA - Mehdi.doc",
      "confidence_score": 0.7289000000000001,
      "used_ocr": false,
      "extracted_fields": {
        "skills": {
          "domain_specific": [
            {
              "name": "engineering",
              "confidence": 0.48,
              "context": "full_text",
              "positions": [
                197,
                350,
                384,
                480,
                1340,
                1933,
                2040,
                3031,
                9997,
                17149,
                73524
              ],
              "experience_weight": 0.6000000000000001,
              "importance_score": 0.7
            }
          ],
          "cloud": [
            {
              "name": "azure",
              "confidence": 0.42,
              "context": "full_text",
              "positions": [
                556,
                1367,
                2630,
                2688,
                2761,
                2791,
                2810,
                2889,
                3169,
                3206,
                3350,
                3360,
                3413,
                3496,
                3526,
                3557,
                3686,
                3729,
                3751,
                3854,
                3871,
                3890,
                3906,
                3927,
                3941,
                3966,
                3979,
                4027,
                4101,
                4116,
                4356,
                4481,
                4506,
                4530,
                4556,
                4573,
                4589,
                4613,
                4637,
                4663,
                4680,
                4709,
                4728,
                4754,
                4774,
                4797,
                4822,
                4844,
                4861,
                4888,
                4908,
                4933,
                4949,
                5087,
                7272,
                10764,
                11565,
                13573,
                13888,
                13926,
                13955,
                14431,
                14524,
                14555,
                14605,
                14619,
                14632,
                14642,
                14663,
                14700,
                14723,
                14755,
                14785,
                14824,
                14870,
                14883,
                14907,
                14960,
                15012,
                15061,
                15091,
                18405,
                18525,
                20535,
                23618,
                23689,
                23741,
                23910,
                24021,
                24051,
                24066,
                24100,
                24170,
                24247,
                24280,
                24397,
                24522,
                24547,
                24571,
                24597,
                24614,
                24630,
                24687,
                24711,
                24737,
                24754,
                24783,
                24802,
                24828,
                24848,
                24871,
                24896,
                24918,
                24935,
                24962,
                24982,
                25007,
                25023,
                25161,
                25732,
                25896,
                26081,
                26273,
                26489,
                26551,
                26622,
                26674,
                26843,
                26954,
                26984,
                26999,
                27033,
                27103,
                27180,
                27213,
                27330,
                27455,
                27480,
                27504,
                27530,
                27547,
                27563,
                27620,
                27644,
                27670,
                27687,
                27716,
                27735,
                27761,
                27781,
                27804,
                27829,
                27851,
                27868,
                27895,
                27915,
                27940,
                27956,
                28094,
                28602,
                28862,
                31792,
                31863,
                31915,
                32084,
                32195,
                32225,
                32240,
                32274,
                32344,
                32421,
                32454,
                32571,
                32719,
                32744,
                32768,
                32794,
                32811,
                32827,
                32851,
                32875,
                32901,
                32918,
                32947,
                32966,
                32992,
                33012,
                33035,
                33060,
                33082,
                33099,
                33126,
                33146,
                33171,
                33187,
                33325,
                35975,
                38571,
                38683,
                40551,
                40852,
                44631,
                44932,
                47040,
                50469,
                51505,
                52454,
                67576,
                67981,
                68159,
                73867,
                74687
              ],
              "experience_weight": 0.4,
              "importance_score": 0.7
            },
            {
              "name": "aws",
              "confidence": 0.42,
              "context": "full_text",
              "positions": [
                562,
                1373,
                5173,
                5210,
                5370,
                5393,
                5745,
                5783,
                5803,
                5848,
                5908,
                7290,
                10384,
                10770,
                11553,
                13589,
                15177,
                15298,
                15549,
                15738,
                16147,
                16167,
                16239,
                16299,
                16377,
                18421,
                18517,
                18757,
                18780,
                18993,
                19059,
                19079,
                19124,
                19184,
                20545,
                31124,
                33734,
                33878,
                33901,
                34114,
                34152,
                34205,
                34250,
                34310,
                35947,
                38563,
                38675,
                45553,
                51517,
                67987,
                73880
              ],
              "experience_weight": 0.4,
              "importance_score": 0.7
            }
          ],
          "methodologies": [
            {
              "name": "devops",
              "confidence": 0.42,
              "context": "full_text",
              "positions": [
                1321,
                1610,
                3384,
                3563,
                7259,
                7278,
                10258,
                14625
              ],
              "experience_weight": 0.4,
              "importance_score": 0.7
            }
          ],
          "data_skills": [
            {
              "name": "hadoop",
              "confidence": 0.42,
              "context": "full_text",
              "positions": [
                2112,
                2369,
                3795,
                4379,
                5328,
                13499,
                14472,
                18700,
                24420,
                25259,
                27353,
                30754,
                32594,
                33850,
                35407,
                39704,
                42646,
                44022,
                47179,
                50193,
                52141,
                66761,
                67825,
                71267
              ],
              "experience_weight": 0.4,
              "importance_score": 0.9
            },
            {
              "name": "spark",
              "confidence": 0.42,
              "context": "full_text",
              "positions": [
                2119,
                2345,
                2463,
                2495,
                4362,
                5335,
                7959,
                11762,
                13506,
                13630,
                13636,
                14479,
                15491,
                17294,
                18165,
                18707,
                20817,
                21504,
                22916,
                24403,
                25266,
                25786,
                27336,
                28367,
                29944,
                30601,
                30761,
                32577,
                33857,
                35414,
                36882,
                37067,
                39698,
                42653,
                66755,
                67832,
                71274
              ],
              "experience_weight": 0.4,
              "importance_score": 0.9
            },
            {
              "name": "hive",
              "confidence": 0.42,
              "context": "full_text",
              "positions": [
                2173,
                2340,
                4315,
                4368,
                5341,
                8095,
                13674,
                14308,
                14501,
                15534,
                18231,
                18713,
                20953,
                24409,
                25343,
                27342,
                30828,
                31324,
                31743,
                32583,
                33863,
                35478,
                39732,
                42659,
                44038,
                47216,
                50209,
                66802,
                67871,
                71309
              ],
              "experience_weight": 0.4,
              "importance_score": 0.9
            }
          ],
          "programming": [
            {
              "name": "scala",
              "confidence": 0.42,
              "context": "full_text",
              "positions": [
                2277,
                2351,
                8128,
                8740,
                13825,
                16707,
                17022,
                18264,
                21010,
                21515,
                21754,
                22882,
                25491,
                29955,
                30194,
                34371,
                34756,
                35617,
                40086,
                43360,
                44478,
                47265,
                50735,
                66876,
                73326
              ],
              "experience_weight": 0.4,
              "importance_score": 1.0
            },
            {
              "name": "java",
              "confidence": 0.42,
              "context": "full_text",
              "positions": [
                2301,
                2357,
                5530,
                8966,
                15686,
                16565,
                25515,
                34395,
                35641,
                36302,
                36726,
                38025,
                40110,
                42153,
                43311,
                43384,
                43569,
                44502,
                47289,
                50574,
                50698,
                52854,
                53661,
                54363,
                71502,
                73134,
                73336
              ],
              "experience_weight": 0.4,
              "importance_score": 1.0
            },
            {
              "name": "sql",
              "confidence": 0.42,
              "context": "full_text",
              "positions": [
                3248,
                3947,
                3972,
                4536,
                6381,
                9143,
                9189,
                12786,
                24577,
                27510,
                32774,
                37839,
                44251,
                46908,
                47752,
                47787,
                48795,
                48903,
                49432,
                51059,
                52197,
                52830,
                53401,
                58636,
                59632,
                61117,
                61896,
                66720,
                66949,
                66991,
                69770,
                69975,
                70683,
                71095,
                71113,
                71143,
                71360,
                71366,
                71373,
                71677,
                72779
              ],
              "experience_weight": 0.4,
              "importance_score": 1.0
            }
          ]
        },
        "first_name": "Mehdi",
        "last_name": "Haghdad",
        "primary_email": {
          "value": "mhaghdad2014@gmail.com",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "secondary_email": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "phone": {
          "value": "9493931150",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "city": {
          "value": "Newport Beach",
          "confidence": 0.7,
          "method": "ner",
          "structured_data": null
        },
        "state": {
          "value": "CA",
          "confidence": 0.9,
          "method": "zip_database",
          "structured_data": null
        },
        "zip": {
          "value": "92612",
          "confidence": 0.7,
          "method": "regex",
          "structured_data": null
        },
        "work_authority": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "resume_link": {
          "value": "data\\input\\.Net - Data Lakes - Data Bricks - CA - Mehdi.doc",
          "confidence": 1.0,
          "method": "file_path",
          "structured_data": null
        },
        "raw_resume": {
          "value": "\nMehdi Haghdad\n62 Parkcrest\nNewport Beach, CA 92612\nCell phone: (949) 393-1150\nmhaghdad2014@gmail.com\n\n                                  EDUCATION\nPHD, UCLA/UNIVERSITY OF CALIFORNIA DAVIS     OBTAINED ON 6/2003\nPhD in Electrical and computer engineering (in Smart Antenna Systems for\nLow Earth Orbit (LEO) Satellites)\n\nThe Royal Institute Of Technology, Stockholm, Sweden\nM.S. Degree: Telecommunications Engineering.\nB.S. Degree: Electrical Engineering\nB.S. Degree: Computer Science.\n\n                                   SUMMARY\nI HAVE ALMOST 20 YEARS OF EXPERIENCE IN ADVANCE ENGINEERING, 10 YEARS OF\nEXPERIENCE IN BIG DATA, 8 YEARS IN CLOUD, MULTI-CLOUD, AZURE, AWS, GCP, 7\nYEARS IN ARTIFICIAL INTELLIGENCE, ML DL CNN. I HAVE WORKED FOR SOME OF THE\nMOST PRESTIGIOUS COMPANIES IN SILICON VALLEY AND OTHER LOCATIONS SUCH AS\nLOCKHEED MARTIN, SPACE SYSTEMS LORAL, MICROSOFT (4 TIMES AS A SENIOR\nSOLUTION ARCHITECT FOR DIFFERENT MULTI-BILLION DOLLAR CLIENTS), HEWLETT\nPACKARD, TEXAS INSTRUMENTS, OPTUM, ACXIOM, CITIBANK, UNITED HEALTHCARE,\nERICSSON, ABB ATOM, ADAPTEC BROAD LOGIC, DELL, ARGONAUT TECHNOLOGIES,\nBAUSCH & LOMB, DARPA, DEPARTMENT OF DEFENSE (DOD) ETC.\nI have been responsible for the development of many systems and\napplications from the designing board to the commercial release.\n\n                                  OBJECTIVE\nPRIMARILY LOOKING FOR CONSULTING AND CONTRACT WORK BUT ALSO OPEN TO\nPERMANENT POSITIONS.\nSenior solution architect Hands On!!, Senior developer, Architect, DevOps\nfor advance engineering in Mulit-Cloud (Azure, AWS and GCP) Big Data,\nArtificial Intelligence. Can also help to build team and lead projects from\nconceptual design to commercial release.\n\nSenior Big Data and Cloud Architect Consultant / BI  Solutions  Architect  /\nData Management and Cloud Lead/ DevOps\n\n\n\n                                   SKILLS\n    . 18 YEARS OF SOLID WORKING EXPERIENCE WITH A PHD FROM UNIVERSITY OF\n      CALIFORNIA UCLA/DAVIS\n    . 17 years of experience as Hands-On Solution Architect, Team Lead,\n      Specialist, Developer, doing Senior Big Data and Cloud Architect\n      Consultant / BI Solutions Architect / Data Management and Cloud Lead\n    . Cloud Architecture, Big Data Engineering, Machine Learning, Deep\n      Learning, Data Scientist, Business Intelligence (BI), Data Warehousing\n      System Engineering multi-tiered applications.\n    . 9+ years of experience in Big Data, Hadoop, Spark, MapR, Cloudera,\n      Hortonworks, Storm, Kafka Confluent, Hive, Impala, Flume, Sqoop,\n      MapReduce, Pig, HBase, NiFi, oozie, Tableau Power BI, Cloudera\n      visualization, QlikView Scala SBT (My Preference), Java Maven etc.\n    . Expert in Big Data: HDFS, Hive, Spark, Scala, Java, Python, Hadoop,\n      Sqoop, Map/Reduce, Hortonworks, Cloudera, MapR, NoSQL HBase,\n      Cassandra, Kafka Confluent, Storm, Spark Streaming, Zeppelin, Kibana,\n      Spark MLib, Mahout, R, NiFi, Falcon, Oozie, Kylin, Atlas, Drill, Solr,\n      ElasticSearch, Ambari,Ranger, Flume, Impala, Pig, HDInsight, SBT etc.\n    . 8 years of Azure cloud and Multi-Cloud Hands on!! Extensive full cycle\n      Azure Cloud experience with full Big Data, Machine Learning Deep\n      Learning, Azure Machine Learning Studio, Azure Power BI, and Azure\n      Search. Comprehensive architecture, development and deployments of\n      massive Azure infrastructures for prestigious companies including 4\n      times for Microsoft as senior solution architect. Full cycle Data Lake\n      Design, Data Engineering and Pipelines, Data Streaming, Data Fabric,\n      Big Data and Artificial Intelligence design and implementation on\n      massive scale. Used Azure Data Factory (ADF Gen1 and Gen2), Azure\n      Datalake Storage (ADLS Gen1 and Gen2), SQL/NoSQL on Cloud, HDInsight,\n      Databricks, Databricks Delta Lake, Infrastructure as Code, Cloud\n      Governance, Azure CLI, Azure PowerShell, Python, DevOps, Kubernetes,\n      Docker, AKS Azure Kubernetes, CI/CD(CICD, CI CD,CI-CD) tools, MLOps,\n      Machine Learning, Deep Learning, Azure Machine Learning Studio, Azure\n      Search, and Elasticsearch, Azure DevOps with automated build and\n      release CI/CD(CICD, CI CD,CI-CD) pipelines utilizing ARM templates,\n      Terraform, Jenkins, Ansible, Azure CLI, Automations, Runbooks, Build\n      Tools, Azure Data Lake Store, Azure Data Lake Analytics, HDInsight\n      (Managed Hadoop), Databricks, Stream Analytics, Machine Learning\n      Studio, Azure Databricks\u00b8 Azure Data Factory, Azure Data Flow, Azure\n      Data Warehouse, Azure Synapse, Azure SQL Data Warehouse (Azure SQL\n      DW), Azure Analysis Services, Event Hubs, Power BI, and Azure Data\n      Catalog, Atlas, Collibra Data Intelligence, Erwin Data Governance.\n      Azure security Azure Active Directory AAD, AD. AAD-DS. ESP, MFA, IR\n      Integration Runtime Gateway, Domain join, Apache Ranger, Ambari,\n      Advanced 4 pillar of security for HDInsight (perimeter VNET, Kerberos\n      AD ESP authentication, Hive policies Ranger and data encryption) Azure\n      Spark Hive HBase Hadoop Kafka Confluent Storm ML Services (R Server)\n      HDInsight clusters, Domain-joined HDInsight clusters, Azure Zeppelin\n      notebooks, Azure Jupyter notebooks, Azure SQL Data Warehouses, Azure\n      Databricks, Azure Data Lake, Azure Data Lake Factory, Azure Data Lake\n      Storage, Azure Data Lake Analytics, Azure Data Links, Azure\n      Integration Runtime (IR), Azure Data Gateway, Azure Kubernetes\n      Services, Azure Storage Blobs, Azure Active Directory, Azure Service\n      Principals, Azure Security Center, Azure Key Vaults, Azure Virtual\n      Network vnet, Azure Log Analytics, Azure Network Interfaces, Azure\n      Cosmos DB, Azure Cortana Intelligence Suite, Infrastructure as a\n      Service IaaS, Platform as a Service PaaS, Microsoft R Server, NLB, Key\n      phrase extraction Azure search, Unstructured text analytics, Event\n      hub, Streaming, Poly Base etc.\n    . 8 years of AWS Cloud Extensive full cycle Cloud AWS Redshift, RDS,\n      EMR, Kinesis, S3, Glue, DMS, Athena, EC2, Lambda, experience with full\n      Big Data, Amazon Elastic MapReduce (EMR), Hadoop, Spark, Hive, Pig,\n      Kafka Confluent, MSK, AWS Management Console, AWS CLI, Amazon EMR File\n      System (EMRFS); Comprehensive CDK and CloudFormation experience with\n      writing script in TypeScript, JavaScript, Python, Java, and C#.\n      Collaborative notebooks Apache Zeppelin, Jupyter, deep learning\n      frameworks like Apache MXNet, RDS Aurora MySQL PostgreSQL,\n      Elasticsearch and SOLR, Machine Learning and Deep Learning development\n      and deployment. AWS Compute E2C, FarGate, Lambda, VMware, AWS\n      Developer Tools, AWS Management Tools, Amazon Machine Learning, AWS\n      DeepLens, Amazon Deep Learning AIMs, Amazon TensorFlow on AWS and\n      other components etc.\n                    .\n    . 5 years of Google Cloud (GCP) Architected and implemented multiple\n      massive projects for different companies Hands On!! Including Big\n      Data, Artificial Intelligence, SOLR on Kubernetes indexing from\n      Dataproc directly. Used among others Cloud Dataproc, BigQuery, Cloud\n      Dataflow, Cloud Data Fusion, Cloud Dataprep, Data Catalog, Google\n      Kubernetes Engine (GKE), Kubernetes CLusters, Dataproc CLusters,\n      Container Registry, Deep Learning Containers, Cloud Bigtable, Cloud\n      SQL, Firebase Realtime Database. For artificial intelligence used AI\n      building blocks, Text-to-Speech, Speech-to-Text, AutoML, Vision AI,\n      Cloud Natural Language, Video AI, AI Platform, AI Hub and AI Platform\n      Deep Learning VM Image etc.\n    . 7 years of Machine Learning, Deep Learning and Artificial Intelligence\n      MLlib, TensorFlow, Keras, Weka Mahout, Multilayer perceptron\n      classifier (MLPC), the feedforward artificial neural network,\n      Convolutional Neural Network CNN, scikit-learn, Pandas,\n      Deeplearning4j, H2o, Sparkling Water ML, Caffe2, MxNet etc. Different\n      algorithms K-Means, Random Forest, Gradient Boosting algorithms (GBM,\n      XGBoost and CatBoost) etc.\n    . 9 years of search engines ELK Stack Elasticsearch, Logstash, Kibana,\n      Filebeat, SOLR from early versions until 8.4.2, SOLR on Kubernetes on\n      GCP!!, Lucene, Rsync, Tika. Also been involved with migration from\n      SOLR to Elasticsearch for at least three companies etc.\n    . Expert in DevOps using Azure DevOps, VSTS, AWS-CodePipeline,\n      Terraforms, Jenkins, Ansible, Git, Maven, Cloudera Navigator, Data\n      Lineage, Kubernetes, Docker.\n    . 5 years of Solid Kubernetes, Orchestration and Micro Services\n      experience Distributed Container based Architecture, Docker, Docker\n      CLI, Kubernetes, Kubernetes CLI, kubeflow, Kube-scheduler, Pods, Pods\n      deployments. Service Deployments, Ingress, Helm Charts, Helm Charts\n      CLI, YAML etc. Successfully installed and deployed for several\n      companies huge Kubernetes Cluster (the latest with more than 2000\n      nodes 16,000 CPUs deployed in 20 minutes) with both Data Pipeline,\n      Data Lake (Raw Data Landing Zone, Processing Zone, Consumption Zone)\n      for Machine Learning, Convolutional Neural Network CNN, Spark on\n      Kubernetes, HDFS on Kubernetes, Kafka Confluent on Kubernetes,\n      Elasticsearch Logstash Kibana ELK on Kubernetes, NiFi on Kubernetes,\n      Hive HBase Jupyter Zeppelin with Scala and Python on Kubernetes. Used\n      Vagrant Terraform HashiCorp for deployment with 2000 nodes, 16,000\n      CPUs with Peta Bytes and 150TB / day capacity etc.\n    . 3 years of IBM Cloud and IBM Cloud Private (ICP) Distributed Container\n      based Architecture, Docker, Docker CLI, Kubernetes, Kubernetes CLI,\n      Pods, Pods deployments. Service Deployments, Ingress, Helm Charts,\n      Helm Charts CLI. Successfully installed and deployed an entire IBM\n      Cloud Private ICP Cluster then implemented and deployed ELK\n      Elasticsearch, Logstash, Kibana, Filebeat, Kafka Confluent, Zookeeper,\n      Cassandra, Curator on ICP IBM Private Cloud, Kubernetes, Pods using\n      Helm Charts, Scala SBT.\n    . 16 years of hands on .NET development, architecture and management\n      experience in application, real time, instrumentation, web, front end,\n      back end, full stack, multiple products out there multiple awards\n    . 16 years of hands on Java development, architecture, front end, back\n      end, full stack\n    . 6 years of Android mobile development and architecture with multiple\n      apps in the app store\n    . 16 years of experience in SQL 7-2016, MySQL, Oracle, and other\n      databases T-SQL, SSIS, SSRS, SSAS, OLTP, OLAP, Multidimensional Cube,\n      MDX, PowerPivot, Tabular Model, SharePoint, PerformancePoint.\n    . Demonstrated experience and understanding of the best practices in all\n      aspects of data modeling, data warehousing (Inmon/Kimball approach).\n      Solid experience in Data Warehouse\n    . Strong knowledge and proven results in Data Warehouse and Data Mart\n      design including Dimensional Modeling (Star & Snowflake Schemas), ER\n      Modeling, 3 Normal Forms, Normalization and Demoralization, Logical\n      Model and Physical Model, Fact/Dimension/Hierarchy identifications.\n    . From Business Case to Data Visualization, I have designed and\n      developed solutions by combining Business Process with Information\n      Technology.\n    . Firmware embedded programming, ARM, PIC, DSP, FPGA, RTOS Linux etc.\n    . Significant management experience including 4 years as the VP of\n      engineering\n\n                                 EXPERIENCE\nMICROSOFT, NEW YORK, NEWPORT BEACH CALIFORNIA      3/2019-PRESENT\nThis was my 4th contract with Microsoft as a high level solution architect\nand expert. Please note I am always hands on, always in addition to doing\nthe architecture I do coding, do DevOps, and do my own developments and\nPOCs. This was a complex hybrid multi projects with Kubernetes, Big Data,\nMulti-cloud (Azue, AWS and GCP), MapR, Kafka, Confluent, SOLR,\nElasticsearch etc. It was for an important Microsoft client, a multibillion\ndollar company in New York with Petabytes of data, and 10 terabyte\nstreaming and data ingestion a day:\n\nI was involved with multiple Kubernetes projects, a hybrid of MapR and\nmulti-cloud system with a massive MapR cluster (with 1400 note 16000 CPUs)\nand Multi-Cloud project (Azure AWS and GCP) with cloud Migration, Kafka\nConfluent streaming using Kappa with massive amount of data in Petabytes\nwith real time streaming using Kappa architecture. This project among\nothers included a comprehensive Data Pipeline, multiple Data Lakes (Landing\nZone, Processing Zone and Consumption zone), high level of domain Join\nsecurity, Big Data Visualizations, Machine Learning, Deep Learning (CNN\nRNN). The architecture that we created is similar to Uber Gen-4\narchitecture with Hudi capable of processing tens of petabytes of streaming\ndata. Please note that due to confidentiality and sensitivity I cannot and\nwill not reveal specific technical details.\n\n    . I lead, architected and helped developing multiple Kubernetes projects\n      on multiple large Kubernetes clusters. Initially we created POCs on\n      AWS (EKS) and Azure (AKS) but finally decided to use the Google\n      Kubernetes Engine (GKE) in GCP for a number of reasons (more than\n      welcome to ask me why in the interview). We created multiple clusters\n      and deployed HDFS Spark in Kubernetes (up to 1000 pods), Kafka\n      Confluent in Kubernetes (created both but decided on using pure Kafka\n      up to 30 pods brokers in Kubernetes), Zookeeper in Kubernetes (up to\n      30 pods ZK), a comprehensive SOLR cluster in Kubernetes (50 SOLR\n      pods), Elastic Search in Kubernetes (50 pods Elasticsearch) and other\n      orchestrations. Please note that due to the StatefulSet nature, these\n      projects were fairly complex using headless services for HDFS, Kafka,\n      Zookeeper, SOLR, Elasticsearch etc however after deployments they were\n      very reliable, scalable. For example with one replica command change\n      in Helm Chart it would go from 5 node to 30 brokers Kafka in minutes.\n      Could index from inside the dataproc into the SOLR kubernetes in a\n      lightning speed. Used other component of the GCP among others Cloud\n      Dataproc, BigQuery, Cloud Dataflow, Cloud Data Fusion, Cloud Dataprep,\n      Data Catalog, Google Kubernetes Engine (GKE), Kubernetes CLusters,\n      Dataproc CLusters, Container Registry, CI/CD (CICD, CI CD,CI-CD)\n      pipelines, Deep Learning Containers, Cloud Bigtable, Cloud SQL,\n      Firebase Realtime Database. For artificial intelligence used AI\n      building blocks, Text-to-Speech, Speech-to-Text, AutoML, Vision AI,\n      Cloud Natural Language, Video AI, AI Platform, AI Hub and AI Platform\n      Deep Learning VM Image etc.\n    . I lead, architected and helped developing a massive MapR cluster with\n      1400 note 16000 CPUs. A comprehensive Data Pipeline with data\n      ingestions from variety of sources RDBMS, hardware logs, images,\n      streaming data, files, documents into the Data Lake. Using Confluent,\n      Kafka Confluent both inside Kubernetes and outside, Kappa architecture\n      with schema evolution, Avro, Parquet (similar to Uber Gen 4\n      architecture) we were able to ingest tens of terabytes of data a day\n      like a charm. The Big Data part used Hadoop, Spark, and Kafka\n      Confluent on mainly MapR but also interacting with Azure HDInsight,\n      AWS EMR, and Kubernetes Cluster. Used also, Spark, Spark Streaming,\n      Storm, Kafka Confluent, Hive, Pig, Impala, Flume, Sqoop, MapReduce,\n      Pig, HBase, NiFi, oozie, Tableau Power BI and, Oozie, QlikView etc.\n      Most of the code were written in Python, PySpark and Scala.\n    . I lead, architected and helped developing a comprehensive Azure\n      infrastructure working with ADF Azure Data Factory gen2, ADLS Azure\n      Data Lake gen2, Data Catalog, Databricks, Delta Lake, Delta tables,\n      Databricks Staging and Merge! Kubernetes, HDInsight, HDInsight\n      Monitoring 4.0, ESP. AD. AAD-DS. ESP, MFA, IR Integration Runtime\n      Gateway, CI/CD (CICD, CI CD,CI-CD) pipelines, Domain join, Apache\n      Ranger, Ambari, Advanced 4 pillar of security for HDInsight (perimeter\n      VNET, Kerberos AD ESP authentication, Hive policies Ranger and data\n      encryption), Advanced ADLS data lake structures, advance Machine\n      learning and CNN, ARM templates, Azure Runbook, notebooks Zeppelin,\n      Jupyter,, Hadoop, Spark, Kafka Confluent, Hive, Hbase. For AI used\n      Azure Machine Learning Service, Azure Machine Learning Studio. For\n      monitoring used Azure Monitor, Azure DevOps, Azure CLI, Azure\n      PowerShell, and Azure Automation. For security I used Azure Active\n      Directory, Azure Role Based Access Control, Azure Subscription\n      Management Azure RBAC, Multi-Factor Authentication, Azure Active\n      Directory Domain Services AAD-DS, Azure Policy, Azure Service\n      Principal, Azure Keys and Key Vault, Enterprise Security Package Azure\n      ESP. For Kubernetes and Micros Services I used Azure Kubernetes\n      Service (AKS), Service Fabric Mesh, Azure Container Instances and\n      Azure Container Registry etc.\n    . I lead, architected and helped developing a comprehensive AWS\n      infrastructure with: Redshift (also tested POC in Snowflake Data\n      Warehouse and Snowpipe), RDS, EMR, MSK (Kafka Confluent), S3 AWS\n      Compute E2C, Glue, DMS, Athena, EC2, RDS Aurora MySQL PostgreSQL,\n      Elasticsearch, Lambda projects with full Big Data, Amazon Elastic\n      MapReduce (EMR), CI/CD (CICD, CI CD,CI-CD) pipeline, CloudFormation\n      CFN,Spark, Kafka Confluent, Oozie, Sqoop, NiFi, Pig, Hive, Hbase, MSK,\n      AWS CLI, Amazon EMR File System (EMRFS). Comprehensive CDK and\n      CloudFormation experience with writing script in TypeScript,\n      JavaScript, Python, Java, and C#. Collaborative notebooks Zeppelin,\n      Jupyter. AWS networking used VPC, Private Subnet, Public Subnet,\n      Internet Gateway, Routing Security groups etc. Visualization and\n      analysis used QuickSight and CloudSearch. Also created a Kubernetes\n      cluster using Elastic Container Service for Kubernetes (EKS), App\n      Mesh, EC2 Container Service (ECS), FarGate. For AI used deep learning\n      frameworks like Apache MXNet Machine Learning and Deep Learning\n      development and deployment, VMware, AWS Developer Tools, AWS\n      Management Tools, Amazon Machine Learning, SageMaker, Alexa Skills\n      Kit, AWS DeepLens, Amazon Deep Learning AMIs, Amazon TensorFlow on AWS\n      and other components. For security used Identity and Access Management\n      (IAM), AWS Organizations, Multi-Factor Authentication. etc.\n    . I lead, architected and helped developing multiple  CDK and\n      CloudFormation projects with writing script in TypeScript, JavaScript,\n      Python and Java.\n    . I lead, architected and helped developing deep learning, machine\n      learning and Convolutional Neural Networks (CNN) using Python, R,\n      PySpark and Scala libraries like scikit-learn, Pandas, Deeplearning4j,\n      Sparkling Water ML, Caffe2, MxNet etc. Different algorithms were used\n      like K-Means, Random Forest, Gradient Boosting algorithms (GBM,\n      XGBoost, XGBoost and CatBoost). Also used GPUs especially with\n      Convolutional Neural Networks CNNs with TensorFlow, Keras.\n    . Development in, Scala, SBT, Eclipse, Python, Pyspark, R, Zeppeline,\n      Jupyter\n\nEricsson, Santa Clara, California 8/2018-3/2019\nDesigned and developed data engineering solutions as a senior Hands-On\nSolution Architect Consultant, Big Data, Data Engineer, Data Scientist,\nsenior developer, Data Warehousing, Spark, HDFS, Kafka Confluent, BI,\nKubernetes, IBM Cloud and IBM Cloud Private (ICP), advanced Machine\nLearning Deep Learning and Convolutional Neural Networks CNN. I am one of\nthe main Architects of GAIA (Ericsson Global Artificial Intelligence\nAccelerator) a $75,000,000 / year new department intended to use Artificial\nIntelligence ML, DL, CNNs to revolutionize cellular communication\nespecially for the G5. Working as senior Solution Architect Consultant,\nsenior developer, senior data engineer, senior data scientist. Implemented\n2000 node 16,000 CPUs Kubernetes cluster and implemented variety of Machine\nLearning, Deep Learning and Convolutional Neural Network CNN. The\nKubernetes Cluster that I created had Data Pipeline, Data Lake (Raw Data\nLanding Zone, Processing Zone, Consumption Zone) with the capacity of Peta\nBytes 150TB /Day Data for Machine Learning, Convolutional Neural Network\nCNN, Spark, HDFS, Kafka Confluent, Elasticsearch Logstash Kibana ELK, NiFi,\nHive, HBase, Jupyter Zeppelin with Scala and Python. Also implemented\nadvanced Machine Learning, Deep Learning, and Convolutional Neural Networks\nCNN and deployed massive POCs on Azure Cloud, GCP, AWS and OpenStack with\nimpressive results.\n\n    . Migrated and architected a Multi-Cloud solution using AWS and Azure\n      using resources such as Redshift, RDS, EMR, Kinesis, S3, Glue, DMS,\n      Athena, EC2, Lambda projects with full Big Data, CI/CD (CICD, CI CD,CI-\n      CD) pipelines, Amazon Elastic MapReduce (EMR), Hadoop, Spark, Hive,\n      Pig, Kafka Confluent, VPC, Subnet, Gateway, AWS Management Console,\n      AWS CLI, Amazon EMR File System (EMRFS), collaborative notebooks\n      Apache Zeppelin, Jupyter, deep learning frameworks like Apache MXNet,\n      Elasticsearch and SOLR, Machine Learning and Deep Learning development\n      and deployment. AWS Compute E2C, RDS Aurora MySQL PostgreSQL, FarGate,\n      Lambda, VMware, AWS Developer Tools, AWS Management Tools, Amazon\n      Machine Learning, AWS DeepLens, Amazon Deep Learning AIMs, Amazon\n      TensorFlow on AWS and other components. etc.\n\n   In a course of less than a year, I have architected and lead some of the\n      most sophisticated Big Data, Deep learning, Machine Learning, and\n      Convolutional Neural Network CNN in the nation for Ericsson Artificial\n      Intelligence Accelerator (Ericsson GAIA) in Santa Clara California.\n      For the first time the technologies were right to create a\n      comprehensive 2000 nodes 16,000 CPUs cluster on Kubernetes with all\n      necessary micro services with automatic orchestration with dynamic\n      deployment for a petabytes Artificial Intelligence system. Some of the\n      results were are impressive. Examples:\n\n\n    . Machine Learning, Deep Learning, Convolutional Neural Network CNN,\n      Anomaly detection on massive amount of real time streaming 5G wireless\n      data with 150TB /day live with high accuracy using the 2000 nodes\n      16,000 CPUs Kubernetes cluster. This would have taken years of\n      processing in the past and is now feasible in minutes on live data.\n    . High accuracy at detecting hack attack, security breach and data\n      breach on live data using CNNs\n    . High accuracy at predicting system availability and reliability and\n      predicting anomalies.\n\n\n***Please note that due to the sensitivity and proprietary nature of these\nprojects and since I am one of the main architect I cannot and will not\nreveal and will not go into too much details!!!\n\n\n    . Created massive Kubernetes clusters on OpenStak, Azure, GCP, AWS with\n      micro service Successfully installed and deployed clusters with more\n      than 2000 nodes 16,000 CPUs deployed in 20 minutes with both Data\n      Pipeline, Data Lake (Raw Data Landing Zone, Processing Zone,\n      Consumption Zone) for Machine Learning, Convolutional Neural Network\n      CNN, Spark on Kubernetes, HDFS on Kubernetes, Kafka Confluent on\n      Kubernetes, Elasticsearch Logstash Kibana ELK on Kubernetes, NiFi on\n      Kubernetes, Hive, HBase, kubeflow, Kube-scheduler, Jupyter Zeppelin\n      with Scala and Python on Kubernetes. Used Vagrant Terraform HashiCorp\n      for deployment with 2000 nodes, 16,000 CPUs with Peta Bytes and 150TB\n      / day capacity.\n    . I lead, architected and helped developing deep learning, machine\n      learning and Convolutional Neural Networks (CNN) systems for 5G\n      wireless data anomaly detection, system availability, Hack attack,\n      Security breach, data breach detection and protection. For peak\n      performance a distributed architecture were created using 16,000 CPUs\n      with amazing results using Spark, HDFS, Scala, SBT, MLlib, TensorFlow,\n      Keras. Some of the development were based on Multilayer perceptron\n      classifier (MLPC) which is a classifier based on the feedforward\n      artificial neural network. Also created other prototypes using Python,\n      R, PySpark and Scala libraries like scikit-learn, Pandas,\n      Deeplearning4j, Sparkling Water ML, Caffe2, MxNet etc. Different\n      algorithms were used like K-Means, Random Forest, Gradient Boosting\n      algorithms (GBM, XGBoost, XGBoost and CatBoost). Also used GPUs\n      especially with Convolutional Neural Networks CNNs with TensorFlow,\n      Keras.\n    . I used TensorFlow, Keras and create similar to YOLO type Convolutional\n      Neural Network on GPUs for detection of different type of anomalies on\n      wireless data. I used the conversion of data to pictures and run it\n      through the Neutral network with amazing results.\n    . I used Acumos AI platform for certain type of ML and CNN projects with\n      cascading Convolutional Neural Networks. I attempted to create an\n      Artificial Intelligence AI environment to facilitate cascading\n      Convolutional Neural Network using.\n    . Created Data Pipeline with real time, intermediate and permanent\n      repositories on Kubernetes. By using huge Kafka Confluent clusters\n      with huge partitions inside Kubernetes, the data was gathered in a\n      Round-robin fashion from variety of sources including real hardware,\n      routers, radios, etc. and was brought into the real time repositories.\n      Using Scala SBT Python Jupyter Zeppelin, Spark, Kafka Confluent, NiFi,\n      ELK, Kubeflow, Kube-scheduler etc. the capacity of the Data Pipeline\n      was 150TB /Day but could be easily extended by simple Kubernetes\n      Orchestration scripts.\n    . Created a Petabytes Data Lake with Raw Data Landing Zone, Processing\n      Zone and Consumption Zone. The Data Lake was specifically designed for\n      ease of use for Artificial Intelligence and therefore the Data\n      Scientist were able to directly access different type of data from\n      real time to intermediate to permanent Data from the Data Lake's\n      Consumption Zone for different type of anomaly detection. The\n      processing in the Data Lake was using Delta and Flip to ensure that\n      the data is accessible at any time even during the processing.\n    . Azure Cloud, I architected, led and did actual implementation of\n      massive Azure Cloud project with extensive full cycle Cloud Azure\n      experience covering the Data Ingestion, Data Transformation and Data\n      Consumption with Machine Learning Deep Learning. This was a massive\n      project on a large scale on Azure which covered a full range of areas\n      including but not limited to Big Data, Machine Learning Deep Learning,\n      Azure Machine Learning Studio, Azure Power BI, Azure Search, and\n      Elasticsearch on Azure development and deployment. Comprehensive\n      deployments of massive Azure infrastructures from inside the Visual\n      Studio 2017 using ARM Templates, Azure Runbook. Deployment of many\n      Azure projects for different companies and here are some of\n      practical!! Hands on!! Component that I have personally used, Azure\n      Spark Hive HBase Hadoop Kafka Confluent Storm ML Services (R Server)\n      HDInsight clusters, Domain-joined HDInsight clusters,, Azure Zeppelin\n      notebooks, Azure Jupyter notebooks, Azure SQL Data Warehouses, Azure\n      Databricks, Azure Data Lake, Azure Data Lake Factory CI/CD (CICD, CI\n      CD,CI-CD) pipelines, Azure Data Lake Storage, Azure Data Lake\n      Analytics, Azure Data Links, Azure Integration Runtime (IR), Azure\n      Data Gateway, Azure Kubernetes Services, Azure Storage Blobs, Azure\n      Active Directory, Azure Service Principals, Azure Security Center,\n      Azure Key Vaults, Azure Virtual Network vnet, Azure Log Analytics,\n      Azure Network Interfaces, Azure Cosmos DB, Azure Cortana Intelligence\n      Suite, Infrastructure as a Service IaaS, Platform as a Service PaaS,\n      Microsoft R Server, NLB, Key phrase extraction Azure search,\n      Unstructured text analytics, Event hub, Streaming, Poly Base etc..\n    . The Big Data part used Hadoop, Spark on mainly MapR but also Cloudera\n      and Hortonworks, Storm, Kafka Confluent, Hive, Pig, Impala, Flume,\n      Sqoop, MapReduce, Pig, HBase, NiFi, oozie, Tableau Power BI and\n      Cloudera visualization, QlikView etc.\n    . Development languages, Extensive Scala SBT (My Preference), Java\n      Maven, Eclipse Intellij (my preference), Python, R, Jupyter, PySpark,\n      Ruby and even some, Linux Shell Script, Shell Scripts\n\nMicrosoft, New York, California   3/2018-8/2018\nSenior Solution Architect Consultant, senior developer, Azure Big Data,\nETL, various databases, Data Warehousing, Spark, Databricks, HDInsight, BI,\nadvanced Machine Learning Deep Learning. I implemented, architected and a\nmassive Azure Cloud infrastructure with big data, machine learning, deep\nlearning, and Artificial Intelligence Neural Network\n\n   This was a project with Microsoft and I worked as Microsoft expert on\n      Azure in New York, other companies involved were Pragmatic Works and\n      Selective in New York. I was involved from the designing board all the\n      way to complete implementation and production release. Azure despite\n      simplicity has enormous amount of details. Many engineers that I\n      interviewed and some of whom that I worked with, may have known bits\n      and pieces but actually creating clusters and implementing systems on\n      Azure require substantial experience and know how, I have that.\n    . Azure Cloud, I architected, led and did actual implementation of\n      massive Azure Cloud project with extensive full cycle Cloud Azure\n      experience covering the Data Ingestion, Data Transformation and Data\n      Consumption with Machine Learning Deep Learning. This was a massive\n      project on a large scale on Azure which covered a full range of areas\n      including but not limited to Big Data, Machine Learning Deep Learning,\n      Azure Machine Learning Studio, Azure Power BI, Azure Search, and\n      Elasticsearch on Azure development and deployment. Comprehensive\n      deployments of massive Azure infrastructures from inside the Visual\n      Studio 2017 using ARM Templates, Azure Runbook. Deployment of many\n      Azure projects for different companies and here are some of\n      practical!! Hands on!! Component that I have personally used, Azure\n      Spark Hive HBase Hadoop Kafka Confluent Storm ML Services (R Server)\n      HDInsight clusters, Domain-joined HDInsight clusters, Azure Zeppelin\n      notebooks, Azure Jupyter notebooks, Azure SQL Data Warehouses, Azure\n      Databricks, Azure Data Lake, Azure Data Lake Factory, CI/CD (CICD, CI\n      CD,CI-CD) pipelines, Azure Data Lake Storage, Azure Data Lake\n      Analytics, Azure Data Links, Azure Integration Runtime (IR), Azure\n      Data Gateway, Azure Kubernetes Services, Azure Storage Blobs, Azure\n      Active Directory, Azure Service Principals, Azure Security Center,\n      Azure Key Vaults, Azure Virtual Network vnet, Azure Log Analytics,\n      Azure Network Interfaces, Azure Cosmos DB, Azure Cortana Intelligence\n      Suite, Infrastructure as a Service IaaS, Platform as a Service PaaS,\n      Microsoft R Server, NLB, Key phrase extraction Azure search,\n      Unstructured text analytics, Event hub, Streaming, Poly Base etc.\n\n\n\nOptum United Healthcare, Santa Ana (partially in Minneapolis), California\n8/2016-3/2018\nLead, Senior Solution Architect Consultant, senior developer, Big Data,\nData Engineer, Data Scientist, Data Warehousing, Spark, HDFS, Kafka, BI,\nKubernetes, IBM Cloud and IBM Cloud Private (ICP), advanced Machine\nLearning Deep Learning on the cutting edge of the Genome, Berkeley Amplab\nAdam Genomics, GATK. Elasticsearch SOLR. Also implemented and deployed a\nmassive Azure Cloud infrastructure with big data, machine learning, deep\nlearning, and artificial intelligence convolutional neural network CNN.\n\n   In a course of one year, I have architected and lead some of the most\n      sophisticated Big Data, Deep learning and Machine Learning, Azure\n      Cloud projects in the nation for Optum in Minneapolis and California.\n      I have had access and utilized thousands of servers, 12,000 CPUs,\n      enormous amount of memories and the results has been astonishing\n      beyond even my own and everybody's expectations. Examples:\n\n\n    . Genomic analysis for prediction of various cancers on 3000 known\n      samples from our genomic bank, 12,000 CPUs, enormous amount of\n      memories the processing time were reduced from 46.7 years to 22\n      minutes 47 seconds! with 99% prediction accuracy, this was recently\n      presented at a conference.\n    . 4 billion records with 53 pre and post processing queries, reduced\n      from days to under a minute!\n\n\n   Please note that due to the highly proprietary and sensitive nature of\n      these projects, I will not be able nor will I disclose the technical\n      details.\n\n\n    . I lead, architected and helped developing deep learning and machine\n      learning systems for genetic analysis and prediction system for\n      occurring of different type of cancers with more than 99% accuracy.\n      For peak performance a distributed architecture were architected using\n      12000 CPUs with amazing results. Spark, HDFS, Scala, SBT, MLlib,\n      TensorFlow, Keras. Some of the development were based on Multilayer\n      perceptron classifier (MLPC) which is a classifier based on the\n      feedforward artificial neural network. Also created other prototypes\n      using Python, R, PySpark and Scala libraries like scikit-learn,\n      Pandas, Deeplearning4j, Sparkling Water ML, Caffe2, MxNet etc.\n      Different algorithms were used like K-Means, Random Forest, Gradient\n      Boosting algorithms (GBM, XGBoost, XGBoost and CatBoost)\n    . For hardware processing of the Big Data, the Deep Learning and Machine\n      Learning different architectures were tested on IBM Neteeza, Teradata\n      and distributed architecture with 12000 CPUs and memory (Spark). The\n      result were absolutely clear, there is no comparison the distributed\n      architecture is far more superior and is the future!\n    . The Big Data part used Hadoop, Spark on mainly MapR but also Cloudera\n      and Hortonworks, Storm, KafKa, Hive, Pig, Impala, Flume, Sqoop,\n      MapReduce, Pig, HBase, NiFi, oozie, Tableau Power BI and Cloudera\n      visualization, QlikView etc.\n    . I lead, architected and helped developing a hybrid system of\n      Elasticsearch and HDFS using Logstash, Rsync and Kafka. The\n      Elasticsearch was sharded over 25 nodes but later deployed on AWS.\n      Three type of data were indexed and inputted into the Elasticsearch\n      (Kibana):\n    o The hardware and system logs for real time (Kafka) hardware and system\n      monitoring\n    o The patient and claim data from HDFS, Hive and HBase for search and\n      quick BI visualization in Kibana.\n    o Data Export files from MarkLogic\n    o For basic search used by different system via an API on top of the\n      Elasticsearch\n    . I lead, architected and helped developing a massive data ingest system\n      from different providers with a permanent and real time pipeline\n      (Kafka) using Delta and Flip methods for an ongoing uninterrupted data\n      ingest. The source data were RDBMS, Hive, HBase, MarkLogic, flat data\n      and log files etc.\n    . Azure Cloud, I architected, led and did actual implementation of\n      massive Azure Cloud project with extensive full cycle Cloud Azure\n      experience covering the Data Ingestion, Data Transformation and Data\n      Consumption with Machine Learning Deep Learning. This was a massive\n      project on a large scale on Azure which covered a full range of areas\n      including but not limited to Big Data, Machine Learning Deep Learning,\n      Azure Machine Learning Studio, Azure Power BI, Azure Search, and\n      Elasticsearch on Azure development and deployment. Comprehensive\n      deployments of massive Azure infrastructures from inside the Visual\n      Studio 2017 using ARM Templates, Azure Runbook. Deployment of many\n      Azure projects for different companies and here are some of\n      practical!! Hands on!! Component that I have personally used, Azure\n      Spark Hive HBase Hadoop Kafka Storm ML Services (R Server) HDInsight\n      clusters, Domain-joined HDInsight clusters, CI/CD (CICD, CI CD,CI-CD)\n      pipelines, Azure Zeppelin notebooks, Azure Jupyter notebooks, Azure\n      SQL Data Warehouses, Azure Databricks, Azure Data Lake, Azure Data\n      Lake Factory, Azure Data Lake Storage, Azure Data Lake Analytics,\n      Azure Data Links, Azure Integration Runtime (IR), Azure Data Gateway,\n      Azure Kubernetes Services, Azure Storage Blobs, Azure Active\n      Directory, Azure Service Principals, Azure Security Center, Azure Key\n      Vaults, Azure Virtual Network vnet, Azure Log Analytics, Azure Network\n      Interfaces, Azure Cosmos DB, Azure Cortana Intelligence Suite,\n      Infrastructure as a Service IaaS, Platform as a Service PaaS,\n      Microsoft R Server, NLB, Key phrase extraction Azure search,\n      Unstructured text analytics, Event hub, Streaming, Poly Base etc..\n    . Genome Analysis Toolkit 4 (GATK4) from Broad Institute, ADAM Genomics\n      Berkeley AMPLab, BAM, SAM, VCF genome variant. Worked also with mango,\n      gnocchi, deca, avocado, quinine, cannoli etc\n    . Using SOLR / Elasticsearch created a detail analytical graphical\n      dashboard in Kibana for Patient Data, Claim Data and Provider Data.\n    . I architected and led multiple AWS projects, Redshift, RDS, EMR,\n      Kinesis, S3, Glue, DMS, Athena, EC2, Lambda, with full Big Data,\n      Amazon Elastic MapReduce (EMR), Hadoop, Spark, Hive, Pig, Kafka, AWS\n      Management Console, AWS CLI, Amazon EMR File System (EMRFS),\n      collaborative notebooks Apache Zeppelin, Jupyter, deep learning\n      frameworks like Apache MXNet, Elasticsearch and SOLR, Machine Learning\n      and Deep Learning development and deployment. AWS Compute E2C,\n      FarGate, Lambda, VMware, AWS Developer Tools, CI/CD (CICD, CI CD,CI-\n      CD) pipelines, AWS Management Tools, Amazon Machine Learning, AWS\n      DeepLens, Amazon Deep Learning AIMs, Amazon TensorFlow on AWS and\n      other components. etc.\n    . Development languages, Extensive Scala SBT (My Preference), Java\n      Maven, Eclipse Intellij (my preference), Python, R, Jupyter, PySpark,\n      Ruby and even some, Linux Shell Script, Shell Scripts\n    . Successfully installed and deployed an entire IBM Cloud Private ICP\n      Cluster then implemented and deployed ELK Elasticsearch, Logstash,\n      Kibana, Filebeat, Kafka, Zookeeper, Cassandra, Curator on ICP IBM\n      Private Cloud, Kubernetes, Pods using Helm Charts, Scala SBT. IBM\n      Cloud and IBM Cloud Private (ICP) is a Distributed Container based\n      Architecture, Docker, Docker CLI, Kubernetes, Kubernetes CLI, Pods,\n      Pods deployments. Service Deployments, Ingress, Helm Charts, Helm\n      Charts CLI.\n\n\nOneStop, El Segundo, California   2/2016-8/2016\nSenior lead, Senior Solution Architect Consultant, senior developer, Big\nData, Data Warehousing, BI, SOLR, Lucene, Elasticsearch, Mahout, Weka\nMachine Learning Lead.\n   I was initially hired at OneStop because of similar experience I had from\n      Dell and Microsoft in Big Data, SOLR Elasticsearch and Machine\n      Learning, taxonomy etc.\n    . I lead, architected and helped developing a Big Data system with 256\n      nodes using Hadoop, Spark on Hortonworks and later migrated to\n      Cloudera, Storm, KafKa, Hive, Pig, Impala, Flume, Sqoop, MapReduce,\n      Pig, HBase, oozie, Tableau Power BI and Cloudera visualization,\n      QlikView\n    . Development languages, extensive Scala SBT (my preference), Java\n      Maven, Eclipse Intellij, Python, R, PySpark, Ruby, C#, Unix Shell\n      Scripts, Linux Shell Scripts\n    . I lead, architected and help developing the SOLR/Lucene Search system\n      which was later migrated to Elasticsearch with 15 nodes sharding. The\n      system was initially developed and maintained in house but later\n      deployed to AWS cloud and prototyped on Azure. The development and\n      test of the 15 node done on VirtualBox machines, physical machines\n      before deployment to the Cloud. The SOLR development was done in two\n      different phases, initially we did indexing directly on top of the\n      metadata extracted from various files with Apache Tika, Apache Flume\n      and scoop. I wrote a scheduler in Java that run delta indexing\n      periodically every few hours. We had customized faceting and then the\n      API would grab the top N results from the XML. The search worked\n      better than expected, the indexing was slow but the search was\n      extremely fast in fraction of a second. On the second phase we stored\n      all the raw documents in HDFS and create indexing and then use HBase\n      to store the index files in HDFS. Also an API was developed in JAVA\n      with a .NET wrapper with SOLR search calls into the SOLR engine.\n    . I lead, architected and help developing an advanced Machine Learning\n      system initially in Spark MLlib then Weka and ultimately a Mahout\n      Machine learning and recommendation system using both ItemSimilarity\n      and UserNeighborhood. I personally favored and created porotypes using\n      Spark MLlib, TensorFlow, Keras, Python libraries like scikit-learn,\n      Pandas but in this case Mahout worked very well. The .NET API would\n      record every time a product was clicked or purchased. The data was\n      recorded in the database and then the metadata was created and the\n      mahout would create a scoring table (0-10) for product and region. The\n      .NET API would select top N highest score and would present it as\n      recommendations.\n    . I lead, architected and helped developing a gigantic amount of data\n      extraction, data warehousing, Big Data. The data was gathered in\n      access of tens of terabytes from more than 40 top of the lines brands\n      Ecommerce sites partnered and operated by OneStop like FRYE, Juicy\n      couture, NYDJ, PAIGE, Splendid, Coffee Beans, Jones New York, Hudson\n      and many more. Used SSIS ETL for SQL to port data to the Data\n      Warehouse and then used Sqoop for extracting from RDBMS to the HDFS,\n      used Flume for extracting from logs files, FTP NAS files to the HDFS,\n      used Apache Tika and Java for extracting metadata from various files\n      into the HDFS, used Nutch for web crawling and for extraction metadata\n      into the HDFS, used SAPI, CMU Sphinx, Kaldi for customer service voice\n      to text conversion into the HDFS\n    . I architected and implemented a real time and streaming component for\n      the Cloudera visualization using Apache Strom and Apache Kafka.\n    . I lead, architected and help developing an elaborate real time\n      visualization using Tableau and Cloudera visualization for the big\n      data portion.\n    . The Big data prototype was deployed both on AWS and Azure. For a\n      number reasons the final decision for the cloud deployment was made\n      for deployment into the AWS not Azure.\n    . I lead and oversaw the conversion of part of the SOLR search project\n      to Elasticsearch and benchmarked the performance. Although I liked\n      working with JASON for various reason SOLR was preferred initially.\n    . I lead, architected and help developing Kibana 4.5 visualization on\n      top of both SOLR and Elasticsearch.\n    . Wrote and oversaw a development of combination of batch files, python\n      and Ruby scripts for SOLR/Lucene and Big Data deployment and\n      configurations. I have to add that I started the conversion of batch\n      file to Python but there were simply not enough time.\n    . Did extensive prototyping and benchmarking and helped evaluating the\n      performance of the big data on Massively Parallel Processing (MPP) and\n      other  Data Warehouse Appliances such as IBM Netezza, Teradata, APS\n      (PDW), Oracle Exadata\n\n\nCanadian Tire, California / Toronto     8/2015-2/2016\nSenior Big Data, DW and BI Lead Solution Architect Consultant.\n    . Led multiple large scale Big Data, Enterprise Data Warehouse EDW and\n      Business Intelligence BI projects on Teradata utilizing Spark, Hadoop,\n      Hortonworks, Cloudera, Hive, Impala, Flume, Sqoop, Map Reduce, Pig,\n      HDInsight, HBase, oozie, and facilitating the real-time data analysis\n      by the data scientist.\n    . Led multiple EDW projects, prototyped and evaluated their performance\n      on the Massively Parallel Processing (MPP), other Data Warehouse\n      Appliances such as IBM Netezza, Teradata, APS (PDW), Oracle Exadata\n    . Development languages, extensive Scala SBT (my preference), Java\n      Maven, Eclipse Intellij, Python, R, PySpark, Ruby, C#, Unix Shell\n      Script, Linux Shell Scripts\n    . Leading the team, I designed architected and implemented the migrating\n      from legacy information warehouse to a modern high performance Big\n      Data and Data Warehouse running on multiple DW appliances. Drafted a\n      BI/DW prioritized implementation roadmap working with the business and\n      finance department.\n    . Leading the team we migrated and deployed 5 projects to Azure Cloud. I\n      was personally involved in the full cycle of vendor selection,\n      requirement gathering, design, development and the deployment of these\n      projects. The migration included different aspects of the projects\n      from front, backend, and integration. We went through thorough\n      research before selecting the Azure cloud for this project and also\n      utilized cutting edge utilities to perform the migration and\n      deployment.\n    . Drafted a BI/DW prioritized implementation roadmap while taking input\n      from internal divisional service plans, business and IT strategy\n      documentation, as well as corporate BI Strategy and the Financial\n      Planning and Reporting System\n    . Designed Enterprise Information Management (EIM) solutions for retail\n      operation. Led technical teams and designed various BI solutions\n      including loyalty programs, card management, POS data management,\n      customer behavioral analysis, store dashboards, finance, ecommerce,\n      cyber security analytic.\n    . Defined the data governance strategy, designed security patterns,\n      implemented data standards and procedures across the enterprise;\n      drafted business specific methodology to establish business\n      stakeholder-driven data stewardship through MDM\n    . Conducted BI maturity assessment of the organization. Architected\n      DW&BI Program Structure, defined the role of DW&BI Program Steering\n      Committee, it's mission, objectives, roles and responsibilities,\n      monitored regular improvements to help manage risks, evaluate trends,\n      and develop capacity and capability to achieve the Program mission\n\nNovaWurks/DARPA, Los Alamitos, California    11/2014-8/2015\nSenior Big Data, DW and BI Lead Solution Architect Consultant, Java Android\nconsultant\nWorked as senior Big Data Solution Architect, team leader and core\ndeveloper on PHOENIX project, an advanced satellite system for DARPA\n(Defense Advanced Research Projects Agency), a network of small satellites\ndue to launch to orbit in 2015. Due to the sensitivity cannot go into too\nmuch details!\n    . Led several Big Data projects on massive amount of transmitted and\n      logged data from the satellite network to the ground station. These\n      projects were developed utilizing Cloudera, Hadoop, Spark, Hive,\n      Impala, Flume, Sqoop, Storm, Pig, HDInsight, HBase, oozie. Due to the\n      real time nature of the project Apache Storm and Apache Kafka was used\n      for handling of the streaming and the real time data feed.\n    . I led the team, designed, architected and implemented an elaborate\n      Data Warehouse and Data Mart using Dimensional Modelling Star Schema\n      for satellite data aggregation, data storage, data log, real time\n      operation status data and other needs.\n    . Utilized the Cloudera Visualizations, Dashboards, and Reports to\n      monitor the operation of the satellites and any warning issues due to\n      any errors, miss functions or failures. Other visualization tools were\n      also created using Java and Android.\n    . Development languages, extensive Scala SBT (my preference), Java\n      Maven, Eclipse Intellij, Python, R, PySpark, Ruby, C#, Shell Scripts\n    . Led the team developed multiple real time Android Apps and middleware\n      using Android Studio and Eclipse, Android SDK and Java, RESTful APIs,\n      Retrofit, GSON, JSON, Regex, JGroups IP Multicast, Apache Thrift,\n      Python. Also used the following technologies and systems, Xilinx FPGA,\n      Verilog, TI DSP, ARM\u00ae Cortex\u00ae-A9 Cores: i.MX 6 Series Multicore\n      Processors etc.\n\nParamit, Morgan Hill, California  7/2013-11/2014\nSenior Big Data, DW and BI Lead Solution Architect, .NET Architect\nConsultant.\n    . Led multiple large scale Big Data, Enterprise Data Warehouse EDW and\n      Business Intelligence BI projects utilizing, Hadoop, Cloudera, Hive,\n      Impala, Flume, Sqoop, Map Reduce, Pig, HDInsight, HBase, oozie, and\n      facilitating real-time data analysis by data scientist.\n    . Leading the team, I designed architected and implemented the migration\n      from legacy normalized SQL, FoxPro, medical device manufacturing, ERP,\n      MRP, CRM, sales, finance and other information warehouses to a\n      consolidated modern high performance Big Data Warehouses running on\n      multiple DW appliances.\n    . Development languages, extensive Scala SBT (my preference), Java\n      Maven, Eclipse Intellij, Python, R, PySpark, Ruby, C#, Shell Scripts\n    . Leading the team we migrated and deployed multiple projects to Azure\n      Cloud. I was personally involved in the full cycle of vendor\n      selection, requirement gathering, design, development and the\n      deployment of these projects. The migration included different aspects\n      of the projects from front, backend, and integration. We went through\n      thorough research before selecting the Azure cloud for this project\n      and also utilized cutting edge utilities to perform the migration and\n      deployment.\n    . Using a combination of WPF C# application GUI and the Cloudera\n      Visualizations, Dashboards, and Reports created advanced data\n      visualization and data entry tools for ERP, MRP, CRM, sales, finance\n      and other departments.\n    . I lead, architected and help developing a SOLR/Lucene Search for the\n      huge amount of ERP, MRP and CRM. The SOLR project was later converted\n      to Elasticsearch. The Elasticsearch /Lucene system was architected\n      with 5 nodes sharding. It was developed and tested on 5 node\n      VirtualBox machines and then deployed to AWS cloud. Created an API in\n      C# .NET for calls to the search engine. Also a GUI was developed in C#\n      .NET for search calls to the Elasticsearch.\n    . Developed a customized SOLR indexing scheduler in C# which would run\n      periodically to do the delta indexing.\n    . Drafted a BI/DW prioritized implementation roadmap while taking input\n      from internal divisional service plans, business and IT strategy\n      documentation, as well as corporate BI Strategy and the Financial\n      Planning and Reporting System\n    . Designed Enterprise Information Management (EIM) solutions for the\n      manufacturing process, customer support and retail operation. Led\n      technical teams and designed various BI solutions including medical\n      device manufacturing tracking process, component reliability analysis,\n      vendor analysis, customer behavioral analysis, finance, ecommerce,\n      cyber security analytic.\n    . Conducted BI maturity assessment of the organization. Architected\n      DW&BI Program Structure, defined the role of DW&BI Program Steering\n      Committee, it's mission, objectives, roles and responsibilities,\n      monitored regular improvements to help manage risks, evaluate trends,\n      and develop capacity and capability to achieve the Program mission\n    . Led the team and developed multiple applications including medical\n      device, ERP, MRP applications with big data architecture. Used NET\n      4.5, C#, WPF, WCF, WF, MVVM Light, Telrik, MVC 4 Razor Entity\n      Framework 6.0 TFS, SQL 2012.\n\nMicrosoft, Redmond WA  2/2013-7/2013\nSenior Big Data, DW and BI Lead Solution Architect, .NET Architect\nConsultant.\n    . Led multiple Azure Cloud Big Data, NoSQL Riak, MongoDB SIP Trunk VOIP\n      projects doing analysis on massive amount of voice to text converted\n      data utilizing Hadoop/HDInsight, PDW, Map/Reduce jobs, Hive, and\n      Sqoop.\n    . Development languages, extensive C++, Scala SBT (my preference), Java\n      Maven, Eclipse Intellij, Python, R, PySpark, Ruby, C#, Shell Scripts\n    . Created real time multithreaded C# code using C++ Dubango Library,\n      SIP, TCP, UDP, RTP the VOIP telephony voice was recorded and using\n      SAPI converted to text. The text was then stored into key value and\n      document tables using Riak and MangoDB. The voice data gathered from\n      Cisco/IPCC telephone systems. Integrated with Cisco Verint for VOIP\n      call recording, quality monitoring (QM), and speech analytics.\n    . Microsoft SQL Server Parallel Data Warehouse (SQL Server PDW) was\n      chosen as the main appliance for the Big Data processing due to its\n      Massively Parallel Processing (MPP) architecture designed for Big Data\n      Processing.\n    . Microsoft Power BI in conjunction with a .NET application is used for\n      data visualization.\n    . Led the design and development of the Workforce Management (WFM) data\n      warehouse and BI solution to optimize adherence and attendance in the\n      contact center. The predictive analytic component accurately forecasts\n      the number of CSRs needed in the call center to fulfill the services.\n    . Led the design and development of an efficient BI auditing framework\n      that collects the data from packages being executed and used in data\n      flows, row counters, versioning, and error handling. The framework is\n      crucial for monitoring, timing, troubleshooting, and auditing. Also,\n      developed Stored Procedures, Views, and Functions for the framework to\n      automate logging the information and error handling in the packages.\n    . Led the design and development of ETL processes and data mapping using\n      SQL server, Master Data Services (MDS), SSIS to extract data from\n      Lagan ECM and division data sources including SQL server and oracle\n      databases, flat files, and excel sheets. The data, then, is\n      transformed and loaded into a data warehouse for reporting.\n    . Led the design and development of data quality ETL packages to correct\n      and cleanse the data and enhance the quality of consolidated data.\n      Wrote hundreds lines of .NET C# code, embedded in the packages, to\n      create a rules engine that loads business rules and apply them to the\n      data efficiently. In addition, the data quality issues are mapped for\n      reporting purposes.\n    . Led the design and development of a SQL Server SSAS Analysis cube\n      utilizing star schema with complex MDX calculated measures, named sets\n      and KPIs to present an analytical view for the data and data quality\n      with multiple dimensions.\n    . Led the design and development of map application and report using\n      ASP.NET/C# web application. The application loads the data from the\n      data warehouse, combines it with geographical information, and\n      displays the data on a map. The application communicates through\n      restful mapping services and uses client side scripts (JavaScript and\n      AJAX) to improve performance and user experience.\n\nDell, Austin, TX 6/2012-2/2013\nSenior Big Data, DW and BI Lead Solution Architect, .NET Architect\nConsultant.\n    . Led a Big Data project on gigantic amount of taxonomy data and\n      customer portfolio using Hadoop,  Cloudera, Hive, Map Reduce, Pig,\n      HDInsight, and facilitating real-time data which was both analyzed and\n      also in real time restructured the Dell website on the demographic\n      portfolio of the customers.\n    . I architected, worked and help developing the SOLR/Lucene Search\n      deployed to Azure. The indexing was done directly on top of the\n      metadata extracted from various files with customized Java code and\n      Apache Tika. Used customized faceting to overwrite the default search\n      criteria.\n    . Development languages, extensive Java Maven, Eclipse Intellij, Python,\n      R, Scala SBT, PySpark, Ruby, C#, Unix Shell Scripts\n    . Developed a customized SOLR indexing scheduler in C# which would run\n      periodically to do delta indexing.\n    . Wrote variation of batch files, python for SOLR/Lucene deployment and\n      configurations\n    . Leading the team, we designed architected and implemented the\n      migrating from legacy normalized SQL taxonomy data, customer portfolio\n      data and other data to a modern high performance Big Data Warehouses\n      running on multiple DW appliances.\n    . Defined the data governance strategy, designed security patterns,\n      implemented data standards and procedures across the enterprise;\n      drafted business specific methodology to establish business\n      stakeholder-driven data stewardship through MDM\n    . Led multiple EDW projects, prototyped and evaluated the performance on\n      Azure cloud, AWS Amazon Cloud, Massively Parallel Processing (MPP)\n      Data Warehouse Appliance\n    . I wrote complicated taxonomy algorithm in C# to load, sort the\n      taxonomy data into huge multidimensional trees on the memory which\n      made the data processing supper fast.\n    . Created Taxonomy data visualization using the Cloudera Visualizations,\n      Dashboards, and Reports to monitor customer profile, demography and\n      other useful data. Other visualization tools were also created using\n      C#.\n    . Created data quality ETL packages to correct and cleanse the taxonomy\n      data and enhance the quality of consolidated data. The consolidated\n      taxonomy data then were segmented using Hadoop and Cloudera.\n    . Led the design and development of a SQL Server SSAS Analysis cube\n      utilizing star schema with complex MDX calculated measures, named sets\n      and KPIs to present an analytical view for the data and data quality\n      with multiple dimensions.\n    . Leading the team we migrated and deployed multiple projects to Azure\n      Cloud. I was involved in the full cycle of vendor selection,\n      requirement gathering, design, development and the deployment of these\n      projects. The migration included different aspects of the projects\n      from front, backend, and integration.\n    . In conjunction with the Big Data I was involved in multiple projects\n      using variety of technologies including MVC 4 Razor, WPF, WF, WCF,\n      TPL, LINQ, SQL 2012, jQuery, Android, Java, J2EE, JRE, Ajax,\n      AngularJS, ExtJS, Entity Framework 5.0,.NET 4.5\n\nBEW / General Electric / 3 Gorges China, San Ramon, California      6/2011-\n6/2012\nTeam Leader, .NET Architect, Hands on Developer Consultant.\n    . Worked as a system architect, core developer on a sophisticated\n      control system for generators and wind turbines lead the software\n      (WPF), hardware (Xilinx FPGA & TI DSP 6000) and firmware (C++\n      Verilog/VHDL) teams.\n    . The high level software controlled a network of generators via TCP/IP.\n      The WPF C# project was architected using MVVM light, Entity Framework,\n      LINQ, WCF Services SQL etc. The Silverlight ASP .NET project was\n      architected using MVVM light, Entity Framework, LINQ, WCF RIA Services\n      Domain Service/Context. Developed equivalent Android application for\n      reading the generator's parameters like RPM, temperature, sensor\n      Voltages etc. Used Java programming and the Android Software\n      Development Kit, Eclipse using the Android Development Tools (ADT)\n      Plugin. Also worked on the firmware and FPGA DSP TMS320C6713\n      TMS320F28335 EMIF, I2C, MCBS, GPIO, RTC UART, Anybus CANbus, DM9000,\n      second level bootloader, EEPROM, code composer 3.3 etc FPGA Xilinx\n      Spartan 6, Xilinx ISE Design Suite 13.2, Verilog and VHDL.\n\nTexas Instruments, Dallas Texas   9/2010-6/2011\nTeam Leader, .NET Architect, Hands on Developer Consultant.\nWorked as the main architect, team leader, and core developer on a\nscientific highly multithreaded WPF C# application for emulation and design\nof advanced communication chips using scientific algorithms. I also worked\non an Android application for the PLL, Java programming using the Android\nSoftware Development Kit, Eclipse using the Android Development Tools (ADT)\nPlugin. The WPF application was architected using propriety MVVM\narchitecture. Utilizing advanced 3D objects the application was similar to\nOrCad and AutoCad. A smaller prototype version was also developed in\nSilverlight.\n\nMultibeam Corporation / Tokyo Electron (TEL), Santa Clara, California\n7/2008-9/2010\nTeam Leader, Architect, Hands on Developer Consultant.\n    . Worked as the project lead, helped designing, architecting, and\n      implementing a revolutionary complex electron beam based instrument\n      for the next generations of semiconductor fabs. Advanced analog\n      digital boards, Embedded Linux, Xilinx & Altera FPGA, Quartus, NIOS,\n      ARM9, ARM11, C, C# .NET, WPF (MVVM), WCF etc. DSP TMS320C67x GPIO, RTC\n      UART, Modbus, DM9000, second level bootloader, EEPROM. Altera FPGA,\n      Stratix, Cyclone Series, Quartus II Nios II.\n\nDepartment of Defense Contract (DOD), Washington DC      3/2007-7/2008\nTeam Leader, Architect, Hands on Developer Consultant.\n    . Architected, developed and led a highly sophisticated\n      hardware/firmware/software system. Due to the classified nature of the\n      project, I can only provide the following generic information: The\n      project involved advanced radio scanners, signal generators using GPS,\n      WCDMA, CDMA, GSM and other systems and protocols. The software\n      application controlling the instruments was a multi-tiered application\n      written in C#, .NET, Visual C++, MFC, CLR, Embedded Linux. It utilized\n      a very advanced multi-threading architecture with sophisticated\n      synchronization, message handling, logging system, serialization etc.\n      Specialized algorithms were devised to speed up the real-time\n      performance of hardware/software. Again because of the defense-related\n      nature of this project I cannot reveal any more details.\n\nPatton Design, Irvine, California 7/2000-3/2007\nVice President.\nWorked as the vice president of software and hardware. I led and developed\nthe software/hardware for a $140,000 instrument medical device - FDA.\nPlease check the website of Patton Design and Busch & Lomb to see this\naward winning instrument for cataract surgery. I designed, architected, led\nthe team and developed the software and also directed the hardware and\nfirmware developments. The software included a sophisticated multithreading\narchitecture, RS232 and TCP/IP communications, managed wrapper for firmware\ncalls, video streaming, voice recognition, database hierarchy encryption\netc. In addition to leading the team and acting as the vice president, I\npersonally wrote the complex core components in C# .NET. Due to the large\nscale of the medical device - FDA projects with hundreds of screens many of\nthe .NET C# libraries and objects had to be used. We also used legacy\nunmanaged code inside the managed code (wrappers). DirecX, DirectShow,\nWindows Communication foundation WCF, Windows Presentation Foundation WPF,\nWF, Silverlight, WCSF, SCSF, Enterprise Library, animation, video, audio\netc were also used.\nIn addition to the main control application I wrote and oversaw the\nfirmware in C++ Embedded Linux, C++ Round Robin, CodeWarrior. I also\noversaw and participated in the hardware development using OrCad 10.\n\n* Patton Design / Cameron Health: Developed the software and participated\nin the hardware design of the heart pacemaker medical device - FDA and the\ncontroller called Q-TECH Programmer. medical device - FDA The heart\npacemaker is transplanted in the heart and controlled by the wireless\ncontroller via Bluetooth. Due to the FDA regulations I could not use the\n.NET framework but had to use Embedded Visual C++ 4.0 and MFC for windows\nCE. More than 140 screens! Very sophisticated programming involving memory\nmanagements, DirectX, DirectShow etc.\n\n* Patton Design / GoVideo: Worked as the Vice President/architect/team\nleader on a joint project between, GoVideo, Patton Design, Daewoo and MTK\nin Taiwan. I led the Patton Design team developed a TiVo style DVD/VCR\ncombo with hard drive recording capability. I was the vice president and\nthe team coordinator between the 4 companies overseeing hardware, software\nand Firmware (Embedded Linux), several patents were filed. The System was\npresented at the CES show in Las Vegas in 2007 and received tremendous\npositive recognitions.\n\n* Contract with usCalibration Inc.: Architected, developed and led a\nsophisticated web based application using C# .NET and Visual Studio 2005,\nSQL Server 2005 and SSRS. I wrote the core part of the application. The\napplication was successfully launched in 2006 for Calibration systems with\nadvanced security systems. Tens of thousands of lines of code with advanced\nnavigation systems with several pending patents.\nHewlett Packard (HP), Cupertino, California  4/2000-7/2000\nTeam Leader, Architect, Hands on Developer Consultant.\nWorked as senior developer/ technical lead on an advanced server client\nbased communication system for server diagnostics. The system was designed\nusing TCP/IP and SNMP protocols for monitoring hardware sensors like\nthermocouples, voltage and current monitoring sensors and other hardware\nsensors installed on HP servers. By reading these sensors, HP was able to\nremotely do detailed hardware/software diagnostics of the HP servers around\nthe globe.\nWorked on hardware, software and the overall system architecture. The\nsoftware had a server and client component and was written in visual C++,\nCOM (ATL), DCOM, ASP, Visual J++, XML, SNMP, MIB, SQL and InstallShield.\n\nBroad Logic, Milpitas, California 7/99-4/2000\nTeam Leader, Architect, Hands on Developer Consultant.\nI was brought to BroadLogic, Inc. by Paul Rudnick because of my expertise\nin satellite communication systems and my experience from Space Systems\nLoral and CyberStar. Prior to this, I had worked closely with Adaptec and\nBroadlogic on the development of the satellite receiver hardware while\nstill a senior manager at Space Systems Loral.\nI worked on the design and implementation of the next generation of two way\nsatellite Express PC transceiver cards, a high speed two way satellite\ncommunication system. I designed, simulated, researched, architected and\nled the project for the development of an advanced two way satellite\ncommunication system (satellite Express PC transceiver cards). Audio, video\ntransfer and high speed internet access over satellite. Using, frequency,\ntime, phase multiplexing. TDMA, CDMA, GMSK, Conditional Access. TCP/IP,\nUDP, DVB, SNMP, MIB and proprietary protocols. Using OQPSK modulation\nimplementation on the Texas Instrument DSP Chip. I have written several\ndocuments related to this system.\n\nHewlett Packard (HP), Mayfield, California   3/99-7/99\nTeam Leader, Architect, Hands on Developer Consultant.\nWorked as the senior architect, technical lead and senior developer on the\nHP Ecommerce site which later became the foundation of the HP website for\nPC and servers. The web application was developed in Visual InterDev 6.0\nusing Active Server Pages (ASP), Microsoft E-Commerce, SQL 7.0, XML, Visual\nC++ 6.0 and Visual Basic 6.0, Visual J++ 6.0, COM (ATL), DCOM, JavaScript\nand VB Script. The web server was Microsoft Internet Information Server\n(IIS), Microsoft site server 3.0, with Microsoft E-Commerce edition 3.0 and\nFrontPage extension running under the NT Server I have written several\ndocuments related to this application.\n\nHewlett Packard (HP), Cupertino, California  5/98-3/99\nTeam Leader, Architect, Hands on Developer Consultant.\nWorked as the senior architect, technical lead and senior developer on the\nHP servers configuration software which later became a major component and\nthe foundation of the HP website for PC and servers configuration. Stand\nalone and the web application in was developed in Visual InterDev 6.0 using\nActive Server Pages (ASP), Microsoft E-Commerce, SQL 6.5, Visual C++ 6.0\nand Visual Basic 6.0, COM (ATL), DCOM, JavaScript and VB Script. The web\nserver was Microsoft Internet Information Server (IIS), Microsoft site\nserver and FrontPage extension running under the NT Server. I have written\nseveral documents related to this application.\n\nSpace Systems Loral (CyberStar), Mountain View, California    11/96- 5/98\nTeam Leader, Architect, Hands on Developer Consultant.\nI was brought into Space Systems Loral from Lockheed Martin by Bob Lapin to\nhelp starting the CyberStar division at the Space Systems Loral. By the\ntime I left the CyberStar in 1998 to finish my PhD in Satellite\nCommunication, the CyberStar division had grown to more than 100 employees.\nI personally interviewed majority of those people.\nI was one of the main architects of the CyberStar project and oversaw the\ndesign, development and implementation of different aspects of hardware,\nsoftware, firmware and the satellite communication at CyberStar.\nI first established a complete satellite communication link both uplink and\ndownlink, using 3rd party modulators, demodulators, encoders, decoders,\ncryptography modules, conditional access, transmitter, receivers,\namplifiers, dampers, data aggregator, data parsers etc. Very soon we were\nable to transmit and receive from and to the satellite. We were primarily\nusing MIB and DVB protocols initially but I was one the first who managed\nin 1998 to implement TCP/IP and high speed internet access over satellite\nusing an ACK table!! (patents)\nTo develop the integrated transceiver hardware we started working with\nAdaptec and I personally was directly involved in the design and\nimplementation of the satellite receiver card hardware using OrCad. This\nlater led to the creation of BroadLogic from Adaptec. I was later hired by\nBroadLogic to continue the improvement of the two way satellite receiver /\ntransmitter.\n\nThis project was personally very important to me and made me understand and\nexperience the satellite communication in a very comprehensive way both\ntheoretically and practically. It helped me to get a PhD in Low Earth Orbit\nSatellite Communication from the University of California, one of very few\nwho did. I travelled extensively in both US and in Europe and came in\ncontact with some amazing people from NASA, Lockheed Martin, BroadLogic,\nEuropean Space Agency etc I wrote many documents in satellite communication\nduring this period for Space Systems Loral.\n\nLockheed Martin, Milpitas, California   6/96- 11/96\nTeam Leader, Architect, Hands on Developer Consultant.\nLockheed Martin at the time in 1996 had the most sophisticated high\nresolution CCDs (Farichild) in the world which were in use in a number of\nsensitive military applications, advanced high resolution digital satellite\nimaging, and few civilian applications.\nDue to the classified nature of some of these projects I cannot in detail\ndescribe what I did. However I was involved in the design and development\nof some of these advanced and sensitive projects. I worked as a senior\nengineer, designing and developing, systems, hardware, firmware and\nsoftware.\n* Hardware: We used OrCad for designing analog and digital circuits,\nfilters, amplifiers, Data collectors from CCDs, interfaces etc.\n*Firmware was written in C++ , flat file Round Robin, on Freescale HC and\nARM family CPUs.\n*Software: developed 32-bits, real time applications in Visual C++ 4.2\nusing MFC and SDKs under Windows 95 for control and testing of an advanced\ndigital camera with high resolution CCD. The GUI software is designed for\ndriving the special digital camera through parallel communication and\ntesting of IPS, ADP, CCD and different part of the system. The tests\nincluded advanced image processing and image quality tests. The project\ninvolved both 16-bits and 32-bits DLLs and VXDs (device drivers), Thunking\nand also conversion from and between 16-bits and 32-bits.\n\n\n                               OTHER PROJECTS\n\nERICSSON (ELLEMTEL), STOCKHOLM, SWEDEN\nTeam Leader, Architect, Hands on Developer.\nDeveloped and designed hardware and a control system for the new generation\nof AXE telephone systems, based on the FUTUREBUS+ bus technology, Using the\nVHDL programming language. I wrote the VHDL program on the SUN platform\n(SUN OS version 3.0).\nI documented the application in a detailed technical white paper entitled\n\"Verification Methods for Hardware Construction\". This paper was released\nto all programmers and hardware engineers at Ericsson and KTH. A copy is\navailable for your review.\n\nABB Atom AB, Vasteras, Sweden\nTeam Leader, Architect, Hands on Developer Consultant.\nElectrical and Computer Lab--section SLC3:\nDeveloped a series of utility programs / application in Quick BASIC\n(version 5.0) used for calibration of computer operated measurement\nequipment in the nuclear power plant reactors. Programs were run on the HP\n9000/300, and Intel 286 platforms.\n\nTECHNOLOGY\n    . Data Warehouse, Data Mart, OLAP, OLTP Databases, Teradata, Netezza,\n      Oracle, Parallel Data Warehouse (PDW), SQL Server, MDM, MDS, Data\n      Quality (DQ), Spark, Hadoop, Hortonworks, Cloudera, Apache KafKa,\n      Hive, Impala, Flume, Sqoop, Map/Reduce, Pig, HDInsight, HBase, Storm,\n      oozie, Python, Scala, HDFS, StreamInsight, PolyBase, Microsoft SSIS,\n      SSAS, SSRS, ETL, BI, MDX, PL/SQL, TSQL, ERwin, Enterprise Architecture\n      (EA),  SQL Servere 2000/2005/2008/2012/2014, Power Query, Power Map,\n      PowerPivot, Power View, IBM Cognos, SPSS, InfoSphere DataStage,\n      Informatica PowerCenter, SAP BusinessObjects (BO), SAP HANA, Crystal\n      Reports, Hyperion, MicroStrategy, SharePoint 2007/2010/2013, Nintex,\n      SharePoint Social, Collaboration, Record Management, Search, Web\n      forms, InfoPath, Branding, CSOM, JSOM, PerformancePoint, Clustering,\n      Failover, Web Analytics, Google Visualization, .NET 1.1 to 4.5, C#,\n      WCF, Restful Services, WPF/Silverlight, WF, VB .NET, ASP .NET,\n      ADO.NET, LINQ, MVC, MVVM, MVP, AJAX, Visual Studio, Dashboard\n      Designer, SharePoint Designer, Visio, TFS, Cloud, Azure, PaaS, SaaS,\n      IaaS, HTML 5.0, DHTML, XML, XSL, WSDL, XSD, JSON, COM, DCOM, MFC, C,\n      Visual C++, Visual Basic, PowerShell scripts, and SDKs, DocXpress, BI\n      Documentation, Nintex, SharePoint Social, Collaboration, Record\n      Management, Search, Web forms, Branding.\n\n                                   SKILLS\n    . BIG DATA, HADOOP, SPARK, CLOUDERA, HORTONWORKS, STORM, KAFKA, HIVE,\n      IMPALA, FLUME, SQOOP, MAPREDUCE, PIG, HDINSIGHT, HBASE, OOZIE, TABLEAU\n      POWER BI AND CLOUDERA VISUALIZATION\n    . Cloud Azure, AWS, , HDInsight, Cortana Intelligence Suite, Data\n      Factory, Data Gateway, Infrastructure as a Service IaaS, Platform as a\n      Service PaaS, Microsoft R Server, NLB, Key phrase extraction Azure\n      search, Unstructured text analytics, Event hub, Streaming, Poly Base\n    . Search engines Elasticsearch, SOLR, Lucene, Kibana. Logstash, Rsync,\n      Tika\n    . Machine Learning and recommendation engines MLlib, TensorFlow, Keras,\n      Weka Mahout, Multilayer perceptron classifier (MLPC), the feedforward\n      artificial neural network, scikit-learn, Pandas, Deeplearning4j,\n      Sparkling Water ML, Caffe2, MxNet etc. Different algorithms K-Means,\n      Random Forest, Gradient Boosting algorithms (GBM, XGBoost, XGBoost and\n      CatBoost)\n    . BI Framework: Strategy and Implementation Plans, Enterprise Metrics,\n      Integration Points, Gap Analysis, BI Portfolio, Performance Management\n      (PM), Analytic and PM Technologies, Defining Business and Decision\n      Process, Building Metadata and Services Centers, Establishing\n      Enterprise Information Management (EIM) Committees, Defining The Role\n      of DW and BI Program Steering Committee, It's Mission, Objectives,\n      Roles and Responsibilities, DAMA DMBOK\n    . Architecture and Data Modeling: Initial Conceptual Solution, Solution\n      Blueprints, Technology Impact Analysis (TIA), Gap Analysis, Technology\n      Roadmap, Dimensional modelling, ER Modelling, Start Schema, Snowflake,\n      Fact, Dimension, Hierarchy, Inmon/ Kimball/ Imhoff, Data Marts, EDW,\n      ERWin 9.5/8.0/7.x, DeZign, Microsoft Visio, Enterprise Architecture\n      (EA), Service Oriented Architecture (SOA), UML, Zachman, TOGAF, Star &\n      Snowflake Schemas, 3 Normal Forms, Normalization and Demoralization,\n      Logical Model and Physical Model, Fact/Dimension/Hierarchy\n      identifications, Data Warehouse Development Lifecycle, Data Mapping,\n      Data Dictionaries\n    . Data Governance: IBM InfoShere MDM, Informatica MDM, MDS, DQS,\n      Profisee Maestro, SAS MDM\n    . Integration and ETL: SSIS/SSRS/SSAS, SQL Server\n      2014/2012/2008R2/2008/2005, Informatica PowerCenter, DataStage,\n      Cognos, ETL Mapping design, Data Profiling, Data Validation, Data\n      Migration, Data Cleansing, Data Structure, Data Quality Services\n      (DQS), BIDS, SQL Data Tools (SSDT), Auditing Framework, Execution\n      Plans, ETL Parallel Processing, Error Handling, Custom Scripting, IBM\n      Cognos, InfoSphere DataStage, Informatica PowerCenter, SAP\n      BusinessObjects (BO)\n    . Data Warehousing and Analysis: OLAP/Cube/MDX/DAX, Dimensional\n      Modelling, Tabular Modelling, KPIs, KPPIs, Data Analysis, SPSS,\n      Predictive Analysis, Data Mining, Machine Learning, SAP HANA,\n      Statistical Analysis, SAS, SAS VA (Visual Analytics), R, XLSTAT,\n      Sentiment analysis, Speech analytics, Teradata. Netezza, Cloudera,\n      PDW, Aginity, Master Data Services (MDS), Master Data Management\n      (MDM), Data Quality (DQ), Analysis of Change (AOC), Metric Engine.\n    . Reporting: Predefined Reports, Ad-hoc Reporting, Analytical Reports,\n      Custom Reporting with .NET/ Report Viewer, SQL Server Reporting\n      Services (SSRS), SharePoint 2013/2010/2007/2003, PerformancePoint,\n      PowerPivot, Power View, Crystal Reports, Hyperion, MicroStrategy,\n      Cognos Report Studio, Framework, Workspace Advanced, DMR, TM1\n    . Data Visualization: Power Map, PowerPivot, Power View, SharePoint,\n      Liferay, PerformancePoint, Google Visualization, Esri's GIS\n      (geographic information systems) , mapping, SAP Lumira, QlikView,\n      Tableau, Data Mapping\n    . Database: MS-SQL, Oracle, Oracle SQL Developer, TSQL, MDX, DMX,\n      PL/SQL, Stored Procedure, View, Function, Erwin Data Modeler, DB2,\n      PowerDesigner, MongoDB, Access, Excel, FoxPro, Informix, NoSQL, Big-\n      data, Hadoop, Spark, HBase, HDInsight, PDW, PolyBase, Hive, HQL,\n      Map/Reduce, HFS, Alert\n    . Programing Languages: SQL, T-SQL, PL/SQL, C#, WCF, Restful Services,\n      WPF/Silverlight, WF, VB .NET, ASP .NET, ADO.NET, LINQ, MVC, MVVM, MVP,\n      AJAX, HTML 5.0, DHTML, XML, XSL, WSDL, XSD, JSON, Java Script,\n      PowerShell, COM, DCOM, VB Script, UNIX Shell Scripting\n    . Others: Agile, Extreme Programing, RUP, Use Cases, SDLC, TCP/IP, CVS,\n      Microsoft Team Foundation Server (TFS), Tortoise SVN, SQL*Plus, TOAD,\n      WinSQL, SilverLight, LightSwitch, Kerberos, Single Sign-On, Datazen,\n      One-Key.\n    . Architecture and Design: Enterprise Architecture (EA), Service\n      Oriented Architecture (SOA), Enterprise Service Bus (ESB), Top-Down/\n      Bottom-Up Design, Structured Design, Object Oriented Design, Multi-\n      tiered and Multi-threaded architecture, Rational Rose, UModel,\n      Patterns: Model/View/ViewModel (MVVM), MVC, MVP, Visio, UML, Zachman,\n      TOGAF, Federal Enterprise Architecture, Gartner Methodology\n    . Business Optimization: Asset management, Information Technology\n      Infrastructure Library (ITIL), customer satisfaction, call center\n      management, service request enhancement, AODA compliance, fraud\n      detection, CRM and ERP optimization, improving marketing\n      effectiveness, portfolio optimization, governance, risk management,\n      compliance, healthcare patient records management, electronic medical\n      records (EMR), optimizing routes and schedules for logistics planning,\n      insurance risk assessment, optimizing manufacturing production.\n    . Integration: Windows API, Biztalk, SOA, WCF, SSIS\n    . Data Access: ADO.NET, LINQ, Entity Framework, Microsoft Enterprise\n      Library, OLE DB, Oracle Data Provider, MS OLAP, SQL Master Data\n      Services (MDS), StreamInsight\n    . Software Development\n    . Methodologies: Test driven programming, Agile software development,\n      Extreme Programming (XP) Microsoft .NET Framework (from 1.0 to 4.0),\n      C#, Visual Basic .NET, VB .NET, ADO .NET, WinFX including Windows\n      communication foundation (WCF), windows workflow (WF), windows\n      presentation foundation (WPF), XAML, XML, HTML, HTML5, Java J2EE,\n      Spring Framework, JavaScript, AJAX, RESTful services, Payment Card\n      Industry (PCI), Image Processing\n    . Visual C++ (MFC, SDKs, COM, DCOM, ATL ActiveXs), VB, C++, Perl, VHDL,\n      Verilog, Shell, Skill, Ocean, SystemC\n    . Scala SBT, Java Maven\n    . Version control tools: Source Safe, Team Foundation Version Control,\n      (TFVC), Subversion Tortoise SVN\n    . Code metrics: Simian, RSM\n    . Type/ industry: financial, banking, biomedical, pharmaceutical,\n      engineering, telecommunication, semiconductor, logistics, health,\n      scientific, e-commerce, instrumental\n    . Internet Development: ASP .NET, MVC, Sliverlight, HTML, DHTML, Web\n      services for marketing and financial applications, AJAX, ASP,\n      JavaScript and VB Script, XML, Microsoft Internet Information Server\n      (IIS), Microsoft E-Commerce, PHP, Webload\n    . Cloud Computing\n    . Windows Azure, Amazon AWS EC2\n    . SharePoint: SharePoint 2013/2010/2007/2003, Multi-machine SharePoint\n      Farm Architecture, Setup, Configuration, Load Balancing, Clustering,\n      Backup Plans, Web Part and module development, Collaboration, Social,\n      Search, Web Content Management, Enterprise Content Management, App\n      Management, PerformancePoint and PowerPivot, PowerView, Application\n      Federation, Secure Store Application, Business Connectivity, Usage\n      Reports, SharePoint Designer, Dashboard Designer, PerformancePoint,\n      dashboard, charts, KPI, Scorecards, reports, filters, Excel Services,\n      PowerPivot Services, Web Analytics, Static Analysis, Hit Counters,\n      Custom Development, PowerShell, SharePoint API, Object Model, web\n      parts web services, workflows, Content Management, site\n      collections/structure\n    . Mobile Development\n    . Android, iOS, Windows\n    . Operating System Used: Windows, UNIX, Windows Azure, Linux, Android,\n      iOS, Windows Mobile, MS-DOS\n    . Hardware and Simulation: Matlab, Cadence Spectre, Spice, Eldo, ANSYS\n    . Algorithms: Genetic algorithm, simulated annealing based algorithms,\n      heuristic search, binary search, quick sort\n    . Automation and Scripting: VB, Perl, Unix Shell\n\n                                 LANGUAGES:\nENGLISH, SWEDISH, PERSIAN AND NORWEGIAN.\n",
          "confidence": 1.0,
          "method": "full_text",
          "structured_data": null
        },
        "tax_term": {
          "value": "CONTRACT",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "source_by": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "designation": {
          "value": "Cloud Architect",
          "confidence": 0.9,
          "method": "ner",
          "structured_data": null
        },
        "experience": {
          "value": "20",
          "confidence": 0.9,
          "method": "regex_total_experience_summary",
          "structured_data": null
        }
      }
    },
    {
      "resume_path": "data\\input\\Akin - PM  - OH.doc",
      "confidence_score": 0.56,
      "used_ocr": false,
      "extracted_fields": {
        "skills": {
          "domain_specific": [
            {
              "name": "retail",
              "confidence": 0.5,
              "context": "education_section",
              "positions": [
                37,
                6876
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "crm",
              "confidence": 0.5,
              "context": "education_section",
              "positions": [
                986,
                1576,
                11082,
                13258,
                14160
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "infrastructure",
              "confidence": 0.5,
              "context": "education_section",
              "positions": [
                1798,
                13786,
                14520
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "design",
              "confidence": 0.5,
              "context": "education_section",
              "positions": [
                5390,
                11961,
                14012,
                14131
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "compliance",
              "confidence": 0.5,
              "context": "education_section",
              "positions": [
                7214,
                10697
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "security",
              "confidence": 0.5,
              "context": "education_section",
              "positions": [
                7646,
                9617,
                12290,
                12970,
                13226
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "energy",
              "confidence": 0.49999999999999994,
              "context": "education_section",
              "positions": [
                4078
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "government",
              "confidence": 0.49999999999999994,
              "context": "education_section",
              "positions": [
                10180
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "ecommerce",
              "confidence": 0.49999999999999994,
              "context": "education_section",
              "positions": [
                11045
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "audit",
              "confidence": 0.49999999999999994,
              "context": "education_section",
              "positions": [
                12456
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "manufacturing",
              "confidence": 0.49999999999999994,
              "context": "education_section",
              "positions": [
                14179
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "education",
              "confidence": 0.49999999999999994,
              "context": "education_section",
              "positions": [
                14738
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "finance",
              "confidence": 0.49999999999999994,
              "context": "education_section",
              "positions": [
                14845
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "soft_skills": [
            {
              "name": "planning",
              "confidence": 0.5,
              "context": "education_section",
              "positions": [
                658,
                947,
                1041,
                10518,
                13326,
                13619,
                13673
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "analysis",
              "confidence": 0.5,
              "context": "education_section",
              "positions": [
                1063,
                1185,
                3379,
                4926,
                5149,
                8841,
                8872,
                12561
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "clarity",
              "confidence": 0.5,
              "context": "education_section",
              "positions": [
                1231,
                3906
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "innovation",
              "confidence": 0.5,
              "context": "education_section",
              "positions": [
                14023,
                14070
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "collaboration",
              "confidence": 0.49999999999999994,
              "context": "education_section",
              "positions": [
                1250
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "initiative",
              "confidence": 0.49999999999999994,
              "context": "education_section",
              "positions": [
                2995
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "documentation",
              "confidence": 0.49999999999999994,
              "context": "education_section",
              "positions": [
                3601
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "consulting",
              "confidence": 0.49999999999999994,
              "context": "education_section",
              "positions": [
                5109
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "methodologies": [
            {
              "name": "agile",
              "confidence": 0.5,
              "context": "education_section",
              "positions": [
                1550,
                4708,
                5543,
                8473,
                11270,
                11509
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "scrum",
              "confidence": 0.5,
              "context": "education_section",
              "positions": [
                1556,
                4259,
                4841,
                8327,
                11032,
                11260,
                11330,
                11519,
                13767
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "waterfall",
              "confidence": 0.5,
              "context": "education_section",
              "positions": [
                1562,
                4698,
                5549,
                8479
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "lean",
              "confidence": 0.5,
              "context": "education_section",
              "positions": [
                13851,
                14007
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "pmi",
              "confidence": 0.49999999999999994,
              "context": "education_section",
              "positions": [
                1523
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "rad",
              "confidence": 0.49999999999999994,
              "context": "education_section",
              "positions": [
                1542
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "rup",
              "confidence": 0.49999999999999994,
              "context": "education_section",
              "positions": [
                1572
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "safe",
              "confidence": 0.49999999999999994,
              "context": "education_section",
              "positions": [
                9698
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "sdlc": [
            {
              "name": "postman",
              "confidence": 0.49999999999999994,
              "context": "education_section",
              "positions": [
                1598
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "cloud": [
            {
              "name": "azure",
              "confidence": 0.49999999999999994,
              "context": "education_section",
              "positions": [
                1645
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "business_skills": [
            {
              "name": "brd",
              "confidence": 0.5,
              "context": "education_section",
              "positions": [
                5324,
                8585
              ],
              "experience_weight": 0.0,
              "importance_score": 0.8
            },
            {
              "name": "reporting",
              "confidence": 0.5,
              "context": "education_section",
              "positions": [
                12378,
                12929
              ],
              "experience_weight": 0.0,
              "importance_score": 0.8
            },
            {
              "name": "srs",
              "confidence": 0.49999999999999994,
              "context": "education_section",
              "positions": [
                5289
              ],
              "experience_weight": 0.0,
              "importance_score": 0.8
            }
          ],
          "networking_equipment": [
            {
              "name": "authentication",
              "confidence": 0.5,
              "context": "education_section",
              "positions": [
                7619,
                7728
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "authorization",
              "confidence": 0.49999999999999994,
              "context": "education_section",
              "positions": [
                7122
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "frameworks": [
            {
              "name": "rest",
              "confidence": 0.49999999999999994,
              "context": "education_section",
              "positions": [
                11470
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ]
        },
        "first_name": "Akin",
        "last_name": "Saheeb",
        "primary_email": {
          "value": "asaheeb@Gmail.com",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "secondary_email": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "phone": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "city": {
          "value": "Westerville",
          "confidence": 0.7,
          "method": "ner",
          "structured_data": null
        },
        "state": {
          "value": "OH",
          "confidence": 0.9,
          "method": "zip_database",
          "structured_data": null
        },
        "zip": {
          "value": "43081",
          "confidence": 0.7,
          "method": "regex",
          "structured_data": null
        },
        "work_authority": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "resume_link": {
          "value": "data\\input\\Akin - PM  - OH.doc",
          "confidence": 1.0,
          "method": "file_path",
          "structured_data": null
        },
        "raw_resume": {
          "value": "\n                              Akin  Saheeb, MBA\n                           Westerville, Ohio 43081\n                              asaheeb@Gmail.com\n\n\n\nSummary of Qualifications:\n|Experienced Project Manager offering Retail and Financial Services domain|\n|expertise                                                                |\n|Extensive experience managing PAYMENTS projects                          |\n|Card Present Payment transaction                                         |\n|Card Not Present Payment transaction                                     |\n|Centralized Payments settlement                                          |\n|Apple Pay implementation                                                 |\n|Mobile wallet                                                            |\n|Integration of loyalty and offer products with payment solutions - Pay,  |\n|Earn, Redeem, Save                                                       |\n|Created and managed project plan for additional functionality request to |\n|enable client to process an 'above site' payment that would allow user to|\n|settle their bill when dining in the restaurant by using their Mobile App|\n|or Mobile Internet site to check out with 'Pay at the Table' capabilities|\n|Project planning and execution, scope definition, change control, risk   |\n|mitigation, stakeholder engagement, and project close out                |\n|Budget management of up to $42M and teams of 25+                         |\n\nTechnical Skills Summary:\n . Budget / Financial Management including Business Case completion,\n   Business Process Reengineering, Budget Planning, Enterprise Content\n   Management, CRM Systems, Cost Control, Schedule Management, Strategic\n   Planning, Earned Value Analysis, Quality Control, Process Improvement,\n   Risk Management, Scope Management, Change Management, Client Relations,\n   Business Analysis, Third Party Provider Risk Management,\n . Clarity, Confluence - Collaboration Software, Jira - Workflow Management,\n   PlanView, OPPM, Microsoft Project, Project Workbench, MS Project\n   Portfolio Management, SAP, SharePoint, PeopleSoft Financial module,\n   Microsoft Visio, Access, PowerPoint, Microsoft Office Suite, Team\n   Foundation Server. Visual Studio\n . PMBOK - PMI Processes, SDLC, RAD, JAD, Agile/Scrum, Waterfall, RUP, CRM,\n   EVA, and Six Sigma.\n . Postman Collection tool - from API point-of-view. Azure DeveOps,\n   Microsoft Office 365\n                             Experience Profile\n\nWendy's, Dublin, Ohio\n                                               10/2019 - Present\nConsultant - Payment Solutions Lead Project Manager\n                     Enterprise IT & Infrastructure Solutions\n . Canada Card Present is payment solutions that spans multiple solutions;\n   On-Line Strategies Services Payments, LLC, GK Software, Moneris, GiveX ,\n   and Wendy's Payment Solutions.\n    Wendy's is seeking to implement an in-store payment processing solution\n   for their Canadian and US locations that will apply hardware-based Point-\n   to-Point Encryption (P2PE) of cardholder data (CHD) within the Ingenico\n   Lane 3000 PIN Encryption Devices (PED) that are integrated with their NCR\n   Aloha POS registers and Customer Self Order (CSO) kiosks.\n o Credit Card Processing\n o On-line Debit Card Processing\n o Gift Card Processing\n o Point-to-Point Encryption Services\n o End-of-Day Processing\n o Tokenization Services\n o Device Management Services\n o Store-to-Host Payment Processing Services\n o Network Connectivity\n\n\n . Canada Card Not Present\n . Centralized Payments Settlement\n\n\nFiserv, Dublin, Ohio\n                                               07/2018 - 08/2019\nConsultant - Biller Solutions Client Implementations Sr. Project Manager\n . Electronic Billing and Payment Solutions.\n    o Electronic Bill drives paperless\n    o Mobile Wallet\n    o Remittance solution to receive and post payment details faster\n    o Team Size 10\n . Bill Discovery and Alert also referred to as Challenger is an ePayments\n   initiative that spans multiple solutions; Fiserv Output Solutions,\n   ePayments, and Billler Solutions. The solution requires Fiserv to get pre-\n   loaded biller data to setup biller as payee. This project focuses on\n   developing an end to end process that will be followed to setup each\n   Biller, across the Biller and Payments Group (BPG).\n . Designed business modelling, requirements modelling, and system analysis\n   to map out the desire workflow to understand it more completely.\n . Engaged with customers and technical staff to create business and\n   technical requirements and other deliverables on projects.\n . Developed and maintained documentation for assigned projects.\n . Partnered with various IT departments to achieve alignment on the\n   supportability and architectural standards related to the technology\n   being implemented.\n . Communicated project status internally and managed communication between\n   the project team and stakeholders. Utilized Clarity to maintain project\n   risks, action items/issues, milestones, and tracked team members' time.\n\nBill Discovery and Alert Projects:\n . PennyMac Loan Services\n . NISSAN/Infiniti\n . TXU Energy\n . AEP (American Electric Power)\n . Frontier Communications\n . AT&T\n . Southwest Gas\n . Jacksonville Electric Authority (JEA)\n\nOCP, San Mateo, CA\n                                               08/2017 - 04/2018\nConsultant - Remote Project Manager/Scrum Master - Software-as-a-Service\nProject\n . Team Size 5 - 7\n . Streamlined systems, and resolved issues for increased profitability, on-\n   time project delivery, and optimized customer satisfaction.\n . Tracking multiple projects with multiple budgets.\n . Effectively managing globally distributed model of delivering solution.\n . Developed business processes and system solutions for various domains\n   from conceptualization to implementation.\n . Managed project using Waterfall/Agile-hybrid framework.\n\nFirst Data Corporation, Hagerstown, MD\n07/2016 - 06/2017\n(Digital Commerce Solutions)\nConsultant - Sr. Project Manager/Scrum Master\nAdvisory Services: Key tasks include client interaction, needs assessment\nanalysis, process improvements, and the ability to execute on key projects\nwithin anticipated schedules.\n . In addition to client facing Project Management responsibilities, I have\n   provided consulting services on Digital Commerce, analysis of Mobile\n   Payment, and Reward programs.  Designed and reviewed of various documents\n   including the Software Requirement Specifications\n   (SRS), Business requirements document (BRD), Use Case Specifications,\n   Functional Specifications (FSD), Systems Design Specification (SDS),\n   Requirement Traceability Matrix (RTM) and testing documents.\n . Created and managed project plan for client solutioning project using --\n   Agile-Waterfall Hybrid methodology, as part of (Digital Commerce\n   Solutions).\n . Developed the project charter, integrated project plan, resource plan,\n   contingency plan and related project management artifacts. Developed\n   communication plans for (day-to-day discussions, team meetings) and\n   written (status reports, change requests) form.\n . Defined and tracked project milestones, maintained and reported on\n   overall integrated delivery plan.\n . Coordinated and facilitated post deployment reviews for continual release\n   improvements.  Managed deployment of concurrent releases; communicated\n   release processes, timelines, requirements, and strategies to\n   stakeholders and project resources.\n . Managed change management requests to ensure release contents were\n   compliant.\n . Maintained communications with QA to ensure problem records were resolved\n   and closed prior to release launch. Created and distributed release\n   processes to cross functional teams.\n\nPay Now APIs Project\n . Budget-450K, Team size- 10+\n . Worked collaboratively with customers, as well as internal and external\n   stakeholders.\n . Created and managed project plan for additional functionality request to\n   enable client to process an 'above site' payment that would allow user to\n   settle their bill when dining in the restaurant by using their Mobile App\n   or Mobile Internet site to check out with 'Pay at the Table'\n   capabilities.\n\nCiti Retail Services Token Service Provider and Integration with Samsung\n . Budget- 1M, Team size- 9-13\n . Track First Data Integration with TCH for Cryptogram Verification\n   associated with Token.\n . Manage connectivity with TCH which include Cryptogram Validation,\n   authorization Advice, and Detokenization.\n\nMasterCard Introduction of New 2 Series BIN Project- (Compliance Project)\n . Acquirers will need to upgrade their systems to be compatible with 2-\n   Series BINs.\n . All merchants will need to be compliant with the 2-Series migration as of\n   January 1st. 2017.\n\nPetro Universal Commerce Enablement Project\n . Budget-2.5M, Team size- 25+\n . Mobile Application solutions developed to integrate loyalty and offer\n   products with payment solutions - Pay, Earn, Redeem, Save.\n . Managed mobile device authentication - enhance the security of merchant,\n   commerce offering providing multiple layers of mobile device\n   authentication.\n\nValuelink TEST MIDs Project (Operational Efficiencies Project)\n . Budget-100K, Team size- 9+\n . The objective of this concept/idea is to create a series of TEST MIDs as\n   a result of anticipated growth; to be used exclusively in a sandbox/test\n   environment.\n . Planned and managed requirements to allow quick boarding of potentially\n   new or standard Valuelink merchants on the uComm gateway and in some\n   cases the merchants are not existing Valuelink clients or are Valuelink\n   standard clients.\n\nDiscover Financial Services, New Albany, OH                         02/2015\n- 12/2015\n(Common Payment Platform)\nConsultant - Sr. Program Manager / Scrum Master: Wallet Service Provider -\nApple Pay\n . Budget-5M, Team size- 25+\n . Created and managed project plan for Apple Pay Implementation project\n   using Agile - Waterfall Hybrid methodology, as part of Common Payment\n   Platform (CPP). Created Business Requirement Document (BRD), Functional\n   Requirement Specification (FRS) document, User Requirement Specification\n   (URS) and Change Request (CR) document for system application\n   development.\n . Collaborated with multiple functional areas to complete required\n   deliverables.\n . Performed contract Gap Analysis.\n . Performed Requirement Analysis and developed Use Cases, Activity Diagrams\n   using Visio diagraming. Worked with a wide variety of internal staff,\n   including senior level management, to ensure process flows and policies\n   are properly defined and documented according to company guidelines.\n . Coordinated and facilitated post deployment reviews for continual release\n   improvements.  Managed deployment of concurrent releases; communicated\n   release processes, timelines, requirements, and strategies to\n   stakeholders and project resources.\n . Maintained communications with QA to ensure problem records were resolved\n   and closed prior to release launch. Created and distributed release\n   processes to cross functional teams.\n\n\n3-D Secure Integration Project (ProtectBuy)\n . ProtectBuy is an added layer of security to Discover card holder that\n   helps make online shopping experience more safe and secure.\n . Defined boarding process and procedures\n . Defined engagement process\n . ACS Vendor Certification; MPI Vendor Certification; Issuing Participant\n   Certification\n . Acquirer/Merchant Certification; Hosted Vendor - OBO (On-behalf-Of);\n   Defined Support Service Model\n . Anti-Money Laundering Project\n . Performed a variety of AML/KYC money laundering activities in support of\n   Bank Secrecy Act (BSA) and /or Anti-Money Laundering regulations designed\n   to prevent financial-related crimes against the government while\n   decreasing client reputational risk.\n . Review cases for adherence to EDD procedures and ensure that\n   recommendations are accurate based on all information available.\n\nCitigroup, Cincinnati, OH\n03/2014 - 09/2014\n(Digital ATM)\nConsultant - Program Manager: ATM Windows 7 Upgrade\n . Budget-42M, Team size- 50+\n . Managed all forecasting and demand planning activities for E01 and E02\n   project size and effort estimates in excess of $41 Million dollars for\n   global initiatives.\n . Managed Windows 7 32-bit software solution that is in compliance with\n   Citi CATE in 23 countries.\n . Managed regional project managers.\n . Utilized HP Quality Center and TFS to manage release plan.\n . Managed Global release to support ATM Windows 7.\n . Performed integration change control.\n . Managed acceptance criteria for regional consumptions.\n\nOne Call Professionals, Columbus, OH\n02/2013 - 11/2013\nConsultant - Scrum Master: eCommerce (Global Distribution System) CRM\n . Worked on one workstreams of a large project to manage GDS - e-commerce\n   (Global Distribution System) implementations.\n . Coached team in optimal time utilization through concepts of scrum and\n   agile management methodologies.\n . Led sprint reviews and daily scrum meetings to touch base with whole team\n   and ensure that all members were performing satisfactorily.\n . Trained team members as well as the rest of the company staff in\n   employing agile and scrum practices to improve work flow.\n . Managed system integrations. Planned the release to ensure that sprints\n   did deliver increments with the right quality.\n\nJPMorgan Chase, Columbus, OH                                  02/2012 -\n10/2012\nConsultant - IT Risk Project Manager:  (Data Management & Analytics)\n . Managed multiple cross-functional projects with globally dispersed teams\n   to manage the risks, issues and change request to minimize its impact to\n   project scope, schedule and cost on the design and build of Integrated\n   Consumer Data Warehouse (ICDW); Enterprise Data Warehouse (EDW); and\n   Info1.\n . Communicated the project status, risks and issues on regular basis to all\n   stakeholders.\n . Collaborated with multiple functional areas to complete required\n   deliverables. Provided day to day direction to team members. Managed\n   Enterprise Security Manager (ESM) remediation review for Info1 Data\n   Management.\n . Managed Guardium Logs Reporting on EDW (Enterprise Data Warehouse)\n   servers, to ensure that there is an audit trail of the changes being\n   performed by the DBA team with elevated access.\n . Designed Business Impact Analysis process for ICDW (Integrated Customer\n   Data Warehouse), Information One, and EDW staffs.\n . Managed ICDW (Integrated Customer Data Warehouse) Control Self-Assessment\n   to ensure that data back-up and retention requirements are defined and\n   agreed upon by key business stakeholders.\n . Managed third Party Provider Oversight-Lifecycle to ensure that there is\n   a clear escalation/reporting process for issues arising (e.g. security\n   breaches, Service Level Agreement problems).\n\nStaffmark, Cincinnati, OH                                     01/2011\n-10/2011\nConsultant - IT Project Manager\n . Provided management with accurate, independent and timely assessments in\n   the area of risk, internal control weakness, information systems\n   security, ERP Systems (PeopleSoft -CRM), database technologies, disaster\n   recovery and Business continuity planning etc.\n . Managed resources during the entire project with strict deadlines and\n   budget constraint.\n . Acted as a point of contact to all stakeholders by providing regular\n   project status updates to avoid communication breakdown and surprises.\n   Facilitated project meetings, change control, resource planning and\n   leveling, risk management and contingency planning.\n\n\n\n\n\nVeyance Technologies, Inc, Marysville, OH\n                                  04/2007 - 08/2008\nConsultant - IT Project Manager /Scrum Master\n . Plant infrastructure upgrade and SAP Implementation\n . Create and execute Lean and Operational Excellence organizational\n   strategies, manage business program initiatives, and identify and\n   implement Best Practices.\n . Led and managed the Lean Design for Innovation Committee for the\n   development of an \"Innovation Methodology Playbook\"\n . Led cross-functional team to design and implement SAP ERP/CRM system for\n   the Manufacturing business using ASAP approach.\n . Worked with and managed off-shore developer teams.\n . Designed a process for managing new data Center applications and\n   initiatives, from concept to implementation.\n\nJPMorgan Chase, Columbus, OH                                  1991 - 2006\nIT Project Manager\n . Managed several medium to large development projects focusing mostly on\n   call center infrastructure, data center applications and enterprise\n   telecommunication offerings with a project team of between 8 and 20, and\n   duration of between 3 months and 12 months: with the aid of MS Project\n   Portfolio Management.\n\n|Education & Certifications                                                 |\n\n\n . Master of Business Administration, Information Technology Project\n   Management, Minor - Finance, Ashland University, Ashland, OH\n\n\n . Bachelor of Science, in Business Administration, Franklin University,\n   Columbus, OH\n . Information Technology Project Management Certificate (MBA, Ashland\n   University) [pic][pic]\n-----------------------\n                           Program/Project Manager\n\n\n",
          "confidence": 1.0,
          "method": "full_text",
          "structured_data": null
        },
        "tax_term": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "source_by": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "designation": {
          "value": "Project Manager",
          "confidence": 0.9,
          "method": "ner",
          "structured_data": null
        },
        "experience": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        }
      }
    },
    {
      "resume_path": "data\\input\\Aleem Ahmed - resume.doc",
      "confidence_score": 0.635,
      "used_ocr": false,
      "extracted_fields": {
        "skills": {
          "methodologies": [
            {
              "name": "xp",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                58,
                5614,
                6651
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "safe",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                103
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "rad",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                801
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "agile",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                4556,
                5443,
                5479
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "tdd",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                4967
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "scrum",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                5449
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "rup",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                7027
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "devops": [
            {
              "name": "svn",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                77
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "git",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                81,
                3001
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "openshift",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                285
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "bitbucket",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                3013
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "cloud": [
            {
              "name": "jenkins",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                152,
                2606,
                3005,
                3828,
                4403,
                5239,
                5572
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "docker",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                160,
                256,
                2375
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "kubernetes",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                187
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "aws",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                270,
                2189,
                2262,
                2312,
                3033
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "ec2",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                274
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "batch",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                2616,
                3960
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "functions",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                5106
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "programming": [
            {
              "name": "chef",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                167
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "puppet",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                172,
                3108
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "gradle",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                222
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "java",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                304,
                549,
                720,
                934,
                1025,
                2569,
                2888,
                3178,
                3882,
                4238,
                4493,
                5497,
                5715,
                6551,
                6742,
                6829,
                7800
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "sql",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                314,
                5078,
                7923,
                7927
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "xml",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                318,
                335,
                354,
                2935,
                3495,
                4309,
                4888,
                5529,
                6193,
                6866,
                6924,
                7539,
                7832
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "xslt",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                344,
                6221,
                6936,
                7836
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "html",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                455,
                1702,
                1789,
                3857,
                6961,
                7899
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "python",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                582,
                3058
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "css",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                1707,
                1809
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "javascript",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                1711,
                1794,
                2954,
                3862,
                4323,
                7888
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "json",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                2970
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "perl",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                3053
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            }
          ],
          "frameworks": [
            {
              "name": "ansible",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                179
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "soap",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                349,
                3499,
                4313,
                4869,
                5524,
                6054,
                6600
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "spring",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                400,
                692,
                1035,
                1300,
                1531,
                1591,
                1756,
                2898,
                2909,
                3586,
                4263,
                4274,
                5022,
                5545,
                6449,
                6561
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "testng",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                2771,
                4115
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "junit",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                2988
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "jquery",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                4334
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "databases": [
            {
              "name": "oracle",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                589,
                606,
                635,
                660,
                758,
                1056,
                2645,
                2994,
                3222,
                3284,
                3437,
                3989,
                4221,
                4352,
                4363,
                5068,
                5580,
                7948
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "domain_specific": [
            {
              "name": "utilities",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                596,
                3229,
                3291
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "insurance",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                891,
                4440
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "design",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                968,
                1161,
                1209,
                1724,
                4844,
                5849,
                6843,
                7078,
                7179
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "architecture",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                1349,
                4589,
                4695,
                4747,
                5960,
                7377
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "infrastructure",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                2087
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "energy",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                3135
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "security",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                3593,
                4860
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "education",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                8005
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "soft_skills": [
            {
              "name": "analysis",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                1152
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "leadership",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                5356
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "presentation",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                6257,
                6905,
                6994
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "consulting",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                6676
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "sdlc": [
            {
              "name": "refactoring",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                1216
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "specialized_domains": [
            {
              "name": "containerization",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                2382
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ]
        },
        "first_name": "Aleem",
        "last_name": "Ahmed",
        "primary_email": {
          "value": "ahmed.8663167@gmail.com",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "secondary_email": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "phone": {
          "value": "6089091931",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "city": {
          "value": "",
          "confidence": 0.0,
          "method": "ner",
          "structured_data": null
        },
        "state": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "zip": {
          "value": "",
          "confidence": 0.0,
          "method": "regex",
          "structured_data": null
        },
        "work_authority": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "resume_link": {
          "value": "data\\input\\Aleem Ahmed - resume.doc",
          "confidence": 1.0,
          "method": "file_path",
          "structured_data": null
        },
        "raw_resume": {
          "value": "\n\nAleem Ahmed\n\n\n(608) 909-1931\n\n\nahmed.8663167@gmail.com\n\n----------------------------------------------------------------------------\n             --------------------------------------------------\n\nProfessionally 14+ experience  in  Java  development  experience  of  Web/N-\nTier/Client Server/Distributed technologies (SOA,  Restful/SOAP,  WSDL,  Web\nServices, JMS, Spring, Hibernate,  EJBs,  Servlets,  JSP,  XML,  JavaScript,\nStruts, WebSphere Application Server, Rational  Application  Developer(RAD),\nWebLogic Application Server) and Client Server technologies (AWT, Swing)  on\nvarious platforms  like  Windows  2k/NT/XP/7/8,  Linux  and  Unix.  He  also\nexperienced in working on DevOps process and tools (Code review, unit test\nautomation, Build & Release automation, Environment, Service,  Incident  and\nChange  Management)  .He  has  developed  software  systems   for   Utility,\nInsurance and  Telecom  domains  with  responsibilities  in  all  phases  of\nsoftware development life cycle  including  Analysis,  Design,  Development,\nTesting, Implementation, Support and Training.\n\n\n .  Strong  experience  in  AWS Cloud  platform  and  its  features  which\n   includes EC2, VPC, EBS, Cloud Watch,  Cloud  Trail,  CloudFormation  AWS\n   Config, Autoscaling, Cloud Front, IAM, S3. Drives Enterprise Application\n   technical blueprints, business requirements & specifications, integration\n   of heterogeneous environments, API implementations. He is an  independent\n   worker  with   strong   problem   solving,   analytical,   interpersonal,\n   communication, presentation and testing skills.\n\n\n . Expertise in developing  inter-operable  Web  Services  and  its  related\n   technologies like SOAP, WSDL, UDDI, XML related technologies/tools such\n   as JAXB, XSD,  XML with  good  understanding of JAX-WS,  JAX-RS,  JAX-\n   RPC inter-operable issue.\n\n\n . Development experience using Java/J2EE ( JSF, JSP, Servlets,  JDBC,  EJB,\n   MDB, JTA, JPA, JNDI, JMS, IBM MQ, Web Services (Axis, JAX-RPC, JAX-WS),\n   MVC Struts frameworks, Hibernate , Spring  ,  Log4j,  JUnit4,  Maven  and\n   ANT).\n\n\nTechnical Skills:\n|Operating Systems   |Linux, Unix, Solaris, Windows /NT/2000/2003/XP      |\n|Version Control     |SVN, GIT, CVS, Visual Source Safe, PVCS, Clear Case,|\n|                    |Bit bucket                                          |\n|Automation/Build    |Jenkins, Docker, Chef, Puppet, Ansible, Kubernetes, |\n|Tools               |Harvest, ANT, Maven, Gradle, Artifactory            |\n|Virtualization      |Docker, Amazon AWS/EC2, VMWare, OpenShift           |\n|Language            |Java, J2EE, SQL                                     |\n|XML Technologies    |XML, WSDL, XSLT, SOAP, XML Schema, Web services.    |\n|Web Technologies    |J2EE, Spring, Hibernate, JSP, JNDI, LDAP, Servlets, |\n|                    |JMS, JDBC, Swing, HTML                              |\n|Web App & Web       |WebLogic, WebSphere, Tomcat, JBoss, JIRA, Service   |\n|Servers             |Now                                                 |\n|Scripting Languages |Java Script, Unix shell scripting, Python           |\n|Oracle Utilities    |Oracle Customer Care & Billing, Oracle MDM, OUAF    |\n|Database            |Oracle 10g/9i/11g                                   |\n|App Framework       |Spring MVC, Struts, Hibernate, Java Persistence API |\n|Data Integration    |Oracle Fusion, MQ, WMB 7.0                          |\n|IDE Tools           |Eclipse, RAD 8.5/7.0/6.1, JBuilder, JDeveloper,     |\n|                    |IntelliJ Idea                                       |\n\n\nProfessional Experience:\nAmerican Family Insurance, Madison WI                               (July\n2015 - Feb 2020)\nSr. Java Developer\nResponsibilities:\n o Design, develop, and maintain web-based applications  using  Java,  J2EE,\n   Spring, Hibernate, and Oracle\n o  Involved  actively  in  the  complete  software  development  lifecycle,\n   including performance analysis, design, development, and testing\n o Participate in design refactoring and enhancement of critical component\n o Developed the application using spring  framework  that  leverages  model\n   view layer architecture, also configured Dependency Injection.\n o Extensively used Hibernate in data access  layer  to  access  and  update\n   information in the database.\n o Involved in the Development of Spring Framework Controllers\n o Developing application using Spring core module and worked  on  Messaging\n   service.\n o Developed user interface using JSP, JSP Tag libraries  JSTL,  HTML,  CSS,\n   JavaScript.\n o Design highly usable GUIs using Spring Framework, JSPs, Struts MVC, HTML,\n   JavaScript, and CSS\n o Worked  on  Single  Page  Applications  using  AngularJS  which  includes\n   Routing, Directives, and Templates.\n o Used JMS for the asynchronous exchange  of  critical  business  data  and\n   events amongJ2EE components and legacy system.\n o Implemented a fully automated CI build and deployment infrastructure  and\n   processes for multiple projects.\n o Involved in performing application deployment to AWS Elastic  Bean  Stack\n   environment.\n o Configured & deployed applications on AWS for a multitude of applications\n   utilizing the AWS stack, cloud formation.\n o Standalone system through SCCM and Docker containerization.\n o Performed  source  control  systems  to  automate  for  Build/deployment,\n   Software Configuration/Continuous Integration/Continuous Delivery/Release\n   Management related tasks in Java EE Environments using Maven and Jenkins\n o Batch/Shell scripting\n o Used Oracle as a transactional database for customer payments.\n o Participated in integration testing and unit testing.\n o Implemented TestNG framework for system integration testing.\n o Performed troubleshooting and fixed production defects.\n\n\n\n\nEnvironment: JAVA, J2ee, Spring MVC,  Spring  Core,  Hibernate,  JSP,  XML,\nAngularJS, CSS3, JavaScript, AJAX, JSON, RESTful  APIs,Junit,  Oracle,  Git,\nJenkins,  Bitbucket,  Maven,  CVS,  AWS,  Shell  scripting,  Perl,   Python,\nGuidewire Applications, Apache Tomcat, JBoss, Puppet, CA Harvest, JIRA.\n\nDTE Energy, Detroit, MI                                           (May 2013\n- May 2015)\nSenior Java Developer\nResponsibilities:\n o Worked on Oracle Utilities Customer Care and Billing Module.\n o Worked on Oracle Utilities Application Framework (OUAF) for usage data\n   management for bill print module.\n o Integrated meter data management with billing module using Oracle Fusion\n   ESB.\n o Implemented web services using JAX-WS, XML, SOAP, and WSDL.\n o Produced and consumed web services from external systems.\n o Implemented Spring security for payment gateway.\n o Performed source control systems to automate for Build/deployment,\n   Software Configuration/Continuous Integration/Continuous Delivery/Release\n   Management related tasks in JavaEE Environments using Maven and Jenkins\n o Developed UI using HTML, JavaScript, CSS3, and Java Server Pages (JSPs).\n o Business Logic was implemented using EJBs and DAOs.\n o Batch/Shell scripting\n o Used Oracle as a transactional database for customer payments.\n o Participated in integration testing and unit testing.\n o Implemented TestNG framework for system integration testing.\n o Performed troubleshooting and fixed defects.\n\nEnvironment: Oracle CCB 2.3.1, Java, J2EE,  JSP,  MVC  Struts,  Spring  MVC,\nSpring Core, Hibernate, Web Services, XML, SOAP, WSDL,  JavaScript,  jQuery,\nHTML5,  CSS3,  Oracle  11g,  Oracle  Fusion  ESB,  WebLogic  Server,  Maven,\nJenkins, ClearCase, UNIX, Windows 8.\n\n\n\nAAA Insurance, Dearborn, MI                            (October 2010 -\nApril 2013)\nSenior Java Developer\nResponsibilities:\n o Developed SOA services using Agile methodology.\n o Designed the architecture for middleware integration with external\n   systems based upon business requirements using J2EE architecture\n   framework with special emphasis on SOA architecture and web services.\n o Implemented web services using JAX-WS and JAX-RS.\n o Developed SOA design using WS-Security, SOAP/HTTP, WSDL, and XML/XSD\n   (schemas).\n o Utilized elements of extreme programming approach (especially TDD, pair\n   programming, and code walkthrough's).\n o Developed Spring and Hibernate applications.\n o Developed Oracle PL/SQL packages, procedures, and functions.\n o Worked with messaging queue applications.\n o Developed web application using Struts.\n o Extensively used Maven for build and Jenkins for continuous integration.\n o Performed code reviews and suggested best coding practices.\n o Provided technical leadership for development team to implement the\n   system.\n o Developed applications using Agile Scrum methodology.\n\nEnvironment: Agile Methodology, Java, J2EE, SOA, Web  Services,  SOAP,  XML,\nWSDL, Struts, Spring, Hibernate, JMS, Maven, Jenkins, Oracle 11g, Linux  and\nWindows 2003/XP/7, Data Power, WebSphere Application Server 7.\n\nSyniverse, Tampa, FL                                       (January 2008 -\nSeptember 2010)\nSenior Java Developer\nResponsibilities:\n o Worked on Direct Operator Billing SOA project and implemented web\n   services using JAX-WS.\n o Performed design, development, and testing of the web services\n   application.\n o Developed web services using service-oriented architecture (SOA).\n o Created and updated WSDLs for web services (produce and consume both).\n o Used SOAP protocol to access the web service - WSDL schemas and JAX-RPC\n   for remote.\n o Procedure calls of the third-party property leads (SOA).\n o Used XML technologies like XPath, XSLT, and JAXB for data exchange and\n   presentation.\n o Developed middleware components like Message Driven Beans for JMS/MQ\n   integration.\n o Built service-oriented platform using web services for client requests.\n o Developed code using spring and hibernate frameworks to access database.\n o Used CVS for source code maintenance.\n\nEnvironment: Java/J2EE, Spring, Hibernate, JPA, Web  Service  (XPath,  SOAP,\nWSDL, JAXB, JAX-RPC), MDB, SAML, JBoss, Windows 2003/XP, UNIX, Solaris.\n\nMindTree Consulting Pvt Ltd, Bangalore, India                   (June 2004\n- December 2007)\nSenior Java Developer\nResponsibilities:\n o Analyzed, designed, and developed application based on Java/J2EE and\n   design patterns.\n o Used XML technologies for data exchange and presentation using XML, XSL,\n   and XSLT.\n o Developed various HTML, DHTML, and JSP pages for the presentation layer.\n o Implemented RUP methodology and designed application using UML design\n   patterns.\n o Visualized and designed use cases, sequence diagrams, and class diagrams.\n o Used J2EE design patterns and developed EJB and DAO components.\n o Used Struts framework for the development of J2EE components.\n o Created servlets for abstraction between the client and server layers\n   (using MVC Architecture).\n o Developed DAO (Data Access Objects) and called them from EJB.\n o Designed and developed the system business logic using Stateless Session\n   Beans.\n o Used XML templates to present the user data in different formats.\n o Created JAR, WAR and EAR files.\n o Deployed EAR file on WebLogic Application Server.\n o Used ClearCase for version control.\n o Created build scripts using ANT.\n o Provided support and maintenance.\n\n\nEnvironment: Java, J2EE, JDBC,  Servlets,  JSP,  EJB,  XML,  XSLT,  WebLogic\nApplication  Server  8.1/6.1,  Struts,  UML,  JavaScript,  HTML,   JBuilder,\nClearCase, SQL, SQL Server 2000/2005,  Oracle  10g/9i,  UNIX,  Sun  Solaris,\nRedhat Linux, Windows 2000.\n\n\n\nEducation: Master's Degree in Computer Applications Osmania University,\nIndia\n\n",
          "confidence": 1.0,
          "method": "full_text",
          "structured_data": null
        },
        "tax_term": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "source_by": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "designation": {
          "value": "Application Developer",
          "confidence": 0.8,
          "method": "regex",
          "structured_data": null
        },
        "experience": {
          "value": "14+",
          "confidence": 0.9,
          "method": "regex_total_experience_summary",
          "structured_data": null
        }
      }
    },
    {
      "resume_path": "data\\input\\Alexander Mai - SAS - VC.doc",
      "confidence_score": 0.66,
      "used_ocr": false,
      "extracted_fields": {
        "skills": {
          "programming": [
            {
              "name": "sas",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                25,
                59,
                2239,
                3803,
                4402,
                6017,
                6684
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "sql",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                2247,
                3807,
                4425,
                6051,
                6688
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "xml",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                2395,
                2411
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            }
          ],
          "domain_specific": [
            {
              "name": "medical",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                95,
                435,
                2329
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "design",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                174
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "insurance",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                297,
                580,
                5239
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "policy",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                3520
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "clinical",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                3679
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "government",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                4824
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "finance",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                5031
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "construction",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                5922
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "security",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                6215
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "sdlc": [
            {
              "name": "surveys",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                423
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "soft_skills": [
            {
              "name": "research",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                728
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "analysis",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                879,
                1021,
                1211,
                2486,
                2594,
                3294,
                5882
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "documentation",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                1637,
                6904
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "organization",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                3380,
                6408
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "consulting",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                3974
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "presentation",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                6891
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "business_skills": [
            {
              "name": "reporting",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                922,
                1702,
                3217,
                3527,
                3853,
                4013,
                6523
              ],
              "experience_weight": 0.0,
              "importance_score": 0.8
            }
          ],
          "networking_equipment": [
            {
              "name": "accounting",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                3988,
                5020
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "cloud": [
            {
              "name": "shield",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                6262
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "data_skills": [
            {
              "name": "excel",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                6812
              ],
              "experience_weight": 0.0,
              "importance_score": 0.9
            }
          ]
        },
        "first_name": "Alexander",
        "last_name": "T. Mai",
        "primary_email": {
          "value": "Alextmai@gmail.com",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "secondary_email": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "phone": {
          "value": "7742326198",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "city": {
          "value": "MA",
          "confidence": 0.7,
          "method": "ner",
          "structured_data": null
        },
        "state": {
          "value": "LA",
          "confidence": 0.8,
          "method": "city_database",
          "structured_data": null
        },
        "zip": {
          "value": "02445",
          "confidence": 0.7,
          "method": "regex",
          "structured_data": null
        },
        "work_authority": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "resume_link": {
          "value": "data\\input\\Alexander Mai - SAS - VC.doc",
          "confidence": 1.0,
          "method": "file_path",
          "structured_data": null
        },
        "raw_resume": {
          "value": "\n                              Alexander T. Mai\n\nAddress     7 University Road                                       Cell\n  (774) 232-6198\n                  Brookline, MA 02445\nEmail     Alextmai@gmail.com\n\nEducation\nNortheastern                                                      University\n                       Boston, MA\n    . Graduated with Bachelor of Science Degree in  Business  Administration\n                                        May 2007\n    . Majored in Dual Concentration: Management  Information  Systems  (MIS)\n      and Finance\n    . Completed six months co-op programs at Zoran Corporation and Raytheon\n    . Honors: Dean's List, Balfour Academy's Scholarship\n\n\nSkills\n    . Applications: Advanced SAS, SQL, R,  Stata,  XML  Mapper,  SSIS,  SAP,\n      ArcGIS, TeamSite, Cognos\n    .  Certifications:  Microsoft  Word  Certified,   SAS   Certified   Base\n      Programmer for SAS 9 Credential\n\nExperience\nHarvard                            Medical                            School\n       Boston,  MA                        Statistical  Programmer   &   Data\nAnalyst                                                                 June\n2016 - Present\n    . Design and develop statistical and data management programs associated\n      with grant funded projects on disability and health insurance.\n    . Perform in multiple types of large cross-sectional and longitudinal\n      data files. Some data stem from population-based surveys (e.g.,\n      Medical Expenditure Panel Survey, Health and Retirement Study). Other\n      data include health care and prescription drug claims from private and\n      public insurance plans (e.g., Medicare and/or Medicaid).\n    . Develop high quality data sets to estimate statistical models and\n      conduct hypothesis testing.\n    . Interpret research protocols and create programs and algorithms that\n      are then discussed with the PI.\n    . Work independently and make decisions about how to implement analysis\n      and provide well written detailed reporting of results, including\n      determination of proper summary statistics, report formats and other\n      analysis considerations.\n    . Responsible for successful completion of these tasks within competing\n      project timelines, and must ensure integrity of data collection, data\n      review, data compilation, and analysis techniques.\n\nSenior                             Whole                              Health\n                Cambridge,    MA                           HEDIS     Manager\n                                                       October  2014  -  May\n2016\n    . Responsible for all aspects of the HEDIS rates production process,\n      including working with internal data sources, external data sources,\n      supplemental data sources and vendor partners to produce and submit\n      the company's HEDIS rates on an annual basis.\n    . Supported the integration of hybrid data, administrative data\n      refreshes and supporting HEDIS documentation in order to meet\n      mandatory NCQA and CMS STARS data reporting requirements.\n    . Analyzed the HEDIS results, compares them to nationally identified\n      benchmarks, and advises the director on opportunities and strategies\n      to continue improving the rates.\n    . Participated on teams working to understand the barriers to strong\n      performance, and identifying opportunities to resolve the barriers in\n      meeting all requirements of the HEDIS program, CAHPS and HOS as\n      needed.\n    . Worked closely with relevant departments to assure understanding of\n      the financial implications of the HEDIS and Star Rating programs.\n    . Utilized SAS and SQL programming to automate and transform multiple\n      reports, reconcile and transfer medical records by file types to\n      different locations, and validate XML files using XML Mapper\n\nCommonwealth of Massachusetts - Center for Health Information  and  Analysis\n(CHIA)                 Boston, MA                        Programmer  Analyst\n                                                            February 2013  -\nOctober 2014\n    . Conducted statistical programming and analysis on All Payer Claims\n      Database (APCD), Acute Hospital Case Mix Databases, Medicaid\n      Management Information System (MMIS), Centers for Medicare and\n      Medicaid Services (CMS), Census, and other data resources to support\n      the analytic and information needs of key audiences.\n    . Worked with the Quality Assurance team to evaluate and standardize\n      APCD by applying statistical analyses related to health care service\n      utilization, health care costs, and quality measurement.\n    . Supported the analytics for the state's consumer health care quality\n      and cost website by calculating cost metrics for providers in the\n      Commonwealth and annual Cost Trends reporting.\n    . Consulted with Subject Matter Experts (SME) to determine appropriate\n      analysis plans and data extraction code specifications, coordinates\n      projects involving organization of data, text and graphics, designing\n      study methodologies; and assisting with the development of new areas\n      of data collecting and policy reporting for the agency.\n    . Applied HEDIS measures to analyze health care quality and utilization\n      data to identify best practices, care delivery trends and clinical\n      practice variation within hospital, provider group and individual\n      physician sites of care\n    . Trained other analysts on SAS, SQL, ArcGIS and helped troubleshoot\n      production reporting issues, identified root causes and implement\n      necessary coding updates\n\nCVS                                                                 Caremark\nWoonsocket,  RI                        Manager  &   Senior   Consulting   of\nAccounting and  Financial  Reporting                            May  2012  -\nJanuary 2013\n    . Responsible for the creation and delivery of multiple Medicare Part D\n      reports that summarize results, identify trends, and develop forecasts\n      for: Beneficiary Premiums, Drug Utilization, Incurred but not Recorded\n      Claims, Risk Scores, Actuarial Metrics and Member Eligibility.\n    . Utilized large databases management in an AIX environment, and applied\n      basic Unix language, SAS Data Step and Proc SQL to turn million of\n      records of raw data from multiple tables/files into summary reports.\n    . Automated majority of the reports used by the accountants to book\n      journal entries. Other reports will be delivering to external business\n      clients as part of their monthly financial package.\n    . Managed raw data from both external and internal by performing data\n      reconciliation and reports variance. External are the government\n      (Center for Medicare and Medicaid Services) and third party vendors.\n      Internal are the Enterprise Data Warehouse and monthly data extracts\n      from AS400.\n    . Interacted effectively with members of accounting, finance, actuarial,\n      forecasting, billing, account management, and Medicare operations\n      groups. In addition, reviewed and assisted junior team members in\n      their workload.\n\nMIB  Solutions,  Risk  Management  Solutions  for  the  Insurance   Industry\n                           Braintree,   MA                          Database\nProgrammer                                                           Analyst\n          December 2010 - April 2012\n    . Performed logic and syntax checks, and standardizes data submitted\n      from various sources. Appropriately documented procedures in support\n      of projects as assigned. Managed interns and worked on multiple\n      projects at the same time.\n    . Streamlined previously written computer code and used or modified to\n      meet the current project objectives. Participated in skills growth\n      path to promote achievement of corporate goals as recommended by the\n      Department Manager.\n    . Interacted with Database Programmer Analysts, Actuaries, and\n      Developers to develop software applications for data analysis.\n      Participated in the evaluation, construction and transition into\n      production of software.\n    . Migrated software applications done in SAS and the legacy environment\n      to SQL Server based applications. Complied with all applicable\n      corporate and departmental software policies, standards, and practices\n      including all standards relating to Security and Disaster Recovery.\n\nBlue       Cross       Blue       Shield        of        MA        (BCBSMA)\n                                   Boston, MA\nData                  Analyst                  &                  Programmer\n                                July 2007 - February 2010\n    . Managed production databases and web/online  content  development  and\n      organization.\n    . Designed/developed, programmed, maintained and  published  operational\n      reports.\n    . Delivered production ready reporting with graphs, charts  and  dynamic\n      drill through capabilities.\n    .  Performed  adhoc  and  customer   reports   for   Informatics/analyst\n      community.\n    .  Developed  tools  using  SAS,  SQL,  and  Cognos,  to  minimize  data\n      manipulation required by informatics analysts.\n    . Created value by integrating current business Excel, Access  processes\n      into DW production process.\n    . Trained interns  and  prepared  presentation,  documentation  and  key\n      management reports.\n\n\n\n\n",
          "confidence": 1.0,
          "method": "full_text",
          "structured_data": null
        },
        "tax_term": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "source_by": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "designation": {
          "value": "Data Analyst",
          "confidence": 0.9,
          "method": "ner",
          "structured_data": null
        },
        "experience": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        }
      }
    },
    {
      "resume_path": "data\\input\\Data Architect - VA - Mehadi.doc",
      "confidence_score": 0.8150000000000001,
      "used_ocr": false,
      "extracted_fields": {
        "skills": {
          "domain_specific": [
            {
              "name": "architecture",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                5,
                5561,
                8164,
                9693,
                9771,
                9897,
                10328,
                11062
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "design",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                41,
                736,
                1870,
                8008,
                8141,
                9746,
                10305,
                11039
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "governance",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                85,
                2669,
                2908
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "mining",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                191,
                2149
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "government",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                378
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "banking",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                419,
                6059,
                6423
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "engineering",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                668,
                6254,
                8375,
                10539,
                11273,
                11608,
                14096,
                14162
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "audit",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                926,
                3124,
                4717,
                5579
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "security",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                5658,
                6809,
                15214
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "retail",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                6407
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "marketing",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                6456
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "defense",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                7828,
                7969,
                8049,
                10560,
                10620,
                13115
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "education",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                9554,
                9857,
                15703
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "military",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                10694,
                10796
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "logistics",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                11668,
                13260
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "soft_skills": [
            {
              "name": "analysis",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                109,
                748,
                2120,
                2279,
                3590,
                4029,
                4046,
                4492,
                4833,
                6745,
                7079,
                8761,
                12227,
                14876
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "documentation",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                1273
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "presentation",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                1287,
                11530
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "research",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                6475,
                7427,
                7473,
                7505,
                8177,
                10341,
                11075
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "business_skills": [
            {
              "name": "reporting",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                118,
                1039,
                5721
              ],
              "experience_weight": 0.0,
              "importance_score": 0.8
            }
          ],
          "data_skills": [
            {
              "name": "etl",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                154,
                757,
                1877,
                2202,
                5787,
                7491,
                7692,
                7707,
                7758,
                11514
              ],
              "experience_weight": 0.0,
              "importance_score": 0.9
            },
            {
              "name": "erwin",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                496,
                4640,
                8475,
                10826,
                14434
              ],
              "experience_weight": 0.0,
              "importance_score": 0.9
            },
            {
              "name": "informatica",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                761,
                1076,
                2832
              ],
              "experience_weight": 0.0,
              "importance_score": 0.9
            },
            {
              "name": "databricks",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                1094,
                2348
              ],
              "experience_weight": 0.0,
              "importance_score": 0.9
            },
            {
              "name": "hadoop",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                1168
              ],
              "experience_weight": 0.0,
              "importance_score": 0.9
            },
            {
              "name": "hive",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                1175
              ],
              "experience_weight": 0.0,
              "importance_score": 0.9
            },
            {
              "name": "excel",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                1316,
                6378,
                14911
              ],
              "experience_weight": 0.0,
              "importance_score": 0.9
            }
          ],
          "programming": [
            {
              "name": "xml",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                531,
                9998
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "sql",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                794,
                801,
                805,
                819,
                1248,
                1393,
                2042,
                6367,
                11774,
                12325,
                12338,
                14806,
                14819,
                14891,
                15384
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "python",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                1027
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "crystal",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                1148
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "sas",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                1191
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "java",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                1404
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "perl",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                14828
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "vba",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                14929
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            }
          ],
          "databases": [
            {
              "name": "oracle",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                956,
                1234,
                2857
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "sybase",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                1241
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "db2",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                1269
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "cloud": [
            {
              "name": "azure",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                1088,
                1105,
                2327
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "aws",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                1164
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "translate",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                9306,
                12587
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "functions",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                9449,
                12730,
                15348
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "batch",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                12301,
                14303,
                14779
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "sdlc": [
            {
              "name": "optimization",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                12512,
                15010
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ]
        },
        "first_name": "Mehadi",
        "last_name": "Hassan",
        "primary_email": {
          "value": "Mehadimh@yahoo.com",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "secondary_email": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "phone": {
          "value": "7039661154",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "city": {
          "value": "Nokesville",
          "confidence": 0.9,
          "method": "address_pattern",
          "structured_data": null
        },
        "state": {
          "value": "VA",
          "confidence": 0.9,
          "method": "address_pattern",
          "structured_data": null
        },
        "zip": {
          "value": "20181",
          "confidence": 0.9,
          "method": "address_pattern",
          "structured_data": null
        },
        "work_authority": {
          "value": "With Excellent Credit And Clean Background Industry",
          "confidence": 0.8,
          "method": "regex",
          "structured_data": null
        },
        "resume_link": {
          "value": "data\\input\\Data Architect - VA - Mehadi.doc",
          "confidence": 1.0,
          "method": "file_path",
          "structured_data": null
        },
        "raw_resume": {
          "value": "\nMehadi Hassan\n11800 Belmont Farm Ln, Nokesville, VA 20181\nMehadimh@yahoo.com\n703-966-1154\n\n\nSYNOPSIS:\n\nA senior level Information Technology professional with years of experience\nin leading and hands-on experience in all phases of database solutions\nrelated projects for government and non-government organizations.\n\nCORE COMPETENCIES:\nData Architecture, Data modeling, Database design and development, Data\nManagement, Data Governance, Complex Data Analysis,\nReporting/Visualization, Data Ingest (ETL) for Datamart, Data profiling,\nData mining, Data Quality, Metadata Management, Master Data Management,\nRequirements gathering, Quality Assurance/Testing\n\nWORK STATUS:\nUS CITIZEN with Excellent Credit and Clean background\n\nINDUSTRY:\nGovernment (DOD, DHS-TSA, HHS, DOE), Financial/Banking/Mortgage/Credit etc.\n\nTECHNCAL SKILLS:\n\nModeling/Methodologies: ER studio, Erwin, Canonical data model (CDM), UML,\nXML, National Information Exchange Model (NIEM), Decision Modeling, Object\nModeling, Ontology, Star Schema, Snow Flake, OLAP, OLTP, Forward/Reverse\nEngineering, Natural Language Processing, Conceptual/Logical/Physical\nDesign\n\nData Analysis/ETL: Informatica (AXON, IDQ, Glossary), PL/SQL, No SQL, SQL\nPlus, TOAD, SQL Developer, IBM Information Server Suite (Information Server\nAnalyzer, Web sphere, Quality Stage, Data Stage, Audit Stage, Business\nGlossary), Oracle Data Quality for Data Integration (Profiling, Time\nSeries, Quality), Python, R\n\nBI/Reporting/Visualization/Cloud: Tableu, Informatica, Azure Databricks,\nAzure Data lakes, POWER BI, Business Objects,  Crystal Reports, AWS,\nHadoop, HIVE, Map Reduce, SAS BI\n\nOperating Systems: Windows, UNIX.\n\nDBMS: Oracle, Sybase, SQL Server, MS Access, DB2.\n\nDocumentation & Presentation: PowerPoint, Word, Excel, Visio, MS Project.\n\n Other: Clear Case, Clear Quest, Control M (scheduler), JIRA, SQL Loader ,\nJava, C++\n\n\nBUSINESS EXPERIENCE:\n\nLeidos/Accelerated Information Management\n                                  11/07 - Present\nClients: DHS-TSA, HHS\n\nLead Data Architect\n\nTask highlights:\n    . Meet clients, stake holders and subject matter experts on regular\n      basis to gather requirements, collect information, understand/clarify\n      business rules, present logical and physical data models, get approval\n      on proposed data models, present sample reports etc.\n    . Construct conceptual/logical/physical data models, compose data\n      dictionary, design ETL and create source target mapping.\n    . Collect, archive, cleanse and validate metadata from various sources\n      to compose master data and various metadata.\n    . Wrote complex PL/SQL scripts such as stored procedures, triggers,\n      transformation logics for data analysis and validation, data\n      mining/cleansing, data load, data conversion/migration, ETL and report\n      validation.\n    . Performed statistical, qualitative, and quantitative analysis on large\n      data sets using tools such as AZURE Data Lakes and Databricks, Tableu\n    . Developed complex Adhoc and canned reports for internal customers,\n      external stakeholders/requests such as press, congressional inquiry,\n      GAO etc.\n    . Provided expert support such as naming standards for database\n      tables/entities, columns/attributes, business glossaries; followed\n      standards such as NIEM.\n    . Compose data governance proposal, roadmap, and policies by meeting\n      with stake holders and learning known issues in various data movements\n      within the enterprise.\n    . Use tools such as Informatica AXON, IDQ and Oracle Data Quality to\n      stand up and implement data governance, data quality, data catalog,\n      data lineage and data profiling.\n    . Analyze data anomalies, develop data standards, compose business\n      glossary, develop, and implement data cleansing rules for data\n      integrity.\n    . Develop data audit rules to measure improvement of data quality as an\n      on-going/continues process (In-band) in important stages of data\n      ingest.\n    . Create data quality plan, data quality baseline metrics for legacy\n      data and data profiling process plans.\n\n\n\nProlink\n                          09/06 - 10/07\nServed Fannie Mae\n\nLead data Analyst\nWorked with the data stewards' team on the data management program under\nthe corporate data strategy to implement data quality solutions at Fannie\nMae.\n\nTask Highlights:\n    . Perform data analysis on multiple systems related to guarantees,\n      Mortgage based securities, foreclosures, risks, originations and\n      create as-is lineage, find problems with data flows both in physical\n      and logical designs and provide \"To be Lineage\" which may be able to\n      correct existing data flow issues.\n    . Profile data on multiple systems by entity and by attribute and\n      analyze overall health of the data in production systems.\n    . Created various reports, statistical analysis, pattern analysis, risk,\n      and benefit matrices on adhoc basis.\n    . Monitor data based on the business rules and find exceptions and turn\n      in reports to the business owners to take measures to correct data.\n    . Create trend graphs and matrices to monitor overall data accuracy.\n    . Collected business glossary/metadata and populated/updated metadata\n      repository.\n    . Managed metadata assets such as data dictionary, data models, naming\n      standards, business glossary and data structure analysis and provided\n      technical support and data handling recommendations.\n    . Created Conceptual/logical/physical models for meta data repository\n      using Erwin modeling tool.\n    . Provided expert guidance on automating Data Quality, data audit,\n      performance matrix collection jobs based on business rules and typical\n      data quality dimensions.\n    . Perform impact analysis on targeted data changes and system changes on\n      upstream and downstream systems.\n    . Provided expert support such as naming standards for database\n      tables/entities, columns/attributes, business glossaries; followed\n      standards such as NIEM.\n    . Create document of the data quality monitoring processes for business\n      users for all data quality profiling or monitoring in production.\n    . Performed some data stewardship duties with enterprise data\n      stewardship team and successfully performed duties such as data\n      quality requirements gathering, designing data improvement solutions,\n      analyzing data, profile, monitor and correct enterprise data,\n      databases, data warehouses, data flows, data lineage, models, and\n      metadata.\n    . Provided support on database model/architecture, data audit controls,\n      data quality controls, metadata management, archival requirements,\n      security controls, cross reference or data lineage, database and\n      reporting performance requirements, export transformation and load\n      (ETL) and specific file handling procedures for various data movement.\n\n\n\n\nChevy Chase bank\n                     07/05 - 08/06\n\n\n\n\nSenior Data Analyst\nThe project involved working with the data management team at CHEVY CHASE\nBANK to build a data warehouse, Business Intelligence and metadata\nrepository for Banking, Mortgage, Credit cards\n\nTask Highlight:\n . Interview SMEs (subject matter experts) during the process of designing\n   the EDW to understand multiple systems, data and processes while\n   designing or re-engineering a system.\n . Profile, analyze and monitor financial data to certify overall accuracy\n   of the data using PL/SQL and MS Excel\n . Analyze General ledger, retail, business banking, mortgage, Home equity\n   and marketing data and research existing reports to provide\n   recommendations to improve data quality in terms of prevention,\n   validation and performance on a EDW for a local bank.\n . Create data a dictionary to provide overviews of the diagrams and data\n   entity, attributes and relationships.\n . Performed analysis and collected matrices on the database activity,\n   growth, security, and performance of the databases within the enterprise.\n . Customize meta-model definitions and user properties to further enhance\n   the data dictionary.\n . Managed metadata assets such as data dictionary, data models, naming\n   standards, business glossary and data structure analysis and provided\n   technical support and data handling recommendations.\n . Creating, changing and maintaining jobs on data stage to migrate data\n   from different sources to the enterprise data warehouse and data marts.\n . Identify stewardship/ownership of the data and processes.\n . Analyzed existing systems and data flows to change or enhance data\n   movements.\n . Research sources of data; identify targets, and research existing ETL\n   processes; research and review final data ownership provides\n   recommendations to improve data quality.\n . Identify existing systems and processes where system allows unwanted\n   data.\n . Designed and developed ETL jobs using ETL tools.\n . Created and edited mapping documents for ETL processes\n\n\n\nComputer Sciences Corp\n       01/05 - 06/05\nServed Department of Defense\n\n\nSenior Computer Scientist\nPerformed duties as a senior software developer in the Enterprise Data\nWarehouse Project for Department of Defense TCAIMS project\n\nTask Highlights:\n . Design and develop EDW for Department of Defense.\n . Create and run daily, weekly, bi-weekly queries to maintain quality data.\n . Identity and design CDA (common data architecture), research sources of\n   the data, shared entities and attributes and provide recommendations.\n . Interview SMEs (subject matter experts) to understand multiple systems,\n   data, and processes while designing or re-engineering a system.\n . Re-engineer fact and dimension tables for the data warehouse IQ\n   databases.\n . Used Erwin and Power Designer for data modeling and used normalization\n   techniques to normalize data.\n . Efficiently performed stewardship duties such as analyzing data, profile,\n   monitor and correct enterprise data, databases, data warehouses, data\n   flows, data lineage, models, and metadata.\n . Perform data analysis on multiple systems and create as-is lineage, find\n   problems with data flows both in physical and logical designs and provide\n   \"To be Lineage\" which may be able to correct existing data flow issues.\n . Profile data on multiple systems by entity and by attribute and analyze\n   overall health of the data in production systems.\n . Monitor data based on the business rules and find exceptions and turn in\n   reports to the business owners to take measures to correct data.\n . Write stored procedures, triggers, and data migration scripts.\n . Backup/restore databases.\n . Translate transaction logs, performed synchronization, dumped the\n   transaction logs and altered databases when required.\n . Created stored procedures/functions, triggers, and queries to perform\n   data modification, data migration and data audits.\n\nDepartment of Education/ Anteon Corp\n                                 09/04 - 12/04\n\nLead Systems Analyst/Data Modeler\nPlayed lead role in the Federal Student Aid (FSA) developing Common data\narchitecture and metadata repository.\n\nTask Highlights:\n . Design a CDA (Common data architecture) conceptual logical model for FSA\n   (Federal Student Aid) under Department of Education using Popkin's SAEM\n   and System Architecture and MS Visio tools.\n . Collect data entities and attributes from multiple documents, systems,\n   XML repository and SMEs to during the process of designing the CDA.\n . Identify improper/wrong data collection processes and provide\n   recommendations on best practices to achieve quality data.\n . Create data a dictionary to provide overviews of the diagrams and data\n   entity, attributes, and relationships.\n . Identity and design CDA (common data architecture), research sources of\n   the data, shared entities and attributes and provide recommendations.\n . Interview SMEs (subject matter experts) to understand multiple systems,\n   data and processes while designing or re-engineering a system.\n\nDefense Systems Inc /WFInet\n   12/03 - 08/04  Client: Department of Defense (DOD)\n\nSenior Data Warehouse Architect\nPlayed mixed roles in the MSC (Military Sealift Command) Enterprise data\nWarehouse project.\n\nTask Highlights:\n . Designed, developed EDW for Military sealift command.\n . Used Erwin and Power Designer for data modeling and used normalization\n   techniques to normalize data.\n . Create data dictionary to provide overviews of the diagrams and data\n   entity, attributes, and relationships.\n . Identity and design CDA (common data architecture), research sources of\n   the data, shared entities and attributes and provide recommendations.\n . Interview SMEs (subject matter experts) to understand multiple systems,\n   data, and processes while designing or re-engineering a system.\n . Maintain standards and cross reference data with SMEs.\n . Customize meta model definitions and user properties to further enhance\n   the data dictionary.\n . Administer production and development databases and maintained\n   collection, ETL, storage and presentation of the quality data for the\n   AWRDS systems.\n . Ran a DQE (data quality engineering) shop to present quality inventory,\n   financial and logistics data/reports to the Army war reserve support\n   command during war and peace time.\n . Write queries in SQL to generate reports.\n . Write reports using Cognos powerplay and impromptu.\n . Identify stewardship/ownership of the data and processes.\n . Creating, changing and maintaining jobs on data stage to migrate data\n   from different sources to the enterprise data warehouse and data marts.\n . Re-engineer fact and dimension tables for the data warehouse IQ\n   databases.\n . Write stored procedures, triggers, and data migration scripts.\n . Generating reports using Cognos tools for data analysis in drillthru.\n . Write data migration scripts in data stage jobs and batch programs by\n   using SQL, Transact SQL.\n . Created a helpdesk problem tracker database to be used by EDW developers\n   and users to track problems and solutions.\n . Involve in memory management, I/O CPU utilization, query optimization and\n   other performance related issues.\n . Backup/restore databases.\n . Translate transaction logs, performed synchronization, dumped the\n   transaction logs and altered databases when required.\n . Created stored procedures/functions, triggers, and queries to perform\n   data modification, data migration and data audits.\n . Perform all the other DBA related duties on regular basis and took full\n   responsibility of the databases.\n . Developed intranet information website for MSC EDW (not in production\n   yet) which will be source and feedback site for EDW developers and EDW\n   users.\n\n\nStanley Associates\n01/98 - 11/03\n Department of Defense\n\n\nSenior Data Analyst/ Database Engineer\nWorked in the AWRDS/ABS project for Army War Reserve support command.  The\nAWRDS/ABS systems provide logistics information to the army in war and\npeace time.\n\nTask Highlights:\n . Designed/developed AWRDSA2k data warehouse and ABS data-mart (read only)\n   to be used by soldiers during war and peacetime.\n . Designed, supported, and administered MWB (maintenance workbench) data\n   mart for Army war reserve while deployed to Kuwait (Arifjan FSC) during\n   Operation Iraqi Freedom.\n . Administered production and development databases and maintained user\n   access both site and off-site.\n . Rebuild databases from Gold(master) shell databases and consolidated\n   databases for sites, copied and restored database on need-to-need basis\n   during development, testing and implementation phases of new AWRDS/ABS\n   modules.\n . Managed data in multiple databases\n . Provide various database support for different application development\n   and operations and maintenance projects.\n . Provide data quality engineering support.\n . Gather DB requirements\n . Ran a DQE (data quality engineering) shop to present quality data/reports\n   to the Army war reserve support command during war and peace time.\n . Wrote queries and created batch executables to be run daily, weekly, BI-\n   weekly to monitor the validity and the quality of the data in AWRDS/ABS\n   databases.\n . Used Erwin and power designer for data modeling and used normalization\n   techniques to normalize data.\n . Interviewed SMEs to identify processes, sources of data and maintain data\n   strategy.\n . Wrote stored procedures, triggers, and data migration scripts during\n   development/module changes/updates on AWRDS, ABS, MWB and other RDBMS.\n . Wrote data migration scripts and batch programs by using MS SQL, Transact\n   SQL, Unix/Perl and DOS scripts as required.\n . Performed data analysis using SQL, ACCESS, Advanced Excel Macros with\n   VBA, and Cognos Powerplay.\n . Involved in memory management, I/O CPU utilization, query optimization\n   and other performance related issues.\n . Translated transaction logs, performed synchronization, dumped the\n   transaction logs and altered databases when required.\n . Installed new update patches and MS security patches both on server and\n   client machines.\n . Used PCAnywhere to access, troubleshoot remote databases.\n . Created stored procedures/functions, triggers, queries, passthru SQL\n   scripts to perform data modification, data migration and data audits.\n . Always maintained the history of data/database change requests by\n   customers both locally and on the network PVCS tracker and version\n   manager.\n . Performed all the other DBA related duties on regular basis and took full\n   responsibility of the databases.\n\n\n\n\n\nEDUCATION:\n\nMS, Computer Information Systems, Strayer University\nBS, Computer Science, West Virginia University\n\n\n\n",
          "confidence": 1.0,
          "method": "full_text",
          "structured_data": null
        },
        "tax_term": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "source_by": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "designation": {
          "value": "Lead Data Architect",
          "confidence": 0.9,
          "method": "ner",
          "structured_data": null
        },
        "experience": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        }
      }
    },
    {
      "resume_path": "data\\input\\Enterprise Architect - NJ - Madan.doc",
      "confidence_score": 0.8084693877551021,
      "used_ocr": false,
      "extracted_fields": {
        "skills": {
          "methodologies": [
            {
              "name": "xp",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                32
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "rad",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                282,
                11203
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "tdd",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                4078,
                5505,
                7027
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "agile",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                5175
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "databases": [
            {
              "name": "oracle",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                84,
                204,
                224,
                11865,
                12842,
                13318,
                19852,
                20110,
                20533,
                20754
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "db2",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                115
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "programming": [
            {
              "name": "sql",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                104,
                111,
                19780
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "java",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                129,
                184,
                429,
                551,
                3362,
                4473,
                4974,
                5853,
                7593,
                7677,
                9447,
                9987,
                10044,
                11069,
                11157,
                13188,
                13233,
                13451,
                14371,
                15663,
                15855,
                16690,
                16847,
                18776,
                19334,
                19655,
                19698,
                20470,
                20690,
                20717
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "html",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                195,
                16459,
                16494,
                16579,
                18837,
                19784,
                20681,
                20761
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "xml",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                200,
                443,
                7749,
                10116,
                11180,
                13367,
                15717,
                16415,
                16562,
                16731,
                17162,
                18820,
                19759,
                20686,
                20766
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "xslt",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                461,
                15725,
                16423,
                16536,
                16739,
                18794,
                19767
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "puppet",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                710
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "json",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                2838,
                4536,
                5916,
                7744,
                10111
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "move",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                6664
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "javascript",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                13330,
                13379,
                15706,
                15743,
                16623,
                16711,
                16744,
                18885,
                19588,
                19711,
                19789
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            }
          ],
          "devops": [
            {
              "name": "svn",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                315
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "git",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                319,
                4552,
                5932,
                7772,
                10139
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "mesos",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                679,
                2881
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "openshift",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                9906
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "frameworks": [
            {
              "name": "spring",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                338,
                350,
                369,
                2785,
                3110,
                3264,
                3489,
                3816,
                4217,
                4259,
                4483,
                4871,
                5105,
                5311,
                5706,
                5748,
                5863,
                6833,
                7310,
                7352,
                7614,
                7687,
                9189,
                9762,
                9804,
                10054,
                12769,
                13260
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "soap",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                447,
                2815,
                4513,
                5893,
                7721,
                10088
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "rest",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                452,
                2797,
                3848,
                4495,
                4742,
                5126,
                5343,
                5875,
                6865,
                7187,
                7703,
                9475,
                9636,
                10070
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "junit",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                18941
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "domain_specific": [
            {
              "name": "design",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                405,
                1102,
                1180,
                1203,
                1518,
                1543,
                3633,
                3783,
                3882,
                4850,
                5204,
                5278,
                5377,
                6049,
                6060,
                6194,
                6726,
                6800,
                6899,
                7633,
                9350,
                9414,
                10832,
                11036,
                12299,
                12391,
                12419,
                16193,
                18387,
                18525,
                19948
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "infrastructure",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                653,
                7525,
                9963
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "architecture",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                847,
                901,
                1163,
                1223,
                1501,
                1554,
                14769,
                18614,
                20303
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "compliance",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                1152,
                1397
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "publishing",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                3376,
                4988
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "security",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                3529,
                3589,
                12055,
                18871,
                19221
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "policy",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                8043,
                8219,
                8451,
                8542,
                8590,
                8807,
                8941
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "sales",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                11488
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "clinical",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                14447
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "banking",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                15903
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "marketing",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                18125
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "engineering",
              "confidence": 0.5,
              "context": "education_section",
              "positions": [
                120,
                162,
                199
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "cloud": [
            {
              "name": "kubernetes",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                668,
                2870,
                4441,
                4593
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "docker",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                685,
                2887,
                3148
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "aws",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                692,
                2847,
                3155
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "jenkins",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                702,
                4186,
                4361,
                5613,
                7135
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "batch",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                10696
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "soft_skills": [
            {
              "name": "initiative",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                982,
                1120,
                2009,
                2305,
                2609
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "planning",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                2119,
                3640
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "analysis",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                2190,
                12290,
                12306,
                16184,
                16200,
                18378,
                18394
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "innovation",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                2222,
                2286
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "documentation",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                16902
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "data_skills": [
            {
              "name": "kafka",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                2858
              ],
              "experience_weight": 0.0,
              "importance_score": 0.9
            }
          ],
          "business_skills": [
            {
              "name": "brd",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                10937
              ],
              "experience_weight": 0.0,
              "importance_score": 0.8
            },
            {
              "name": "reporting",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                13778,
                16947
              ],
              "experience_weight": 0.0,
              "importance_score": 0.8
            }
          ],
          "networking_equipment": [
            {
              "name": "authentication",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                12064,
                12199
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ]
        },
        "first_name": "Madan",
        "last_name": "Vemurie",
        "primary_email": {
          "value": "madanvemurie78@gmail.com",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "secondary_email": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "phone": {
          "value": "8563694341",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "city": {
          "value": "Jersey",
          "confidence": 0.7,
          "method": "ner",
          "structured_data": null
        },
        "state": {
          "value": "OH",
          "confidence": 0.8,
          "method": "city_database",
          "structured_data": null
        },
        "zip": {
          "value": "",
          "confidence": 0.0,
          "method": "regex",
          "structured_data": null
        },
        "work_authority": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "resume_link": {
          "value": "data\\input\\Enterprise Architect - NJ - Madan.doc",
          "confidence": 1.0,
          "method": "file_path",
          "structured_data": null
        },
        "raw_resume": {
          "value": "\n\n\n                                Madan Vemurie\n                              Phone: 8563694341\n                     Email Id:  madanvemurie78@gmail.com\n        LinkedIn: http://www.linkedin.com/pub/madan-vemurie/8/441/599\n\n\nObjective:\n\n      Highly industrious and self-motivated, creative individual seeking a\n      challenging position in Managing and Designing/Delivering client\n      solutions that best utilizes my past expertise in Technical Sales and\n      Management. Experienced technical product manager, technical project\n      lead, and business analyst with over 16 years of experience in the\n      health and fitness, excellent communication skills and a track record\n      of successful product releases working in cross - functional teams.\n\nSummary:\n\n    . 16 plus years of experience in  application  design,  development  and\n      testing\n    . Extensive experience on Spring Boot Microservice design and  hands  on\n      development.\n    . Worked on NodeJS Microservice design and development.\n    . Working as an Enterprise Architect in recent assignments\n    . Worked as an Architect with BPM Architecture.\n    . Worked on Swagger API Design model and WS02 for publishing the API's.\n    . Worked as a Release Manager and Operations Manager during  my  Comcast\n      Project.\n    . Strong Command on Weblogic 11.03, Websphere 8,  Tomcat,  Jboss  7  and\n      other Application Servers, Open Shift\n    .  Extensive  experience   in   developing   applications   using   J2EE\n      technologies such as JPA, Servlets, EJB,  Spring  Boot,  Struts,  JMS,\n      JDBC, Hibernate.\n    . Experience in REST and SOAP Web services.\n    . Experience with  Mockito  unit  testing  framework  and  Cucumber  for\n      automation testing and Integration testing.\n    . Experience in MVC (Model View Controller) architecture by using Struts\n      Frameworks, Spring Framework, Spring Integration, Spring Data,  Spring\n      Boot, Service Oriented Architecture, Design Patterns and UML.\n    . Extensive knowledge of XML, XPATH, XSLT, SOAP, WSDL,  SOA,  REST  API,\n      JASON, Jersey.\n    . Expert in BPM 10.2 and BPM 5.7.\n    . Experience in Pivotal Cloud Foundry installation of micro services and\n      testing them.\n    . Experience in AWS EC2 Cloud set up for micro services.\n    . Expert in Spring Framework, Spring Integration,  Spring  Data,  Spring\n      Boot.\n    . Expert knowledge in Core Java and Multi Threading.\n    . Experience in Unix Shell Scripting.\n    . Knowledge in all phases of Software Development Life Cycle (SDLC)  and\n      AGILE methodology.\n    . Experience in SVN, GIT Repository, CVS, ClearCase, MS Visual Source.\n    . Experience in UML.\n    . Team player with very good communication  skills  and  inter  personal\n      skills.\nEducation:\n    . Masters of Science, Computer and Information Sciences Jan 03'-Dec 04'\n      North Illinois University, Dekalb, IL USA\n    . Bachelors of Engineering, Electronics and Communication Engineering\n      Dr. Ambedkar Institute of Engineering, Bangalore, India\n\n\n\n\nTechnical Skills:\n\n|Operating     |:|Windows 95 / 98 / 2000 / NT/XP/ 7/8, Sun Solaris8.0,     |\n|System        | |Linux, Red Hat                                           |\n|Database      |:|Oracle 11, MS-Access, SQL, PL/SQL, DB2.                  |\n|Languages     |:|JAVA, C, C++, J2EE, EJB, Applets, Servlets, JDBC, AWT,   |\n|              | |UML, Swings, Java Beans, HTML, XML,                      |\n|Oracle        |:|BPM 10g, JBpm, Oracle DB 10g, Weblogic 11.03             |\n|Products      | |                                                         |\n|IDE/Version   |:| Eclipse, RAD, IntelliJ, MS Visio, MS VSS, CVS, SVN, GIT.|\n|Controller    | |                                                         |\n|Web           |:|Spring Boot, Spring Integration, Spring Data, Struts,    |\n|Applications  | |MVC, Design Patterns, EJB, JSP, Java Servlets, XML, SOAP,|\n|              | |REST API, XSLT, XPATH, JDBC, SOA, Hibernate.             |\n|Micro Services|:|Sprint Boot, Node Js                                     |\n|Web Servers   |:|BEA Weblogic, Java Web Server, Apache TomCat, Websphere  |\n|              | |V4, IIS.                                                 |\n|Cloud         |:|Pivotal Cloud Foundry PaaS(Platform as a Service),       |\n|Infrastructure| |Kubernetes, Mesos, Docker, AWS                           |\n|CI/CD         |:|Jenkins, Puppet                                          |\n\n\n\n\n\nProfessional Experience:\n\n\n\n      Client Royal Caribbean Cruise Line\n                                         Manager (Enterprise Architect), PA\n                                                          Jan 2019-Till Date\n\n\n      Leading the project Architecture and project delivery in the Advisory\n      and Architecture group for Royal Caribbean Cruise on Employee track. I\n      am leading the initiative to improve Employee efficiency and\n      experience and ease of doing business on cruise.\n\n\n      EMPLOYEE Track\n      Working on design of 3 major initiative\n     1. Scheduling and ILO compliance Architecture and Design.\n        I am working on design and complete Architecture which will be used\n        by delivery and executives to bring in and integrate with out of\n        box tool.\n        This will automate all the crew members Scheduling, Time Keeping\n        and Compliance rules engine on the ships and help to ease the\n        employee experience\n     2. Stateroom Attendant App Architecture and Design.\n        I am working on a design and Architecture of an app which will be\n        developed by RCCL team. This app will be used to assign to assign\n        guest rooms and areas of the attendants for cleaning. It provides\n        them with checklist to ensure that all areas are covered. And once\n        attendant completes the work these are send to floor supervisors\n        for inspection.\n        Supervisor has the ability update the status of inspection and may\n        be reject it and reassign with comments.\n     3. JDE HCM replacement with SAP HCM initiative and working on cut over\n        strategy of all the Ship and Shore products which are dependent on\n        HCM. And planning the gradual systems cut over with data synching\n        strategy, risk analysis and roll back strategy.\n\n\n             Innovation and Emerging Technology Track\n       1.  Started working with Innovation Team on initiative for Kids\n          tracking on ship during voyage. This is very important for kids\n          safety and kids will wear a WOW band and they can use the band\n          which has a Bluetooth beepers which can be tracked by Auruba/Cisco\n          listeners on ship.\n          The location is compiled and tracked on guest app.\n       2. Mustering 2.0 was an initiative to remove the guest pain point by\n          removing the safety mustering at one time and giving them\n          provisions to view videos on app or stateroom TV.\n\n\n      Environment: UML, draw.io, Spring Boot, REST Web Services, SOAP Web\n      Services, GSON/JSON, XSD, AWS, VMware, Kafka, TIBCO, Kubernetes, Mesos\n      Docker.\n\n\n\n    > Prudential (Cognizant Technology Solutions)\n                                                                       Sr.\n      Manager/Delivery Executive\n                  Nov 2017- Dec 2018\n      Working as Technical Delivery and Sr. Architect with hands on\n      experience for a project to convert Web Method Based Services to\n      Spring Boot services and deploying on Docker AWS.\n      My team has delivered a very fast pacing project where we have\n      converted 100 plus Documentum Services to Spring Boot Micro Services.\n      Introduced Swagger model to write swagger Jason file and convert it to\n      Java Code and Publishing Swagger Jason file on WS02 for testing and\n      viewing and discovering.\n      This whole project is converted to spring boot Micro Services with all\n      the security, common feature and adding Apigee API Gateway based\n      security with a very small team.\n      . Involved in Design, Planning, and Co-ordination with offshore team\n        and helped in Coding of the application common features in Service\n        layer environment.\n      . Involved in Design and coding involving core Spring Technologies.\n    . Involved in REST Web services development and design\n      . Trained the team with Mockito unit testing framework and Cucumber\n        for automation testing and integration testing and implement the\n        testing frame work with 85% of test coverage.\n      . Experience in TDD environment and created unit test cases before\n        writing the business logic using Mockito. Used Sonar with Jenkins\n        for code coverage.\n      . Used Spring Boot framework for development and spring annotations.\n      . Created Custom Annotations and used Lombok Annotations for POJO's.\n      . CI/CD builds with Jenkins\n      . Worked with Dev Ops team for Dockerization of services and\n        deploying on Kubernetes.\n\n\n      Environment: UML, Core Java, J2EE, Spring Boot, REST Web Services,\n      SOAP Web Services, GSON/JSON, XSD, Tomcat, GIT, STS IDE, Swagger IO,\n      APIGEE, UNIX, CI/CD, Kubernetes\n\n\n\n\n    > TD Ameritrade (Cognizant Technology Solutions)\n                                                                       Sr.\n      Architect/Sr. Manager\n                        May 2017- Nov 2017\n\n\n      Working as API Designer and Development lead for a Rest API Pilot\n      project in BOS Service layer team in TD Ameritrade. Worked on a Pilot\n      project to initiate API design standards and Spring framework in the\n      project. Introduced Swagger model to write swagger Jason file and\n      convert it to Java Code and Publishing Swagger Jason file on WS02 for\n      testing and viewing and discovering.\n      This was a Pilot project to set up the Spring Framework for REST API\n      development and introduce the team with Agile Principals.\n      . Involved in Design, Coding of the application in Service layer\n        environment.\n      . Involved in Design and coding involving core Spring Technologies.\n    . Involved in REST Web services development and design\n      . Experience with Mockito unit testing framework and Cucumber for\n        automation testing and integration testing.\n      . Experience in TDD environment and created unit test cases before\n        writing the business logic using Mockito. Used Sonar with Jenkins\n        for code coverage.\n      . Created and updated stories worked on stories on Jira tracker.\n      . Used Spring Boot framework for development and spring annotations.\n      . Created Custom Annotations and used Lombok Annotations for POJO's.\n      . Environment: UML, Core Java, J2EE, Spring Boot, REST Web Services,\n        SOAP Web Services, GSON/JSON, XSD, Tomcat, GIT, STS , Swagger IO,\n        WS02,UNIX\n\n\n\n    > FDC (Cognizant Technology Solutions)\n      Sr. Architect/Sr. Manager\n                           Aug 2016- April 2017\n\n\n      Solution design and Design work for integration with Fraud monitoring\n      Solution with a third Party Machine Learning and Monitoring Response\n      System Feedzai. Design micro services which are currently in\n      Production.\n      Also integration with another Smart Phone device Fraud alert system\n      InAuth Service.\n      The client which this products are used are mainly from Exxon Mobile,\n      Chickfilae, Tacobell.\n\n\n      This new integration is designed completely as an independent micro\n      services which are deployed separately and have their own purpose.\n      These services are also currently on PCF (Pivotal Cloud Foundry) PaaS\n      as a test services and intention was to move completely to PCF in\n      future.\n   Responsibilities: -\n      . Involved in Design, Coding of the application in Service layer\n        environment.\n      . Involved in Design and coding involving core Spring Technologies.\n    . Involved in REST Web services development and design\n      . Experience with Mockito unit testing framework and Cucumber for\n        automation testing and integration testing.\n      . Experience in TDD environment and created unit test cases before\n        writing the business logic using Mockito. Used Sonar with Jenkins\n        for code coverage.\n      . Developing micro API's and REST web services for the businesses and\n        vendors to use.\n      . Created and updated stories worked on stories on Jira tracker.\n      . Used Spring Boot framework for development and spring annotations.\n      . Created Custom Annotations and used Lombok Annotations for POJO's.\n      . Worked on POC for Pivotal Cloud Foundry PaaS as for deployments of\n        applications on Cloud Infrastructure.\n      . Worked on POC for API Gateway Apigee on PCF.\n      . Used JPA (Java Persistence API) Spring Data for DB design and\n        development.\n\n\n   Environment: UML, Core Java, J2EE, Spring Boot, JPA, REST Web Services,\n   SOAP Web Services, GSON/JSON, XML, XSL, XSD, Web Sphere, GIT, STS,\n   Mocito, Apigee , UNIX\n\n\n\n    > United Health Care, Horsham, PA\n\n\n         Sr. Architect/Software Engineer\n                       Mar 2015- June 2016\n\n\n      Digital Service Medicare and Retirement\n\n\n            Solution Summary document for a new project was developed.\n      Provider Web is a web site residing within the SHIP Comprehensive\n      Policy Administration System (COMPAS) with self-service features for\n      providers.  When it was built 11 years ago, no enterprise-wide\n      provider portal existed, so it was built into the Policy\n      Administration system.  Today, there are better options, specifically,\n      leveraging the UHC-wide Provider Portal, also known as Optum Provider\n      Touch Point (PTP). Our recommendation is to align to the enterprise\n      standard by retiring the Policy Administration system Provider Web in\n      favor of leveraging Optum PTP.\n           In addition, the Policy Administration system has an outstanding\n      policy exception (PEX) with UHC Corporate IT for accessing the United\n      Claim Processing System (UCPS) system directly in order to maintain\n      information in the UCPS Provider Database. To resolve the outstanding\n      PEX issue, the Policy Administration system will integrate with ACES\n      provider maintenance services in order to pass information between\n      UCPS and the Policy Administration system.\n\n\n           Hands on a development of micro API Services and Restful Web\n      Service, which provides services and to business and other teams for\n      looking up claims, Provider Demographics, plans, eligibilities,\n      preferences, fulfillments. Used Spring Boot framework with extensive\n      unit testing using mockito framework, automation testing using\n      Cucumber and integration testing.\n\n\n\n\n\n\n   Responsibilities: -\n      . Involved in Design, Coding of the application in SOA environment.\n      . Involved in Design and coding involving core Java technologies.\n    . Worked on REST Web services.\n      . Worked on writing unit test cases with Mockito framework and\n        Cucumber for automation testing and integration testing.\n      . Developing micro API's and REST web services for the businesses and\n        vendors to use.\n      . Created and updated stories worked on stories on Pivotal tracker.\n      . Used Spring Boot framework for development and spring annotations.\n      . Created Custom Annotations and used Lombok Annotations for POJO's.\n      . Worked on POC for Openshift automatic deployments of applications\n        on Cloud Infrastructure.\n      . Used JPA (Java Persistence API) for DB queries\n\n\n   Environment: UML, Core Java, J2EE, Spring Boot, JPA, REST Web Services,\n   SOAP Web Services, GSON/JSON, XML, XSL, XSD, Web Sphere, GIT, IntelliJ ,\n   UNIX\n\n\n\n    Bank Of America, Newark\n    Sr. Architect/ Technical Lead\n                     Jan 2013-Feb 2015\n\n\n        I am currently working as a Sr. Architect and Technical Lead in Bank\n   of America in Card Technologies and Common Services team. I work closely\n   with Business team and other teams on quarterly releases. I have worked\n   on Fraud Activity Verification project where Business is moving the Fraud\n   Verification service in house. CTCS team is mainly an Orchestration Layer\n   between different teams and we provide Wed Services, which are consumed\n   by different teams.\n   I have also worked on Batch processes project, which is used mainly for\n   our Mailers and Agreement printing services for customers.\n\n\n   Responsibilities: -\n      . Involved in Design, Coding of the application in SOA environment.\n      . Involved in Business requirements meeting to collect BRD\n        requirements.\n      . Involved in Integration testing with all the teams in SOA\n        environment.\n      . Involved in Design and coding involving core Java technologies.\n      . Worked on JBPM 6 projects for Card fraud project.\n\n\n   Environment: UML ,Core Java, J2EE, Web Services, XML, XSL, XSD, Web\n   Sphere, RAD 7.5, JBpm 6, RTC, UNIX.\n\n\n\nComcast, Mount Laurel, NJ\n Sr. Architect/ Technical Lead\n                               Jan 2007-Dec 2012\n\n\n  I have started working in Comcast from Jan 2007 and from very first week\nthe work was exciting. I have worked on lot of Major project development\nand releases for Commercial Work Bench Portal. Sales team to create and\nProvision Business customers with Comcast Line of Businesses products use\nthis product. I worked on all the major Product release in Commercial\nWorkbench both in designing and development phase. We have also done lot of\nupgrades to our projects from Weblogic 8.1 to Weblogic 10.01. Also last\nyear we have done a major upgrade from Aqua-logic BPM 5.7.3 to Oracle BPM\n10.\nWorked as a Release Manager, Enterprise Architect and Technical Lead for\nall development release.\nSome of my responsibilities in the application:-\n    . Integration with LDAP server for security authentication for  SSO  and\n      also for SalesForce integration. Also for one of our  external  client\n      we have used Public Key and Private Key authentication for getting the\n      Login Tokens.\n    . Involved  in  Requirements  gathering,  Requirement  analysis,  Design\n      analysis, Integration and deployment for enhancements and changes.\n    . Involved High Level design documents and Detail design  documents  for\n      our projects in every monthly release.\n    . Performance evaluation with the performance team before every release.\n    . Involved in  coding  and  Integration  testing  with  clients  of  the\n      application in SOA environment. We have different vendor  clients  who\n      are consuming out  services  like  Salesforce,  IBM  CPQ  (Evolution),\n      Century 2.1.\n    . Application uses Spring frame work and Struts framework  for  existing\n      services and we have Oracle BPM 10g as a middleware.\n    . Integration our application with external  Business  Class  Voice  and\n      Business Class Trunking provisioning system.\n    . Created UI page for Trunking product using GWT\n    . Integration with Enterprise services  like  billing  systems,  account\n      services, offer management services, order management services.\n    . Used multi threading and core Java for parallel processing.\n\n\n\n   Environment: UML, Java, J2EE, BPM 10.3, BPM 5.7, Spring, Struts, BEA\n   Weblogic 10.03, BEA Weblogic 10.1 Portal, Oracle 10G2, JavaScript, GWT,\n   DWR, AJAX, Web Services , XML, XSL, XSD, JavaScript, Hibernate,  Apache\n   Tomcat,  UNIX.\n\n\n\n > Pfizer Inc. New London, CT\n      Senior Java Developer/Architect\n                       April 2006- Dec 2006\n   1.   Data Standard Catalog         Team Lead\n   Working on a Data Standards Catalog project and this application is used\n   all over the world Pfizer end users and study teams to post issues on\n   Core data standards and efficacy standards. This application is\n   interfaced with an extra view product for reporting the issues and\n   tracking issues. This application is deployed in Weblogic 8.1.\n\n\n       .  Directly involved with business team to maintain the whole\n         application. First point of contact for application support and\n         upgrades.\n       .  Involved in coding, interfacing and testing with the extra view\n         team.\n       .  Written stored procedures and packages for the versioning.\n       .  Documenting and maintaining the requirements and the application\n         documents.\n       . Maintaining the production and staging application server boxes\n         and directly co-ordination with the middle tier team for new\n         releases.\n\n\n   2. Dictionary extract project\n      Developed a java application using Live Link API, which is used to\n   upload files from the clinical trials servers every day to the Pfizer\n   Live Link servers, which are used by the business and end user for\n   review.\n      . Involved in coding, interfacing and testing of the project.\n      . Written stored procedures and packages.\n      . Documenting and maintaining the requirements and the application\n        documents.\n      . Implemented Service oriented Architecture for web service\n        communications with the different Pfizer clients.\n      . Maintaining the production and staging application server boxes and\n        directly co-ordination with the middle tier team for new releases.\n\n\n   3. WHO- Drug Dictionary and TMS servers front end web interface\n      Developed web front end using JSP and Struts frame work for a front\n      end interface were a business person can come and login using Pfizer\n      NT account username and password and upload the spreadsheets or\n      download the spread sheet for the dictionary terms.\n\n\n       . Involved in designing, coding, interfacing and testing of the\n         project.\n       .  Documenting and maintaining the requirements and the application\n         documents.\n       .   Used Adobe photo shop for look and feel images.\n       . Maintaining the production and staging application server boxes\n         and directly co-ordination with the middle tier team for new\n         releases.\n       . Used Hibernate for mapping.\n   Environment: Core Java,Rational Rose, UML, WingNut, J2EE, Struts,\n   JavaScript, XML, XSL, XSLT, VB scripting, JavaScript, Hibernate,  Apache\n   Tomcat,  Adobe Photoshop, UNIX, CVS for version controlling.\n\n\n\n\n   FUNDTech, Norcross, GA\n   Senior Java Developer\n                              Nov 2005- April 2006\n   Working on a Banking project and our main job is Maintenance and\n   production support. So we are dealing with small failures and code\n   Enhancements. The project is both web based and swing based so I am\n   dealing with both the sides issue on client side.\n\n\n    .  Involved in  Requirements  gathering,  Requirement  analysis,  Design\n      analysis, Integration and deployment for enhancements and changes.\n    . Involved in  coding  and  Integration  testing  with  clients  of  the\n      application\n    . The application uses the Swing and Servlets. The views are  programmed\n      using XML and XSLT pages which are converted into HTML. And  use  AJAX\n      for changing the Html page without reloading the page.\n    .  Used XSLT for transforming the XML content into HTML\n    . Used Starteam for version control.\n    . Used JavaScript as the scripting language for the View.\n\n\n      Environment: Core Java, J2EE, Struts, JMS, JavaScript, JBuilder,  XML,\n      XSL,  XSLT,   JavaScript,   Apache  Tomcat,  UNIX,  SOA,   Star   Team\n      Enterprise.\n\n\n   Cingular Wireless, Alpharetta, GA\n                                               Senior Analysts(Java\n   Developer)\n      Dec 2004- Nov 2005\n\n\n   1. EDD (Enterprise Data Documentation), EDD Resend Web  Application,  EDD\n      Reporting Tool. In Production from August 2005\n      Designed and developed along with my team members  an  automated  tool\n      which can receive customer payment information from clients queue in a\n      bean format and convert it in XML string and put it in the local queue\n      and store it LDAP database.  After  that  sending  a  notification  of\n      payment to the customer via SMS, Email or Post Card according  to  the\n      preference.\n      Designed and  developed  a  web  application  which  is  used  by  CSR\n      (Customer Service Representatives) to resend the payments  information\n      a customer for past two years. This web  application  takes  customers\n      information and displays the records of all the previous payments  and\n      then the customer can choose one or more  than  one  record.  And  the\n      message is again sent via SMS, Email or Post Card which ever  customer\n      prefers.\n      Developed  and  developed  web  application  which  is  used  by   the\n      production support team and the  administration  to  view  the  totals\n      records of each client and the resend records. It will also  tell  how\n      many records have been successfully sent and how many failed.\n\n\n   2. Corporate Email System\n      Designed and developed a web application which is going to be used  by\n      Cingular agents or Marketing people to send emails to  individuals  or\n      group of people or to a Category of people. The agents  can  create  a\n      new template or use the  existing  template  to  send  emails  to  the\n      clients.\n\n\n   Responsibilities:\n\n\n    . Involved  in  Requirements  gathering,  Requirement  analysis,  Design\n      analysis, Integration and deployment\n    . Involved in  coding  and  Integration  testing  with  clients  of  the\n      application\n    .  Responsible  for  the  design  and  development  of  the  application\n      framework\n    . Designed and Developed UI's using MVC architecture\n    . The application uses the STRUTS framework. The  views  are  programmed\n      using JSP pages with the struts tag library. Model is a combination of\n      EJB, DAO and Java classes.\n    . Used XSLT for transforming the XML content into HTML and used SOA  for\n      web service security.\n    . Used JavaScript for much functionality of the web  pages.  Used  JUnit\n      for unit testing of the system and Log4J for logging\n    . Used EJB and DAO a middleware in designing and developing a three-tier\n      distributed application\n    . Used web services with session bean for Synchronous communication with\n      the client and even used a vendor tool for web service security.\n    . Used Hiberate as a mapping tool\n    . All the queries are written and tested on TOAD for this application.\n    . The Java Message Service  (JMS)  API  is  used  to  allow  application\n      components to create, send, receive, and read messages\n    . Installed Web Logic for the application is UNIX environment.\n    . Used ClearCase for version control.\n    . Used AJAX to modify the page asynchronously.\n    . Used JavaScript as the scripting language for the View.\n\n\n   Environment: Core Java, J2EE, Struts, JSP, EJB, Servlets, JMS, JDBC, Java\n   (JDK 1.4), JavaScript, JBuilder, Rational Rose, Hibernate DAO, XML,  XSL,\n   XSLT, AJAX, PL/SQL, HTML, JavaScript, WebLogic 8.1 SP4, Web  Sphere  4.1,\n   JBOSS, Apache Tomcat, Oracle 9i/8i, UNIX (HP)\n\n\n > Sonata Software Ltd., Bangalore, India     Software Developer\n   Nov'01 - Jan'02\n\n      For the design and simulation  of  the  Electronic  Control  Units  at\n   Continental Teves during their R&D phase, Sonata proposed and developed a\n   web-based system with  a  central  Oracle  database.  Web  interface  and\n   interface  to  other  existing  systems  were  provided.   Offline   data\n   manipulation for field engineers and  overseas  database  synchronization\n   was also implemented. MVC architecture was used with JSP,  Servlets,  and\n   JavaBeans to build the interface for the database.\n   Responsibility:\n    . Designed and developed session and entity beans\n    . Developed Java Servlets used for the server end\n    . Installed and maintained Oracle Server\n    .  Involved  in  unit  testing,  module  testing,  product  testing  and\n      development of test cases.\n    . Involved in the web interface development using HTML, XML,  Java  AWT,\n      Swing\n   Environment: Java, JSP, C++, C#, Servlets, JDBC, JavaBeans, Oracle, HTML,\n   XML, Jawa AWT, Swing Windows NT\n\nReferences:  Available  or  please  check  out  my  LinkedIn   profile   for\nreferences.\n\n\n\n\n",
          "confidence": 1.0,
          "method": "full_text",
          "structured_data": null
        },
        "tax_term": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "source_by": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "designation": {
          "value": "product manager",
          "confidence": 0.9,
          "method": "ner",
          "structured_data": null
        },
        "experience": {
          "value": "16",
          "confidence": 0.9,
          "method": "regex_total_experience_summary",
          "structured_data": null
        }
      }
    },
    {
      "resume_path": "data\\input\\Finance Manager - CA - Stepfan.doc",
      "confidence_score": 0.7613548387096775,
      "used_ocr": false,
      "extracted_fields": {
        "skills": {
          "soft_skills": [
            {
              "name": "analysis",
              "confidence": 0.84,
              "context": "project_section",
              "positions": [
                362,
                675,
                1031,
                1147,
                3946,
                4032,
                4162,
                6039,
                7026,
                8503,
                8702,
                10247,
                11695,
                12260,
                12506,
                12630,
                12802,
                12979,
                13259,
                14032,
                14041,
                14495,
                14952,
                19476
              ],
              "experience_weight": 0.2,
              "importance_score": 0.7
            },
            {
              "name": "prioritization",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                419
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "leadership",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                2259,
                19702
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "planning",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                2782,
                7249,
                7418,
                7867,
                8693,
                10432,
                10766,
                12377
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "accountability",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                3089,
                5461
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "research",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                3381,
                4153,
                6021,
                7901,
                8322
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "documentation",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                6725,
                14777,
                17151
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "training",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                7587
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "collaboration",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                9041
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "organization",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                9145
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "influence",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                12394
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "cloud": [
            {
              "name": "functions",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                653,
                1156,
                4419,
                7184
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "comprehend",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                8191
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "business_skills": [
            {
              "name": "reporting",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                1207,
                3791,
                4583,
                5516,
                5741,
                5827,
                6242,
                7052,
                7229,
                7273,
                7459,
                8986,
                12643,
                13285
              ],
              "experience_weight": 0.0,
              "importance_score": 0.8
            }
          ],
          "domain_specific": [
            {
              "name": "finance",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                1318,
                2351,
                2474,
                4099,
                4449,
                7563,
                19737
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "infrastructure",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                4000,
                4245
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "contracts",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                4081,
                6810,
                9286
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "compliance",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                5480,
                13752,
                14605,
                17004
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "legal",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                5735,
                8864,
                13103
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "policy",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                7952
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "manufacturing",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                8764,
                10675,
                14170,
                14223,
                14457
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "sales",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                8888,
                9986,
                12059,
                12254,
                12350,
                12830,
                13648
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "engineering",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                9488,
                10359
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "construction",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                13055,
                13081
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "government",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                15736,
                17042,
                17322
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "security",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                18713
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "networking_equipment": [
            {
              "name": "accounting",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                4107,
                4373,
                4408,
                7941,
                8275,
                8339,
                11321,
                13966,
                14021,
                14514,
                14574,
                14631
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "programming": [
            {
              "name": "assembly",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                10374
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            }
          ],
          "frameworks": [
            {
              "name": "express",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                16455,
                16602,
                17121
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "databases": [
            {
              "name": "oracle",
              "confidence": 0.84,
              "context": "project_section",
              "positions": [
                19512
              ],
              "experience_weight": 0.2,
              "importance_score": 0.7
            }
          ],
          "data_skills": [
            {
              "name": "excel",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                19626
              ],
              "experience_weight": 0.0,
              "importance_score": 0.9
            }
          ]
        },
        "first_name": "Stepfan",
        "last_name": "Jiles",
        "primary_email": {
          "value": "Stepfanjiles@hotmail.com",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "secondary_email": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "phone": {
          "value": "9016908956",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "city": {
          "value": "San Ramon",
          "confidence": 0.7,
          "method": "ner",
          "structured_data": null
        },
        "state": {
          "value": "CA",
          "confidence": 0.9,
          "method": "zip_database",
          "structured_data": null
        },
        "zip": {
          "value": "94583",
          "confidence": 0.7,
          "method": "regex",
          "structured_data": null
        },
        "work_authority": {
          "value": "Meetings",
          "confidence": 0.8,
          "method": "regex",
          "structured_data": null
        },
        "resume_link": {
          "value": "data\\input\\Finance Manager - CA - Stepfan.doc",
          "confidence": 1.0,
          "method": "file_path",
          "structured_data": null
        },
        "raw_resume": {
          "value": "\n                                Stepfan Jiles\n                         2218 Canyon Village Circle\n                             San Ramon, CA 94583\n                          Stepfanjiles@hotmail.com\n                                (901)690-8956\n\nOBJECTIVE\nTo obtain a position with a company that seeks an assertive, energetic\nfinance professional resource. The ideal position will allow me to utilize\nmy leadership and strong financial planning & analysis skills to have a\npositive impact on profitability and productivity.\n\nEDUCATION\nChamplain College\n       Burlington, VT\nCost Accountant Certification (CCA)\n                      July, 2018\nFlorida Institute of Technology\n\n   Melbourne, FL\nExecutive MBA Finance & Accounting\n\n   June, 2011\nBryan College\n\n                      Dayton, TN\nBachelors of Science in Business Administration\n\nMay, 1998\nMajor: Management\nWentworth Military Academy\n\nLexington, MO\nLiberal Arts Associate Degree\n\n              May, 1993\nMajor: Accounting\n\nEXPERIENCE\nPacific, Gas & Electric (PG&E) Utilities\nInsight Global Inc.- Finance Consultant\n                                                           January 2020 -\nPresent\nSr. Project Control's Analyst\n                                           San Ramon, Ca\nSchedule Development and Analysis:\n . In coordination with Project Manager develop project schedule and cash\n   flow/forecast plans with functional department input.\n . Attend Project and Job kickoff and walk down meetings.\n . Create, maintain and update schedules.\n . Document, monitor and communicate project milestones and risks with\n   appropriate stakeholders.\n . Participate in project status meetings, collect progress data and revise\n   project plan as needed.\n . Monitor financial progress and maintain Project Manager's order group.\n   Develop, maintain and control project schedule plans using appropriate\n   software (for ex: SAP Project System, SAP Work Management System, and MS\n   Project).\nCost Plan Development and Analysis:\n . Use the cost and schedule plan to determine if project objectives are\n   achievable.  Create, maintain, and update monthly forecast budgets.\n   Prepare various cost reports and maintain forecast accuracy. Prepare\n   project performance analysis, cost, and schedule status reports. Identify\n   cost and schedule variances from objectives and recommend corrective\n   action.\n . Assess and report on project performance using established industry\n   standards.\n . Ensure that responses to project budget, and accruals' cost requests\n   reflect accurate and current project cost information and stakeholders\n   are in concurrence.\n . Provide regular communication on project cost, schedule and risk status\n   to project team members, stakeholders and public.\n . Interface with design and planning resources and software systems.\n . Review and assist in the preparation of Advance Authorizations, Job\n   Estimates and Re-Authorizations and assist PM in routing for approval.\n . Prepare journal entries and coordinate or process goods receipts in\n   current software system.\n . Ensure that responses to budget and cost requests reflect correct project\n   cost information and stakeholders are in concurrence.\n . Input data into various programs and prepare various cost and forecasting\n   reports.\nProject Documentation & Reporting:\n . Verify that project cost and schedule milestones were/were not attained\n   and provide input to identify future process or business improvements and\n   work with responsible parties to implement.\n . Manage orders from inception through completion and all required\n   documentation is entered in current software system (Ex: EDRS).\n . Resolve all open items, ensure compliance requirements are met and\n   settlement rules are entered, and close out order.\n . Maintain scope change, contingency release, change order, and journal\n   entry logs.\n . Maintain written and electronic project documentation and records for\n   required aspects of the project: Maintain project files in accordance\n   with established guidelines and requirements (Ex: utilizing the\n   electronic document management system or EDMS).\n . Document change order requests, project status, key issues, risks and\n   resolution, priority changes and approvals.\n . Provide Project Manager with monthly report of project costs and/or\n   schedule information including variance analysis according to an agreed\n   upon level of detail and prioritization.\n . Provide Project Team members with current status report containing\n   schedule and cost information. Comply with Utility Operations Policies,\n   Standards and Guidelines.\n . Assist PM with Post Job Critique.\nCentral Data Management Functions - Quality and Analysis.\n . Prepare various monthly and weekly project management reports and report\n   performance reports to management group including scorecards.\n . Ensure quality of reports for the department, to report out to\n   stakeholders.\n . Validate weekly and monthly data sets.\n . May provide support to project manager(s) as necessary.\n . Fulfill ad-hoc requests for cost-related data analysis.\n . Participate on (or lead) various process improvement initiatives within\n   Project Management.\nCentral Financial Analysis Functions - Project Budget & Cycle Forecast\nMonitoring/Reporting\n . Prepare various monthly and weekly project performance reports - budget\n   and cycle forecast reports; and finance status reports, utilizing the\n   financial tracking software (e.g. SAP/BW system and EPM).\n . Coordinate and work with all stakeholders, on creating new reports, that\n   best measure and support good business decisions, to meet the Year End\n   Annual Budget target.\n . Coordinate and summarize project cycle forecast variance explanations for\n   the department, to report out to stakeholders.\n . Ensure that all responses to budget, quarterly cycle requests reflect\n   accurate and current project cost information and stakeholders are in\n   concurrence.\n . Post and update all the project performance; budget and cycle forecast;\n   and status reports on the Project Management website.\nFinancial and Performance Management\n . Monitor, analyze and report out on the Department's Forecasting\n   Performance, with weekly performance reports, utilizing the financial\n   tracking tools (e.g. SAP/BW).\n . Analyze financial results on a monthly basis, providing explanations of\n   significant cost drivers to PM Leadership - reports such as the Green-Red\n   Scorecard and Lessons Learned Reports.\n\nCordoba Corp.- Finance Consultant\nBART/ Santa Clara Valley Transit Authority Phase II Extension Project\n        December 2017 - January 2020\nProject Finance Consultant\n                          San Jose, Ca\n . Lead the tracking of the capital needs of the BART/ Santa Clara Valley\n   Transit Authority Phase II Extension Project, including project\n   commitments, project spend, project budgets, and overhead.\n . Maintain and update annual GAAP and cash operating forecast models.\n . Support the strategic planning process for the Project Controls'\n   division.\n . Support the Budget process and monitor the capital needs of the BART/\n   Santa Clara Valley Transit Authority Phase II Extension Project.\n . Lead and track KPI's and financial metrics to inform senior leader\n   decision making, measure divisional performance, and drive\n   accountability.\n . Lead Project FP&A to roll-up and map task and sub-task division models to\n   corporate consolidated models.\n . Prepare monthly project scope reviews and track actuals to budget;\n   present and communicate to senior management.\n . Perform various other financial analyses and industry related research,\n   tracking and compiling industry trends and competitive analyses, to\n   inform senior management strategic decision-making; effectively\n   communicate findings to team/colleagues through presentations, memos, and\n   other deliverables.\n . Manage and assist in the preparation of annual project business reviews\n   and variance analyses.\n . Manage sub contract level ultimate's, tracking against actual activity\n   from third party reporting.\n . Track invoice rights and availabilities for project, supporting\n   comprehensive agreement strategy.\n . Support Program Management Team (PMT) Strategy and Analysis in the setup\n   of the processes and procedures infrastructure for ongoing deal analysis\n   for new third party venders and ongoing contracts.\n . Perform finance/accounting, cost economics, customer and market research\n   analysis. Analyzing, interpreting and presenting data related to\n   transportations and infrastructure market operations.\n\n\nCalifornia High Speed Rail Project\n        April, 2017 - November 2017\nSr. Financial Analyst\n              Sacramento, Ca\n.     Accounting duties - Performs complex accounting functions for High\nSpeed Rail Finance and Budget\n       department, internal office departments, and program. Analyzes and\nassigns transaction codes in accordance\n       with state reporting and local management requirements. Reviews and\nreconciles account and fund balances;\n        reviews documents and claims for accuracy, completeness, and\nuniformity to rules, regulations, and laws;\n       coordinates, prepares and reviews year-end closing process and\ndocuments. Performs periodic review of\n       financial reports and verifies accuracy and project solvency.\n.     Budget Monitoring - Support the development of budgets for internal\nand external customer; support the\n       creation of new budget coding; provides information and assists\nstaff and administrators in budget preparation,\n       implementation, and control; prepares, balances, compiles, and\nenters budget data. Support the preparation of\n       quarterly revisions and billings; in accordance with Federal\nRailroad Administration (FRA) guidelines, monitors\n       and evaluates federal and state fund budgets and cash flow for\nappropriateness to ensure solvency and\n       accountability for compliance with state law.\n.     Financial Reporting - Prepares complex financial reports as mandated\nby the administration, California High\n       Speed Rail Authority, and other agencies; assists internal and\nexternal business customers in compiling data and\n       interpreting legal reporting requirements and regulations. Monitors\ntimelines to meet strict deadlines in\n       reporting regulations; reviews grant letters, funding and\nentitlement reports; reviews and verifies statistical and\n       financial information including independent reports and long-term\ndebt documents.\n.     Research and Data Analysis - Researches and analyzes financial data\nfor internal and external clients; creates,\n       organizes, and maintains files using database and spreadsheet\nprograms; retrieves and organizes data into\n       required reporting formats; collects, retrieves and organizes data\nto identify financial discrepancies and resolve\n       client inquiries; recommends solutions to clients and staff ensuring\nthat corrections or changes are implemented\n       properly.\n.     Communications and Technical Support - Maintains communication with\ninternal and external clients regarding\n       financial matters; provides information and technical support in the\ndevelopment and revision of policies and\n       regulations; assists in the development and documentation of office\nprocess updates and revisions to procedures;\n      reviews proposed contracts for adherence to rules and regulations.\nSupport procedures for monitoring grants.\n\nCBRE                                                         August, 2015 -\nJan., 2017\nSr. Project Analyst/Sr. Financial Analyst\n San Francisco, Ca\n.      Manages a broad range of complex financial analysis and/or financial\nreporting activities to measure\n        profitability for a region, line of business or large, complex\nclient. Includes the management of such functions as\n        budgeting forecasting, financial reporting, strategic planning and\nmanagement reporting processes and work\n        product.\n.      Manages one or all of the following: capital budgeting process,\ncapital forecasting process, capital strategic\n        planning process, and standard management reporting. Establishes\noverall departmental priorities and ensures\n        that all deadlines are met.\n.      Supervises finance staff including training. Reviews and approves\ncertain HFM monthly journal entries.\n        Conducts other special financial and business studies and other\nduties as assigned by management.\n.     Produces a variety of routine and ad hoc financial reports, packages\nand pro forma analyses for senior\n       management and planning unit. Participates in the research,\ndevelopment and preparation of accounting policy\n       and procedures, as required. May act as consultant to management on\nfinancial policies, procedures, and\n       applications. Manages special projects and prepares presentations\nfor senior management. Performs other duties\n       as assigned.\n.    Ability to comprehend, analyze, and interpret complex financial\ninformation and transactions and accounting\n      principles. Ability to independently research complex accounting\ntransactions/issues. Ability to problem-solve,\n      both independently and working as a team. Perform advanced analytical\nand quantitative skills. Draws upon the\n      analysis of others and makes recommendations that have a direct\nimpact on the company.\n\nSiemens, Inc.\nNovember, 2013 - May, 2015\nCommercial Program Manager\nSacramento, Ca\n.     Directly lead the financial planning & analysis of the U.S.\noperations lite rail and locomotive bogie\n       manufacturing team, supporting my technical bogies' program manager\nin relation to all commercial and legal\n       issues. Performing sales and expense forecasting and budgeting for\nall Bogie projects.\n.     Perform weekly & monthly program reporting/project status\ndiscussions/milestone reviews. Collaboration on the\n       project completion report and summarizing the lessons learned with\nfeedback to the organization executive\n       management staff. Collaborating with internal & external customers\nin negotiations and also formulating and\n       implementing contracts in the program.\n.     Analyzing and assessing complex, possibly international suppliers,\ncontract agreements'. Managing claim and\n       change order management: Asserting own claims and warding off\nunjustified engineering claims.\n.     Maintain contractual changes with regard to the scope of delivery and\nservices, prices, deadlines or other\n       contractual agreements, as well as forecast opportunity and risk\nmanagement: Identifying and financially\n       assessing opportunities and risks, defining and implementing\nsuitable measures for reducing risks or realizing\n       opportunities and taking precautions for remaining risks.\n.     Drawing up the order receipts, concurrent and final costing\ncalculations for quotes and bids for one off sales\n       request. Performing asset management, correcting assignment and\nmonitoring of cost. Drawing up invoices and\n       tracking claims. Maintaining internal project controlling\n(deadlines, costs and quality).\nFinancial Analyst III\n.     Performs complex financial and operational analysis to support short\nterm and long range strategic plans and\n       operating budgets. Act as a key partner to the Engineering, UT\nAssembly Production & Scheduling cost centers\n       for financial planning and controlling activities. I am the\nHyperion/Essbase/Smartview lead for this company.\n.     Conducts work order variance analyses on actual expenditures to\nbudget estimates. Evaluates data, prepares\n       forecasts, and analyzes trends in operations, manufacturing, general\nbusiness conditions and other areas.\n.     Verifies labor cost and workload planning by comparing plan vs.\nactual costs. Identifies voucher-\n       related or other transactions mistakenly applied to the projects\nthat need to be removed.  Coordinates the\n       correction through the EZ Suite feeder system or as a journal entry.\n.    Works closely with executive management to assist in hourly rate\ninternal calculations. Provide timely and\n       accurate invoices and billing information to selected outside\ncustomers. Monitor projects for Project Status EAC\n       vs. actual related issues. Ensures appropriate action is taken to\nupdate statuses and prepares required accounting\n       entries. I am the site Hyperion, Essbase, Smartview lead support for\nthe OBA department.\n.    Responds to various ad hoc requests as well as requests for additional\ninformation from customers both Internal\n       and External.  Coordinates the development of additional cost\ncontrol reports. Perform monthly budgeting,\n       forecasting, ETC/EAC monitoring, revenue recognition and variance\nanalysis.\n.    Assists Department Managers in the timely and accurate preparation of\nannual capital budgets and mid-month\n      forecasting. (ERP JD Edwards, Hyperion, Essbase and Smart View\nretrieve software).\n\nUnited Technology Corporation\nAugust, 2012 - July, 2013\nFinancial Analyst II\nFairfield, CA\n.     Develop and maintain financial models and metrics to measure\n      performance for company sales mix that\n       accurately predict business performance and highlight key issues for\n      senior management. Complete key\n       financial reports including but not limited to: Monthly performance\n      recap, weekly sales analysis, monthly gross\n       margin and other key metrics. Provide ad hoc analytical support for\n      Sales & Inventory Operations Planning\n       (SIOP) to influence decision making with financial perspective.\n.     Support monthly A/P & A/R invoicing for SBU and direct labor\n      analysis. Manage and prepare monthly account\n        commission journal entry and accrual, process check request and\n      maintain account analysis for reporting.\n        Manage and update a percentage of our prepaid and accrual sub-\n      ledger. Reconcile all general ledger accounts\n        and prepare/update general ledger account analysis.\n.      Build and maintain sales and cost schedules to track actual vs.\nbudget variance on a MTD and YTD basics.\n        Support budgeting and forecasting processes by performing trends\nanalysis via moving average schedule and\n        models.  Audits Capital Projects (e.g. construction applications,\nconstruction invoices, legal services invoices,\n        etc.) for the purpose of ensuring proper account codes,\nmathematical correctness and availability of\n        funds. Perform revenue recognition analysis for monthly sale\nreporting under GAAP. Maintain\n        lease register for all lease agreements and assets on site.\n.      Provide financial support to Program Managers and Pricing Analyst by\nmonitoring program financial\n        performance and developing cost effective strategies to meet budget\ncost performance. Establish and\n        maintain cost of goods sold schedule to determine estimate at\ncompletion for sales order programs.  Databases &\n        software being utilized daily Hyperion Financial Management(HFM),\nSAP, eSOX compliance module, Auto\n        time, SAP BO/BW, SmartView, Microsoft office.\n\nParamount Staffing\nMenlo- Nike Golf                                               January,\n2011- April, 2012\nCost Accountant/Financial Analyst\n      Memphis, TN\n.     Support off-site corporate Controller in all accounting duties, i.e.\n      month end closings, accruals, cost accounting\n       analysis, analysis of actual against budget & forecast time\n      period(s) revenues and cost of materials, standard cost\n       vs. actual cost variance of manufacturing production cost. Analyzes\n      financial and manufacturing cost data on\n       local level to produce relevant decision making tools and provide\n      recommendation to senior management to\n       make sound business decisions. Support the on- site General Manager\n      & Site Production Manager of the golf\n       manufacturing operations in financial analysis, inventory accounting\n      and financial decision making as directed.\n.     Ensure accounting transactions are in compliance with Nike Golf\n      accounting policies and US GAAP.  Assist in\n       the maintenance of complete and accurate standard operating\n      procedures and SOX related internal control\n       documentation.  Assist in the maintenance of domestic and/or\n      international ledgers including the preparation of\n       A/P related journal entries and reconciliations.  Decision support\n      analysis of financial transactions to determine\n       present and future financial performance. Perform bookkeeping to G/L\n      for month end closing using SAP, HFM,\n       Smartview.\n\nRobert Half & Associates\n                                                                      Nov,\n2005 - Jan, 2014\nContract Financial Consultant\n                                                 Memphis, TN / San\nFrancisco, Ca\n.    Analyzes transaction data for specific account codes for the purpose\nof identifying potential budget variances,\n      compiling statistical information, developing procedures, and\nconforming to established financial practices and\n      regulatory requirements. Maintains spreadsheets for the purpose of\nensuring cash balances are managed on a\n      weekly basis, reconciling monthly expenditures and revenues and\ngeneral ledger cash balances.\n.    Responds to inquiries from a wide variety of internal and external\nsources (e.g. staff, supplier companies,\n      government agencies, etc.) for the purpose of providing information,\ndirection and/or appropriate referrals.\n      Review vendor activity to ensure all monthly invoices have been\nreceived and processed. Review contract\n      obligations to ensure all contract invoices have been received and\nprocessed. Perform the operating cash account\n      reconciliation and related journal entries. Perform various detailed\nmonth-end allocations for fringe benefits,\n      shared costs and indirect cost allocations. Record the release from\nrestrictions revenue journal entry.  Perform\n      various account reconciliations and work to resolve any\ndiscrepancies. Other duties / special projects as assigned.\n\nFedEx Trade Networks Transport & Brokerage, Inc.\n                                               April, 2008 - Jan, 2012\nSr. Express Clearance Operations (ECO) Import Agent\n                                                         Memphis, TN\n.    Serves as the primary link between FedEx Trade Networks Transport &\n      Brokerage, Inc. and FedEx Express for\n      account and shipment specific issues. Sorts and assigns departmental\n      incoming work and monitor daily workflow\n      to ensure regulatory timeframes are met. Failure to meet these\n      timeframes could cost the company and our\n      customer dollars in potential liquidated damages and additional duty.\n      Refer potential major problems to the\n      manager.\n.    Maintains uniform business practices and procedures to ensure\n      compliance with U.S. customs and other\n      government agency (DEA, FDA, DOD, F&WA) laws and regulations and\n      according to FedEx Express\n      expectations. Analyzes documentation provided to determine that\n      harmonized tariff number and whether the\n      shipment qualifies for special tariff treatment, NAFTA, reduced duty\n      rates and ensures all government agency\n      regulations are followed. Also, ensures that the required information\n      is submitted or transmitted electronically.\n.    Completes the classification of imported merchandise by determining\n      the proper tariff classification and associate\n      duty rate, and calculating the entered value using t/b and FedEx\n      systems.\n\nMemphis Marriott East\n                                                                  June,\n2003 - Sept, 2005\nNight Auditor Supervisor/Night Manager.\n\nMemphis, TN\n.     Responsible for reconciling all hotel cashier transactions;\nreviewing, organizing and compiling management\n        reports on a timely basis and ensuring the accuracy of guest\nbillings and hotel ledger transactions.  I was also\n        responsible for effectively communicating concerns and/or related\nissues to all levels of management and\n        performing guest service agent duties as required, including check-\nin, check-out, switchboard operation and\n        reservations.\n.     Oversaw the overall operation of a 14 floor 319 room hotel property\nduring the evening and/or night shift to\n        ensure guest satisfaction and safety. I had full hotel authority in\nthe absence of the General Manager during the\n        night shift. Responsible for following up and making decisions\naffecting guest and team member issues.\n        Maintain direct supervision over all night shift team members.\n.    Supervised and trained night Guest Services Agents to ensure guests\n      are satisfied with their stay and reports are\n      accurately completed. Immediately address any security issues found\n      on property. Ensuring the property is well\n      maintained and free from any safety hazards. Consistently walking\n      through all departments to ensure that all\n      staff members are in proper uniform and present in work areas.\n.\nCOMPUTER SKILLS\nAs a Leader, I have fostered a team approach on consistently executing a\nsolid partnership by developing strong relationships with internal and\nexternal clients with an objective of meeting all clients' financial\nexpectations from a simplified approach. Excellent critical thinking,\nanalytical and spreadsheet modeling skills. Demonstrate the ability to\ndocument relevant facts and information to support testing and conclusions\nso other reviewers can follow the auditor's logic and methodology.\nProficient in Ariba, SAP analysis for office (AO), Primavera P6, Oracle,\nSAP, SAP BW/BO experience, Hyperion (HFM retrieve), Smart View, JD Edwards,\nPower BI, QuickBooks, advance Microsoft Excel (Pivot tables, V-lookup,\nSUMIFS, AVERAGEIFS, COUNTIFS).\n\nADDITIONAL SKILLS\nMy leadership in the past 10 years in Finance has allowed me to acquire\noutstanding qualities throughout the process of leading the delivery of\nfinancial solutions and services to meet the client's financial budgets. As\na Leader, it's imperative that I am effective at collaborating with all\nlevels of individuals to ensure the delivery of a customer centric\nenvironment.\n\n\n",
          "confidence": 1.0,
          "method": "full_text",
          "structured_data": null
        },
        "tax_term": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "source_by": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "designation": {
          "value": "Project Manager",
          "confidence": 0.9,
          "method": "ner",
          "structured_data": null
        },
        "experience": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        }
      }
    },
    {
      "resume_path": "data\\input\\Grant Compliance Manager - MA - Joan.doc",
      "confidence_score": 0.5650000000000001,
      "used_ocr": false,
      "extracted_fields": {
        "skills": {},
        "first_name": "Joan",
        "last_name": "E. Eline",
        "primary_email": {
          "value": "Joan.e.eline@gmail.com",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "secondary_email": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "phone": {
          "value": "4013392244",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "city": {
          "value": "Fall River",
          "confidence": 0.7,
          "method": "ner",
          "structured_data": null
        },
        "state": {
          "value": "RI",
          "confidence": 0.9,
          "method": "zip_database",
          "structured_data": null
        },
        "zip": {
          "value": "02871",
          "confidence": 0.7,
          "method": "regex",
          "structured_data": null
        },
        "work_authority": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "resume_link": {
          "value": "data\\input\\Grant Compliance Manager - MA - Joan.doc",
          "confidence": 1.0,
          "method": "file_path",
          "structured_data": null
        },
        "raw_resume": {
          "value": "\n\n                                Joan E. Eline\n\n                               Fall River, MA\n                                (401) 339-2244\n                           Joan.e.eline@gmail.com\n\n\n\nEXPERIENCE\n\n\n\n\n\n\n              Primrose Center, Inc\n                                  2015-Present\n\n              Orlando, FL\n\n              Accounting and Grant Compliance Manager\n\n               . Accounts Receivable, including invoicing and collections\n               . Posting of payments to customer accounts\n               . Processing Credits\n               . Payroll & Benefit processing\n               . Supervision of Accounts Payable and Reception\n               . Journal Entries as needed\n               . Preparation of YE Audit Workpapers\n               . Grant Compliance, Federal, State and City Level\n               . Projects as needed\n\n\n\n\n\n              Airsports, LLC & Kong USA, LLC\n                                  2014-2015\n\n              Bristol, RI\n\n              Accountant/Customer Service\n\n               . Full Charge Bookkeeping for multiple entities\n               . Processing of all payables and receivables, including\n                 invoicing\n               . Posting of payments to customer accounts\n               . Compiled monthly financial statements, including balance\n                 sheets, income statements, general ledger details and cash\n                 flow\n               . Payroll & Benefit processing\n               . Reconciling of bank accounts\n               . Processing of monthly and quarterly Payroll & Business\n                 Taxes\n               . Projects as needed\n\n\n\n\n\n              Island Carpet & Floor Covering, LLC\n                                  2004-2015\n\n              Middletown, RI\n\n              Office Manager/Bookkeeper\n\n               . Processing of all payables and receivables, including\n                 invoicing\n               . Posting of payments to customer accounts\n               . Compiled monthly financial statements, including balance\n                 sheets, income statements, general ledger details and cash\n                 flow\n               . Payroll & Benefit processing\n               . Reconciling of bank accounts\n               . Processing of monthly sales tax\n               . Worked with certified public accountants on tax issues\n\n\n                    Promptus Communications, INC\n                       1996 - 2004\n\n              Portsmouth, RI\n\n              Accounting Manager\n\n               . Processing of all payables and receivables, including\n                 invoicing\n               . Posting of payments to customer accounts\n               . Compiled monthly financial statements, including balance\n                 sheets, income statements, general ledger details and cash\n                 flow\n               . Payroll & Benefits\n               . Fixed Asset Accounting, including depreciation schedules\n               . Worked with certified public accountants on tax issues\n               . All duties associated with Human Resources\n\n\n\n\n\n\nJoan E. Eline\nPage 2\n\n\n\n\nEDUCATION\n\n\n                   BACHELOR OF SCIENCE, MANAGEMENT\n                      2003\n\n                   Fisher College, Boston, MA\n\n                                    ASSOCIATES OF SCIENCE, ACCOUNTING\n                            1994\n                    Fisher College, Boston, MA\n\nSKILLS\n              Microsoft Excel, Word, Outlook, Internet Explorer, QuickBooks\n              Premier, Quickbooks Enterprise and Microsoft Dynamics\n\n\n\nREFERENCES\n\n            Paul Fredette                                     401-683-6100\n            215 Ethel Dr\n            Portsmouth, RI  02871\n\n            John Dunn                                    401-253-3759\n            62A Ballou Blvd\n            Bristol, RI  02809\n\n            Kenneth W Fain                                    401-465-0693\n            28 Greaton Dr\n            Providence, RI  02906\n",
          "confidence": 1.0,
          "method": "full_text",
          "structured_data": null
        },
        "tax_term": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "source_by": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "designation": {
          "value": "cco",
          "confidence": 0.8,
          "method": "regex",
          "structured_data": null
        },
        "experience": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        }
      }
    },
    {
      "resume_path": "data\\input\\HD - VA - Ovais.doc",
      "confidence_score": 0.615,
      "used_ocr": false,
      "extracted_fields": {
        "skills": {
          "domain_specific": [
            {
              "name": "security",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                1397,
                1859,
                2102,
                2345,
                7170
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "design",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                2458,
                11772
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "infrastructure",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                2534
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "policy",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                9227
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "education",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                12320
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "soft_skills": [
            {
              "name": "consulting",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                1729,
                1868
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "analysis",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                2449
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "accountability",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                3519
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "planning",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                4459
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "training",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                8535,
                10249,
                11044
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "documentation",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                8600,
                9129,
                10080,
                10200,
                10533,
                11819
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "security_tools": [
            {
              "name": "tenable",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                1983
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "business_skills": [
            {
              "name": "reporting",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                2783,
                6872
              ],
              "experience_weight": 0.0,
              "importance_score": 0.8
            }
          ],
          "programming": [
            {
              "name": "sql",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                2796,
                6693,
                6885
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            }
          ],
          "methodologies": [
            {
              "name": "agile",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                3037,
                7283
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "itil",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                4757,
                12412
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "safe",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                7053
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "sdlc": [
            {
              "name": "optimization",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                11750
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ]
        },
        "first_name": "Ovais",
        "last_name": "Khan",
        "primary_email": {
          "value": "Ovais.kit@gmail.com",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "secondary_email": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "phone": {
          "value": "7035052053",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "city": {
          "value": "",
          "confidence": 0.0,
          "method": "ner",
          "structured_data": null
        },
        "state": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "zip": {
          "value": "",
          "confidence": 0.0,
          "method": "regex",
          "structured_data": null
        },
        "work_authority": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "resume_link": {
          "value": "data\\input\\HD - VA - Ovais.doc",
          "confidence": 1.0,
          "method": "file_path",
          "structured_data": null
        },
        "raw_resume": {
          "value": "\n                                 Ovais Khan\n                                703-505-2053\n                             Ovais.kit@gmail.com\n\nSummary:\n    . Verified and supported hard disk issues\n    . Identified browser and connectivity issues\n    . Configured local administrator user account and network connections.\n    .  Managed  to  identify  Apple  devices  connection  issues  and  acted\n      accordingly.\n    . Administered Software Update utility\n    . Utilized the appropriate purpose and format of Internet Protocol  (IP)\n      addresses and subnet masks.\n    . Utilized Access Network services  for  email,  Internet,  and  instant\n      messaging.\n    . Resolved accessing services problems.\n\nSkill                                        Years Used           Last Used\nInstalling and imaging new computers and loading of\nappropriate   software    for    customers.                               16\n        2021\nMigrating   data   and   user   profiles                                  16\n                             2021\nStrong  Microsoft  productivity  software  skills                         10\n                                   2021\nExperience  providing   end   user   system   support                     16\n                             2021\nInstalling and imaging new computers and loading of\nappropriate   software   for    customers.                                10\n                             2021\nExperience  troubleshooting  browser  dependency  issues                  10\n                                   2021\nExperience  with  providing   direct   customer   service                 16\n                             2021\nExperience     with     bug     documentation     and     bug      lifecycle\n  10                                       2021\nExperience            with            workstation            troubleshooting\n     10                        2021\nExcellent   analytical   and   logical   skills                           16\n        2021\nExtensive experience  with  windows  operating  systems                   16\n                                   2021\nExperience      with      MAC       OS                                     3\n                  2021\nExperience  with  Android  OS  for   mobile   device                       4\n                             2021\nWorking knowledge of  application  and  desktop  security  concepts        4\n                                   2021\nExperience  with  remote  desktop   control   tools                       10\n        2021\nExperience   writing   SQL   Queries                                       5\n        2021\nITIL Certification                                 Course in Progress\n\nProfessional Experience:\n\nWashington       Radiology       -       FairFax,       Virginia        USA.\n     Aug 2021 - Current\nHelpdesk Technical Support\n    . Generates top results in terms of resolving calls &  tickets  quickly,\n      efficiently, and effectively.\n    . Unlocking accounts and resetting user password in Active Directory\n    . Installing and  imaging  new  computers  and  loading  of  appropriate\n      software for customers.\n    . Respond to user's tickets, e-mails and assist in resolving Office 365\n      related issues.\n    . Answering around 50 calls in timely manner and logging accurately.\n    . Closed 95% of trouble tickets on the first call without escalation.\n    . Escalating technical issues correctly and effectively to right area of\n      support\n    . Work within a team to provide technology support to Radiologists\n    . Flexible working hours to accommodate a 24/7 shift, including rotating\n      weekends and holidays\n\nConsult     America     Inc     -     Great     Falls,     Virginia     USA.\n     Nov 2018 - Apr 2021\nNIH Bethesda MD\nTechnical Support Engineer\n    . Deploying, installing, and configuring PCs, printers, mobile  devices,\n      and other peripheral equipment\n    . Deployed new hardware and  software,  leveraging  automated  processes\n      established.\n    . Installing and  imaging  new  computers  and  loading  of  appropriate\n      software for customers.\n    . Migrating data and user profiles\n    . Successful pc windows migration completed\n    . Working within customer SLA guidelines\n    . Commended for quickly resolving complex issues  including  application\n      issues, system  crashes,  network  slowdowns,  connectivity  problems,\n      security breaches, virus infections and more.\n    . Consistently logged  and  monitored  ticket  status  to  ensure  fast,\n      quality resolution of every issue.\n    . Maintained bug  status  reports  and  drove  to  resolution  including\n      verification of fixes.\n    . Provide assistance for ongoing maintenance, backups, system health  of\n      local servers and network gear.\n\nSaapa      Consulting      Technology      Abu       Dhabi       -       UAE\nOct 2016 - Aug 2018\nTechnical Engineer\n    . Provide a variety of highly visible, complex,  and  valuable  Security\n      consulting services in  support  of  the  SaapaTech  Projects  (Bolden\n      James, Digital Guardian, Fidelis, Secunia, Nutanix, and  Tenable)  and\n      Business Development pursuits.\n    . Focus on providing added value  to  Saapa,  pursuits,  proposals,  and\n      projects with Security IT solutions.\n    . Provide technical delivery to support Saapa Responses to  RFIs,  RFQs,\n      and RFPs.\n    . Prioritized & coordinated multiple projects to maximize  efficiency  &\n      achieve critical timelines.\n    . Extensively used ServiceNow for ticketing.\n    . Provide IT Security technical qualifications and solutions as part  of\n      SaapaTech Proposal content.\n    . Lead the Business  Analysis,  Design,  Development,  and  Delivery  of\n      technical  solutions  (incl.  Applications,  IT   Infrastructure)   to\n      Projects.\n    . Analyze  business  needs,  recommend  solutions,  facilitate  solution\n      delivery, and work with the project teams to ensure Saapa  ability  to\n      meet client deliverables.\n    . Designed and populated specific table  for  collection,  tracking  and\n      reporting of SQL queries data.\n    . Upon Project  award,  work  on  IT  mobilization  activities.  Manage,\n      execute, monitor, and control Projects.\n    . Manage Project Scope, Budget, and Schedule.  Engage  team  members  as\n      needed.\n    . Documented dead date  test  case  (UAT)  while  working  in  an  agile\n      software development life cycle.\n    . Build client relationships while managing  multiple  initiatives  with\n      competing priorities.\n    . Maintained bug  status  reports  and  drove  to  resolution  including\n      verification of fixes.\n    . Ensure  the  delivery  of  support  services  necessary  for  projects\n      successful  execution  while   also   promoting   standardization   of\n      applications and systems to the extent possible.\n\nComputer       Sciences       Corporation       -       CSC        Australia\n      Jun 2015 - Sep 2016\nTechnical Analyst\n    . Maintaining full accountability for handling all aspects of day-to-day\n      activities as the  primary  point  of  contact  for  users  to  report\n      technical issues and incidents for pioneer  accounts  for  clients  as\n      BHP, AMP Bank, BMC Coal.\n    . Contribute technical expertise  in  evaluating,  troubleshooting,  and\n      resolving routine computer incidents for internal and external users.\n    . Successfully handled all facets  of  day-to-day  incident  and  change\n      management  for  routine  PC,  server,  mainframe   applications   and\n      hardware.\n    . Prioritized & coordinated multiple projects to maximize  efficiency  &\n      achieve critical timelines.\n    . Demonstrated a strong proficiency in assessing upcoming  releases  and\n      identifying support gaps, risks, and opportunities.\n    . Steered a wide array of different IT initiatives, including escalating\n      complex IT issues, evaluating and tracking interactions, incidents and\n      requests within the IT Service Management tool,  and  providing  users\n      with incident status and outages.\n    . Apply strategic planning,  problem  solving,  and  project  management\n      skills  toward  consistently  achieving   critical   deadlines   while\n      maintaining high quality standards.\n\nDepartment     of      Family      and      Community      Services      NSW\nMar 2015 - May 2015\nClient Service Officer\n    . Providing Technical support to over 25,000 users across NSW  per  ITIL\n      framework. I performed the following tasks:\n    . Managing client expectations by ensuring applications are supported to\n      agreed service levels.\n    . Create user accounts and assign group rights  using  Microsoft  server\n      tools (ADS) Active Directory\n    . Manage Citrix Applications access through group policies.\n    . Answering 30-40 ticket calls in timely manner and logging accurately.\n    . Support MS Office Suite 2010 related issues.\n    .  Responsible  for  obtaining   required   information   from   service\n      requesters,   querying   clients   to    ensure    accurate    product\n      identification,  and  logging  the  information   into   proper   call\n      management system.\n    . Escalating calls to correct department.\n\nINTA            serve             -             Sydney             Australia\n  Jul 2012 - Feb 2015\nTechnical Support Engineer\n    . INTA serve is Australia's premiere  hosting  and  domain  specialists.\n      Providing Shared, Co-Location  and  dedicated  hosting  to  Australian\n      business and domains. I performed the following tasks:\n    . Managing technical issues on the DNR (Domain  Name  Registration)  and\n      Shared Hosting servers.\n    . Maintaining daily, weekly and monthly data  backups  as  per  schedule\n      given by management.\n    . Deploying maintenance routine job including data backup  and  restore,\n      upgrade and patch.\n    . Installing Antivirus server/client machine.\n    . Management of mission critical email servers (IMAIL, @MAIL),  DNS  and\n      web server (IIS).\n    . Installation and administration  of  all  software  used  to  run  web\n      services including Microsoft IIS.\n    . Installation & configuration of DNS, Mail and WWW services\n    . Provide technical support to  wide  range  of  dedicated  server,  Co-\n      Location Clients and end users using different means of communication,\n      remote access, telephone and in person support methods.\n    . Diagnosis & resolve  client's  queries  in  specific  time  frame  for\n      hardware, software and peripherals.\n    . Work closely with other groups within company  like  server,  printer,\n      desktop and remote resolution group  to  maintain  smooth  running  of\n      network system.\n    . Document incident and resolution  information  in  multiple  ticketing\n      systems.\n    . MS SQL, Linux control panels (c-panel, Plesk)\n    . Implementation, management & maintenance of all departmental desktop &\n      servers.\n    . Designed and populated specific table  for  collection,  tracking  and\n      reporting of SQL queries data.\n    . Installation & configuration of Windows 2000, 2003 and 2008 servers.\n    . Installing servers (IBM, DELL, HP) in racks in Data center.\n    . Ensuring the implementation  of  safe  systems  appropriate  to  their\n      operational responsibility.\n    . Maintenance and troubleshooting of servers & workstations  security  &\n      identifying & solving the hardware, software issues\n    . Documented dead date  test  case  (UAT)  while  working  in  an  agile\n      software development life cycle.\n    . Coordination with team members to ensure  efficiency  and  quality  of\n      service.\n    . Major achievements include:\n    . Worked on the Data  Centre  Migration  Project  as  a  Junior  Project\n      Officer.\n    . Dramatically improved problem-solving skills  through  troubleshooting\n      and customer enquiries\n    .  Providing  technical  support  to  users  using  different  means  of\n      communication.\n\nHostway              Australia-              Sydney,              Australia.\n       Apr 2007 - Jun 2012\nTechnical Service Specialist\n    . Hostway is one  of  the  biggest  client  service  providers  covering\n      Australia and countries of Far East. Working as client service manager\n      my main duties included project deployment and technical expertise  to\n      various teams. I reported directly to head office in Chicago,  USA.  I\n      performed the following tasks:\n    . Performing key account management duties that apply to over 80 managed\n      hosting clients.\n    . Purchase new hardware from vendors IBM & Dell & sourcing new  partners\n      to achieve cost reduction.\n    . Managing aspects of service delivery - making sure  that  hardware  is\n      sourced, technical team receives hardware and installation is complete\n      before the client is notified.\n    . Providing technical assistance to Technical Support team.\n    . Managing Microsoft License and  submitting  monthly  reports  to  Head\n      Office in Chicago- USA\n    . Training new technical staff with  procedures  &  providing  technical\n      documentation.\n    . Day-to-day operations  involving  technical  aspect  are  carried  out\n      smoothly.\n    . Bug status reports and drove to resolution including  verification  of\n      fixes were documented.\n    . Document incident and resolution  information  in  multiple  ticketing\n      systems.\n    . Maintaining &  Developing  client  support  procedures,  policies  and\n      standards for IT department.\n    . Point of contact for Client technical complaints and resolve technical\n      issues in a smooth manner.\nMajor achievements include:\n    .  Able  to  produce  and  maintain  high  quality  accurate   technical\n      documentation.\n    . Able to utilize technical knowledge in the continuous  improvement  of\n      procedures and policy.\n    . Gained Analytical and structured problem-solving skills and  now  able\n      to analyze, diagnose and troubleshoot root cause  technical  problems.\n\n\n    . Delivered high-quality technical solutions to  client  with  excellent\n      feedbacks\n\nIndigo  Solutions  -  Sydney,  Australia.                                Feb\n2006 -Mar 2007\nTechnical Support Engineer\n    . Indigo is an Australian hosting company serving the Enterprise clients\n      hosting needs in dedicated servers and cloud  computing.  I  performed\n      the following tasks:\n    . System Maintenance and builds for: Dell Productions Servers in an  IDC\n      environment.\n    .  Server  Configuration:  IBM  servers  and  OEM  servers  for  an  IDC\n      Environment.\n    . System Maintenance and builds for: IIS Web server, Linux Web  servers,\n      DNS, CSR and SSL Certificates,\n    . Data Centre Support: 24/7 Pager Alert Monitoring on Enterprise Servers\n      & Backup Monitoring.\n    . Remote Power Rail Management and Remote Server Management.\n    . Documentation of IP, Subnet, NAT and Rack diagrams\nMajor achievements:\n    . Produced & maintained high quality accurate technical documentation.\n    .  Updated,  modified,  and  created  users  training,   technical   and\n      administration manuals.\n\nMax            SI             -             Brisbane,             Australia.\n          Jan 2004 - Dec 2005\nSystem Analyst\n    . Max SI is a  software  development  company  which  developed  systems\n      according to client's requirement  and  offered  software  testing  to\n      client's software. I performed the following tasks:\n    .  Documentation  of  C3M  (Customer  Call  Centre  Management)   system\n      developed by MAX-SI.\n    . Script writing for regression test critical program components.\n    . Preparing Requirements Document  for  purpose  of  system  integration\n      using UML (Unified Modeling\n    .  Language).  Workflow  modeling/process  mapping  of  the  C3M  system\n      (Microsoft Visio)\n    . Successful implementation of Unified  Modeling  Language  (UML),  Data\n      Feed Diagrams (DFD), Entity\n    . Relationship Diagrams (ERD) and Program  Evaluation  Review  Technique\n      (PERT).\n    . Updating,  modifying,  and  creating  users  training,  technical  and\n      administration manual.\n    . Debugging, problem solving  and  analyzing  user  interface  form  for\n      better friendly usage.\nMajor achievements include:\n    . Confident application of UML for system mapping (various modules)\n    . Part of developer team to launch MAX-SI's first successful  in  house-\n      built software C3M (Call Centre\n    . Customer Management) etc.\n\nNet          Technologies          Solutions          -           Australia.\nOct 2002 - Sep 2003\nWeb Developer/Tester:\n    . Net Technologies is a Gold Coast  based  web  application  development\n      company, designing & developing  local  business  static  and  dynamic\n      websites. I performed the following tasks\n    . Development of web based, component based and database application.\n    . Database designing, management, administration, and optimization.\n    .   Software   design,   development,   and   technical    specification\n      documentation.\n    . Software Proposal and time estimates preparation.\n    . Code review and application performance improvement.\n    . Test designing, development, and execution of manual test cases.\n    . Participation in Software Process Improvement activities.\n    . Coordination with development teams and project managers and  client's\n      coordination.\nMajor achievements include:\n    . Understanding Team, Project managers & Clients coordination.\n    . Development and execution of manual test cases.\n    . Demonstration of Database designing, management skills.\n\nEducation:\n    .  MIT.  Master  of  Information  Technology  -   Griffith   University.\n      Australia.\n    . Certifications:\n    . ITIL. V4 Foundation - Course in Progress\n\n\n",
          "confidence": 1.0,
          "method": "full_text",
          "structured_data": null
        },
        "tax_term": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "source_by": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "designation": {
          "value": "system support",
          "confidence": 0.9,
          "method": "ner",
          "structured_data": null
        },
        "experience": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        }
      }
    },
    {
      "resume_path": "data\\input\\HealthRules - configuration  - GA - Violet Jones.doc",
      "confidence_score": 0.46740000000000004,
      "used_ocr": false,
      "extracted_fields": {
        "skills": {
          "cloud": [
            {
              "name": "shield",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                74
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "functions",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                1735
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "soft_skills": [
            {
              "name": "analysis",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                266,
                757,
                2524
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "documentation",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                298,
                2246,
                2318,
                3154
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "productivity",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                3633,
                4525
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "domain_specific": [
            {
              "name": "compliance",
              "confidence": 0.96,
              "context": "experience_section",
              "positions": [
                6253
              ],
              "experience_weight": 0.2,
              "importance_score": 0.7
            },
            {
              "name": "healthcare",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                866,
                5599
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "security",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                1056,
                1077,
                1095
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "contracts",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                1847,
                3080,
                3609
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "architecture",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                2093
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "design",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                4159
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "insurance",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                4804,
                4991,
                5153,
                5242,
                5534,
                5579
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "medical",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                4859,
                4922
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "education",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                5833
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "programming": [
            {
              "name": "sql",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                2947,
                6074
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            }
          ],
          "managerial_skills": [
            {
              "name": "mvp",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                3438
              ],
              "experience_weight": 0.0,
              "importance_score": 0.8
            }
          ],
          "databases": [
            {
              "name": "oracle",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                5935
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "frameworks": [
            {
              "name": "soap",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                5977,
                5985
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "electron",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                6095
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "data_skills": [
            {
              "name": "excel",
              "confidence": 0.96,
              "context": "experience_section",
              "positions": [
                6293
              ],
              "experience_weight": 0.2,
              "importance_score": 0.9
            }
          ]
        },
        "first_name": "Violet",
        "last_name": "Jones",
        "primary_email": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "secondary_email": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "phone": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "city": {
          "value": "Quincy",
          "confidence": 0.7,
          "method": "ner",
          "structured_data": null
        },
        "state": {
          "value": "MA",
          "confidence": 0.8,
          "method": "city_database",
          "structured_data": null
        },
        "zip": {
          "value": "",
          "confidence": 0.0,
          "method": "regex",
          "structured_data": null
        },
        "work_authority": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "resume_link": {
          "value": "data\\input\\HealthRules - configuration  - GA - Violet Jones.doc",
          "confidence": 1.0,
          "method": "file_path",
          "structured_data": null
        },
        "raw_resume": {
          "value": "\nViolet Jones\n\nSummary:\n\n    . 15 years Healthcare Payer Experience\n    . 10+ years Facets and NetworX Pricer experience (versions 2.96 to 5.10,\n      both front and back end expertise for configuration and testing)\n    . 10+ year experience in HSD Diamond 950, Xcelys,  Amysis, Amysis\n      Advance, QNXT and Health Rules\n    . Comprehensive experience in healthcare data analysis encompassing\n      systems reporting and testing. Superior claims adjudication and\n      auditing expertise resulting in excellent performance evaluations.\n\n\nExperience:\n\n10/2014-Present (Traveling Position) Jacobson Staffing; HCSC-Blue\nCross/Blue Shield Of Texas Medicare Contract Office; Facets Benefits\nConsultant, UAT Testing Analyst, Configuration Analyst\n\n    . Ensure updates of CPT, HCPCS, DRG, and ICD-10 codes as well as\n      pricing, billing rates, analysis, data entry, testing and\n      documentation of configuration set up for all Facets applications.\n    . Translates business rules into effective and efficient Facets\n      configuration\n    . Gathering and analyzing data in support of business cases, proposed\n      projects and system requirements.\n    . Prepare and deliver reports, recommendations or alternatives in an\n      effort to streamline processes and improve operational efficiencies\n    . Define and deploy operational requirements and system goals based off\n      of conducted analysis and agreed upon recommendations\n\n\n7/2015- 9/2015 (Traveling Position) Catalyst Solutions- Lumeris Essence\nHealthcare; Facets Benefits Consultant, Testing Analyst, Sr. Facets\nConfiguration Analyst\n\n    . Set up & configure ITS; Synchronize ITS Home- Facets Configuration\n      Analyst; Configure MQ series\n    . Configure SA/Security; ITS Product Security; ITS User Security\n    . Facets to ITS code Conversions; ITS COB Method Conversion; ITS Codes\n      Description; ITS Error codes description\n    . ITS Home Administrative Rules Application; ITS HRA Administrative\n      Rules\n    . ITS Home Products; ITS Proxy provider applications\n\n\n\n\n\n3/2011-8/2014 (Traveling Position) Dell Staffing, Harvard Pilgrim Health\nQuincy, MA.\nBusiness Analyst , Amysis  Advance & Facets Configuration Analyst, UAT\nTesting Analyst\n\n    . Provides oversight of business system requirement/ specification\n      analyst & maintenance in accordance with customer deliverables.\n    . Document and test  Facets test scenarios, cases, and step-by-step\n      instructions based on Harvard pilgrim health business functions.\n    . Developed and maintained business system configuration solutions in\n      accordance with provider/employer contracts and customer requirements.\n    . Conceiving and delivering Claims Mass re-Adjudication (CMA)\n    . Administered the data/ configuration maintenance of systems\n      parameters, specifications and data linkages to other systems involved\n      in SOA component based architecture.\n    . Query and analyze data backend table data from Facets and NetworX\n      tables.\n    . Developed, maintained, and disseminated internal and external system\n      documentation, including status updates; prepared procedural and\n      customer documentation when required.\n    . Responsible for data maintenance in Facets Tables which includes but\n      not limited to backend tables, writing queries through data dictionary\n      and stored procedures\n    . Test and quality analysis validation of configuration building test\n      claims\n    . Able to convert CORE Facets pricing to NetworX pricing\n\n\n\n9/2009-3/2011(Traveling Position) Dell Staffing   Universal America\nHouston, TX\n        Facets Configuration Analyst, Facets Testing Analyst, Senior\nAppeals and Grievance Specialist\n\n    . Implementation of Medicare Line of Business into Facets 5.01\n    . Performed Facets front and back end configuration and inquiries\n      activities using SQL\n    . Responsible for timely and accurate configuration build and test\n      activities related to Benefits, Payments, Pricing, Claims, Provider\n      contracts & Membership/Enrollment, including ITS.\n    . Creation and maintenance of documentation related to build, test,\n      policies and processes related to implementation efforts\n    . Provided status reports on weekly basis\n    . Responsible for monitoring effectuations of all resolutions as a\n      result of appeal or grievance.\n\n3/2007-9/2009; 5/2013-3/2014 (Work @ Home) Jacobson Staffing    MVP\nRochester, NY\n          Senior Facets Claims Specialist and Adjustment Representative\n\n    . Researched, analyzed and adjusted claims in accordance with the plans\n      provision and provider contracts\n    . Produced high productivity and accuracy rate processing claims using\n      Facets version 4.81\n    . Adjudicated and adjusted claims using Facets Medicare, Medicaid,\n      Manage Care and PPO claims\n    . Strong problem solving techniques with excellent verbal and written\n      communication skills\n    . Outstanding organizational skills and ability to meet deadlines\n\n\n\n3/2005-3/2007(Traveling Position) Jacobson Staffing    Lovelace Health Plan\n   Alb, NM\n       Facets Testing and Configuration Analyst, Appeals and Grievance\nRep.; Senior Claims Processor\n    . Followed established requirements and design for Facets configuration,\n      testing and debugging, provided build and release project/product\n      components\n    . Facets  membership (enrollment) and adjudication of test claims for\n      eligibility and benefit configuration validation\n    . Responsible for identifying necessary steps and processing claims of\n      various complexity levels for Medicare, Medicaid, HMO, PPO, SNF, Rehab\n      etc.\n    . High productivity and accuracy rate processing overturned appeals;\n      processed and adjusted claims in accordance with the plans guidelines\n\n\n10/2003-3/2005: Southern Crescent Brain Injury Hospital Stockbridge, GA.\nSenior Biller and Collections Specialist for Medicare, Medicaid and\nCommercial Insurance\n\n    . Providing appropriate coding on the patient's medical history,\n      diagnosis, tests and treatments\n    . Using the coded medical records to create invoices that are sent to\n      the patients and insurance companies.\n    . Responsible for submitting and following up on all claims which\n      includes electronically filing Medicare, Medicaid, Commercial claims\n    . Liaison to insurance companies, accepting and posting payments\n    . Accurately recording the patient's and insurance company details into\n      the system.\n    . Facets patient enrollment, Billing, case management and follow-up\n\n\n\n\n6/2000-10/2003: The Shepherd Center Spinal and Brain Injury Hospital\nAtlanta, Ga.\nBiller and Collection Specialist for Medicare and Medicaid\n\n    . Used coded data to produce and submit claims to insurance companies\n    . Worked directly with the insurance companies, healthcare providers,\n      and patients to get claims processed and paid\n    . Reviewed and appealed unpaid or denied claims; handle collections on\n      unpaid accounts\n    . Manage the facility's accounts receivable reports; answer patients\n      billing questions\n\n\nEducation:\nGeorgia State University Atlanta, Georgia\nA.S., Math\nGraduated Cum Laude\n\nSystem Experience:\nHIX, Oracle, RFT/QTP functional automation tool, Soap UI, Soap UI Pro,\nAMISYS, NASCO, Facets (2.96 through 5.10), Rims, Xcelys, NextGen, QNXT,\nDiamond, TOPPS, SQL, CSC, Macess, EMDON, Electron and Navicure clearing\nhouse, Florida Shared Systems, Mutual of Omaha Electronic Claims, Trizetto\nNextworX Pricer, Care Planner, Citrix, ICD-9, ICD-10, HIPAA Compliance,\nProficient in Microsoft Word, Excel, PowerPoint, Outlook E-mail, Database\nManagement & Intermediate computer skills.\n\n\nCertification:\n\n    . CPAR, 2005\n\n\n\n\n",
          "confidence": 1.0,
          "method": "full_text",
          "structured_data": null
        },
        "tax_term": {
          "value": "CONTRACT",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "source_by": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "designation": {
          "value": "Business Analyst",
          "confidence": 0.9,
          "method": "ner",
          "structured_data": null
        },
        "experience": {
          "value": "10+",
          "confidence": 0.9,
          "method": "regex_total_experience_summary",
          "structured_data": null
        }
      }
    },
    {
      "resume_path": "data\\input\\HealthRules - Data Analyst - MI - John.doc",
      "confidence_score": 0.4016216216216216,
      "used_ocr": false,
      "extracted_fields": {
        "skills": {
          "programming": [
            {
              "name": "java",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                36
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "sql",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                41,
                197,
                221,
                1877,
                1907,
                1936,
                1956,
                2969,
                3658,
                3697,
                3824,
                4548,
                5123,
                5459,
                5633,
                6449,
                6663,
                7121,
                8046,
                8141,
                9411,
                9599,
                10254,
                10403,
                10715,
                12094,
                12344,
                13064,
                13110,
                14611,
                15414,
                15667,
                15894
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "html",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                45
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "xml",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                50,
                3051,
                7257,
                7824,
                11184,
                12519
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "sas",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                3780
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "python",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                4817
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            }
          ],
          "frameworks": [
            {
              "name": "selenium",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                68
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "soap",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                77,
                7795,
                7841
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "business_skills": [
            {
              "name": "reporting",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                89,
                2481,
                6674,
                10726,
                13551,
                14923
              ],
              "experience_weight": 0.0,
              "importance_score": 0.8
            },
            {
              "name": "frd",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                14159
              ],
              "experience_weight": 0.0,
              "importance_score": 0.8
            }
          ],
          "methodologies": [
            {
              "name": "agile",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                163,
                3186,
                10880
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "scrum",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                169
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "waterfall",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                175,
                3172,
                13935
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "data_skills": [
            {
              "name": "etl",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                208,
                2521,
                2658,
                2718,
                2776,
                2883,
                11517,
                13418
              ],
              "experience_weight": 0.0,
              "importance_score": 0.9
            },
            {
              "name": "informatica",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                950,
                1122,
                1461
              ],
              "experience_weight": 0.0,
              "importance_score": 0.9
            },
            {
              "name": "erwin",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                6199,
                8282,
                12997
              ],
              "experience_weight": 0.0,
              "importance_score": 0.9
            },
            {
              "name": "excel",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                10536
              ],
              "experience_weight": 0.0,
              "importance_score": 0.9
            }
          ],
          "soft_skills": [
            {
              "name": "analysis",
              "confidence": 1.2,
              "context": "skills_section",
              "positions": [
                448,
                484,
                2256,
                4460,
                5816,
                5940,
                5966,
                6022,
                7000,
                7582,
                10671,
                11761,
                13561,
                13838,
                13959,
                14933,
                15043
              ],
              "experience_weight": 0.2,
              "importance_score": 0.7
            },
            {
              "name": "organization",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                2392
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "documentation",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                7390,
                9209,
                10346,
                11349,
                11521,
                13040
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "planning",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                12950
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "domain_specific": [
            {
              "name": "design",
              "confidence": 1.2,
              "context": "skills_section",
              "positions": [
                535,
                1713,
                2089,
                2594,
                5834,
                6078,
                6115,
                7293,
                7383,
                8291,
                8732,
                13709,
                13847,
                15564
              ],
              "experience_weight": 0.2,
              "importance_score": 0.7
            },
            {
              "name": "compliance",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                690,
                7421,
                7658
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "engineering",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                3332,
                8343
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "medical",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                3447
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "insurance",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                4414,
                13471
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "healthcare",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                5363,
                14177
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "governance",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                7443,
                11148
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "publishing",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                8355
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "architecture",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                9914
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "mining",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                11384
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "databases": [
            {
              "name": "oracle",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                3007,
                4537,
                5620,
                11700
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "db2",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                4544
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "snowflake",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                6611
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "cloud": [
            {
              "name": "batch",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                4803,
                4853,
                5871,
                13624,
                15001
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "functions",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                8077,
                9431,
                12109
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "workflows",
              "confidence": 1.0,
              "context": "skills_section",
              "positions": [
                14188
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ]
        },
        "first_name": "PROFESSIONAL",
        "last_name": "SUMMARY",
        "primary_email": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "secondary_email": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "phone": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "city": {
          "value": "",
          "confidence": 0.0,
          "method": "ner",
          "structured_data": null
        },
        "state": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "zip": {
          "value": "",
          "confidence": 0.0,
          "method": "regex",
          "structured_data": null
        },
        "work_authority": {
          "value": "",
          "confidence": 0.8,
          "method": "regex",
          "structured_data": null
        },
        "resume_link": {
          "value": "data\\input\\HealthRules - Data Analyst - MI - John.doc",
          "confidence": 1.0,
          "method": "file_path",
          "structured_data": null
        },
        "raw_resume": {
          "value": "\nPROFESSIONAL SUMMARY:\n .  8+  years'of  diverse  experience  as  Data  Analyst   across   multiple\n   organizations.\n . Experience with mapping and validating various EDI  files  such  as  834,\n   820, 835, 837, etc.\n . Strong knowledge of  software  development  methodologies  including  the\n   agile methodology, Waterfall, and RAD Methodology.\n .  Knowledge  and  experience  working  with  FACETS  and   FACETS   claims\n   processing.\n . Skilled in bug reporting and tracking using  ALM  (Quality  Center),  IMB\n   Clear quest and JIRA.\n . Experience in developing, trouble shooting and customizing  test  scripts\n   using Quick Test Professional QTP /HP UFT for Functional and Regression\n   Testing.\n . Experienced in  using  management  tools  such  as  HP  ALM,  JIRA-Zephyr\n   combination, CA Agile Central (Rally) for writing the  test  cases,  test\n   data, executing the test cases, tracking, logging and  reporting  defects\n   as well as developing Test Analysis Report (TAR).\n . Performed backend database testing by writing SQL and PL/SQL  scripts  to\n   verify data integrity.\n .  Tested  the  HIPAA  EDI  transactions  834,  837/835,  276/277,  270/271\n   according to the requirement test scenarios.\n . Assist with managing FACETS of project  life  cycle,  including  design,\n   development, testing, and deployment.\n . Good working knowledge of HL7 standards and implementation.\n . Worked with Microsoft SQL Server. Thorough knowledge and understanding of\n   Dimensional Data Modeling, Star schema, Snow-Flake  schema,  creation  of\n   Fact  and  Dimension  Tables,  OLAP,  OLTP  and  other   Data-Warehousing\n   concepts.\n .    Solid    understanding    of Membership,    Claims     Processing,\n   Benefit/Eligibility, COB, Authorization /Referrals, and have  experience\n   in HIPAA standards and corresponding EDI transactions.\n . Hands on experience with  solid  understanding  of  Business  Requirement\n   gathering,  Business  Process  flow,  Business   Process   Modeling   and\n   database/data warehouse experience.\n . Excellent communication and organizational skills  with  the  ability  to\n   adapt to a new environment.\n . Experience in facilitating meetings with clients to discuss and  sign-off\n   on the document.\n . Good experience in Back-End Testing using SQL queries on Oracle,  MS  SQL\n   Server to validate the consistency of data.\n . Expertise in  Unit  testing,  Integration  testing,  Regression  testing,\n   System testing, User Acceptance testing, and Implementation testing.\n . Extensive working experience in Functionality, System, Integration, Front-\n   End, GUI, Back-End, Recovery Testing,  Compatibility  Testing,  Usability\n   Testing, Regression, Tracking Bug Reports using Bug tracking  Tools  like\n   Quality Center.\n . Experience in usage of defect tracking tools  Quality  Center/ALM,  Clear\n   Quest, Rally, JIRA for logging the defects,  generation  of  reports  and\n   analyzes the reports.\n . Used Agile-testing methodology for achieving deadlines in UAT.\n . Performed planning and development of Test Plans, Test  Strategies,  Test\n   Cases and Test Scenario to meet product's business requirement\n . Good knowledge of SQL and experience at conducting backend testing.\n . Expertise in Testing of Client/Server and Web based applications.\n . Experience in Functional Testing, System Integration  Testing,  Back  End\n   Testing, GUI Testing, configuration Testing, User Acceptance Testing.\n . Tested web applications in Agile, waterfall development processes.\n . Strong experience in Front-end Testing and Database  (back-end)  Testing,\n   GUI testing black box testing, white  box  testing,  gray  box  and  User\n   acceptance testing.\n . Possess strong problem-solving skills with the ability to adapt to a  new\n   environment and meet deadlines.\n . Motivated self-starter with exceptional team  building,  leadership,  and\n   interpersonal skills. Good team player with the ability to work  in  time\n   sensitive environments.\n . Experience  in  creating  and  maintaining  the  Requirements  definition\n   documents   that   included   Business   requirements   and    Functional\n   requirements.\n . Solid experience in creating and validating TCS letters  through  various\n   FACETS  Applications  (Utilization  Management,  Claim   Processing   and\n   Customer Service Applications) in the QAE1 environment.\n\n\nTECHNICAL SKILLS:\n\n Operating Systems:          Windows\n Languages:            Java, SQL, HTML, XML\n Testing Tools:              Selenium, SOAP UI\n Bug Reporting Tools:                 Bugzilla, JIRA, HP ALM/ Quality Center\n\n Project Methodologies:      Agile Scrum, Waterfall\n Database:             MS SQL server\n ETL Tools:            MS SQL server integration Services\n\n\n\n\nPROFESSIONAL EXPERIENCE:\n\nUnited            Health            Group,            Phoenix             AZ\n                  Sep 2018 - Present\nData Analyst\nI have worked on the project for Health Claims Scanning and Data  Extraction\n(OCR).  I developed and managed needs analysis, requirements gathering,  gap\nanalysis,  creation  of  vendor  disaster  recovery  plan,  design,  layout,\nbusiness rule development and associated software interface development  and\ntest  documents,  development,  and  EDI   implementation   insuring   HIPAA\ncompliance.   The  project  had  a  budget  of  $800K  and  the  system  was\nanticipated to have an annual volume 1.5M claims.\nResponsibilities:\n . Assisted the project manager in the creation of  the  project  charter  &\n   vision document during the inception phase of the project.\n . Used Informatica Power Center to extract, transform and load data  from\n   various source systems to staging and target Data warehouse.\n . Tuned the performance of mappings by following Informatica best practices\n   and applied several methods to get best performance by decreasing the run\n   time of workflow.\n . Imported Source/Target Tables from the respective databases  and  created\n   reusable  transformations  (Joiner,  Routers,  Lookups,   Rank,   Filter,\n   Expression, and Aggregator) in a Mapplet and created new  mappings  using\n   Designer module of Informatica.\n .  Developed Test Cases for Testing the Facets Model.\n . Analyzed  business  Processes,  Subscribers  -  group  -  plan  -  county\n   structure, current processes, Facets configurations  and  FACETS  backend\n   processes.\n . Coordinated with the developers and IT architects to design the interface\n   of the new system according to the X12 (270, 276, 278, 834,  837  (I,P,D)\n   and 820) standards.\n . Performed daily tasks including backup and restore by  using  SQL  Server\n   2008 R2 tools like SQL Server Management Studio, SQL Server Profiler, SQL\n   Server Agent, and Database Engine Tuning Advisor.\n .  Worked  with  Facets,  e-Billing  and  EDI  HIPAA  Claims  (837/835/834)\n   processing\n . Develop, design & implement department plan to operationalize new  FACETS\n   integrated processing system, to include but not  limited  to,  workflow,\n   management oversight and performance analysis.\n . Translated  business  requirements  into  functional  specifications  and\n   documented the work processes and information flows of the organization.\n . Extensive use of OLAP Function as well as complex joins to  extract  data\n   for reporting purposes.\n . Designed the overall ETL solution including analyzing  data,  preparation\n   of high level, detailed  design  documents,  test  plans  and  deployment\n   strategy.\n . Implemented ETL process to load and extract data.\n . Written Test Cases for ETL to compare Source and Target database systems.\n . Created ETL execution scripts for automating jobs.\n . Created test cases and test procedures for  various  stages  of  the  ETL\n   process including source to extract, source to  staging  and  staging  to\n   target using SQL, Quality Center 10.00 and TOAD for Oracle 10g.\n . Administration of HL7 Interfaces  (XML/CCD,  Demographics,  Appointments,\n   Orders and Results, Billing, HIE) in various environments.\n . Worked in all phases of SDLC in Waterfall  and  Agile  environments  with\n   business stakeholders, platform leads, and test leads  to  define  scope,\n   gather user stories and requirements.\n . HL7 v3 interface engineering to process  ambulatory  and  acute  EMR  HL7\n   messages.\n . Implemented HL7 interfaces to process EMR messages of medical POC  (Point\n   of Care) devices.\n . Created views to make data available on need to know basis.\n . Created queries to  fetch  data  from  different  tables,  using  derived\n   tables, Sub  queries,  as  well  as  Correlated  queries  on  SQL  Server\n   platform.\n . Performed numerous SQL queries that were highly tuned using concepts like\n   Explain, Stats and CAST.\n . Used SAS to import and export data from and into SQL Server platform.\n . Used UML Diagrams to graphically capture system functionalities.\n .  Periodically  Designed  Conceptual,  Logical  and  Physical  Models   to\n   effectively improve Business processes.\n . Modified various data sources,  writing  complex  new  queries  based  on\n   business needs.\n . Maintained departmental reports on daily basis  and  managed  performance\n   issues.\n\nCareSource,                            Dayton,                            OH\n    Oct 2016 - Jul 2018\nDATA  ANALYST\nI worked as a Data Analyst on Provider  information  delivery  team,  Health\ncare application Dashboard and involved  in  troubleshooting  and  resolving\nerrors in 834 and  820  transactions  for  health  insurance  exchanges  and\nperforming root cause analysis.\nResponsibilities:\n . Analyzed the source data coming from  Legacy  system,  Oracle,  DB2,  SQL\n   server and flat files. Worked with  Data  Warehouse  team  in  developing\n   Dimensional Data Model.\n . Worked on solving the errors of EDI  834  load  to  Facets  through  MMS.\n   Created keyword files to have member data bulk  loaded  into  the  FACETS\n   system through the MMS batch\n . Built a python class where the objects were batch jobs depending on their\n   severity.\n . Worked with data investigation, discovery and mapping tools to scan every\n   single data record from many sources.\n . Optimized the performance of SSIS packages by filtering  data  at  source\n   level, using fast load options and tuning embedded SQL queries.\n . Worked extensively on EDI transactions 837 and 835  Involved  in  writing\n   test cases for different LOB's (ITS, FEP and Regular) for  SIT,  Parallel\n   and UAT.\n . Set Claim processing data for different FACETS Module.\n . Analyzing  Medicare,  Medicaid  healthcare  programs  and  performs  data\n   transformations, data manipulations, data validations using SQL,  Access,\n   VLOOKUP, and Index Matches for  dealing  with  data  inconsistencies  and\n   maintain data systems with accurate data.\n . Converted syntax of diverse queries from Oracle to MS SQL Server in order\n   to reach the business requirements\n . Accomplished the data mapping documentations for database updates\n . Experienced in full life cycle MDM development including  data  analysis,\n   database design, data mapping and data load in batch.\n . The data was integrated with the Medicaid data after extensive analysis.\n . Involved  in  Data Analysis,  Data  Cleansing,  Requirements  gathering,\n   Business Analysis,  Entity  Relationship  diagrams  (ERD),  Architectural\n   design docs, Functional and  Technical  design  docs,  and  Process  Flow\n   diagrams\n . Handled and maintained model versioning using Erwin.\n .  Created  entity  /  process  association  matrices,  entity-relationship\n   diagrams, functional decomposition diagrams and data flow diagrams.\n . Analyzing the reference of data which is extract from  source  table  and\n   transform to the target table by using the SQL query.\n . Experiences in Designing, modeling, and creating database and Normalizing\n   or de-normalizing data according to business  requirements  and  Creating\n   Star and snowflake schemas.\n . Designed Enterprise reports using SQL Server Reporting Services (SSRS)\n . Did Transformation and data cleansing activities use various Control flow\n   and data flow tasks in SSIS packages during data migration\n . Created and managed  Event  Handlers,  Package  Configurations,  Logging,\n   System and User-defined Variables for SSIS Packages.\n . Analyzed the FACETS Requirements and conducted gap analysis.\n . Responsible for Medicaid Claims Resolution/Reimbursement for peach  state\n   health plan using MMIS.\n . Used SSIS  and  T-SQL  stored  procedures  to  transfer  data  from  OLTP\n   databases to staging area  and  finally  transfer  into  data  marts  and\n   performed action in XML.\n .  Participated  in   Complete   Formal   Design   process   from   initial\n   specifications and requirement. Involved  in  creating  technical  design\n   documentation.\n . Worked with data compliance teams, Data governance team to maintain  data\n   models, Metadata, Data Dictionaries.\n . Analyzed results and EDI ANSI X12 file mapping and reported  on  standard\n   analysis spreadsheet. Reviewed EDI companion guides  for  all  payers  to\n   ensure compliance, edit integrity and maintain up-to-date list  of  payer\n   contacts. Acted as a liaison between client and payer/intermediary.\n . Tested SOAP request and response in XML format using SOAP UI.\n . Extracted data from a data source and performed ad-hoc queries.\n . Extracted data  from  existing  data  stores,  Developing  and  executing\n   departmental reports for performance and response purposes by  using  SQL\n   Server procedures, packages, functions, database triggers\n . Created numerous views with complex SQL statements to retrieve data  from\n   the Tables.\n . Devising a planned approach  to  maintain  the  month  long  segmentation\n   process.\n .  Extensively  used  ERWIN  to  design   Logical/Physical   Data   Models,\n   forward/reverse engineering, publishing data model to acrobat  files  and\n   Data Cleansing.\n . Validated the positions and the characters after the data gets translated\n   through TIBCO.\n . Analyze test cases and defects being loaded in HP  ALM  by  QA  teams  to\n   ensure the link entities and accuracy of data.\n .  Involved  in  designing,  development  and  testing  of  Interfaces   to\n   communicate with third party data.\n .   Prepared   technical   design/specifications   for   data   Extraction,\n   Transformation and Loading.\n\nTUFTS            HEALTH             PLAN,             BOSTON,             MA\nMay 2014 - Jul 2016\nDATA ANALYST\nTHP is going to build a new system called Health Rules Payer where our  team\nis working on the transformation/conversion of historic data.\nResponsibilities:\n . Worked on the Data Warehouse team analyzing data files  that  had  to  be\n   compiled from disparate  non-production  sources  and  readied  them  for\n   production. Tasks included: comparing data to requirements documentation,\n   creation of data layouts and data dictionary.\n . Coordinates HIE maintenance events with application owners  and  partner\n   systems; executes maintenance routines in the system\n . Developed  various  T-SQL  objects  such  as  Functions,  Tables,  Views,\n   Triggers, Indexes, Constraints, Stored Procedures and Queries to be  used\n   in the project.\n . Optimized the performance of queries with modification in T-SQL  queries,\n   creating sub queries, establishing joins and  creating  indexes  wherever\n   necessary.\n . Lead client  discussions  relating  to  the Facets configuration  of  a\n   standard Medicare Advantage plan.\n . Supported new business requirements by extending the functionality of the\n   core Facets system using the Facets extensibility architecture feature.\n . Involved in Facets Implementation, including end to end testing of Facets\n   Billing, Claim Processing and Subscriber/Member module.\n . Receives reviews and enters  data  and  source  documents  into  Medicaid\n   billing system\n . Participated as member of a project  team  performing  various  lifecycle\n   tasks  for  an  intranet  application  with  SQL  Server  2012  database.\n   Developed logical  and  physical  database  models  and  data  dictionary\n   documentation.\n . Created Stored Procedure and Views, Indexes, SQL joins and Sub queries\n . Worked with integration services for transferring and reviewing data from\n   different sources like (Flat file, Excel, CSV)\n . Responsible for acquisition of Medicaid  data  from  MDW  (Medicaid  Data\n   warehouse)\n . Generated Daily, Weekly, Monthly reports for the analysis of managers and\n   end users by using SQL Server Reporting Services.\n . Co-ordinate with different application team to  develop  and  standardize\n   enterprise wide data model, created dimensional data  model  using  Agile\n   Methodology,  Designed  and  developed  dimension  tables,  fact  tables,\n   conformed fact and dimension tables.\n . Analyzed  the  member  eligibility,  plans  including Medicaid/Medicare,\n   refill, pricing and drug edits.\n . Participate in  weekly  data  analyst  meeting  and  submit  weekly  data\n   governance status\n . Examined existing XML with TIBCO Resources to solve any  existing  issues\n   or provide new direction\n . Data profiling and Data  quality  checks  along  with  integration  rules\n   determination and documentation.\n .  Experience  with data  mining, data  cleansing, data  modeling, data\n   manipulation, record matching, table linking, table structures, importing\n   and exporting.\n . Created ETL documentation such as EDI X12  837(P,I),  834  and  835  Data\n   Mapping, Transformation logic for Main Frame Layout, Updating  Meta  data\n   documents for new Platform.\n . Extracted raw data from an Oracle database and used SSIS to read it  and\n   run statistical analysis using SSIS.\n . Normalization to 3NF/de-normalization techniques for optimum  performance\n   in relational and dimensional database environments.\n . Wrote Test Plans, Test Scenarios, Test Cases and the Test Matrix.\n . Participated in the tasks of data migration from legacy to  new  database\n   system\n . Responsible for creating and modifying the PL/SQL  procedures,  functions\n   according to the business requirement.\n .  Re-engineered  and  captured  EDI  transactions  with   legacy   systems\n   [Enrollment -834, Eligibility Transaction (270/271), Claims (837),  Claim\n   Status Request and Response (276/277), Remittance (835)].\n . Created SQL Server Reports, handling sub-reports and writing  queries  to\n   perform drill down, drill-through operations in SSRS.\n . Used shell commands to create serialized C# class files  from  XML  based\n   schemas Involved in performing the  unit  testing,  Regression  and  User\n   Acceptance Testing.\n\nPremera            Blue            Cross,            Seattle,             WA\n                                                      Mar 2013 - Mar 2014\nData Analyst\nThe project deals with Mainframe Applications that has  different  types  of\nTransactions that supports the HC - Claims Adjudication  Process  System.  I\nworked as a Sr.  Data  Analyst  for  the  Meredian-Accums  Project  for  the\nconversion of claims to FACETS.\nResponsibilities:\n . Involved in Planning, Defining and  Designing  database  using  Erwin  on\n   business requirement and provided documentation.\n . Installed SQL Server 2008 R2 and Management tools using SQL Server  Setup\n   Program.\n . Created SSIS packages to extract data  from  OLTP  to  OLAP  systems  and\n   scheduled Jobs to call the packages and Stored Procedures.\n . Applied various data  transformations  like  Slowly  Changing  Dimension,\n   Aggregate, Sort, Multicasting, Conditional Split, Derived column.\n . Developed SSIS packages for ETL to migrate data from  different  sources\n   including  insurance  groups  etc  to  the  data   Involved   in   FACETS\n   configuration, Customization, reporting,  analysis  and  enhancement  and\n   worked on membership, claim module, Batch Processing, Pricing Module.\n . Coordinated with the developers and IT architects to design the interface\n   of the new system according to the X12 (270,  276,  278,  834,  835,  837\n   (I,P,D) and 820) standards.\n . Performed  end  to  end  analysis,  design,testing,  implementation,  and\n   support for enhancements of complex systems utilizing Waterfall SDLC.\n . Detailed Analysis of the HIPAA 4010, 5010 along with the 6020 version  of\n   the 834 and 820Companion guides.\n . Created the logic for flat  files  and  837I,  837D,  837P  according  to\n   business requirements and documented in FRD.\n . Supported all healthcare workflows through HL7.\n . Performed manual  testing,  including  validation/smoke  testing  of HL7\n   interface messages on each new build before  delivering  to  the  quality\n   assurance team.\n . Perform the system testing to see whether FACETS is passing the data thru\n   Application Enablement to Provider Portal or Member Portal as per need or\n   not.\n . Validated that the 270/271 generated  is  in  accordance  with  the  5010\n   implementation guide.\n . Wrote T-SQL queries, Stored Procedures and used them to build packages.\n . Handled slowly changing dimensions to maintain the history of the data.\n . Followed HL7 standards to match the  International  Standard.  Documented\n   the Functional Specifications Document using Standards of HL7.\n . Involved in FACETS configuration, Customization, reporting, analysis  and\n   enhancement  and  also  worked  on  membership,   claim   module,   Batch\n   Processing, Pricing Module.\n . Have done analysis on HL7 that establishes criteria for representing  and\n   communicating data associated with health care.\n . Coordinated the efforts  to  implement  appropriate  HL7  changes  on  an\n   ongoing basis.\n . Analyzed the change  detection  process  on  FACETS  database  tables  to\n   capture  the  daily  changes  done  by  Users   through   Online   FACETS\n   Application.\n . Responsible for optimizing all indexes, SQL queries, stored procedures to\n   improve the quality of software.\n . Designed, reviewed, and created primary objects such  as  views,  indexes\n   based  on  logical  design  models,  user   requirements   and   physical\n   constraints.\n . Designed, Developed and Deployed reports in  MS  SQL  Server  environment\n   using SSRS-2008 R2.\n . Generated Sub-Reports, Drill down  reports,  Drill  through  reports  and\n   Parameterized reports using SSRS.\n . Created reports to retrieve data  using  Stored  Procedures  that  accept\n   parameters.\n . Used SQL Server profiler for auditing  and  analyzing  the  events  which\n   occurred during a particular time horizon and stored in script.\n . Planned a complete backup on the  database  and  restored  database  from\n   disaster recovery.\n\n\n",
          "confidence": 1.0,
          "method": "full_text",
          "structured_data": null
        },
        "tax_term": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "source_by": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "designation": {
          "value": "Data Analyst",
          "confidence": 0.9,
          "method": "ner",
          "structured_data": null
        },
        "experience": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        }
      }
    },
    {
      "resume_path": "data\\input\\Help Desk - VA - Andre Potts.doc",
      "confidence_score": 0.7500000000000001,
      "used_ocr": false,
      "extracted_fields": {
        "skills": {},
        "first_name": "Andre",
        "last_name": "L. Potts",
        "primary_email": {
          "value": "andrepotts1234@aol.com",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "secondary_email": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "phone": {
          "value": "5716068186",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "city": {
          "value": "Woodbridge",
          "confidence": 0.7,
          "method": "ner",
          "structured_data": null
        },
        "state": {
          "value": "VA",
          "confidence": 0.8,
          "method": "city_database",
          "structured_data": null
        },
        "zip": {
          "value": "16756",
          "confidence": 0.7,
          "method": "regex",
          "structured_data": null
        },
        "work_authority": {
          "value": "US Citizen",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "resume_link": {
          "value": "data\\input\\Help Desk - VA - Andre Potts.doc",
          "confidence": 1.0,
          "method": "file_path",
          "structured_data": null
        },
        "raw_resume": {
          "value": "\n\n                             Andre L. Potts, MCP\n                   16756 Flotilla Way Woodbridge, VA 22191\n                           Cellular:  571.606.8186\n                       Email:  andrepotts1234@aol.com\n\n                                 Clearance:\n                             DOD Top Secret - June 28, 2019\n                  DOD Top Secret \\ SCI - Granted July 2019\n\nFull legal name: Andre Potts\nCurrent Location (with zip code): 2O774\nWork Authorization: US Citizen\nCurrently working on Contract/Fulltime:\nCurrent project end date: Oct 2021\nExpected per Hr. pay Rate:\nBest time to reach:\nVaccination - Yes\n\n $30/hour on 1099 (30 Min commute)\n\nCareer Summary\n\nOver 20 years of experience as a Network, Desktop, Helpdesk and Migration\nLead.  Additional experience includes over 8 years as a System\nAdministrator with an extensive background in networking technology and\ncustomer relations.  Experienced in planning, designing, implementation and\nmanagement of Windows 7, 8 and 10. Windows Technology XP and 2003, NetWare\n3.x/4x networks.\n\nTechnical Skills\n\nConfiguration of Vista, Windows XP, 2000, NT, Windows 7, 8 and 10; MS\nOffice 2003 and MS Outlook; AT&T 7290 Blackberry; MS VPN; Remedy 5.0;\nWindows; Active Directory; HP Jet Direct; Symantec Ghost image 7.5; Net IQ;\nNetWare 3.x, 4.x, 5.0; Dell Server and Workstations 2003 & 2008; Compaq\nServer and workstations; RAS, PAL and VPN, VM Ware IPad\n\nCertifications\n\nITIL v3, MCP (Workstation and Server) A+, Security+ - 12-2020 (Dell,\nCertification on Dimension, Optaplex and Latitude Professional Workstation.\n\nEmployment History\n\nMetronome \\ State Department - Senior Computer Administrator      Jan. 2020\n-  Oct 2021\n\n    . Created user and admin accounts with the aid of Active Directory\n    . Assist teleworking clients with connection and general computer\n      problem.\n    . Assist client at the shop with classnet \\ jwic\n    . Support client with computer need with major application\n    . Created clients email accounts in exchange for all 3 networks\n    . Utilize ITIL best practices to enhance and optimize the services\n      provided to end users\n\n\n\n\n\n\nSSI \\ JSP DOD - Senior System Engineer Support          Oct. 2018 - Jan\n2020\n\n    . Lead a team of Deskside Support Technicians providing technical\n      support for system users.\n    . Assist in Creating work schedules, updating the front-end voicemail\n      when necessary, complete the daily attendance and shift handoff\n      reports, and provide ideas for improvement in daily operations of the\n      client Service Center.\n    . Assist in the development of performance measurements\n      (goals/metrics/reports) utilizing the current service desk platform\n      and facilitate a feedback system to team members on issues such as\n      customer service, communication, and technical skills, in order to\n      enhance the quality of support delivered.\n    . Utilize ITIL best practices to enhance and optimize the services\n      provided to end users\n    . Complete and provide regular Service Desk metrics reports\n    . Expert knowledge in the implementation of an enterprise knowledge base\n      and knowledge management best practices\n    . Coordinate with IT support teams to resolve customer\n      incidents/problems, fulfill service requests, and institute continuous\n      service improvement initiatives and methods.\n    . Resolve customer issues effectively or escalate them to appropriate\n      support tiers.\n    . Work with management in assessing staff performance/reviews/changes.\n    . Assist in the management of Service Desk resources for optimal\n      performance.\n    . Assist in the professional and technical development of the team\n    . Meet customer requirements for Service Desk performance.\n\n\n\n\nSAIC / State Department - IT Operations Watch Office Analyst        Sept.\n2015 - Oct. 2018\n\n    . Monitor network and server performance; assess potential problems\n         o Networks, Applications, Hosting, Voice, and security\n         o Work with Service Operations team staff during the installation,\n           upgrade, or decommissioning of infrastructure equipment or\n           software.\n    . Monitors and analyzes critical incident alerts regarding outages or\n      degradation in service affecting the enterprise using monitoring\n      tools.\n         o NeuralStar\n         o vCenter\n         o SCOM\n         o Other Enterprise Available Tools\n    . Assist with the development and maintenance of standard operation\n      monitoring procedure\n    . Provide in-depth analysis of nework performance alerts on the\n      messaging and management\n    . Networks and well as associated servers and applications\n    . Assist with Major Incident Severity Level calls (standing up\n      conference call, tracking down service line resources, creating a\n      timeline of events)\n    . Preparing, updating, and reporting Situational Awareness to the IT\n      Operations Center Watch Officer\n\nClearbridge /AT&T/State Department    Network and Port Validation\nMarch 2014 - Sept. 2015\n . Responsible for providing on-site assistance.\n . Participate in the assessment, inventory, and circuit tracing and desktop\n   support.\n . Facilitate office and network distribution closet equipment.\n . Physical installation of desktop experience, office systems and network\n   equipment including all telecom and network,  cabling cat 5 and 6.\n . Inventory tracking and management for initial survey and when equipment\n   is  added or removed.\n . Create and/or update as-built site documentation and inventories.\n . Configure and patch management of servers, switches, routers and other\n   network devices with remote assistance from engineers.\n\n\n\nAvenion / USMC    OCIO VIP Support Analyst April 2012 - October 2013\n\n1. Participate in system implementation and all team related activities.\n2. Interface with the federal client on base daily.\nResponsible for conducting a preliminary survey before each tech refresh.\n4. Effectively communicate any issues that could impede the success of the\n   refresh.\n5. Work with VIP's and contractor PM's before beginning the work.\n6. Responsive to all calls and other activities surrounding the Help Desk\n   (described in detail below).\n7. Facilitate a resolution to any problems.\n8. Track and process system file interfaces for external stakeholder\n   entities.\n9. Distribute strategic correspondences to internal and external\n   stakeholders to communicate system outages and other significant issues.\n10. Update and distribute weekly minutes and agenda for the Operations\n   Status meetings (occurs 3 times per week).\n11. Quality control and data entry for all new/modified user access\n   request.\n12. Monitor operations email inbox.\n13. Create, update and maintain Help Desk tickets, report status of open\n   Help Desk Tickets.\n14. Archive completed user access forms on a weekly basis.\n15. Interface with the team Database Administrators.\n\n16. Provide Tier II support per request from various constituencies.\n   Investigate and troubleshoot issues.\n\n17. Perform daily system monitoring, verifying the integrity and\n   availability of all hardware, server resources, systems and key\n   processes, reviewing system and application logs, and verifying\n   completion of scheduled jobs such as backups.\n\nRemote into system using Borgate, configure VPN, IPAD's and Blackberry\nServe as 2nd tier support on the Service Desk team which is responsible for\nthe triage and resolution of a wide variety of IT issues and requests.\n\nResponsible for monitoring enterprise performance, conducting audits and\nresolving complex problems related to network and desktop equipment and\nsoftware applications.\n\nTroubleshoot and utilize problem solving skills and previous experience\nmaintaining and supporting complex systems that are deployed nationally.\n\nWyle/Insight Global/Pentagon - Windows Refresh Analyst   September 2011 -\nFebruary 2012 (temp assignment)\n21. Performed basic system administrator duties.\n22. Created user accounts.\n23. Disabled accounts in AD (Active Directory).\n24. Placed user in the correct OU (Organizational Unit).\n25. Removed workstation in and out of the correct containers.\n26. Closed, updated and produced Remedy Daily Report.\n27. Added user on and off the domain.\n28. Managed exchange mailboxes, configured mailboxes on user workstations.\n29. Created user mailboxes for Outlook 2010.\n30. Reemerged Windows 7 Laptops using the Ghost 7.5 system from XP, Windows\n   2000.\n31. Back-up workstation using Windows Migration by retrieving data off the\n   machines and storing in a container on the server.\n32. Migrate from Windows XP to Windows 7, configured the user desktop.\n33. Troubleshot tickets using Remedy Ticketing System and new issues with\n   re-image Migrate 7 migration tool.\n\nATR/General Dynamics/FBI System Installer and Network Engineering (3\nmonths) tasks March 2011 - September 2011\n\nResponsible for conducting tech refresh and run preliminary survey report\nbefore each.\n35. Effectively communicate any issues that may harm the success of the\n   refresh.\n36. Work with contractor PM's before beginning the work.\n37. Made sure replacement components are in place before replacing CISCO\n   switches and routers.\n38. Installed all upgraded components and switch routers to ensure a smooth\n   transition with the taclane.\n39. Performed installs of Dell Desktop, Laptops and office Devices.\n40. Installed workstation in SIPRNET & NIPRNET environment.\n41. Troubleshot system for any malfunction.\n42. Install and update patches and upgrade of Share Point.\n43. Update and data entry in Share point.\n44. Administrator Share Point install, configure and monitor.\n45. Ran trouble tickets from help desk Remedy ticketing system.\n46. Back up user data for deployment.\n47. Supported trouble tickets from helpdesk Tier II and Tier III.\n\nFYI/General Dynamics/Coast Guard    System Engineer   June 2010 - March\n2011 Transfer\n\n48. Perform Active Directory function in a SIPRNET & NIPRNET environment.\n49. Support user on base of over 2000 user and active DOJ Employees.\n50. Help Desk environment Tier I and Tier II tech using Remedy ticket\n   system.\n51. Resolved user issues from a Tier II - Tier III aspect ticketing system.\n\n\n52. Testing application, Run Gold Disk and run report on all pre-installed\n   application.\n53. Administer Vista and XP desktop OS for users and test lab, Administer\n   Server 2003, 2007 and 2008 OS.\n54. Execute patches and updates on Windows enterprise environment via WSUS.\n55. Ran complete and full report of results of updates and patches every\n   month.\n56. Document work performed and disseminate to Administrator and Engineers.\n57. Administer a SIPR environment in a VMWare ESX network.\n58. Execution of application management utilizing WISE.\n59. Submit and execute all applications through SMS, package application in\n   Win-zip and deploy thought SMS.\n\nTech-System / Lockheed - Martin/Air Force System Engineer    June 2009 -\nJune 2010\n\n       Environmental, Vulnerability and Mitigation Engineer (EVM) Team\n60. Executed patches and update on Window environment enterprise.\n61. Monitored and created tickets thought Remedy ticketing system.\n62. Assisted with Information Assurance Vulnerability Assessment (IAVA)\n   efforts.\n63. Administer Vista, XP and Windows 7 desktop OS.\n64. Performed Active Directory function and remote connection through SMS\n   in a Siprnet & Niprnet environment.\n65. Planned TCNO implementation strategy, packaged and deployed TCNO\n   patches.\n66. Implemented TCNO instructions throughout the enterprise, documented\n   TCNO implementation IAW AFI 33-138 and advised VAS when implementation\n   has been completed.\n67. Tested and evaluated TCNO for effect on network.\n68. Assigned resources to perform patch application management.\n69. Provided updates during weekly status meeting as a key contributor.\n\nPerot Systems; /BTA/DFAS System Administrator II/Blackberry Enterprise\n   Administrator: January 2007 - June 2009\n70. Served as 2nd tier support on the Service Desk responsible for the\n   resolution of a wide variety of IT issues, requests and operating system\n   upgrades for the Business Transformation Agency (BTA), a DOD entity.\n71. Responsible for monitoring enterprise performance, conducting audits\n   and resolving complex problems related to network and desktop equipment\n   and software applications.\n72. Performed daily check of the backup system (Symantec) and\n   troubleshooting any failed scheduled backup.\n73. Served as a System Administrator in a test lab environment (DOD\n   SIPRNET& NIPRNET environment).\n74. Assisted with Information Assurance Vulnerability Assessment (IAVA)\n   efforts.\n75. Responded with technical assistance to requests from walk-ins, callers\n   and remote site users as required.\n76. Maintained data files and control procedures for a system of 20\n   networked servers with a Windows, UNIX, Linux and VMware mix.\n77. Monitored use of resources, back-up files and responded to management\n   requests for information.\n78. Simulated or re-created user problems to resolves use of problems\n   management database and helpdesk systems.\n79. Provided configuration support of laptops, blackberry's, cell phones\n   and VPN clients for on/remote site users as required.\n80. Responsible for the in-house network and communication environment;\n   controlled the system inventory of all equipment, software and licenses\n   to all on and off site employees.\n81. Provided server and system support on a Wireless network and configured\n   all equipment issued clients with Vista operation system as the standard\n   also configured VMware.\n82. Developed standard weekly reports as required regarding progress and\n   attended weekly meeting via phone.\n83. Installed and maintained DOD software applications while utilizing\n   appropriate IT procedures, standards, and guidelines for all tasks\n   performed.\n84. Responsible for 24 hour technical support via a DOD provided pager on a\n   rotational schedule.\n85. Managed the IT Service Center Call Tracking System, (REMEDY ver. 5.0)\n   by removing in user system with the use of SMS Client.\n86. Installed existing or new systems, including initial software loading\n   and configuration.\n87. Diagnosed issues related to the operating system, modems, printers,\n   network interfaces, software applications and other system level\n   problems.\n88. Coordinated with network technicians and self-maintenance groups to\n   install, trouble shoot and relocate computer related equipment and\n   peripherals.\n89. Oversaw maintenance of User Accounts as applicable.\n90. Supported local networking, telephone, and voicemail equipment to\n   include additions, moves and changes for remote BTA locations.\n91. Responsible for the Oversight and execution of LAN/Server migrations.\n92. Configured and loaded the software on a 6400 Cisco router\n93. Active Directory was used to create and managed user accounts.\n94. Managed mail accounts using Exchange 2003\n\nEducation\nColorado Technical University, CO (On-line)  February 2011 - Present\nUniversity of Phoenix, Alexandria, VA,  October 2004 - 2006\nLiming Computer School Stafford, VA.\n                          June 2000 - 2001\nTEST Technology College, Hyattsville MD,\n                  September 1986 - October 1988\n\n\n\nTraining\nBasic Program Management, CBT, Unisys University\nEffective Communication, CBT, Unisys University\nNovell 3.x, 4.x Operation and Networking Essentials, Crystal City, VA\n\nSecurity + Certification 12- 2020\n\nSkill\nPossess ability to troubleshoot and utilize problem solving skills, with\nproficient experience in maintaining and supporting complex systems that\nare deployed nationally.  Possess working knowledge of Helpdesk systems,\nand a working knowledge of accounting practices.  1-3 years' experience\nwith the latest MS Office suite 2010, 2007.  Excellent communication\nskills, including strong writing skills, and experience writing emails and\nupdating technical documentation with expert attention to detail.\n\nREFERENCES AVAILABLE UPON REQUEST\n\n",
          "confidence": 1.0,
          "method": "full_text",
          "structured_data": null
        },
        "tax_term": {
          "value": "1099",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "source_by": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "designation": {
          "value": "Senior System Engineer",
          "confidence": 0.9,
          "method": "ner",
          "structured_data": null
        },
        "experience": {
          "value": "20",
          "confidence": 0.9,
          "method": "regex_total_experience_summary",
          "structured_data": null
        }
      }
    },
    {
      "resume_path": "data\\input\\.Net  Architect - DE - Muthu.docx",
      "confidence_score": 0.43500000000000005,
      "used_ocr": false,
      "extracted_fields": {
        "skills": {
          "domain_specific": [
            {
              "name": "security",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                21,
                990,
                2426,
                4873,
                6019,
                7442,
                16892,
                17931,
                17953,
                18018,
                18377,
                18579,
                18949,
                19271,
                19554,
                20324
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "insurance",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                67
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "manufacturing",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                86
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "government",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                166,
                11669
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "architecture",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                1789,
                1819,
                2179,
                2546,
                3047,
                6990,
                7035,
                9848,
                10733,
                11701,
                15759,
                19469
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "infrastructure",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                2485,
                7501,
                19445
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "design",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                2808,
                3040,
                3671,
                7559,
                7999,
                11723,
                12452,
                14191,
                15942,
                17295,
                19880
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "sales",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                4962
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "finance",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                4988
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "crm",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                5048,
                6010,
                7967,
                8088,
                9542
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "mining",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                13093
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "programming": [
            {
              "name": "java",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                496,
                1655,
                3216,
                5423,
                5877,
                6259,
                9455
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "python",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                579,
                1660,
                3070,
                5455,
                12941,
                20719
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "sql",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                678,
                4648,
                5584,
                6429,
                8252,
                9193,
                12017,
                12326,
                12781,
                13074,
                13802,
                14545,
                14791,
                17682,
                17868,
                19891,
                19898,
                20282,
                20587,
                20594,
                20663
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "xml",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                694,
                2767,
                5632,
                6445,
                7256,
                9241,
                11269,
                11894,
                12483,
                12657,
                12980,
                13002,
                13006,
                13679,
                14210,
                14376,
                14724,
                14737,
                14741,
                15751,
                15961,
                16103,
                16444,
                16453,
                16457,
                17060,
                17403,
                17407,
                17701,
                17740,
                17744,
                20224
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "json",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                698,
                2771,
                5615,
                6449,
                7260,
                9224,
                12988
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "javascript",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                712,
                5604,
                5884,
                6463,
                9213,
                9462,
                9878,
                13039
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "gradle",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                3960
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "css",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                5600,
                9209,
                12993
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "puppet",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                5680,
                8604,
                9392
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "chef",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                5687,
                9387
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "ruby",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                8422
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "html",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                19535
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            }
          ],
          "frameworks": [
            {
              "name": "spring",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                506,
                6269
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "rest",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                703,
                2565,
                6454,
                7054
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "serverless",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                1541
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "nifi",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                1683,
                5919
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "soap",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                2574,
                7063
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "jquery",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                5620,
                9229,
                9867
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "cloud": [
            {
              "name": "aws",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                1414,
                1533,
                2275,
                2397,
                4187,
                5727,
                6316,
                7273,
                7413,
                9297,
                11338
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "azure",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                1425,
                1527,
                2279,
                11297
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "vpc",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                2913,
                5735,
                7664,
                9305
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "jenkins",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                3952,
                5746,
                8592,
                9379
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "ec2",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                4599,
                5731,
                7277,
                8792,
                9301
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "docker",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                5565
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "data_skills": [
            {
              "name": "hadoop",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                1576,
                5901,
                6289
              ],
              "experience_weight": 0.0,
              "importance_score": 0.9
            },
            {
              "name": "hive",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                1678,
                5914
              ],
              "experience_weight": 0.0,
              "importance_score": 0.9
            },
            {
              "name": "spark",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                1688,
                5929
              ],
              "experience_weight": 0.0,
              "importance_score": 0.9
            },
            {
              "name": "kafka",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                5895
              ],
              "experience_weight": 0.0,
              "importance_score": 0.9
            },
            {
              "name": "etl",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                9473,
                9546,
                14728
              ],
              "experience_weight": 0.0,
              "importance_score": 0.9
            }
          ],
          "databases": [
            {
              "name": "hbase",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                1672,
                5908
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "solr",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                1694,
                5924
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "elasticsearch",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                9009,
                9477
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "oracle",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                11249,
                13059,
                14776,
                15300,
                16272,
                16508,
                17586,
                17791,
                19722,
                20496,
                20534,
                20678
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "sybase",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                18837,
                19715,
                20209
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "soft_skills": [
            {
              "name": "analysis",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                1769,
                1855
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "devops": [
            {
              "name": "github",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                3979
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "svn",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                4118
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "business_skills": [
            {
              "name": "reporting",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                4850
              ],
              "experience_weight": 0.0,
              "importance_score": 0.8
            }
          ],
          "methodologies": [
            {
              "name": "devops",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                4977
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "cmmi",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                11016
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "xp",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                12911,
                14662,
                16382
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "safe",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                13127,
                14821,
                16537
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "prototype",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                19937
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "networking_equipment": [
            {
              "name": "authentication",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                19203
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "authorization",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                20731
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ]
        },
        "first_name": "",
        "last_name": "",
        "primary_email": {
          "value": "muthukumar@consultant.com",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "secondary_email": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "phone": {
          "value": "4433161103",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "city": {
          "value": "",
          "confidence": 0.0,
          "method": "ner",
          "structured_data": null
        },
        "state": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "zip": {
          "value": "",
          "confidence": 0.0,
          "method": "regex",
          "structured_data": null
        },
        "work_authority": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "resume_link": {
          "value": "data\\input\\.Net  Architect - DE - Muthu.docx",
          "confidence": 1.0,
          "method": "file_path",
          "structured_data": null
        },
        "raw_resume": {
          "value": "Introduction\nKumar is an Enterprise and Solution Architect / Senior System Architect / Certified Solution consultant (SME) with, around 20 years of architectural, systems analysis and development experience in Enterprise & Solutions Architectures, Technical Evaluations and Solutions using technologies such as products and enterprise wide solutions using Java, J2EE, Python, API Gateways, Visual C# /.NET / in Unix, Windows, IBM, Mobile iOS, Android, Windows CE, Linux, Hadoop & Cloud such as AWS and Azure Cloud environments. \n\nHas extensive experiences in AS-IS analysis, Transition Architectures and Future State architectures using TOGAF and FEAF\nSuccessfully worked on CBA, TCOs, Budgets, TBM, Asset Managements and Cost modellings\nInvolved in Technical Proposals, Pricing, Acquisitions, RFIs and RFP activities\nWorked extensively on Technical evaluations, Solutions proposals, Implementations planning and design artifacts\nWorked on CCBs, Risk Assessments, Controls, Oversight on SDLC, Agile and SAFe 4.0 & 4.5\nWorked on Data-center operations including Cloud and Government Datacenters and focused on networking operations, monitoring systems and applications\nHave good working knowledge on BI tools within the Datacenter using analytics, logger functions and enterprise services management\n Applied Architectural principles and produced successful solutions with, design pattern modeling, analysis, & programming SME for real time in enterprise and public facing production systems on varied local and international projects.  \nWell-versed in programming API\u2019s and module implementation to support mobile, wireless programming and communications with Prototyping\nHas extensive work in .NET, Java, MS C#, VC++, NET Components, jQuery, Test Driven Development, Pre-Sales, JSON, HTML5, MVC, MVVM, COM/COM+, ADO (.NET), BizTalk, OLEDB, MQ Series, DCOM, MFC, ATL, MTS, IIS, UML, Rabbit MQ, Spring Boot, Puppet, Chef, ADO/DAO, SOA, OLE Automation, MS Agent (Speech API), Virtual machines, Docker, Synchronize / A-synchronize Programming, XML, PL/SQL, ETL, Micro Services, Data Modeling, Ruby, Meta data, Object Oriented Analysis and Object Oriented Design using Visio / Rational Rose Products.\nWith better communication and leadership skills, could handle any project in N-tier, multi-tier, SOA, Domain and Window DNA or Complex architectures. \n\nEducational Qualifications and Certifications\nMaster\u2019s in Business Administration, UMUC, MD\nMasters in Computers, Bharathidasan University, India\nHonors Diploma in Systems Management, NCC, UK\nCertified Application Developer, MCAD, Microsoft, 2003\nProject Management Professional (PMP), PMI, 2008\nInformation Technology Infrastructure Library v3 Foundation (ITIL), 2011\nAWS Certified Solutions Architect - Associate (2013)\nAWS Certified Solutions Architect \u2013 Professional (2018)\nAWS Business Professional Accreditation (2013)\nAWS Technical Professional Accreditation (2013)\nTOGAF Certified Enterprise Architect (2016)\nAzure Associate MCP (2017)\n\nDevelopment Toolset:\nMicrosoft Visual C# 2013, 2012, 2010, 2008, 2005, Microsoft Visual C++ 2008, 6.0, 5.0, 4.0,1.52, Visual Basic 6.0, 7.0, Android 1.x, 2.x, 3.x, 4.x, iPhone, iPad, iPod Touch, Azure, Amazon Web Services (AWS), Active Server Pages (ASP 2, 3), BizTalk 2004, 2006, MTS, Business Objects 5.x, MS Share Point, MFC/ATL, COM / COM+/Serviced Components, Managed Services and Components, Threads, Sockets, Events, OLE Automations, OCX, ActiveX, Active Reports, Crystal Reports, Microsoft Agent, Managed Controls, SDK, J-Script/JSP, AJAX, Server-lets, J2EE, J2Me, MOSS, WSS, SharePoint, VBScript, HTML, XML, XSD, XSLT, XPath, WebSphere, UML, C, C++, Java, PKI, Digital Certs, RSA, SHA128, html5, jQuery, JavaScript, JSON, Arc , ESRI Maps\n\nDatabases:\nOracle (7.3, 8i, 9.0, 10g, 11g), Sybase (11.x, 12), SQL Server 6.5, 7.0, 2000, 2008, 2012, Informix 9.0, ActiveX Data Objects (ADO 2.5, 2.6), ADO.net, SQL Plus and MS Access with ODBC.\n\nDevelopment Platforms:\nMicrosoft Windows XP Professional, Windows Vista, Windows 7, 8.1, 10, 2008, 10 Servers, 2000 Professional, NT 4.0/98/95, Windows CE, iOS 5, Windows Tablet PC,  Unix [HP-UX (10.x, 11), Solaris (8x), AIX (4.3.x) SVR4, Linux (2.4.x), SCO (3.2.x)], VMS, Windows 3.11 Workgroup, DOS and OS/2, Internet Information Server (IIS), Apache/Tomcat, WebSphere, Netscape Enterprise, IBM MQ Series\n\nDesign Tools & Methodologies:\nVisual Studio UML, SAFe 3.x, 4.5, TOGAF, Zachman, Visio, Rational Rose, Rational Clear Case, OOA, OOD, OOP, Rational ClearQuest, RequisitePro, Visual Modeler, Visio, UML, Booch, WinRunner, LoadRunner, Sheridan, Infragistics Controls, Bounds Checker, Clear case, Install Shield, PVCS, VSS, RCS, SCCS, Tivoli, Python, Perl, Verisign, WLBS, UDP, LDAP, TCP/IP, 802.11b, CDPD, Toad, ISO, CMMI process methodology, NLB, IIB, Layer 7,ISO, MVC (model-view-controller), Agile (XP, Kanban, SCRUM), TDD, Continuous Integration, Continuous Delivery \n\nIndustry Experience:\n-Financial Management\t\t\t- Security Management\t   -Payroll\n-Claims Management/Insurance Industry\t- Manufacturing\t  \t   -Hotel Management\n-Hospital Management\t\t\t- Communication/Mobile     -Federal Government \n\nPROFESSIONAL EXPERIENCE\n\nGSA, Treasury, USDA, HUD Coe\u2019s \tJul 14 \u2013 Till Now\nEnterprise Solutions Architect, SME Consultant\n\nAs a Chief Enterprise Architect, responsible for enterprise portfolio of applications.  I have worked on multiple systems and integrated many applications with technical systems such as Enterprise, Mobile, PEGA, JAVA, J2EE, Spring Framework & Technologies including SharePoint 2010-2013, API Gateway, Python, Enterprise Service Bus, .net 4.0, 4.5, 4.8, .net core 1.8 and 2.0 Visual Studio 2010, 2015, 2019 MS-SQL Server 2008, XML, JSON, REST API, JavaScript and Objective C, Business Objects. I worked independently and as a part of the team that builds and integrates interactive Web sites, applications, and services for both internal and public sites.  Worked on the latest version of the mobile and deployed using Samsung Knox security suite for Androids.  My technical domain includes the ability to architect websites, designing data-driven applications, and developing efficient client-server solutions & systems administrations in a cloud and hybrid environment.\n\nCloud Centers of Excellence \u2013 Cloud CoE \u2013 Senior Solutions Architect\nWorked as Senior Solutions Architect for the GSA Centers of Excellence\nWorked in all aspects of Phase I of Cloud Adoptions in AWS and MS Azure Platforms\nCreated and implemented TBM as part of GSA CoE.\nWritten multiple proofs of concepts in Azure, AWS and Serverless Architectures\nWorked on Hadoop Stack, EMR, HDInsight big data architectures\nWorked on HDP platform using Java, Python, HDFS, HBase, Hive, Nifi, Spark, Solr, Impala\nEnterprise Architect \u2013 EA Role\nPerformed extensive work on As-Is analysis, Transition architecture and Future-State architecture\nWorked on Cost-Benefit analysis (CBA), TCO, budgeting and cost modelling\nWorked on Technical recommendations, evaluations and proposals\nWorked on Risk management, Control implementations and Change control boards\nArchitected and Recommended solutions and implementation plans for mobile and desktop cross device cross platform browser-based application architecture using iPhone/iPad/iPod touch iOS devices and android devices and .NET windows nodes.\nAWS, Azure, SOA, PEGA & Admin Role:\nWorked on the team that helped RATB in moving enterprise systems to Amazon cloud technology (AWS) solutions to improve its security and agility, consolidate systems, and streamline IT infrastructure and overhead costs. Worked on Service Oriented Architecture using REST and SOAP methods.  Worked on end-points using Mule ESB and worked on various Enterprise Integration Patterns.  Used various conversions methods using J2EE and .NET to various conversion formats such as XML, JSON, CSV etc., Worked on agency\u2019s cloud design includes leveraging Microsoft Office 365 for messaging services, and Amazon\u2019s virtual private cloud (VPC) for all other enterprise services. \nWork on the mobile product and architect them for the app store\nWorked on CA API Gateway Design/Architecture\nWorked on Python, J2EE/.NET application Architected, designed and developed solution with Micro services\nWork on mobile, web trends and cloud integration using Java/.NET Suites\nHands-on on PEGA PRPC, Data Pages, UI/UX with Services and Agents\nWorked on multiple Share Point Projects, services to deploy web application and Migration\nWork on the mobile product and deploy them on the app store\nExperience in ESB and message integration techniques, like aggregation, transformation, dynamic routing, content-based routing, re-sequencer, etc., using Mule ESB\nExperience in scalable Mule flows, error handling etc.\nExperience in the design and implementation of Mule transactions, Async flows, message filters, message translators, content enricher and integration patterns\nExperience in the development of testing suites using Mule MUnit and JMeter to test regular mule flow\nExperience with CI/CD tools such as Maven, Jenkins, Gradle, Artifactory, Github, etc.\nWorked on Share Point Projects, services to deploy web application and Migration\nExperience with version control systems such as SVN, CVS, Harvest, and IBM ClearCase\nExpertise working with API Gateway (AWS, Ax-way and CA APIs), Runtime Manager, etc.\nWork on mobile and cloud integration\nInstall and maintain Microsoft Windows 2008 R2, IIS, SP2010, Project Server 2010, Site Collections and COTS Software regular upgrades and applying patches\nInstall and maintain DNS Services (Primary and Secondary DC) and manage Windows 2008 R2 Server admin accounts\nMaintaining and managing firewall rules on servers running on the Amazon EC2\nSystems and applications monitoring IIS Logs, SQL Logs, alerts notification, issues identification and corrective actions \nSalesforce.com\nDeveloped custom code, user interface, business applications, shared rules on Salesforce.com platform to facilitate reporting and enhanced security\nDesigned and implemented dashboards, dynamic dashboards and tailored reports for sales, governor, DevOps, and finance departments\nLeveraged clients existing IT framework/CRM system for better data visibility and system integration.\nWorked on controllers, messaging, service facets of the Apex\nAssisted in developing Project Scope, Business Requirements and Functional Requirements with internal team and clients.\nFacilitated the creation of test scenarios and test scripts and worked with data maintenance\nEnvironment: \nMicrosoft .NET Framework 3.5, 4.0, Java (Eclipse) J2EE, PEGA 7.x, J2Me, Python, TOGAF, Microsoft Visual Studio 2010-2019, SOA, Microsoft SharePoint 2010, Windows Web development with C#, C++, Docker, Objective C, SQL Server 2008, CSS, JavaScript, JSON, jQuery, dojo, XML, XHTML, XPATH, Adobe Flash, , ASP.net 4.0, ADO.net, Puppet, Chef, Rabbit MQ, Knox, Amazon Web Services (AWS), EC2, VPC, VPN, F5, Jenkins, Salesforce.com, ArcGIS, ESRI GIS Integration, OS X (Lion, Mountain Lion), iPad, iPod Touch and iPhone, iOS 4.x, 5.0, 5.1, Windows 2008, Java, C#, JavaScript, Kafka, Hadoop, HBase, Hive, Nifi, Solr, Spark, HDInsight, Apache Tomcat, Android, iPad, iPod Touch and iPhone iOS 4.x, 5.x, 6.x, CRM, Knox Security, Knockout, MVVM, WCF, Objective C\n\nRecovery.gov, Treasury.gov, USASpending.gov, \tJun 12 \u2013 Jul 14\nSolutions Architect, Integrator, Consultant\n\nAs a technical Solutions consultant, have worked on the systems and integration in Mule Soft ESB, Mobile, JAVA, J2EE, Spring Framework, HA Hadoop, Amazon Web Services (AWS) and Open Stack cloud & Microsoft Technologies including SharePoint 2010-2013, .net 3.5, 4.0, Visual Studio 2010, MS-SQL Server 2008, XML, JSON, REST API, JavaScript and Objective C, Business Objects.  I worked independently and as a part of the team that builds and integrates interactive Web sites, applications, and services for both internal and public sites.  Worked on the latest version of Recovery.gov app in Objective \u201cC\u201d and deployed to the Apple\u2019s app store.  My technical domain includes the ability to architect websites, designing data-driven applications, and developing efficient client-server solutions & systems administrations in a cloud and hybrid environment.\n\nMule Soft and SOA Architecture Role:\nWorked on Service Oriented Architecture using REST and SOAP methods.  Worked on end-points using Mule ESB and worked on various Enterprise Integration Patterns.  Used various conversions methods using J2EE and .NET to various conversion formats such as XML, JSON, CSV etc.,\n\nAWS EC2, SharePoint/CMS & Architectural Admin Role:\nWorked on the team that helped RATB in moving enterprise systems to Amazon cloud technology (AWS) solutions to improve its security and agility, consolidate systems, and streamline IT infrastructure and overhead costs. Worked on agency\u2019s cloud design includes leveraging Microsoft Office 365 for messaging services, and Amazon\u2019s virtual private cloud (VPC) for all other enterprise services. \n\nWorked on Technical Evaluation, Recommendations and Solution Implementation Plans\nWorked on Cost Benefit and TCO calculations\nWork on the mobile product and deploy them on the app store\nWorked on .NET application Architected, designed and developed solution \nWorked on CRM/SharePoint 2010 application design pattern and architectures \nWork on mobile and cloud integration\nWorked on multiple CRM/Share Point Projects, services to deploy web application and Migration\nDesigning and Documenting Servers, Systems, Applications configurations using Visio Modeler\nSQL Server database backup and restore on EBS Volumes\nVirtual Servers backup and restore, NLB, Layer 7\nManaging on-site and off-site backup storage\nWorked on Website using Ruby on Rails\nInstall and maintain Microsoft Windows 2008 R2, IIS, SP2010, Project Server 2010, Site Collections and COTS Software regular upgrades and applying patches with Jenkins and Puppet\nInstall and maintain DNS Services (Primary and Secondary DC) and manage Windows 2008 R2 Server admin accounts\nMaintaining and managing firewall rules on servers running on the Amazon EC2\nElastic-Search\nArchitected solution for Elastic Search solution for USA spending\nImplemented solution on Elastic search using multiple nodes\nCompared for the accuracy of results between previous search engine and Elasticsearch\nEnvironment: \nMicrosoft .NET Framework 3.5, 4.0, ASP.net 4.0,  TOGAF, Microsoft Visual Studio 2010, SOA, Microsoft SharePoint 2010 (CMS), Windows Web development with C#, C++, Objective C, SQL Server 2008, CSS, JavaScript, JSON, jQuery, dojo, XML, XHTML, XPATH, Adobe Flash, ADO.net, Amazon Web Services (AWS), EC2, VPC, VPN, ArcGIS, ESRI GIS Integration, OS X (Lion, Mountain Lion), Apache Tomcat, Jenkins, Chef, Puppet, iPad, iPod Touch and iPhone, iOS 4.x, 5.0, 5.1, Windows 2008, Java, C#, JavaScript, ETL, Elasticsearch, Android, iPad, iPod Touch and iPhone iOS 4.x, 5.x, 6.x, CRM, ETL, MVVM, WCF, Objective C\n\nUSDA - FS, VA, Mar 08 \u2013 Apr 12\nMobile Technology Consultant / Mobility Architect/ Lead\nDescription:\nArchitected and developed mobile cross device cross platform browser-based application for iPhone/iPad/iPod touch iOS devices and android devices with Visual studio model view controller architecture, HTML5, jQuery and JavaScript.\n\nThe Integrated Mobile Applications (IMA) suite provides applications that allow the user to enter data while in the field, and upload the data to NRM when they return to the office and connect to the network. Some front-desk applications also require the ability to work in disconnected mode, in order to continue serving public customers and users in cases where field office networks are very slow or unstable. IMA applications run on a variety of mobile devices, including notebooks and tablet computers running Windows, and handheld devices running Pocket PC, Android and Windows Mobile.  Mobile Workflow software in an integrated shared platform connected via windows shared services.\n\nThe mobile devices utilize a common Mobile Data Builder application for downloading data from I-Web and for uploading data back into the database. Used Service orient Architecture (SOA) in check-out and check-in process is defined for each mobile application in the Integration Mobile Framework, a database schema containing metadata describing the data required by each application, and a series of procedures for data validation and loading.  Project is CMMI Level 2 certified.\n\nEnvironment: \nMicrosoft .NET Framework 2.0, 3.5, 4.0, Microsoft Visual Studio 2008, 2010, J2ME, J2EE, SOA, MOSS Enterprise 2007, WSS 3.0, Windows Mobile 5.6/6.0, Windows Forms development with C#, C++, Windows Mobile development, Oracle 9.x, 10.x, 11g, xml, ADO.net, ODP.net, Windows Azure (Cloud Services), Amazon Web Services (AWS), ArcMaps, ESRI GIS mobile Integration, IBM Work light, IBM MQ Series, Salesforce Lightning Connect, Apex, Sencha Touch, Windows CE, iPad, iPod Touch and iPhone, iOS 4.x, 5.0, Android 1.x, 2.x, 3.x, 4.x.\n\nIRS, DC & MD, July 05 \u2013 Feb 08\nCI & CDE, Business / Technology Consultant\nDescription:\nCDE and CI both are. Net-based application implemented for Federal government.  I was involved in CI Architecture, from the design & developmental stage to implementation into production stage.  In CDE project, I was involved from the beginning of the project till the implementation into production. XML is the preferred language to construct the messages and XSD is used to validate the structure.  This interfaces with the SQL server engine that triggers the process to write the messages into the local and remote queues.  The service at the other end of the channel constantly listens for the messages and dispatches to the respective end user application\n\nRole: \nAs Senior Consultant and SDM, Designed and Developed C# Business objects, SQL Stored Procedures and MQ for middle tier and interacted with Site minder  components using predominantly C#\nResponsible for Design, Development and Testing XML / XSD validation Request / response structure of the Business and client message communications. \nWorked on BizTalk middle tier\nDeveloped C# components using XmlDb, Data set and XML Web Service\nUI for Windows \nMQ Messaging interfaces to interact with various queues and host systems\nCommunications With SQL Server .net and Access using messages\n\nEnvironment: \nVisual C# 7.0, BizTalk 2004 & 2006, ADO (.NET), ASP. Net, COM+, Windows 2000, 2003, Windows XP Prof., Windows NT, Windows CE, Python, Windows Tablet PC, MS Access, UML, XML, IIS, JSON, CSS, HTTP, XML/XML Schema DOM, IBM MQ-Series, UML, JavaScript, VBScript, Oracle 8.x, 9.x, SQL Server 2000, DB Mining and Engineer, Visual Source safe, Version Manager, Rational Rose\n\n\nUnited Parcel Service Company, MD Feb 05 \u2013 Jun 05\nPackage Operations, Consultant\nDescription:\nThis project is core MQ programming using C#, C++ on Windows platform. Programmatic approach and using MQ Calls have been used to create local queues, remote queues, Xmit queues and channels.  Encoding and decoding the data packets between communications is the heart of this core engine.  Unicode and multi-bytes encoding support is provided.\n\nAt one layer above, we should able to put and get messages using the regular programming methods.  XML is the preferred language to construct the messages and XSD is used to validate the structure.  This interfaces with the SQL server engine that triggers the process to write the messages into the local and remote queues.  The service at the other end of the channel constantly listens for the messages and dispatches to the respective end user application.\n\nRole: \nAs a Senior Consultant, Designed and Developed Business objects and MQ Objects for middle tier and Web sphere components using C# and VC++\nResponsible for Design and Testing XML / XSD validation Request / response structure of the WebSphere MQ Server and Message client communications. \nDeveloped C#, MC++, VC++ components using Data set, Record set and XML Web Service\nUI for Windows CE using windows CE and CE Emulation 4.2 .net\nMQ Messaging interfaces to interact with various queues and host systems\nCommunications With SQL Server .net and Access using messages\n\nEnvironment: \nVC++ 7.0, Visual C# 7.0, MC++, ADO (.NET), ASP. Net, COM+,  Windows 2000, Windows XP Prof., Windows NT, Windows CE, Windows Tablet PC, MS Access, UML, XML, ETL, HTTP, XML/XML Schema DOM, IBM MQ-Series, Sibel,  Oracle 8.x, 9.x, SQL Server 2000, Visual Source safe, Version Manager, Rational Rose,  MS SharePoint\n\nBaltimore Gas & Electric, MD\tFeb 03 \u2013 Feb 05\nField Resource Management, Consultant\nDescription:\nWireless work order dispatch system between Wireless Mobile PC\u2019s and Dispatcher Desktops.  Works with PCAD and CDPD systems.  The work orders are dispatched to the work engineers from the dispatcher workstations using MDS Dispatcher/Mobile Systems.\n\nMiddle layer is managed components written in Visual C# and COM Components with using Visual Basic.  Oracle database is used as the backend database.  Wireless mobile notebooks use MS Access as their local database and this is updated by wireless radio.  COM and serviced components have been used.  UI for windows CE has been created using CE .net emulation software.\n\nFor messaging, MQ Messaging Server used between communication ends.  Worked on Queue poll and pull.  Local message server encapsulates the MQ Messaging server and communicated via Message clients.  XML DOM architecture is heavily used for sending messages.\n\nRole: \nAs a Senior Consultant, Designed and Developed Business objects in Middle tier COM+ components using VB .NET and C#\nResponsible for design and Testing XML Request / response structure of the MQ Server and Message client communications. \nDeveloped C#, VB.NET components using Data set, Record set and XML Web Service\nUI for Windows CE using windows CE and CE Emulation 4.2 .net\nMQ Messaging interfaces to interact with various queues and host systems\nCommunications With Oracle and Access using messages\n\nEnvironment: \nVB.NET 6.0, Visual C # 7.0, ADO (.NET), ASP. Net, COM+,  Windows 2000, Windows XP Prof., Windows NT, Windows CE, Windows Tablet PC, MS Access, UML, XML, HTTP, XML/XML Schema DOM, IBM MQ-Series, RS6000 AIX 4.3.3 with Oracle 8.x, 9.x, Visual Source safe, Rational Rose, Pragma COM, Business Objects 5.x, MS SharePoint\n\nAMS - Asset Management Company, PA\tJan 02 \u2013 Jan 03\nGlobal plus, Global plus Messaging\nDescription:\nAsset Management using Global plus software that makes buy and sell of securities, multi currencies, multi-lingual software to manage the asset owned by individuals, corporations and banks.  Manages the security brokers, fixed assets, liquid assets with user friendly GUI.  Makes trades up to trillion dollars.  Execution Tickler and Transaction Tickler handling is done using XML on outbound message on the MQ-Series using MHH Monitor.\n\nRole: \nAs a Senior Developer, Designed and Developed the Middle tier VC++/C components. \nDeveloped and debugged modules of Inbound/Outbound messaging in IBM MQ-Series\nResponsible for Design and Testing of Global plus in AIX and G-plus Messaging using AIX MQ-Series\nDeveloped interface using XML/XML Schema for Outbound Messaging\nUI for Windows CE and Emulations\nDeveloped application in VC++/VB for unit/integration testing of the services.\nPortability testing performed using Oracle 8.x\n\nEnvironment: \nVisual C# 7.0, VC++ 7.0, ADO(.Net), ASP. Net, COM+, Windows 2000, Windows CE, VMS, Unix, SQL Server 7.0, UML, XML, HTTP, .net WEB Service C++/C, GNU C++, GDB, XML/XML Schema, IBM MQ-Series, RS6000 AIX 4.3.3 with Oracle 8.x, e-ChangeMan, Customer First, X-Windows CDE, Global Plus, ClearCase, PL/SQL\n\nLegg Mason, Financial Brokerage Company, MD \t\tFeb 00 to Dec 01\nSecurity Lite / PerfMon /Security Related Products, Consultant, Project Manager\n\nDescription:\nSecurity related product, written in VC++/C++ that authenticates and validates its user whoever login to the application after domain login or direct application login.  Perform variety of tasks like has access to various applications, has access to Branch, Division, Region, Firm and Super firm.  The different modules are LMADO, LMCrypt, LMDBLite, PerfMon, Sec2000, and Data Security Form.\nLMADO / LM Crypt / LMDB Lite: \nData service layer for the above components, written using ADO connectionless objects. Encryption components written in MTS, using RC4, 128 bit.  Uses default Microsoft security context for encryption/decryption. Work with stream/block algorithms conversion switch.  Initially implemented with 1/1 cipher. Digital certificates using Veri-sign and thawte another MTS component used to retrieve asset management set of data from the Sybase database.  Has variety of calls with respect to Asset management.  All calls are sync.\nPerf-Mon / Sec 2000 / Data Security Form\nSec2000 is the visual basic front end/VC middle tier application that updates the database thru forms and grid controls.  Retrieves/ Stores values from / to the database.   Has the User interface for values to be stored in the repository. Does auto authentication based on the NT user login and validates against the security-lite software component.\n\nRole: \nAs a Lead/Senior Developer, Designed and Developed the Middle tier COM components for SecurityLite/PerfMon/Sec2000/Cryptography using PK Infrastructure. \nDesigned architecture document using Visio UML\nDeveloped the front-end ASP/HTML file for Data Security Form using ATL components and user interaction and optimized the implementation.\nImplemented the Stored procedure to run MTS Components interacting with Sybase / Oracle multiple data connections.\nResearched and implemented PKI RSA128 Bit encryption using RC128 /SHA128 and certificates using Verisign\nInvolved in Database design and SQL, PL-SQL/Stored Procedure scripts.\nDeveloped prototype for MSMQ message queue\nDeveloped Perfmon integration of components using system Perf-Mon utility\nDeveloped Test applications in VB for Unit Testing and System testing and Win Runner\n\nEnvironment: \nVC++ 6.0/C++, ATL/MFC/SDK, COM/COM+, ADO, MTS, IIS, VB 6.0, ASP, MSMQ, Windows NT, Sybase 11.x/12, XML, HTTP, HP 10.2x, AIX, Win-Runner, UML, RSA, Certificates, VSS, SQL Server 7.0\n\nInternet Dynamics, Internet Security Company, CA\t Nov 99 - Feb 00\nISAPI/NSAPI Filter, Consultant\n\nAs a Senior Team Member, developed/Debug ISAPI filter using IIS/Apache/Netscape web servers interacting with Oracle Virtual DB\nInteracted with the Oracle Database using Stored Procedures, Triggers and SQL-PL/SQL Scripts.\n\nEnvironment:  VC++/MFC, IIS, Netscape Enterprise Server/Apache, SQL Server 7.0, Oracle 7.3 & Windows NT, Windows 2000, Linux, Python.\n\n\nWORK AUTHORIZATION\nUS Citizen\n\nCLEARANCE\nMBI\nActive Public Trust\n\nCONTACT\nEmail-ID  muthukumar@consultant.com  & mk3787037@gmail.com  \nHerndon, VA\nPrimary Phone: 443-316-1103\n",
          "confidence": 1.0,
          "method": "full_text",
          "structured_data": null
        },
        "tax_term": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "source_by": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "designation": {
          "value": "Senior System Architect",
          "confidence": 0.9,
          "method": "ner",
          "structured_data": null
        },
        "experience": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        }
      }
    },
    {
      "resume_path": "data\\input\\.Net - MD - Raj.docx",
      "confidence_score": 0.48500000000000004,
      "used_ocr": false,
      "extracted_fields": {
        "skills": {},
        "first_name": "Kallidumbil",
        "last_name": "Raj Rajnarayanan",
        "primary_email": {
          "value": "k.rajnarayanan@gmail.com",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "secondary_email": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "phone": {
          "value": "3016933175",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "city": {
          "value": "",
          "confidence": 0.0,
          "method": "ner",
          "structured_data": null
        },
        "state": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "zip": {
          "value": "",
          "confidence": 0.0,
          "method": "regex",
          "structured_data": null
        },
        "work_authority": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "resume_link": {
          "value": "data\\input\\.Net - MD - Raj.docx",
          "confidence": 1.0,
          "method": "file_path",
          "structured_data": null
        },
        "raw_resume": {
          "value": "Kallidumbil \u201cRaj\u201d Rajnarayanan\nPhone: 301 693 3175\nEmail: k.rajnarayanan@gmail.com\n\nSUMMARY\t\n\n13+ years of IT experience in Web and Client Server Development and Management.\nStrong at Object Oriented \u201cAnalysis and Design\u201d and Programming skills along with ability to learn new emerging technologies and adapt quickly \nExpertise in JavaScript development using Node, jQuery, Angular, React and Backbone\nWebsite development using HTML5, CSS3, AJAX, JSON, LESS and SASS\nExperience in XML, XPATH, XQUERY, XSL, WSDL, SOAP, REST and XSLT\nExperience in web development using Drupal\nExtensive experience in Shell Scripting, Cron job scheduling in both Linux and Unix\nExperience in database development using DB2, Oracle, SQL Server, MySQL and MS Access\nExperience in NOSQL databases MongoDB, Dynamodb, Solr and ElasticSearch\nExperience in CDN configuration and management\nExperience in implementing enterprise search using Solr, ElasticSearch and RDBMS Full-Text\nExpertise in .NET forms, MVC, API and Core\nExperience in Core and JEE Platform development\nExperience in JSP, Servlets\nExpertise in JPA and Hibernate\nExperience in Spring MVC, Spring Boot, Maven and Apache Ant\nExpertise in ETL Development using Informatica and SSIS\nExperience in Data warehousing and Master Data Management\nExperience in using a variety of reporting tools such as Oracle Reports, Crystal Reports, Microsoft Reports, SSRS and Active Reports\nExcellent Interpersonal, Analytical and Communication skills\nKnowledge in various states of System Development Life Cycle (SDLC)\nWell versed in Agile Development Methodologies \nHigh proficiency in working with users to gather requirements, analyze them, and subsequently use design tools to model the requirements\n\nTECHNICAL SKILLS\n\nLanguages: \tJavaScript, Java, Python, VB.NET, C#, Scala, TypeScript\nWeb Technologies: \tHTML5, CSS3, Nodejs, ASP.NET, .NET Core, Spring Framework, Flask, Django, Drupal, REST, SOAP, XSL, XML, XPATH, Web Services, jQuery, Angular, React, Backbone, AJAX, LESS, SASS\nMobile:\tXamarin, ionic.\nCloud:\tAWS, AWS Lambda, Azure and Azure Functions\nDatabase Tools: \tSQL Enterprise Manager, SQL Profiler, Query Analyzer, TOAD, SQL Developer, phpMyAdmin\nORM Frameworks:\tHibernate, JPA, NHibernate, LINQ, Entity Framework\nRDBMS: \t\t\tIBM DB2, MS SQL Server, Oracle, MySQL, SQLite, Access, Postgres \nBigData/NOSQL:\t\tApache Spark, Mongodb, Dynamodb, \nSearch Technologies:\t\tSolr, Elastic\nOperating Systems: \t\tWindows, Mac OS X, Linux, Unix, IBM z/OS, Android and IOS\nTechnology: \tCOM+, CORBA, MSMQ, Web Services, WebDAV, Win Forms, WCF, WPF, WF, Servlets, Enterprise Java Beans, Struts, Spring, JPA, Flex\nDatabase Scripting: \t\tTSQL, PL/SQL\nScripting Languages:\tJ\tavaScript, VB Script, Action Script, Shell Scripting\nDatabase Connectivity: \tADO.NET, OLEDB, ODBC, JET, ADO, JDBC\nSource Control: \tGit, Visual SourceSafe, Visual Studio, Team Foundation, Subversion\nReporting Tools: \t\tSQL Server Reporting, Crystal Reports, Active Reports, SSRS\nVirtualization: \t\tVMware, Virtual PC, VirtualBox\n\nSECURITY CLEARANCE\nPublic Trust\nEDUCATION\nCurrently pursuing MS in Computer Science from Hood College, Frederick, MD\nMSc Business Information Technology, Middlesex University, London UK \nDiploma in Advanced Computing: CDAC Pune, India\nBSc Mathematics, University of Calicut, Kerala, India\n\nPROFESSIONAL EXPERIENCE\nKTBSonline, Sparks,MD\nSenior Software Developer\t\t\t\t\t\t\tJun 2019 - Present\n\nKTBSonline is a fully customizable web portal that facilitates the enrollment and management of employee benefits for employers of all sizes. Since 1998, KTBSonline has been renowned for its proven efficiencies and exceptional employer/employee experience, as well as the critical data it provides and uncompromising security.\n\nKTBSonline provides a benefits management system for HR professionals and a web portal for employees. Through KTBSonline, employers reduce benefit spend, increase the value of their health care plans, and become more efficient and productive \u2014 while increasing compliance, data security, and employee engagement. Simply put, it helps increase the efficiency of any Human Resources department. It is a fully customizable web portal that facilitates the enrollment and management of employee benefits and payroll for businesses of all sizes.\nResponsibilities:\nDevelop ASP.net WebForms based application\nPort WebForms to Razor\nPort WCF services to GRPC\nAzure PaaS\nImplement and consume WCF and GRPC based web services\nImplement DevExtreme and Vue based user interface components\nImplement MSMQ and NServiceBus based Message queue\nSQL Server 2017 Programming\n\nEnvironment:\nC#, TypeScript, Vuejs, NodeJs, Dotnet Core, ASP.NET MVC, SQL Server 2017, LINQ, Entity Framework,CSS3, HTML5, jQuery, Visual Studio 2019, DevExtreme UI, , Agile, Scrum, MongoDB, NServiceBus, MSMQ, GRPC, WCF\n\nHarmonia Inc, Tysons Corner VA\nSenior Software Developer\t\n\t\t\t\t\t\tSep 2017 - Jun 2019\nFSIS APM\nAPM is the Product recall module that intends to replace the current paper based process with a web based portal that will be integrated with FSIS.\nResponsibilities:\nDevelop ASP.net Core based application\nDevelop Xamarin based mobile application for Android and IOS\nImplement and consume REST based services using ASP.NET Core 2.0\nParticipate in architecture design process\nIdentify and solve complex problems using design patterns such as factory, singleton, Strategy, observer  and builder patterns\nImplement Telerik Kendo UI based user interface components\nImplement Anguklar7.0 based UI\nImplement IText7 based PDF generation and signing functionality\nSQL Server 2017 Programming\n\nEnvironment:\nC#, TypeScript, Angular, NodeJs, Dotnet Core, ASP.NET MVC, SQL Server 2017, LINQ, Entity Framework,CSS3, HTML5, jQuery,4.5, Visual Studio 2017, Telerik Kendo UI, IText 7, Agile, Scrum, Xamarin forms for Android, TDD, BOD\n\n\nFSIS Application Portal\nFSIS implemented the Public Health Information System (PHIS) as part of our effort to collect,\nconsolidate, and analyze data. PHIS is a user-friendly, web-based application that replaces many\nof FSIS\u2019 existing systems\nResponsibilities:\nDevelop ASP.net WebForms based application\nAzure PaaS\nImplement and consume WCF based web services\nImplement Telerik based user interface components\nImplement RabbitMQ and Masstransit based Message queue\nSQL Server 2014 Programming\n\nEnvironment:\nC#, WCF, ASP.NET MVC, ASP.NET Web Forms, SQL Server 2014, LINQ, Entity Framework, CSS3, HTML5, jQuery,4.5, Visual Studio 2015, Telerik, RabbitMQ, Masstransit, Agile, Scrum,m Azure\n\n\nLHNCB at National Library of Medicine, NIH Bethesda, MD\t \nSenior Web Developer\t\t\t\t\t\t\tSep 2015- Sep 2017\n\nPersonnel Management System\nThe project Personnel Management System help manage Researchers, Interns and Associates at the National Library of Medicine at NIH. It tracks their academic achievements such as degrees and awards and helps the NLM management track their career development at NLM. The system was originally developed in ASP and is in use for over 15 years. Support for some components such as Crystal Reports and Windows 2003 are no longer available. The system lacked user interface improvements due to the proliferation JavaScript and lacked important data security measures such as data encryption and secure login. \n\nResponsibilities:\nConvert ActiveX Based Crystal Reports to .NET\nImplement sensitive data encryption in the database\nImplement LDAP based authentication\nImplement XSS and SQL injection prevention throughout the website\nAdd search and data download functionalities\n\nEnvironment:\nC#, ASP.NET MVC, ASP.NET Web Forms, Classic ASP, Crystal Reports, SQL Server 2012, CSS3, HTML5, jQuery, COM+, VB Script, .NET 4.5, Visual Studio 2015. \n\nLforms\nLForms is a lightweight, feature-rich, open-source widget that creates input forms for Web-based medical applications or to integrate into electronic health records (EHRs), personal health records (PHRs), and mobile health apps. LForms is a completely free widget toolkit that leverages LOINC's rich content model for panels, forms, and survey instruments. Developed by LHNCBC in collaboration with the Regenstrief Institute, LForms can read any of the 1,700+ panels defined in LOINC\u00ae and render a powerful data entry form. It works across all the domains (laboratory panels, survey instruments, etc.) included in LOINC. LForms power comes through its support of detailed form attributes, including: data type, cardinality, default value, units of measure (if numeric), answer lists (if multiple choice), ability to make multiple choice answer lists function as \"select one\" or \"select all that apply,\" relationship (in a nested hierarchy) to other questions, default value settings, validation checks, skip logic and help messages.\n\nResponsibilities:\nDevelop node js based REST services that serves data stored in the elastic index to the user. The rest services can be customized to return specific fields only\nDevelop ElasticSearch based indexes. These indexes are tuned to maximize response time and handle high volume traffic\nImplement Elasticsearch based search on the Gene Index.\nCreate a Shell script using tools such as AWK, sed, sort and such to de-duplicate, group, truncate and sort raw data\nCreate Jasmine and Mocha Tests based test for LForms\n\nEnvironment:\nNodejs, Angular 2.0, Elasticsearch, REST, Mocha, Jasmine, Grunt, AWK, Shell Script, Red hat Linux, Oracle 11g, ASP.NET MVC, ASP.NET Web Forms, ASP, SQL Server 2012, SQL Server 2012, Jira, BitBucket\n\n\n\n\nSocial Security Administration, Gwynn Oak MD\t\t\t\t\t\nApplication Developer\t\t\t\t\t\t\tJul 2014- Jul 2015\n\nThe purpose of this Project is to provide support to the Office of the Chief Strategic Officer (OSCO) at the Social Security Administration in connection with OCSO\u2019s executive information site and all related data tables and reports. LOCKHEED MARTIN will seamlessly update and transition Executive and Management Information System (EMIS) from the current version to a new one by suggesting improvements including intranet web interfaces, database design, report design, and user tools.\n\nResponsibilities:\nDevelop tools that allow OSCO staff to update raw data\nCreate new reports when necessary\nDevelop a system that allows modifications to report structure, data elements, and charts/graphs without hard coding or new programming, through user-friendly interfaces\nDevelop new program(s) with the flexibility for direct database access\nDevelop interactive graphical user interfaces) (GUI) for the executive information site on EMIS, Agency Priority Goals (APG) and Strategic Objective data where, through data visualization (i.e., dashboards), users can easily identify overall progress and clearly see those areas or timeframes where additional information and analysis is required\nProvide business analyst support to document code, maintain, and archive program development and requirements changes by producing a data dictionary and a user manual\nProvide data analytics support as requested\n\nEnvironment:\nASP.NET MVC, ASP.NET Web Forms, ASP, WCF, Web Services, SQL Server 2008, SQL Server 2012, SSIS, jQuery, Highcharts, Bootstrap, D3.js Library, JavaScript, CSS 3, HTML 5 C#, REST, ASP.NET Web API, Java, Spring 3.0, WebSphere, 508 Compliance. \n\n\nThe Great Courses, Chantilly VA\t\t\t\t\t\t\t\nSenior Web Developer\t\t\t\t\t\t\tFeb 2013- Jul 2014\nThe Great Courses is a series of college-level audio and video courses produced and distributed by The Teaching Company, Chantilly, Virginia company, via mobile apps, CD, DVD, or MP3 and MPEG-4 download formats\n\nResponsibilities:\nMaintain existing website and making frequent updates upon business requirements\nWork closely with the marketing team to implement new requirements and bug fixes\nDeploy website across dev, stage and pre-production environments\nDevelop SQL stored procedures as needed for implementation\nDesign, implement and maintain code that supports all browsers\nImplement tagging needed for tracking user behavior and conversion rates\nImplement split version of the website for testing purposes\nImplement user Interface using JavaScript and HTML5\nCreate and maintain SSIS packages for the MIS Data Mart\nMaintain and enhance Apache Solr Search Platform and integrate it with the Website\nDevelop and integrate email campaign using Strong Mail\nDevelop REST API for mobile integration\n\nEnvironment:\nASP.NET MVC, ASP.NET Web Forms, ASP, Visual Basic 6, Windows Froms, SQL Server, 2008, SQL Server 2012, SSIS, Solr, jQuery, JavaScript, CSS3, HTML5, WCF, C#, VB.NET, VB Script, REST, ASP.NET Web API\n\nMaricom, a CSC Company, Windsor Mill MD\t\t\t\t\t\nETL Developer\t\t\t\t\t\t\t\tAug 2012-Feb 2013\nMaricom, a CSC company, is an IT solutions firm, specializing in Data Management, Software Engineering, Business Intelligence, and Contact Center solutions. Located within the Baltimore-Washington corridor Maricom has worked successfully with Government, Healthcare, and Commercial sectors, providing management, maintenance, and support of large and complex data environments.\n\n\nResponsibilities\nDeveloped Mappings and Sessions based on user requirements and business rules to load data from source flat files and RDBMS tables to target tables.\nInvolved into design the logical data model from the technical design documents and translating to a physical data model using ERWIN\nDesigned the ETL processes using Informatica to load data from DB2, Oracle, SQL Server, Flat Files, XML Files and Excel files to target Oracle Data Warehouse database.\nIdentified and loaded sources and targets metadata into repository\nCreated Shell Scripts to control and initiate Informatica workflows, Process flat files and email status messages\nDeveloped Mappings and Sessions based on user requirements and business rules to load data from source flat files and RDBMS tables to target tables\nDesigned the ETL processes using Informatica to load data from DB2, Oracle, Flat Files, XML Files and Excel files to target DB2 Database.\nCreate SOAP Java web services using Apache Axis 2.\n\nEnvironment:\nInformatica Powercenter 9.1.0, Oracle 11g, 10g/9i, DB2 10 for z/OS, UNIX Shell Scripting, Windows NT/2000, Unix, Sun Solaris, Eclipse, Java, Apache Axis 2. \n\nHertz Rent2Buy Park Ridge, NJ\nAnalyst/Programmer\t\t\t\t\t\t\t\tFeb 2012- Aug 2012 \nHertz Global Holdings Inc.\u00a0is an American\u00a0car rental\u00a0company with international locations in 145 countries worldwide. Rent2Buy is an innovative program launched by Hertz Car Sales with the purpose of selling high-quality, active rental cars online at very competitive prices. Hertz Rent2Buy\u00ae provides customers with the opportunity to test the vehicle for 3 full days by renting it at a low special rate, which will be waived if the car is purchased.\n\nResponsibilities:\nCreated components using SOAP wire protocol for the communication between Hertz partners and the Rent2Buy\nDeveloped Solr based search service for cars in the rent2buy database.\nConsumed and exposed data to and from several clients using Web Service (SOAP and JSON) \nDesigned and developed the messaging framework for communication between Rent2Buy, other Enterprise Applications and client data feeds over XML and MQ-Series \nDeveloped\u00a0web interface using JSP, Servlets, JavaScript, CSS and JDBC for administering and managing users, vehicles and clients\nDeveloped and deployed various Entity EJBs and session EJBs\nImplemented a responsive website which can run on Desktops, Tablets and Smartphones using HTML5, JavaScript and CSS3\nMaintained and enhanced Rent2Buy website and Database\nAdded and maintained various data consistency checks and alerts to the Rent2Buy Database\nCreated Middle Tier to enable data transfer between Frontend and Database\n\nEnvironment: \nEnvironment: Java, JSP, Java Server Faces, Servlets, EJB, JDBC, ASP.NET MVC, SQL Server 2008, HTML, CSS 3, HTML5, XML, JSON, Subversion, WebSphere Application Server 6.1, jQuery, GWT, Backbone, Handlebar, Web services (SOAP, WSDL), JIRA, JSON, JUnit,\n\nIPSOS, Parsippany NJ\nAnalyst/Programmer\t\t\t\t\t\t\t\tAug 2011- Feb 2012 \n\nIPSOS is a global market research company focusing on the production, interpretation and distribution of information gathered from individuals about their opinions, desires, attitudes and behaviors. IPSOS operates over 80 countries worldwide. The Project I am working on is for Walgreens Customer Loyalty Research.\n\n\nResponsibilities:\nMaintained and enhanced Walgreens Customer Loyalty website and Database\nAdded and maintained various data consistency checks and alerts to the survey database\nAdded or Removed questions to the customer survey database and website\nCleaned, Transformed and Imported data from the survey to the Data warehouse using SSIS \nCreated various data consistency checks and alerts on the imported data\nPrepared stored procedures to pre-calculate data for reporting\nCreated and maintained ASP.NET Web forms based Website for reporting\nCreated and Maintained various SQL Server 2008 Jobs for data processing and SSIS\nDeveloped ASP.net, Windows Forms and Service applications using Vb.Net and C#\nDeveloped a flex based application for Mobiles with .net Service based backend\n\nEnvironment: \nASP.NET Web forms, Windows Forms, WPF, JavaScript, jQuery, Dojo, ASP.NET AJAX, JSON, C#, Visual Studio.NET 2008, Visual Source Safe, Share point 2007, XML, XSLT, CSS, SQL Server 2008, Windows Services, SSIS, Flex, ActionScript.\n\n\nUnited Nations, New York, NY\t\t\t\t\nInformation Systems Assistant\t\t\t\t\t\tAug 2010- Jul 2011\nThe Headquarters of the United Nations in New York City employs a variety of technologies to fulfill the mandates of different departments and enable effective communications between its branches all over the world. \n\nResponsibilities:\nActively participated in United Nations Non-Governmental Organizations (NGO) Branch website usability enhancement studies and design\nDeveloped and maintained NGO branch website using Java, HTML, JavaScript and Servlets\nCreated and maintained various scripts to host the application in Apache Tomcat container\nMaintained and enhanced custom MVC framework using JSP's, internally developed Custom Tag Libraries, JSP Standard Tag Libraries (JSTL), HTML, and JavaScript\nCreated User interface using JSP, HTML, and JavaScript\nMaintained and enhanced NGO Registration Portal\nCreated and generated reports using Pentaho reporting tool\nMaintained and enhanced event management system for UN agencies in New York and Geneva\nCreated and maintained Tables, Stored Procedures, Triggers and Indexes for Oracle 10g database\nEnhanced the Paperless website to handle the volume of information submitted by Non-Governmental Organizations around the world\n\nEnvironment: \nJAVA, JSP, Servlets, Tomcat, JDBC, Spring Framework, JPA, Hibernate, JavaScript, jQuery, Dojo, Oracle, Eclipse, NetBeans, SQL Developer\n\nEmblem Health, New York, NY\t\t\t\t\nAnalyst/Developer\t\t\t\t\t\t\t\tJuly 2009 \u2013 Aug 2010\nEmblem Health provides health insurance to over 3.4 million members across New York, New Jersey and Connecticut.\n\nResponsibilities:\nCreate ASP.NET website for data gathering and presentation\nMaintain and support existing WinForms application\nAnalyze requirements for the Request tracking intranet website\nPrepare Project Documentation\nDesign SQL Server 2008 database for the request tracking application website\nCreate database objects such as tables, views, stored procedures and indexes\nCreate complex yet efficient stored procedures for Search and Reporting\nCreated AJAX enabled User Interfaces for the Request Management Application using XHTML, ASP.NET MVC and jQuery\nImplement a Data Repository using ADO.net Entity Framework\nTest Driven Development using ASP.NET Test Framework\nCreate SSIS packages for Extracting and loading data from legacy applications\nDesign and Create Local and Remote reports using SSRS\nDeploy SSRS and consume reports in the web application\n\nEnvironment: \nASP.NET, WinForms, WPF, JavaScript, jQuery, Dojo, Structuremap, ASP.NET AJAX, JSON, C#, Visual Studio.NET 2008, Visual Source Safe, Share point 2007, XML, XSLT, CSS, Active Reports, SQL Server 2008, Windows Services, SSIS, SSRS, ADO.NET Entity Framework, LINQ\n\nI3 Solutions, Sterling, VA\t\t\t\t\t\nSenior Developer\t\t\t\t\t\t\t\tMar 2009 \u2013 Jul 2009\nI3solutions is an IT consulting firm specializing in strategic application development, data management and systems integration, and collaborative solutions.\n\nResponsibilities:\nCreated ASP.NET web forms for data gathering and presentation\nCreated and scripted Database components such as tables, functions and stored procedures to store data for effective retrieval and manipulation\nCreated SSIS packages to extract from Excel and Text files, Cleanse and Upload to Oracle 10g database\nCreated and consumed WCF services, web services and HTTP Handlers for asynchronous communication between the server and client\nCreated detailed technical specification and documentation of the project\nCreated AJAX user interface using DOJO Library\nAuthentication using Active directory \nCustomization ASP.NET Membership API to facilitate form based authentication and rule based authorization\n\nEnvironment: ASP.NET, JavaScript, ASP.NET AJAX, WCF, JSON, VB.NET, Oracle 11.i, 10g, Visual Studio.NET 2008, Visual Source Safe, SharePoint 2007, XML, XSLT, CSS, Active Reports.\n\nFINRA, Rockville, MD \t\t\t\t                         \t\nSenior Web Developer \t\t\t\t\t\t\tJul 2008 \u2013 Feb 2009\nFinancial Industry Regulatory Authority (FINRA) is the largest independent regulator for all securities firms doing business in the United States. FINRA operates Web CRD\u00ae, the central licensing and registration system for the U.S. securities industry and its regulators.  It contains the registration records of more than 6,800 registered broker-dealers and the qualification, employment, and disclosure histories of more than 660,000 active registered individuals, making it the world\u2019s largest and most sophisticated online registration and reporting system\n\t\t \t\t         \nResponsibilities:\nCreated ASP.NET components and pages to add new functionality to the WebCRD application\nAnalyzed their impact to the application\nCreated and maintained XML documents and support functions for legacy application\nCreated and maintained XSL Transformations to consume XML data\nCreated and maintained oracle functions and procedures in packages to incorporated changes to the schema\nCreated complex PL/SQL Stored Procedures to check the completeness of form filings\nExtensively used PL/SQL tables, cursors and exception handling\nCreated database scripts to delete or update inconsistent data to maintain data Integrity\nFixed performance issues and bugs within packages, forms, and reports using DBMS_OUTPUT, TOAD and SQL Developer debuggers, explain plan and TKPROF\nImplemented AJAX Control extensions to enhance user experience\nMaintained and implemented Java Script functions for improved user experience\nSharePoint customization and development\nMaintain and debug old codebase\n\nEnvironment: ASP.NET, ASP, C#, VB Script, JavaScript, AJAX, JSON, Oracle 11i, 10g, Eclipse, CVS, SharePoint 2007 XML, XSLT, CSS.\n\n\nMIC BUSINESS SOLUTIONS, INC, New York, NY           \nSenior Web Developer \t\t\t\t\t\t\tJan 2008 \u2013 Jul 2008\nMIC Business Solutions, Inc. (MIBS), a wholly owned subsidiary of Mitsubishi International Corporation. MIBS provides information and communications technology consulting, as well as system and network integration, focusing mainly on medium-sized Japanese corporations with a presence in the U.S. The project was to design and implement a warehouse management system using barcode to manage Inventory.\n\nResponsibilities:\nParticipate in Requirements Analysis\nCreated Database diagrams for the database\nCreated Class diagrams, sequence diagrams and use case diagrams for the project\nCreated tables, stored procedures and functions for the warehouse management system\nCreate SSIS packages to Extract, standardized and upload product delivery notes from different suppliers to the warehouse database\nDesigned and implemented User interface for the web application using server side AJAX\nDeveloped user interface for the WinCE based barcode reader\nDevelopment and customization of SharePoint Site\nImplemented Barcode data manipulation and database interaction\nImplemented barcode reader behavior for the handheld device\nCreate reports using Crystal reports\nImplemented modules to interact with SAP Web module to update stock\nHosted site on client intranet\n\nEnvironment: ASP.NET, JavaScript, ASP.NET AJAX, C#, SQL Server 2005, Visual Studio .NET 2005, Visual SourceSafe, Crystal Reports and Win CE 5.\n\nWORLD TRAVEL HOLDINGS, Woburn MA \t       \nAnalyst Programmer\t\t\t\t\t\t\t\tJul 2007 \u2013 Dec 2007\nWorld Travel Holdings is a multi-brand travel distributor with executive offices in Port Washington, New York and corporate offices in Woburn, Massachusetts. The company's owned brands include CruisesOnly, Cruises.com, Rooms.com, Vacation Outlet, CruiseOne, Cruises Inc., and Villas of Distinction. WTH's portfolio of licensed partner brands includes BJ's Vacations, Hotwire.com Cruises, Overstock.com Travel, Priceline.com Cruises, American Airlines Cruises, continental.com cruises, and many others.\n\nResponsibilities:\nMaintain the existing ASP code base, with an emphasis on JavaScript\nCreated Class diagrams, sequence diagrams and use case diagrams for the project from the specification provided\nImplemented .NET libraries to interact with client\u2019s web services\nImplemented COM+ wrapper classes for this library to interact with legacy ASP code base\nImplemented Http based routines to interact with legacy web resources\nCreated PL/SQL packages to incorporate changes to the database\nCreated PL/SQL stored procedures and Functions\nFixed performance issues and bugs within packages, forms, and reports using DBMS_OUTPUT, TOAD and SQL Developer debuggers, explain plan and TKPROF\nImplemented testing cases for classes used for the gateway\nImplemented a gateway to Interact with WTH partners for data exchange using XML and HTTP\nImplemented web trends and 7 billion people analytic tools for the website\nDeveloped Java based middleware libraries to interact with Sabre web services\nDeveloped, debugged and maintained JSP and Servlet code\n\nEnvironment: ASP, ASP.NET, JavaScript, VBScript, C#, Oracle, Visual Studio.NET 2005, Visual Source Safe, Visual C++, MFC, COM+.\n\t\n\nINPHONIC INC. Largo, MD\t\t\t\t\t\tDec 2006 \u2013 Jun 2007\nAnalyst Programmer\nInPhonic is one of the Internet's No. 1 authorized retailers of cell phones, cell phone plans, and other wireless devices. InPhonic operates Wirefly.com, one of the 5 most visited electronics shopping sites in the world, and manages private label e-commerce sites for national retailers (such as RadioShack and Best Buy), internet retailers (such as Overstock.com and Buy.com), cell phone manufacturers (such as Motorola and LG), membership organizations (such as airlines and associations), and thousands of other partners and affiliates. The aim of the project I was involved was to enhance their Back-Office application with new functionalities called Invitation Codes.\n\nResponsibilities:\nDesigning and developing modules for the application\nMaintenance of legacy application in ASP code \nDesigning application interface for Customer promotions and Invitation codes\nImplemented application logic for product return and damaged Items handling\nCoding application logic in ASP.NET using VB.NET\nEvaluating the impact of the new functionalities on the existing Application\n\nEnvironment: ASP.NET, C#, ASP, VB Script, VB.NET, SQL Server 2000, 2005, Windows XP Professional, Visual Studio .NET, Visual Source Safe, MS Application blocks, SQL Server Reporting Services, Crystal Reports.\n\nSURFPIN LTD., London, UK\t\t\t\t\nSoftware Engineer\t\t\t\t\t\t\t\tApr 2004 \u2013 Nov2006\nSurfpin Ltd, UK is the market leader in providing Micropayment services in Europe, North America and Australia. Their business model includes mobile phone companies or aggregator, ecommerce and gaming websites and customers. The complexity involved in successfully completing a transaction in enormous. Also, there is a pressing need to provide the partner website with state of the art business intelligence about their payment pattern. Projects included designing and implementing transactional, highly available web services, business intelligence and customer service solutions. \n\nResponsibilities: \nCreated views, stored procedures and functions for the database\nCreated COM+ components\nCreated MSMQ components\nCreated Windows services to manage Message queues\nConverted XML to HTML mark-up using XSLT\nImplemented Data marts for faster report processing\nCreated MDX queries\nConverted Multidimensional queries to XML using ADOMD for reporting tools\nCreated Smart client based customer support package using WebDAV and Exchange Server\n\nEnvironment: ASP.NET, C#, COM+, MSMQ, SQL Server 2000, SQL Server 2005, SQL Server Business Intelligence Studio, Crystal Reports, SQL Server Reporting Services, WebDAV, XML, XSLT, XPATH, Web Services, Crystal Reports, WinForms, Smart Client Applications, SSIS, SSAS, SSRS, MS CRM.\n\n\nPet Projects\nCoupon Application \t\t\t\t\t\tDev 2017 - Present\nSmall pet project for small businesses to issue discount coupons and track customer reviews.\nResponsibilities:\nDevelop NodeJS and Angular 6 based application to offer coupons\nImplement and consume REST services \nImplement IONIC based mobile application for IOS and Android\nImplement Firebase datastore\n\n\nEnvironment:\nJavaScript, Nodejs, Angular 6, Firebase, Mongo,  Ionic, TypeScript, Linux, Docker, AWS, Microservices\n\n\n\nBinary Sort (ongoing startup pet project), Frederick MD\nSenior Software Developer\t\t\t\t\t\t\tJan 2015 - Present\n\nBinarySort is an early stage start-up where responsibilities include, design and development of modular and scalable N-tier enterprise application. The application performs real-time social media monitoring. The application monitors and aggregates popular and trending articles from various social media platforms and present to the user via responsive web page.\n\nEnvironment:\nNode.js, .NET, Angularjs, Bootstrap, MongoDB, OAuth 2.0, CORs, Cloud9 IDE\n\n\n\n\n\nMMDB (My Movie DataBase)  ongoing fun/ learning project \t\t\nDeveloper\t\t\t\t\t\t\t\t\tMar 2017 - Present\n\nThe MMDB aims to create an index of publicly available movie datasets. The project aims to combine sophisticated search on a number of criteria such as name, year, genre, studio, actor, country, language. For Elasticsearch is used for indexing the data. The project exposes a number of REST APIs for querying, adding and updating the movie data. The project consumes APIs from IMDB and TMDB to query movie details and reviews. In addition, the project exposes its API using Azure API Management Gateway. The APIs are designed as microservices using Spring boot and .NET core, \n\nEnvironment\n.NET core, Java, Nodejs, MongoDB, Elasticsearch, Azure, Azure Storage, Azure Functions, Angular, Jenkins, GitHub, Docker.\n\nCustom Album Designer (ongoing startup pet project)\ncustomalbumdesigner.com\nSenior Software Developer\t\t\t\t\t\t\tJan 2016 - Present\n\nCustom album designer helps professional and amateurs to create stunning albums with their photos for any occasion or event. The site gives option to upload photos, choose designs, binding types and previews and suggestions. The project is aimed to move the entire operation to AWS with modern JavaScript based user interface and cloud storage and hosting.\n\nEnvironment:\nFlask, React, AWS, Python, JavaScript, S3, DynamoDB, Bootstrap, OAuth 2.0, CORs, Cloud9 IDE, CI CD, Jenkins\n",
          "confidence": 1.0,
          "method": "full_text",
          "structured_data": null
        },
        "tax_term": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "source_by": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "designation": {
          "value": "Senior Software Developer",
          "confidence": 0.8,
          "method": "regex",
          "structured_data": null
        },
        "experience": {
          "value": "13+",
          "confidence": 0.9,
          "method": "regex_total_experience_summary",
          "structured_data": null
        }
      }
    },
    {
      "resume_path": "data\\input\\ABDIRAHIM MASLAH   - SD - OH.docx",
      "confidence_score": 0.65,
      "used_ocr": false,
      "extracted_fields": {
        "skills": {
          "domain_specific": [
            {
              "name": "security",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                204,
                2067
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "retail",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                980
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "infrastructure",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                1103
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "medical",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                2778
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "vendor_certifications": [
            {
              "name": "ccnp",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                217
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "soft_skills": [
            {
              "name": "documentation",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                1366
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "methodologies": [
            {
              "name": "safe",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                1710
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "xp",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                2154
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "data_skills": [
            {
              "name": "excel",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                2230
              ],
              "experience_weight": 0.0,
              "importance_score": 0.9
            }
          ],
          "security_tools": [
            {
              "name": "mcafee",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                2285
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ]
        },
        "first_name": "ABDIRAHIM",
        "last_name": "MASLAH",
        "primary_email": {
          "value": "abdirahim008@gmail.com",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "secondary_email": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "phone": {
          "value": "6143774454",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "city": {
          "value": "Columbus",
          "confidence": 0.7,
          "method": "ner",
          "structured_data": null
        },
        "state": {
          "value": "OH",
          "confidence": 0.8,
          "method": "city_database",
          "structured_data": null
        },
        "zip": {
          "value": "",
          "confidence": 0.0,
          "method": "regex",
          "structured_data": null
        },
        "work_authority": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "resume_link": {
          "value": "data\\input\\ABDIRAHIM MASLAH   - SD - OH.docx",
          "confidence": 1.0,
          "method": "file_path",
          "structured_data": null
        },
        "raw_resume": {
          "value": "ABDIRAHIM MASLAH  \n\n                                           Columbus Ohio\nHome: 614-377-4454 Cell: 614-377-4454 abdirahim008@gmail.com \nPROFILE\nAspiring IT professional actively pursuing an aggressive education to obtain Industry specific certifications.  Highly experienced in customer service protocols and processes.  Comfortable working directly with the customer/client to resolve their issues.  Able to de-escalate problematic situations whether face-to-face or via phone.  Called upon to take the lead in multiple situations to train and mentor peers.  \nEDUCATION\nMy Computer career, Columbus, Ohio \t\t\t\t\t\tGraduated June 2016\nInformation Technology system Administration \nCurrent Certification: MTA Network fundamentals, CompTIA Network+, CompTIA A+, CompTIA Server Plus and MTA Windows OS. \nCertification in progress: Mobility + CompTIA, CNNA, MCTS (Window 7), MCTS (Window server Enterprise Administrator), CompTIA Security+, and CCNP \nTECHNICAL HIGHLIGHTS\nResponds to customer technical problems/issues related to various OEM hardware  and software platforms\nAssists customers by diagnosing problems and providing resolutions for technical issues\nUses troubleshooting techniques and tools to identify products that are defective and follow guidelines in issuing service calls/contacts\nUtilized Service Now to open, document, and resolve helpdesk tickets in accordance procedures.\n\u00a0Performs PC, laptop, and printer moves and installations. \n\u00a0Advises/educates customers within procedural guidelines to ensure a complete solution to their technical or service questions\nUtilized ServiceNow ticketing system.\n\nproviding phone support for Windows 7 & 8 desktop environment including Active Directory (AD), within a Retail Financial Services environment\nIn-depth knowledge of several technology disciplines including PC, Server, and Network infrastructure as well as Mobile Messaging/Communication software and hardware such as smart phones, broadband cards and other wireless technologies\nUtilized ServiceNow ticketing system.\nStrong customer service skills via phone, instant message (IM), along with strong documentation skills\nConfigure and troubleshoot UPS\u2019s and Micro pods. \n\nField Nation \t\t\t\t\t\t\t\t          5/25/2015 to present \nFreelance contractor/ Help Desk Analyst\nColumbus, Ohio \nInstalling CAT 5 and CAT 6\u00a0cables, Testing\u00a0and terminating cables, Patching\u00a0panels, \nSolving\u00a0issues related to cables and networks\nTerminate phone jacks\nDemonstrate proper procedures, encourage safe use, document in ServiceNow.\nWorked with TCP/IP, assigning/reassigning IP addresses and with Novell Servers, Windows NT and DNS.\nInstall and trouble PC, Printers, router, servers, and switches\n     \n SomTech    \n Network Technician  \n Columbus,Ohio  \t\t\t\t\t\t\t\t02/15/2014 to 1/30/2016\nFamiliarity with servers - Virtual, blade and physical.\t \t\t\t    \nInstalling router and switches                                                               \nSetting up computer security measures \t\nTroubleshot software and hardware issues via phone such as Windows 7, XP operating systems, Microsoft standard desktop applications including Word, Excel, PowerPoint, Exchange and Visio, Firefox and Chrome, McAfee, Kapersky and Norton anti-virus software\t\t\nbackup processes and retention periods\nRacking, un-racking, and installing servers, UPS, and surge protectors \nHands-on experience with pulling cable\nVery strong part of any data center support team\n\nAFRICAN COMMUNITY CENTER \t\t\t\t\t       07/2008 to 01/2013\nFront desk and Tech support\nMinneapolis, Minnesota\nMade outgoing/received incoming calls to clients to request and clarify information. \nHelped elderly and non-English speaking customers to fill out job, housing, medical, or Immigration applications. \nReceived Company invoices, scanned and emailed to Microsoft Outlook.\nRecord keeping and maintaining an organized work environment \nTroubleshoot, diagnose, repair/replace computer issues. ",
          "confidence": 1.0,
          "method": "full_text",
          "structured_data": null
        },
        "tax_term": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "source_by": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "designation": {
          "value": "system Administration",
          "confidence": 0.8,
          "method": "regex",
          "structured_data": null
        },
        "experience": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        }
      }
    },
    {
      "resume_path": "data\\input\\Abhishek-devops-mig.docx",
      "confidence_score": 0.6816153846153847,
      "used_ocr": false,
      "extracted_fields": {
        "skills": {
          "devops": [
            {
              "name": "bitbucket",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                111,
                547,
                2372,
                4955
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "bamboo",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                255,
                4940
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "splunk",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                410,
                7881
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "tempo",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                607,
                2432
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "github",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                726,
                1086
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "openshift",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                1828,
                2155,
                2252,
                3640,
                3869,
                4886
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "svn",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                4947
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "git",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                4951,
                7381,
                7782
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "cloud": [
            {
              "name": "workflows",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                642,
                996,
                2831,
                3219
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "azure",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                1863,
                1957,
                4880,
                6686,
                6735,
                6773,
                6889,
                6945,
                6971,
                7804
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "jenkins",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                1873,
                2103,
                4381,
                5963,
                7786
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "aws",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                6277,
                6462,
                6567,
                6870,
                7800,
                11209,
                11284
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "ec2",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                6281
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "vpc",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                6285
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "s3",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                6289,
                6466
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "cloudformation",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                6616
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "docker",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                6714,
                7464,
                7502,
                7522,
                7640,
                7839
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "kubernetes",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                7022,
                7481,
                7846
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "lambda",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                11288
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "methodologies": [
            {
              "name": "devops",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                1963
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "agile",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                2443
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "rad",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                12701
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "business_skills": [
            {
              "name": "reporting",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                2058,
                2546
              ],
              "experience_weight": 0.0,
              "importance_score": 0.8
            }
          ],
          "programming": [
            {
              "name": "java",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                2534,
                11052,
                11657
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "python",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                6343
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "bash",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                6354
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "powershell",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                6779
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "move",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                11099
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "sql",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                11642
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "javascript",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                11678,
                12667
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "xml",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                11970,
                12685
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            },
            {
              "name": "html",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                12218,
                12662
              ],
              "experience_weight": 0.0,
              "importance_score": 1.0
            }
          ],
          "domain_specific": [
            {
              "name": "security",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                2658,
                2858,
                6154,
                10730
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "infrastructure",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                3141,
                4778,
                6571,
                9163
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "compliance",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                3503
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "banking",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                11188
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "design",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                11503,
                11710
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "architecture",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                12166
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "soft_skills": [
            {
              "name": "onboarding",
              "confidence": 0.84,
              "context": "project_section",
              "positions": [
                4204
              ],
              "experience_weight": 0.2,
              "importance_score": 0.7
            },
            {
              "name": "leadership",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                2982
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "mediation",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                9972
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "planning",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                10691
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "organization",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                11237
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "analysis",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                11701
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "frameworks": [
            {
              "name": "ansible",
              "confidence": 1.1199999999999999,
              "context": "project_section",
              "positions": [
                4101,
                4136,
                4157,
                4697,
                4896,
                6335
              ],
              "experience_weight": 0.6000000000000001,
              "importance_score": 0.7
            },
            {
              "name": "terraform",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                6325,
                6602,
                7823
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "bottle",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                10334
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "jquery",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                11689,
                12678
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "soap",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                11850
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "spring",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                12404,
                12444,
                12641
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "junit",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                12705
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "databases": [
            {
              "name": "mysql",
              "confidence": 0.7,
              "context": "project_section",
              "positions": [
                12635
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ]
        },
        "first_name": "ABHISHEK",
        "last_name": "Heera",
        "primary_email": {
          "value": "abhishek.heera11@gmail.com",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "secondary_email": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "phone": {
          "value": "7814864747",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "city": {
          "value": "",
          "confidence": 0.0,
          "method": "ner",
          "structured_data": null
        },
        "state": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "zip": {
          "value": "",
          "confidence": 0.0,
          "method": "regex",
          "structured_data": null
        },
        "work_authority": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "resume_link": {
          "value": "data\\input\\Abhishek-devops-mig.docx",
          "confidence": 1.0,
          "method": "file_path",
          "structured_data": null
        },
        "raw_resume": {
          "value": "ABHISHEK Heera\nDevOps/Migration Engineer\nPhone: (781)-486-4747 | Email: abhishek.heera11@gmail.com\n\nPROFESSIONAL SUMMARY\nAround 7 years of experience in Cloud Infrastructure and DevOps experience building CI/CD pipelines and automation.\nHands on experience with configuration management tools such as Chef, Ansible. Version control using GIT, SVN. Containerization using Docker and Kubernetes, continuous integration using Jenkins, Bamboo and worked on build tools like ANT and Maven.\nExperience in Continuous Integration and deployment using various like Jenkins, Hudson, Bamboo, Chef, and Puppet.\nExpert in Jira Atlassian admin. Have worked extensively with Jira plugins like Tempo, WBS, Jira Automation, Docker.\nConfigures and administers\u00a0Jira\u00a0Software,\u00a0Jira\u00a0Core and/or\u00a0Jira\u00a0Service Desk, and related products such as apps.\nExperience in working on AWS and its services like\u00a0AWS IAM, VPC, EC2, ECS, EBS, RDS, S3, Lambda, ELB, Auto Scaling, Route 53, Cloud Front, Cloud Watch, Cloud Trail, SQS, and SNS.\nExperienced in Cloud automation using AWS Cloud Formation templates to create custom sized VPC, subnets, NAT, EC2 instances, ELB and Security groups.\nManages global settings including users and groups,\u00a0roles, global permissions, and schemes.\nCreates projects, sets up project permissions, and assigns\u00a0Jira\u00a0project\u00a0administrators.\nExpertise in all areas of SCM processes, including version, build\u00a0and issue management, build and release management.\u00a0\nExpert in deploying the code through web application servers like WebSphere/Web Logic/ Apache Tomcat/JBOSS \nGood experience in middleware services JDBC, SSL certificates in WebLogic.\nExpertise in Deploying and troubleshooting the J2EE Applications (WAR & EAR) in JBOSS Application Server\nExperience with maintaining the 24x7 production environments on public Cloud environments (AWS, Azure) and       on prem.\nExtensive experience in setting up the CI/CD pipelines using\u00a0Jenkins, Maven, Nexus, GitHub, CHEF, Terraform and AWS.\nDeveloped\u00a0Python\u00a0scripts for software build conduct and management support.\nExtensive experience using MAVEN and ANT as build tools for the building of deployable artifacts (jar, war, ear) from raw source code.\nExtensively worked on Jenkins for continuous integration and End-to-End automation for all build and deployments.\nExperience working on several Docker\u00a0components like Docker\u00a0Engine, Hub, Machine, creating Docker\u00a0images, Compose, Docker\u00a0Registry and handling multiple images primarily for middleware installations and domain configurations.\nBuilding/Maintaining Docker container clusters managed by\u00a0Kubernetes. \nWorking on On-Premises and AWS platforms utilizing DevOps/Agile operational processes.\nExperience in designing, configuring and deploying solutions on Microsoft Azure using ARM Templates, AZURE PowerShell Module and, Azure CLI focusing on high-availability and auto-scaling.\nGood working knowledge of Creating, the AWS VPC network for installed instances and configured security groups and Elastic IP\u2019s accordingly.\nExperience in installing Logstash, Elastic search, Kibana on containers and creating Logstash config file to get all the logs from the server.  \nExperience in using Bug tracking tools like JIRA, and HP Quality Centre.\nExperience in working with configuration management tools ANSIBLE.\nCreated\u00a0Ansible playbooks to automatically install packages from a repository, to change the configuration of remotely configured machines and to deploy new builds.\nProficient in tracing complex build problems, release issues and environment issues in a multi-component environment.\nExperience in configuring clusters and managing domains in Application Servers\nKnowledge of using Routed Protocols: FTP, SSH, HTTP, TCP/IP, and HTTPS.\nExperience in deploying JDBC pool connections using SSI\u2019s.\nExcellent communication skills, leadership abilities, strong architectural skills, hardworking and a very good team worker. \nEDUCATION\nMasters in Computer Science - Colorado Technical\u00a0University\u00a0\u2013 2017 \n\nTECHNICAL SKILLS\n\n\n                                                               \nEXPERIENCE SUMMARY\n\n\nState of TN, Nashville, TN\nDepartment: HealthCare                                                                                                                    MAY 2021 \u2013 Current\nRole: Sr DevOps Engineer \nProject: TFS Migration\n\nResponsibilities\nConfigured and Managed repositories for various components of application using Bitbucket for Source code management, version control and managing various requirements for code promotion.\nCreated various plans and tasks using bamboo to build and deploy code to various environments and configured it to be automated based on the requirements.\nMonitor various application logs using Splunk to handle or resolve any issues real time.\nResponsible for administration, maintenance, upgrades and enhancements to JIRA, Confluence, Bitbucket and integrations with other tools or plugins like Tempo.\nDevelop & maintain custom JIRA workflows based upon business requirements\nMigration of code from TFS repository to GitHub repository for flexible version control.\nConverted various version control system specific directives and necessary mappings as required.\nTask management and workflow to track issues, bugs and various application requests using the Jira.\nDevelop & maintain custom JIRA workflows based upon business requirements\nPerformed migration of repositories from TF to GitHub.\nPerformed various builds on a regular basis and patches required by the application teams.\nRaised firewall rules and user access requests for various servers for targeted and destination and made use of Jira for tracking progress.\nUsed NuGet packaging for versioning support.\nMaintained servers and provided on the go support to application teams to resolve issues.\nStarting and stopping various nodes and slaves for environments with appropriate changes.\nInvolved in merging code for different components of application including enhancements.\nManaged Jboss servers to configure and resolve issues. \n\nUPS, Lutherville- Timonium, MD     \t\t\t\t\t\t\t              JUL 2019\u2013APR 2021\nRole: Systems Migration Engineer\n\nSYNOPSIS: Migrating various applications from tomcat and JBOSS to OpenShift container platform using azure and Jenkins pipelines to build and deploy.   \n\nResponsibilities\nSource code management using Azure DevOps server to perform various tasks including version control project management builds and reporting.\nBuilt multi branch pipelines using Jenkins to build and deploy various applications to OpenShift.\nScaling pods according to requirements and performing restarts when needed for apps in OpenShift to resolve any issues.\nResponsible for administration, maintenance, upgrades and enhancements to JIRA, Confluence, Bitbucket and integrations with other tools or plugins like Tempo.\nJIRA Agile (formerly green hopper), installed plugins for Confluence, experience maintaining custom Java custom reporting in JIRA\nAdministration of Atlassian Add-ons and 3rd party integrations, plugins, and extensions\nVet the security of those add-ons and integrations, etc.\nGather requirements for business processes, and determine ways to optimize/improve JIRA/Confluence build and configure complex workflows and screen, field, security, and notification schemes\nDevelop metrics dashboards and advanced filters in JIRA to provide end-users and business leadership with meaningful operational/performance metrics and status reports\nImplement Atlassian Tools upgrades, and partner with other IT staff to coordinate infrastructure maintenance and system migrations.\nDevelop & maintain custom JIRA workflows based upon business requirements\nCreated custom permission schemes, notification schemes, screens, and similar configuration changes on a project-by-project basis.\nProvided User Management and support for 1000+ local and remote users, manage system access across groups to ensure compliance, and maintain best practices.\nPerformed soft launches for various application using the stage and live routes implementation in OpenShift, which would only be toggled upon successful testing of the staged application.\nInvolved in raising firewall rules by providing necessary source, destination, and ports for various apps.\nMonitoring applications using APM in OpenShift which would require filtering at the pod\u2019s environment variables level. \nMaintained high availability clustered and standalone server environments and refined automation components with scripting and configuration management (Ansible) and experienced in writing Ansible scripts.\nUsed Ansible tower to run various job templates for onboarding applications and to generate the required com configs where necessary resources can be set for each environment for an app.\nInvolved in integrating taurus plugin with Jenkins for functional and performance test automation.\nInvolved in editing the existing ANT/MAVEN files in case of errors or changes in the project requirements.\u00a0\nUsed visual studio code for integrated development environment to run and execute apps.\nPerformed route switching from stage to live using job templates in ansible tower to switch traffic.\nWork with development/testing, deployment, systems/infrastructure and project teams to ensure continuous operation of build and test systems.\n\nEnvironment: Azure, Openshift, Ansible, JBOSS, ANT/MAVEN, JIRA and Confluence, Bamboo, SVN, GIT, Bitbucket, SDK.\n\nFreddie Mac, McLean, VA                                 \t\t\t\t\t\t\tJUN 2018 \u2013 JUN 2019\n\t\t\t\t\t\t                                                  \nRole: Sr. Systems Administrator\nResponsibilities\nResponsible for Installation, Configuration and Administration of WebSphere Process Server 7.0 including tracking of where the current business process is and troubleshooting issues with other services.\nInstalled, configured, administered and supported WebSphere Application Servers 7.0 on Linux and Windows.\nInstalled Fix packs, Cumulative Fixes and Patches on the Base and ND Versions.\nInstallation of WebSphere and IHS through silent installation using response files.\nCreated the WebSphere Process Server Cluster (Golden topology), ME Cluster, and CEI Cluster and performed a manual install of CEI Applications and changes on data sources.\nResponsible for day-to-day build and deployments in pre-production and production environments.\nOperating and maintaining multiple Data Centers including data clusters across data centers.\nBuild out server automation with Continuous Integration - Continuous Deployment tools like Jenkins/Maven for deployment and build management system.\u00a0\nConfigure Elastic Load balancer (ELB) including high availability of ELB using various subnets in various availability zones, configured security settings and health check for application.\nActing as coordinator during the process of Troubleshooting.\nInvolved in AWS EC2, VPC, S3, SQS, SNS based automation through Terraform, Ansible, Python, and Bash Scripts. Adopted new features as they were released by Amazon, including ELB & EBS.\nInvolved in configuring AWS S3 versioning, lifecycle policies, backup files and archive files in the glacier.\nWritten Templates for AWS infrastructure as a code using Terraform and CloudFormation to build staging and production environments.\nWorked on Azure Fabric, Micro services & Docker Containers in Azure. Ability to create scripts using Azure PowerShell during automation and built-in process.\nUsed cloud\u00a0providers and API\u2019s for Amazon (AWS) with Microsoft Azure and expertise to orchestrate the jobs to and from Azure Automation.\nHands-on AZURE, Migration of all servers from on-premises to Kubernetes containers, writing the scripts in Shell Scripts for managing various enterprise applications.\nWorked on middleware services JDBC, SSL certificates in JBOSS.\nInstalled, configured, and maintained web servers like JBOSS, Apache Web Server.\u00a0\u00a0\nCoordinate/assist developers with establishing and applying appropriate branching, Labelling /naming conventions using GIT source control.\nDesigning and implementing container orchestration systems with Docker Swarm and Kubernetes.\u00a0\nWorked on Docker hub, creating Docker\u00a0images and handling multiple images primarily for middleware installations and domain configurations.\nWorked on Docker\u00a0container snapshots, attaching to a running container, removing images, managing directory structures, and managing containers.\u00a0\t\n\nEnvironment: GIT, Jenkins, JBOSS, AWS, Azure, Shell script, Terraform, Maven, Docker, Kubernetes, Apache Tomcat, New Relic, Splunk, Jira, Confluence.\n\t\t\t                                             \t\nClient: SunTrust Bank      \t\t\t\t\t\t\t\t          SEP 2017 \u2013 APR 2018                                                                       \nSystems Administrator\nResponsibilities\nResponsible for Installation, Configuration and Administration of WebSphere Process Server v6.2 and WebSphere Application Server v7.0  \nInstalled and configured clustered 64bit-silent WebSphere Process Server v6.2.0.3 successfully on Red Hat Linux.\nInstalled and Configured IHS 6.1 and ran multiple instance of IHS for Log Viewer and \t\t       Application Requests\nUsing Build Forge 6.1 (Automation tool from IBM) involved in creating the WebSphere Process Server Cluster (Golden topology), ME Cluster, and CEI Cluster and Application cluster.\nConfiguring WebSphere resources like J2C, JMS, JDBC, Resource adapters, mail providers and shared libraries.\nDeveloped Jython scripts on performing the configuration\u2019s like MQ Queues and Queues Connection Factories, JVM Custom Properties\nTuned Activation specifications, JDBC Connection Factory\u2019s for long running BPEL\u2019s in WebSphere Process Server clustered environment.\n Configured of BPE container and Task Container using JACL Scripts in WebSphere Process Server clustered environment.\nConfigured Business Rules Manager, Relationship Manager and Task manager in WebSphere Process Server clustered environment.\nConfigured Business Process Choreographer and Common Event Infrastructure in WebSphere Process Server clustered environment.\nConfigured of Business Rules using Scripts in WebSphere Process Server clustered environment.\nPrepared BPC Task Exceptions using BPC Explorer in WebSphere Process Server clustered environment.\n Handled long running BPEL and Human Task instances by using BPC Explorer in WebSphere Process Server clustered environment.\nUsing Tivoli Provisioning Manager (TPM) performance the maintenance operation\u2019s as Installation of WPS and WAS and IHS, Also for Installing Fix packs, Cumulative Fixes and Patches on the WPS and WAS.\nDeveloped deployment scripts with support from IBM specialist for automating deployment of SOA applications on WebSphere Process Server and WebSphere Application Server environments.\nConfigured message logging and error processing using mediation modules components in WPS.\nResponsible for migration of WPS instance to new multiple Golden Topology environments.\nConfigured Monitoring agents on various Environments\u2019 like ITCAM for WebSphere, CA Wily Introscope and HP Diagnostics.\nInvolved in performance load testing for various applications. As part of the PLT, I was involved in Identifying Application bottle necks using Monitoring tools, Identifying System Bottleneck using NMON and Monitoring tools, Along with new recommendations for Connection Pool size and thread pool size , Analyzing heap dumps and Thread Dumps for any application related issue during PLT\u2019s \nExtensive troubleshooting experience using ISA and Application relevant logs.\nInvolved in Capacity Planning and Architectural decision on Security configuration and SSO.\nWorked in troubleshooting issues with prod and non-prod of WebSphere Process Server logs.\nWorked on SIB Explorer for identifying the queue depth and purging queue data in WebSphere Process Server default buses queues and customized MQ queues.\n\n\nHSBC Group, Hyderabad India     \t\t\t\t\t\t\t         SEP 2014\u2013 AUG 2015\nRole: Java Developer\n\nSYNOPSIS: The Company planned to move all of its public web assets supporting 50 percent of customer traffic and internet banking workloads to AWS in the coming years. The organization also started its examination with AWS Lambda, machine learning, grid computing and data-analytics workloads as part of its digital-transformation journey. \n\nResponsibilities:\nActively participated in the complete Software development life cycle starting from design phase to the implementation phase. Involved in requirements gathering and designed high and low-level designs using UML.\nDeveloped PL/SQL procedures, Java and Backbone Js, JavaScript, JQuery code.\nAnalysis Design and Development, Testing and Production Support.\nDeveloped Web Services for sending and getting data from different applications using SOAP messages, such as Loan Applications to transfer data from Branch Server to Head office Server, then used SAX and DOM XML parsers for data retrieval.\nDeveloped functional model, object model and dynamic model using UML.\nExtensively worked on generating complex reports.\nDeveloped code for various activities using MVC architecture.\nDeveloped client side screen using JSP, HTML and DHTML. \nWorked on development of Hibernate, including mapping files, configuration file and classes to interact with the database.\nInvolved in injecting dependencies into code using spring core module concepts like IOC of Spring Framework\nDeveloped web service (which talks with web method) for uploading CSV (using web methods) files and validating and later inserting into the corresponding tables.\n\nEnvironment: J2EE, MySQL, Spring, Hibernate, JSP, HTML, JavaScript, JQuery, XML (SAX and DOM), RAD, JUNIT, and JNDI.\n",
          "confidence": 1.0,
          "method": "full_text",
          "structured_data": null
        },
        "tax_term": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "source_by": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "designation": {
          "value": "Cloud Infrastructure",
          "confidence": 0.8,
          "method": "regex",
          "structured_data": null
        },
        "experience": {
          "value": "7",
          "confidence": 0.9,
          "method": "regex_total_experience_summary",
          "structured_data": null
        }
      }
    },
    {
      "resume_path": "data\\input\\Adam Turner - .Net - VA.docx",
      "confidence_score": 0.665,
      "used_ocr": false,
      "extracted_fields": {
        "skills": {},
        "first_name": "Adam",
        "last_name": "Turner",
        "primary_email": {
          "value": "Adam.turner.jobs1@gmail.com",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "secondary_email": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "phone": {
          "value": "7176545847",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "city": {
          "value": "Lemoyne",
          "confidence": 0.9,
          "method": "address_pattern",
          "structured_data": null
        },
        "state": {
          "value": "PA",
          "confidence": 0.9,
          "method": "address_pattern",
          "structured_data": null
        },
        "zip": {
          "value": "17043",
          "confidence": 0.9,
          "method": "address_pattern",
          "structured_data": null
        },
        "work_authority": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "resume_link": {
          "value": "data\\input\\Adam Turner - .Net - VA.docx",
          "confidence": 1.0,
          "method": "file_path",
          "structured_data": null
        },
        "raw_resume": {
          "value": "344 Plum St. Lemoyne, PA 17043\nCell Phone: 717-654-5847\nAdam.turner.jobs1@gmail.com  \nAdam Turner\nProfessional Summary\nSenior .net enterprise engineer with over 17 years of industry experience\nExperienced Lead Programmer with end-to-end project lifecycle (SDLC) \nPrimary expertise in full stack multi-tier architecture.\nExperienced Trainer\nComptia Security+ Certified\nExperience\n10/16 \u2013 Present     V Group Inc @ SERS     Harrisburg, PA\nSenior .Net Developer (6-12 month contract) 2 \u00bd years later\nComplete redesign of public facing site https://employers.sers.pa.gov \nDeveloped ASP.net MVC Ad hoc app (AGC) to work with 3rd party CMS product (Process360)\nDeveloped two associated ASP.net Web Forms Applications\nOAP \u2013 Public facing site to apply for agency specific jobs.\nOAPA \u2013 Intranet site to administer the application\nDeveloped JNET WCF SSL Service. Shared code with 2 other agencies and assisted them with setup.\nNew development and maintenance of .Net VTA\u2019s\nDeveloped a WinForms application and .dll to encrypt-decrypt sensitive data i.e. connection strings (AES)\nASP.net Web Forms & SOAP Web Services (ASMX & WCF) Maintenance and Bugs Fixes for intranet app\u2019s (DAWPM & CMS)\nCurrently working with 2 teams for SERIS 2.0 migration as lead .net developer.\nTechnologies used: .Net Framework 2.0/4.0, Visual Studio 2015, VB.net & C#, JavaScript/JQuery TFS 2015 Web & Intergrated, T-SQL/Oracle, XPath, LINQ, AJAX, HTML, CSS, Bootstrap\n\n4/16 \u2013 8/16      Accenture/Navy Depot      Mechanicsburg, PA\n.Net SME/Trainer (6-12 month contract) \nLead Trainer of a 4 week (160 hours) of training class of 9 experienced developers on the following topics in .Net (OOP Concepts and UML, Software Analysis Design & Build, SOLID Principles, Software Development Environment & Tools, Programming Language: C#, Error Handling/Assertions, Reuse: Procedural Classes & Objects, Programming Inheritance, Programming Polymorphism, Programming Collections, Programming Exception Handling, OOP Paradigm, Unit Testing, Assembly and Production Testing, Database Input/Output, Database Overview, Getting Started with LINQ, Getting Started with Entity Framework, Database Connectivity, Elements of Enterprise Architecture, Enterprise Architecture Patterns, Web Based Programming, XML & XSLT, MVC, Securing Applications, Working with MVC Models, Designing ASP.NET MVC Application Architecture, Coding Fundamentals for C#, Define, Revise, & Prepare Unit Test, And a few other smaller topics\nFollowing training, I'm provided over the shoulder support for the entire depot regarding .net technologies. (85+ developers)\nPersonally created 6 hours of Powerpoint training slides for an Advanced Javascript class (over 500 slides).\n\n1/16 \u2013 4/16      NRA/Appalachia Technologies      Harrisburg, PA\nSenior Software Developer (6-8 week contract) \nNew Development on VB.net WinForms Application\nTechnologies include: VB.net, SQL Server 2008R2, LINQ, CTE, GIT, XML\n\n3/14\u201307/15   Deloitte   Camp Hill, PA\nSenior Software Developer (Phase 1 Complete) \nWorking Lead developer/mentor of 5+ developers on a team of 50+\nConsultant for new development on CWIS project\nExpert on LiveCycle/XML Correspondence Generation\nAwarded 3 Performance Awards\nTechnologies include: Oracle SQL, Bootstrap, Asp.net Webforms, C#, VB.net, jQuery, AngularJS, WCF, TFS\n\n4/13\u201311/13   RR Donnelly   Lancaster, PA\nSenior Software Developer (6 month contract)\nMaintenance and enhancements on intranet ASP.net/c# web applications.\nMaintenance and enhancements on WCF/Windows Services\nWCF, Windows Services, XML, XSL, XSLT, .NET 2.0-4.0, C#/VB.net, Javascript, AJAX, LINQ2SQL, ADO.net, SQL Server 2005-2008, MSMQ, SOA, ASP.net, IIS6, TFS\n\n3/12\u20134/13   PA Dept. Of Transportation   Harrisburg, PA\nSenior Software Developer on the MM team (1 year contract)\nSenior Software Engineer on the .net Managed Maintenance team.\nLead/mentor for 5+ mid-level/junior developers.\nPrimary Lead developer supporting 10+ applications for 1,000+ users\nExpert in Windows7 migration - Primary engineer for all departmental application migration from WindowsXP.\nPerformed documentation including, but not limited to, requirements, function point counting, user manuals, technical, and AKT's (Application Knowledge Transfer)\nTechnologies: ASP.net, SQL Server, VB.net, C#, Web Services, MS Access, Javascript, AJAX, XML, Crystal Reports(BOBJ/BOXI)\n\n12/10\u20132/12   PA Dept. Of Health   Harrisburg, PA\nSenior Systems Engineer (Contract Team Replaced with internals)\nMaintenance and new development on QuickWIC intranet site\nMaintained public facing Vendor Assistance website\nSupport and development on CheckScan and MICR readers for vendors.\nGeneral support and development for various departmental applications.\nSupporting 250+ ad hoc laptop/tablet synchronization for satellite clinics.\nDesigned adhoc application for laptop monitoring/tracking.\nSQL Server 2005/2008, ASP.net 3.5, VB.net, XML, Linq2SQL, DHTML, Javascript, OOP, MS Access 2007\n\n11/09\u201310/10   Wells Fargo N.A.   Phoenix, AZ\nSenior Systems Engineer (Contract terminated - Position eliminated)\nMaintained existing MS Access 97/2000 applications during migration process with Wachovia\nMaintained vb.net winforms financial application supporting 1,000 users\nLead SQL Server 2005 Developer for back-end systems\nSolely responsible for development and support of 3 applications that support 2,000+ users\nIntegral role in full SDLC business logic and ad-hoc development for financial services department.\nMinor Oracle queries accessed through a datalayer.\n\n4/09\u201310/09   Berry Wave   Phoenix, AZ\n.Net Architect/Engineer (6 Month contract)\nAd-Hoc software development\nN-tier Architecture design\nVS 2008 .net 3.0 Win/Web Forms development/C#\nDeveloped an FTP job application (winforms C#)\nSQL Server DBA\nDeveloping an application to communicate via text messaging bidirectionally with a PC\nASP.net(Linq to SQL/JQuery) registration site\n\n04/08\u201303/09   American Express   Phoenix, AZ\n.Net Windows Developer (6-12 month contract)\nAutomated manual processes via .net Windows Services and console applications with VB.net 2.0/3.5 Framework\nDeveloped and maintained ad hoc and existing ASP.net intranet pages supporting 50+ users - C#\nDeveloped and maintained .net 2.0/3.5 3-tier WinForms applications that communicated with Unix Servers\nLead developer in SQL Server 2000/2005 optimizations in a highly transactional batch processing environment. (execution plans, transactional control, normalization, etc\u2026)\nLead developer in investigating and optimizing existing application performance.\nIntegral author in documentation and enforcement of programming standards\n\n01/07\u201302/08   Salt River Project   Phoenix, AZ\nContract Software Engineer (6 month contract)\nMigrated all functionality from several MS Access environments to SQL Server 2000/ASP.net(C#) \nSolely redesigned all project management proprietary/legacy software for the Project Services department into a single vb.net application.\nConverted paper contract system into an ASP.net/C# environment.\nAutomated manual validations that reduced man hours from hours into minutes\nProvided thorough documentation to management that outlined both high level and low level design.\nMigrated 4 VB6 applications to VB.net Winforms\nMinor interaction with Oracle\n\n03/06\u201310/06   OEM Logistics Support Inc.   Tempe, AZ\nIT Manager/Software Developer (replaced in merger)\nMigrated MS Access inventory management system into SQL Server 2000/ASP.net C# Win/Web forms.\nSQL Server Administrator (DBA) \u2013 administrate and maintain SQL Server 2000\nNetwork Administrator \u2013 administrate and maintain 150+ employee workstations, 2 Windows 2003 servers, and an in-house IIS web server\nMS Project Server Administrator \u2013 configure and maintain MS Project Server\nSoftware Developer \u2013 developed a company intranet from a nonexistent environment through JAD sessions\nAutomation designer \u2013 automated all communication for Boeing procurement processes.\nTelco telephone administrator \u2013 responsible for all telephone related setup and troubleshooting.\nWebmaster and designer for www.kitmyschoolsupplies.com (currently removed)\nMigrated VB6 applications to VB.net \n\n10/04\u201303/06   Global Fulfillment Services, Inc.   Phoenix, AZ\nProgrammer Analyst/Code Approver (Co. relocated to Minnesota)\nDeveloped stand alone VB6 WinForms applications that generated reports in MS Excel, MS Word, (CDO objects) for MS Outlook.\nReviewed and approved code to be released into production for 13 analysts\nAutomated repetitive tasks with VB6 applications and VBA macros that heightened the throughput of production while diminishing the common human errors.\nUsing SQL Server 2005 technologies, investigated and resolved batch exceptions, reported potential problem areas, and performed general SQL related tasks in response to immediate needs. Complex T-SQL.\nCode limits, compliance, and conditions to ensure rebate offers were paid/unpaid in accordance with client specifications.\nAlpha tested final offer to confirm all necessary flags are thrown prior to implementation.\nModified existing C# code to adjust batch importer file.\n2.0 Framework\n\n04/02\u201302/04   University of Phoenix Online   Phoenix, AZ\nJunior Software Engineer (Left for Analyst Position)\nMaintained C# .net WinForms applications using ADO.net, OOP, Regular Expressions, and mild T-SQL 2000/Oracle.\nMaintained ASP.net intranet using HTML, XML, Javascript and mild T-SQL 2000.\nIntegral involved in migrating VB6 applications into .net\nWrote VBA macros for various MS Office applications for management.\nReceived an award for outstanding achievement for the 3rd quarter of 2002.\nParticipated in iHeat Beta Testing, TechWeb2 Focus Group, and the Supervisory Interview Panel.\nEducation\n2016 COMPTIA Security+ Certified\nhttps://www.certmetrics.com/comptia/public/verification.aspx?code=7WS52S5E3DE1Q2D4\n1999-2002   DeVry University   Phoenix, AZ                               \nBachelor\u2019s in C.I.S.\n\n",
          "confidence": 1.0,
          "method": "full_text",
          "structured_data": null
        },
        "tax_term": {
          "value": "CONTRACT",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "source_by": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "designation": {
          "value": "Application Architect",
          "confidence": 0.8,
          "method": "regex",
          "structured_data": null
        },
        "experience": {
          "value": "17",
          "confidence": 0.9,
          "method": "regex_total_experience_summary",
          "structured_data": null
        }
      }
    },
    {
      "resume_path": "data\\input\\Data Entry - PA - Christopher.docx",
      "confidence_score": 0.6727142857142858,
      "used_ocr": false,
      "extracted_fields": {
        "skills": {
          "soft_skills": [
            {
              "name": "organization",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                397
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "planning",
              "confidence": 0.49999999999999994,
              "context": "education_section",
              "positions": [
                230
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "domain_specific": [
            {
              "name": "sales",
              "confidence": 0.96,
              "context": "experience_section",
              "positions": [
                986
              ],
              "experience_weight": 0.2,
              "importance_score": 0.7
            },
            {
              "name": "finance",
              "confidence": 0.5,
              "context": "education_section",
              "positions": [
                57,
                421,
                668
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "education",
              "confidence": 0.5,
              "context": "education_section",
              "positions": [
                511,
                610
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "frameworks": [
            {
              "name": "spring",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                1378
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "networking_equipment": [
            {
              "name": "accounting",
              "confidence": 0.49999999999999994,
              "context": "education_section",
              "positions": [
                284
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ]
        },
        "first_name": "Christopher",
        "last_name": "T. Krozel",
        "primary_email": {
          "value": "Chris69ss350@verizon.net",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "secondary_email": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "phone": {
          "value": "8148865626",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "city": {
          "value": "Gallitzin",
          "confidence": 0.9,
          "method": "address_pattern",
          "structured_data": null
        },
        "state": {
          "value": "PA",
          "confidence": 0.9,
          "method": "address_pattern",
          "structured_data": null
        },
        "zip": {
          "value": "16641",
          "confidence": 0.9,
          "method": "address_pattern",
          "structured_data": null
        },
        "work_authority": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "resume_link": {
          "value": "data\\input\\Data Entry - PA - Christopher.docx",
          "confidence": 1.0,
          "method": "file_path",
          "structured_data": null
        },
        "raw_resume": {
          "value": "Christopher T. Krozel\n731 Tunnelhill St.\nGallitzin, PA 16641\n(814)-886-5626\nEmail: Chris69ss350@verizon.net\n\nObjective:  To earn a position that will enable me to utilize my finance, management, accounting, and analytical skills, along   with my prior job experience with the hope of possibly advancing in the future. \n\nEducation:\n\nAugust 2003-May 2005\nSaint Francis University, Loretto, PA\nFinance Major.  Development of numerous professional skills that would enable me to be a pleasant and welcomed addition to any organization, including skills for investing and planning for the future, in addition to management and accounting.  I graduated with the title of Cum Laude in May 2005 with a QPA of 3.681.\n\nAugust 2001-May 2003\nPenn State University, Altoona, PA\nFinance Major.  At Penn State I was originally an undeclared major, and completed my general education requirements including: English, Speech, Calculus, Art, Biology, History, etc.  During my general education classes I developed an interest in the business/finance field which I decided to pursue at Saint Francis University in the fall of 2003.\n\nEmployment:\n\nCustomer Service / Account Representative\nNovember 2006-present\nMeadWestvaco/Cenveo Williamsburg, PA\nResponsibilities included customer service, order entry, inventory maintenance, price quoting, and purchasing.  Specialized in high profile customers that required expedited services.  Regularly backed up coworkers\u2019 accounts and their responsibilities.  Developed excellent professional skills with a focus on organization, accuracy, efficiency and proper customer etiquette.  \n\nGrounds Crew\nApril 2000-present\nBlair County Ballpark, Altoona, PA\nResponsibilities include maintenance and preparation/repair of a professional baseball playing surface.  Experienced with radar observations along with storm preparation and recovery efforts.  I also acquired skills providing weather front updates to umpires, players, and managers, while also forming pleasant and lasting relationships with fans.\n\nData Entry/Stock Specialist\nFebruary 2001-present\nSehrer Mill & Hardware, Gallitzin, PA\nPossess 5 years of experience in the sales and cashier field.  Main skills lied in product stocking and distribution, inventory maintenance, and data entry.  Acquired skills in solving problems quickly and efficiently while improving the overall shopping experience for customers.\n\nAcademic Accomplishments:\n\nRegularly achieved both the Penn State and Saint Francis Dean\u2019s lists\nInducted into the Penn State University Honor Society in the spring of 2003\nNamed to Who\u2019s Who Among American College Students in 2003\nAgain, named to Who\u2019s Who among American College Students in 2004\nInducted into the Saint Francis University Honor Society in 2005\nGraduated Cum Laude with a 3.681 QPA\n\n\n(References Available Upon Request)",
          "confidence": 1.0,
          "method": "full_text",
          "structured_data": null
        },
        "tax_term": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "source_by": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "designation": {
          "value": "it",
          "confidence": 0.8,
          "method": "regex",
          "structured_data": null
        },
        "experience": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        }
      }
    },
    {
      "resume_path": "data\\input\\Deborah Jefferson - BA - GA.docx",
      "confidence_score": 0.5750000000000001,
      "used_ocr": false,
      "extracted_fields": {
        "skills": {},
        "first_name": "DEBORAH",
        "last_name": "JEFFERSON",
        "primary_email": {
          "value": "medidata15@gmail.com",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "secondary_email": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "phone": {
          "value": "8032383169",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "city": {
          "value": "Savannah",
          "confidence": 0.7,
          "method": "ner",
          "structured_data": null
        },
        "state": {
          "value": "GA",
          "confidence": 0.9,
          "method": "zip_database",
          "structured_data": null
        },
        "zip": {
          "value": "31406",
          "confidence": 0.7,
          "method": "regex",
          "structured_data": null
        },
        "work_authority": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "resume_link": {
          "value": "data\\input\\Deborah Jefferson - BA - GA.docx",
          "confidence": 1.0,
          "method": "file_path",
          "structured_data": null
        },
        "raw_resume": {
          "value": " DEBORAH JEFFERSON\nmedidata15@gmail.com \u2022 Savannah, GA 31406 \u2022 803.238.3169 \nA detail-oriented, driven, and analytical Analyst with a meticulous eye for detail, a diversified skill set, and an aptitude for project management, analytical thinking, creative problem solving, and data claims analysis, investigations, administration, and implementation. Adept in customer service, six sigma concepts, business intelligence, and creating mutually beneficial relationships with clients and stakeholders. Well-versed in healthcare services, Medicare fraud, and fraud waste; capable of completing all industry related tasks through effective time management, judgement, confidentiality, and prioritization skills. Communicative and attentive, able to work in alone or in teams, learn new practices and procedures, and adapt to new environments easily and quickly with absolute professionalism\nDatabase Management \u2022 Business Finance \u2022 Regulatory Compliance \u2022 Project Management\u2022 Performance Optimization \u2022 Time Management \u2022 Data Review (AHIMA) & Statistical Analysis \u2022 Business Requirement Documentation \u2022 Complex Problem Solving \u2022 Interpersonal Skills \u2022 Verbal & Written Communication \u2022 Cross-functional Team Leadership\u2022 End-to-End Training \u2022 Presentation Skills \u2022 Managed Care \u2022 Fraud Waste & Abuse \u2022 Data Claims Analysis (AHIMA) \u2022 \n\nBUSINESS ANALYST IV, PA Health and Wellness, Pittsburgh, PA\t6/2017 to 11/2019\nExecuted various duties including, but not limited to, gathering internal business requirements, extracting and translating data into practical formats, conducting UAT, and managing reconciliation tasks and/or analytical projects from front to back end. Reviewed all routine state reports, prepared Business Requirement Specifications, and ensured timely delivery and compliance of all reports, requirements, and conditions with state and contractual regulations. Collaborated with functional internal coordinates and data analysts to develop SOPs, resolve all identified issues, and implement associated projects. \nActed as a Subject Matter Expert; responsible for facilitating the initiation, realization, and completion of various company projects (in collaboration with the Director and departmental Vice President).  \nDeveloped and implemented a validation process that ensured the smooth execution of reconciliation processes, management of BH/PH files, and extraction of data as per state regulations. \nGuaranteed timely deployment of data by partnering with data professionals to research and formulate criteria for Omni configuration requests. \nSuccessfully finalized acceptable extracted data after administering the configuration of the disenrollment/enrollment file.\nImproved organizational workflow by redesigning business requirement specification forms and designing data change request forms. \nBUSINESS ANALYST CONSULTANT (REMOTE), Optuminsight, Eden Prairie, MN\t4/2015 to 2/2016\nEmployed health plan data knowledge, proficiency in medical terminology, and effective communication skills to enroll members, formulate data driven metrics, and support management teams, clients, and the EDIS Data Exchange by translating, integrating, and evaluating data acquired from several SQL dba resources. Partnered with internal project managers to guarantee adherence to timeline stipulations. Executed contract population mapping, Input Specifications and NCQA measure summary analyses HEDIS/STARS data validations, and other systemic assessment to identify software errors related to data output. \nActed as a SME; examined client questions with the aim of resolving data issues, meeting allotted timelines, and adhering to NCQA measures.\nParticipated in daily and routine meetings with external clients, internal departments, and members of the medical coding team; to discuss and guarantee delivery of results that are consistent to client\u2019s needs.\nBoosted key performance measurement results pertaining to after identifying gaps and implementing interventions.\nIncreased understanding of supplemental HEDIS data concepts by formulating working knowledge of STARS contracts. \nFRAUD WASTE AND ABUSE PROJECT MANAGER, McKesson Health Solutions, San Francisco, CA\t\t        \t5/2012 to 2/2015\nOversaw several different tasks that pertained to the development of a standard gathering requirement template for the Service Center of Excellence. Cooperated with developers to assess FWA predictive analytic results and improve processes by determining false-positive data outcomes. Interfaced remotely with executives, business and financial risk analyst, managers, offshore medical coding teams, and instructional designers to conduct daily and weekly meetings with internal teams. Collated and analyzed data to develop Advisory Change Memos (ACM) for the offshore claims triage team; which resulted in the implementation of policy changes. \nAfter one year of employment, promoted from a Contracted Business Analyst to a Project Manager.\nProvided subject matter expertise during data mapping workshop and configuration workshops, developed implementation best practices, tools and documentations and critically evaluated information from multiple sources.\nActed as a Liaison between internal and external customers and the product software delivery team; supported sales product and data analysis presentations.\nFunctioned as a Trainer; created program materials, job aides, syllabus, and webinars to provide detailed instruction to clients on appropriate software use, ensure validity of analytic data, observe triage medical coders workflow process, and provided best practices feedback to FWA management team.\nDecreased the rate of false positive referrals by 89% after identifying gaps in analyses and reconfigured job aide.\nDATA ANALYST, Select Health of South Carolina, Charleston, SC\t\t\t\t\t                4/2010 to 4/2012\nExecuted a collection of tasks, associated with collation of data, development of customized reports, and preparation of analytical needs for corporate audits, HEDIS and the performance of qualitative and quantitative studies. Regulated queries following the creation of maps and measurement flowcharts, assessment of data, and the identification of data trends. Partnered with inter-departmental personnel to investigate, examine, and validate the accuracy of all data results. \nTrained outsourced LPNs and RNs on the fundamentals of data entry, scanning, and document transfers of a HEDIS project.\nEffectively re-tooled provider credentialing and delegated efforts to comply with NCQA standards.\nContacting +71K members, via sound bite reminders and incentive letter, to accurately identify non-compliant members for HEDIS outreach campaign program.\nSuccessfully tracked 12-month rolling HEDIS measures YTD from AIG software for Managed Care Medicaid programs.\nOther experiences as a Medicare Fraud/Data Analyst for Palmetto GBA/BCBS of SC, Columbia, SC\n\nBachelor of Arts, Fine Arts, Emmanuel College, Boston, MA\nAssociate of Arts, Computer Science, Limestone College, Gaffney, SC\nProfessional Memberships\nAmerican Health Information Management Association (AHIMA) \n",
          "confidence": 1.0,
          "method": "full_text",
          "structured_data": null
        },
        "tax_term": {
          "value": "CONTRACT",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "source_by": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "designation": {
          "value": "BUSINESS ANALYST",
          "confidence": 0.9,
          "method": "ner",
          "structured_data": null
        },
        "experience": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        }
      }
    },
    {
      "resume_path": "data\\input\\Finance Manager - NC - Tamika.docx",
      "confidence_score": 0.5950000000000001,
      "used_ocr": false,
      "extracted_fields": {
        "skills": {
          "domain_specific": [
            {
              "name": "finance",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                38,
                2926,
                5661,
                6834,
                7823,
                9073,
                9512,
                9620,
                11024,
                12399
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "compliance",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                348,
                3614,
                4280,
                4996,
                5137,
                8772,
                10731
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "contracts",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                2824,
                4744
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "banking",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                3221,
                4787
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "government",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                3654,
                4324
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "legal",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                4629
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "security",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                5414
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "sales",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                6374,
                6492,
                6596,
                10352,
                10415
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "industrial",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                9418
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "education",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                12303
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "soft_skills": [
            {
              "name": "analysis",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                703,
                916,
                5515,
                5778,
                6040,
                6602,
                7045,
                7865,
                8195,
                9125,
                9402,
                9930,
                10566,
                11078,
                11406,
                11882,
                12036
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "leadership",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                1174,
                1199,
                6401,
                6887,
                10930
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "documentation",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                3426
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "negotiation",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                4453,
                5375
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "planning",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                5769,
                5875,
                7856,
                11434
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "innovation",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                6363
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "organization",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                7139
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "training",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                7450,
                12212
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "research",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                11483,
                11781
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "business_skills": [
            {
              "name": "reporting",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                716,
                2428,
                5895,
                6124,
                11186,
                11511,
                11771
              ],
              "experience_weight": 0.0,
              "importance_score": 0.8
            }
          ],
          "data_skills": [
            {
              "name": "excel",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                2324,
                12192
              ],
              "experience_weight": 0.0,
              "importance_score": 0.9
            }
          ],
          "cloud": [
            {
              "name": "functions",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                2934
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "networking_equipment": [
            {
              "name": "accounting",
              "confidence": 0.8,
              "context": "experience_section",
              "positions": [
                4639,
                9081,
                9648,
                9674,
                10056,
                11032
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ]
        },
        "first_name": "Tamika",
        "last_name": "L. Porter",
        "primary_email": {
          "value": "tmkporter@gmail.com",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "secondary_email": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "phone": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "city": {
          "value": "Charlotte",
          "confidence": 0.7,
          "method": "ner",
          "structured_data": null
        },
        "state": {
          "value": "NC",
          "confidence": 0.9,
          "method": "zip_database",
          "structured_data": null
        },
        "zip": {
          "value": "28215",
          "confidence": 0.7,
          "method": "regex",
          "structured_data": null
        },
        "work_authority": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "resume_link": {
          "value": "data\\input\\Finance Manager - NC - Tamika.docx",
          "confidence": 1.0,
          "method": "file_path",
          "structured_data": null
        },
        "raw_resume": {
          "value": "\nTamika L. Porter, MBA\nCharlotte, NC 28215 |Phone: (407) 879 -1819 | Email: tmkporter@gmail.com \nSummary:\n*A multi-discipline Corporate Finance leader with 15+ years of global experience in the manufacturing and financial services industries.  * Drive strategic business results and profit margins utilizing trend and competitive impact analysis.   * Enterprise-wide consolidation of financial forecasting, budgeting and capital investment.  *Oversee daily accounting and cash functions including financial reporting, P&L and Balance Sheet analysis, and financial modeling.  * Own start-to-end budgeting, actual and forecast.  * Business partner to Executive Management providing recommendations, insights and best practices.  * Highly organized with critical thinking abilities at both the strategic and tactical levels. * Strong presentation skills and ability to navigate at all levels in the organization * Executive level reporting and collaboration on risk mitigation and opportunity assessments * Manage teams through strong leadership skills and employee development. * Power Excel user building financial models using sensitivity analysis, data consolidation and VBA to automate repetitive actions and functions * Also skilled in pivots tables, logical/lookup functions, conditional formatting and advanced charting.  Yellow belt trained and certified, Lean Six Sigma and Agile Principles.\nFinancial Systems:\nSAP Front End Portal\nSAP Business Object\nSAP BI/Warehouse \nSAP BEX Analyzer \nOracle Hyperion Planning \nOracle Hyperion Smart View \nOracle Financial Mgt (HFM) \n\nOracle Essbase/Workspace\nOracle Discoverer\nQlikView Dashboard\nHP PPM Capital/Project Mgt \nPPM Capital System\nMicrosoft Visual Basic\nSalesforce\n\nLIBRA \nTreasury Connect\nSTAMP Project Mgt Tool\nWARP System\nIBM Lotus Notes\nYellow Belt Certified\nAgile Principles\n\n\nProfessional Experience:\nBank of America (NTT Data), Charlotte, NC\t\t\nFinance Manager, Traded Product Liquidity Management\t              \t                                                  10/20 \u2013 Present\t\nManage and consolidate the daily liquidity figures from multiple broker dealers across Global Liquidity.\nSupport CFO-Treasury Executive Operating Review by creating and providing deck materials.\nParticipate in periodic corporate audits to ensure compliance with Treasury Risk standards/corporate policies.\nProcess Improvement \u2013Partner with Treasury IT as business needs change on metadata revisions, test requirements and enhanced specifications to create a customizable view in LIBRA/Treasury Connect.\n\nWells Fargo Bank (Randstad), Charlotte, NC\t\t\nFinancial Strategy Consultant                           \t\t\t              \t                              \t        9/18 \u2013 09/20\t\nLed the consolidation, analysis, and reporting of monthly actual and forecasted performance for a suite of complex projects under a high-profile federated program with total value of $220M.\nDeveloped financial models and directed variance analysis for A/R collections and capital expenditures.\nLed development of the annual business and long range plan, providing support preparation of action plans and recommendations for business and operational decisions.\nPrepared operational reviews for senior leadership and supported leadership presentations discussing the financial/resource position of Program.\nPrepared and delivered performance appraisal for a staff of 5.  Mentored and coached team members to further develop competencies. \nRecommended educated and measurable steps to achieve overall business growth, while focusing on a multitude of individual and overlapping projects.   \nCreated financial and statistical models to show executives the return of certain investments or other financial recommendations including making changes to internal operations, suggestions in product development or repricing strategies.\nConducted regional financial evaluations of proposed projects and investments by gathering assumptions, making financial projections and creating regional business cases that substantiate the rationale for certain investments.                                                                                                       \nIdentify, forecast and challenge financial and resource assumptions not aligned with Program guidelines.\nIdentify funding gaps and work with program sponsor to determine if funds can be re-appropriated.  \nProcess Improvement \u2013 Revamp the financial process by leading the automation of month end reports using Advanced Excel techniques.  Worked with IT to merge synergies from different ERP Systems to produce a consolidated reporting view and provide a solid source of truth.  Vendor Management process revamp for timely invoice and time sheet submissions.  Frequent communication with vendor on PTO timing/employee conversions/new hires and roll-offs to help with forecasting accuracy.  Proposed automated invoicing options through AP versus manual processing. \n\nBabcock & Wilcox, Charlotte, NC \u2013 Corporate Downsizing\t\nGlobal Treasury Contracts Securities Manager, Corp Headquarters\t              \t                              \t          8/17 \u2013 7/18\t\nManaged all key global treasury contract and finance functions and relationships with banks and other 3rd party financial providers.  Managed a $1.6B portfolio of company project surety instruments including surety bonds, letters of credit and bank guarantees with foreign currency exposure management. \nManaged cash flow forecasting, investments, banking, and oversight of 3rd party relationships.\nLed efforts between staff and domestic and international contracting officers on all contractual matters in order to maintain consistency in communications, documentation, and minimizing cost risk due to misunderstandings and/or non-compliances\nResponsible for directing all contract activities to ensure that contractual obligations are met and compliance is maintained with mandatory government regulations, applicable laws and Corporate policies/procedures\nOversaw and administered contractual provisions by directing appropriate departmental staff and by project management of the organizations to ensure supporting established requirements\nPartnered with the Treasurer on strategizing critical business needs and analytics; Managed 2 Treasury Analysts.  Manage subordinate staff in day to day operations as well as personnel actions.  Line management responsibility for timesheet and expenses approval, PTO balances and annual PDRs.\nCoordinated, reviewed, and provided approval on all contractual documents to ensure compliance with the proposal and applicable government regulations and laws.\nBusiness Partner to Commercial Execs, Contract Managers, Attorneys representing contractors in the negotiation of bank guarantees, surety bonds, letter of credits, parent company guarantees.  Provided harmonization and extensive coordination with subsidiaries, global treasury ops, legal and accounting departments and financial institutions.\nReviewed and advised commercial terms of proposals and contracts and work closely with surety and banking relationship and senior management team for issuance.\nInvolved in Project Life cycle, negotiations and Language Terms from Proposal to Contract to Project Lifecycle.\nManaged quarterly and annual covenant compliance for debt & credit facilities, evaluating financial and operating information to support 10Q debt portfolio Calculations and covenant compliance mandated by SEC.\nForeign currency exposure management; Verify correct FX rates are applied to the securities based on monthly projections and industry changes.\nProcess Improvement - Partnered with company Attorney in the successful negotiation of terms and closeout of a security that was held in litigation.  Partnered with CFO & restructure consultant by providing critical analysis, industry knowledge and decision making for international project in risk of being defaulted.\n\nSealed Air, Charlotte, NC \u2013 Corporate Downsizing\t\t\nFinance Manager, Global R&D, Corp Headquarters\t\t              \t                                                        1/16 \u2013 7/17  \t\nResponsible for all output associated with Financial, Planning & Analysis (FP&A) including but not limited to annual budget preparation, monthly forecasts, strategic planning, management reporting, and special projects to the Global R&D department with total company revenue of $7B dollars. \nReviewed and approved balance sheet account analysis, statement of cash flows and supporting schedules and standard consolidated reporting packs as well as HFM monthly journal entries.\nProvided guidance and counsel in development of long range plans involving strategy development, resource allocation, work plans and timelines and financial outcomes.\nManaged the monthly Innovation Sales Report and Executive Leadership Quarterly deck; Analyzed total company key performance metrics and project level sales profitability & validity by country, product line and sku number extracting and building informative sales analysis via macros.  \nOversaw financial aspects of vendor negotiations, funding issues and related efforts.\t\nPrepared and delivered performance appraisal for a staff of 5.  Mentored and coached team members to further develop competencies. \nFinance business partner to the Global R&D Executive leadership team communicating trends and impacts, risk and opportunity assessments and resolve exceptions.\nLed monthly operational reviews and present variance analysis of actual results versus budget, forecast, and prior year at varying levels of the R&D organization and across different information systems.\nMaintained fixed asset and depreciation schedules and request capital as needed during funding reviews.  \nProcess Improvement - Efforts contributed to accelerating the monthly forecast from the 7th to the 5th calendar day of the month.  Developed and led end user training for the SAP rollout as well as the Capital/Project Management investment spending and tracking system to Global R&D users.\u00a0\u00a0Support R&D Functional expense review by having one on one reviews with VPs of R&D for actuals review, forecast recommendations, best practices and provide financial guidance to help drive decisions.\n\n\nUnited Technologies Corporation, Charlotte, NC\t\t\nFinance Lead, Corporate Financial Planning & Analysis\t\t\t\t                    11/12 \u2013 12/15 \t\nProvided financial and operational expertise for a heating/cooling company with yearly revenues of $800M; Total Company revenue $65.1B.  Directly supported the VP Ops, CFO, Functional Mgrs., and Corp HQ.\nBusiness partner to Functional Managers leading calls in the monthly closing process for reserves, accruals & expense analysis as well as monitoring overhead budgets.\nMaintained fixed asset and depreciation schedules and oversaw cash management strategies requesting capital and preparing distributions as needed.\nPrepared annual Plan and monthly forecasts including identifying risks and opportunities and loading results to HFM.\nManaged the incentive accrual/payout and review results with Incentive Committee (i.e. VP OPS, HR & Acct).\nManaged the monthly Capital Expenditure process and IT Project Spend and analyze and forecast risks and opportunities.\nParticipated in periodic branch audits to ensure compliance with standards and corporate policies.\nProcess Improvement - Developed, identified and implemented improvements in current processes through the use of ACE, Kaizans, VSMs and seminars.  Efforts contributed to accelerating the monthly forecast from the 7th to the 5th calendar day of the month.  Lead Finance & Accounting SAP Expert providing support and analysis to onsite Operations team.  Participated in a forecast Kaizan and Incentive Management Kaizen to improve results and provide efficient service to internal customers.\n\nPall Corporation, Deland, FL\t\t\t\t\t\t\nFinancial Controller \t\t\t\t             \t\t        \t\t\t        11/10-10/12  \nProvided operational & financial expertise and analysis for an industrial management company with yearly revenues totaling $32M; Total Company revenue is $2.4B.  \nFinance leader supporting the Regional Controller, local Executive team and corporate headquarters.\nManaged a Finance Analyst and General Accounting Clerk on daily accounting operations.  Line management responsibility for timesheet and expenses approval, PTO balances and annual PDRs.\nLed month-end close process, including processing journal entries, account reconciliation, accruals/prepayments, revenue adjustments, variance analysis of the P&L and Balance Sheet.\nPrepared accurate daily, monthly, and annual financial statements on the accrual basis of accounting, including workpapers to support external review.  Maintain fixed asset and depreciation schedules\nCreated and analyzed financial statements from trial balances for multiple entities in order to create consolidated financial statements.\nPerformed short and long-term forecasting to ensure sales, margins and expenses are in line; analyzed projections of sales and margin against actual figures, as well as budgets against expenses.\nBusiness partner to Cost Center owners in monthly expense reviews, variance analysis and forecasting/budgeting.\nAssisted in the annual physical inventory, including reconciliation of results.  \nSupport and manage relationships, including contract compliance and proper invoicing, with customers, vendors, investors, and external auditors\nProcess Improvement - Identified and implemented improvements in current processes through the use of Kaizans and Leadership seminars.  Improved the monthly close checklist based on new corporate policies. Lead Finance & Accounting SAP Expert providing expertise and analysis to onsite Operations team during SAP implementation.\n\nGES Exposition Services, Orlando, FL\t\t\t\t\t\t\nForecasting & Reporting Analyst                                                                                                                               2/09-11/10\t\nResponsible for the financial management of 2 regions that produce exhibition and event services with yearly revenues totaling $50M; Total Company revenue is $740M.\nPerformed financial statement analysis including recovery planning, expenditure activity and purchase order research. \nExecuted month end reporting, forecasting, annual budget and account reconciliations in accordance with SOX and GAAP \nProcess Improvement - Improved the monthly close checklist based on new corporate policies.  Lead Analyst on Forecasting Improvement Kaizen to reduce risk in financial reporting & research profitability measures.\n\nOperations Analyst                                                                                                                                                      2/07-02/09   \nDirected/Coordinated labor projections & analysis to identify capital needs & resource sharing opportunities.\nAnalyzed results in comparison to Plan & Forecast & provide Senior Management with variance analysis statements.\nIndirectly supervised two Account and Operations Coordinators on invoicing procedures & labor projections.\nProcess Improvement - Implemented Excel tutorials and training for project and account managers to help them manage their files more effectively.  \n\n\nEducation\n\nUniversity of New Orleans, New Orleans, LA\t\nMaster of Business Administration Degree (MBA), Finance, 2005\n\nDillard University, New Orleans, LA\nBachelor of Arts (BA), Psychology Degree\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           ",
          "confidence": 1.0,
          "method": "full_text",
          "structured_data": null
        },
        "tax_term": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "source_by": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "designation": {
          "value": "Support CFO",
          "confidence": 0.8,
          "method": "regex",
          "structured_data": null
        },
        "experience": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        }
      }
    },
    {
      "resume_path": "data\\input\\IT Security - Cyber Security - DC - Houman.docx",
      "confidence_score": 0.6741666666666667,
      "used_ocr": false,
      "extracted_fields": {
        "skills": {
          "domain_specific": [
            {
              "name": "security",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                81,
                172,
                1660,
                1921,
                2222,
                2428,
                2529,
                2706,
                2948,
                3192,
                3325,
                3474,
                3502,
                3676,
                3803,
                4126,
                4816,
                5816,
                6886,
                7179,
                7389,
                7458,
                7664,
                8557,
                9047,
                9834,
                10237,
                11305,
                11529,
                11729,
                11783,
                11891,
                11985,
                12178,
                12196,
                12276,
                12300,
                12395,
                12409,
                12547,
                12722,
                12830,
                13068,
                13252,
                13439,
                13472,
                13586,
                14159,
                15303,
                15900
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "infrastructure",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                436,
                5654,
                8771,
                8900,
                9370,
                13101
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "education",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                1430,
                16635
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "compliance",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                2553,
                2961,
                3262,
                3338,
                7475,
                7687,
                8047,
                9850,
                12763,
                12839,
                12884
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "design",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                2641,
                3511,
                3618,
                14893
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "legal",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                3002
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "policy",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                3536,
                6914,
                8489
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "defense",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                3717,
                4789,
                12481
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "architecture",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                3734,
                4825,
                5188,
                5788,
                6468,
                7735,
                11314,
                16465
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "streaming",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                7379
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "audit",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                8618
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "governance",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                11400
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "engineering",
              "confidence": 0.49999999999999994,
              "context": "education_section",
              "positions": [
                63
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "cybersecurity",
              "confidence": 0.49999999999999994,
              "context": "education_section",
              "positions": [
                133
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "methodologies": [
            {
              "name": "pmp",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                32
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "itil",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                116
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "cmmi",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                1076
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "pmi",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                9462
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "vendor_certifications": [
            {
              "name": "cissp",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                103
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "soft_skills": [
            {
              "name": "consulting",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                979
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "leadership",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                1600,
                2366,
                3226,
                3284
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "organization",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                3366,
                5840,
                10941,
                14194
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "initiative",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                3982,
                7554,
                10183
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "documentation",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                4428,
                9674
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "analysis",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                6831,
                9283,
                12924,
                15820,
                16274
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "influence",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                7903
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "planning",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                11459,
                12113,
                12430
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "cloud": [
            {
              "name": "iap",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                4966
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "networking_equipment": [
            {
              "name": "solarwinds",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                5284
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "authentication",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                16098,
                16202
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "security_tools": [
            {
              "name": "mcafee",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                6436,
                6618,
                7161
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "tenable",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                7648
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "siem",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                13500
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            },
            {
              "name": "snort",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                15932
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ],
          "business_skills": [
            {
              "name": "reporting",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                8355,
                12251
              ],
              "experience_weight": 0.0,
              "importance_score": 0.8
            }
          ],
          "sdlc": [
            {
              "name": "interviews",
              "confidence": 0.6,
              "context": "certification_section",
              "positions": [
                9018
              ],
              "experience_weight": 0.0,
              "importance_score": 0.7
            }
          ]
        },
        "first_name": "Houman",
        "last_name": "Hadaegh",
        "primary_email": {
          "value": "houmany@gmail.com",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "secondary_email": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "phone": {
          "value": "3016020708",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "city": {
          "value": "",
          "confidence": 0.0,
          "method": "ner",
          "structured_data": null
        },
        "state": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "zip": {
          "value": "27000",
          "confidence": 0.7,
          "method": "regex",
          "structured_data": null
        },
        "work_authority": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "resume_link": {
          "value": "data\\input\\IT Security - Cyber Security - DC - Houman.docx",
          "confidence": 1.0,
          "method": "file_path",
          "structured_data": null
        },
        "raw_resume": {
          "value": "Houman Hadaegh\n\nMobile: (301) 602-0708\nEmail: houmany@gmail.com \nhttps://www.linkedin.com/in/houmanh/\n\nSUMMARY:\n\nDetail-oriented executive leader with 20 years of expertise in IT infrastructure projects and security management for defense, civilian, and commercial clients. A Level 5 leader as coined by Jim Collins, with a professional philosophy based on the pillars of empathy, commitment, discipline, resiliency, and integrity.\nLeads and delivers highly technical, innovative solutions to the federal sector in the realms of cybersecurity and cloud computing.\nDevelops technology initiatives that optimize operations, align with business objectives, and promote growth capabilities.\nProvides strategic direction and oversight for the design, development, operation, and support of IT systems and programs that fulfill business needs, including enterprise architecture management, application management, security and risk management, and infrastructure and operations support management.\nAdvises senior executives on their overall technology strategy and leads its execution; engages teams and works collaboratively across departments to proactively solve problems, build consensus, and deliver critical projects.\nTechnical specialties include security architecture and program development, risk management, vulnerability management and assessment, incident response, and policy development. Vast experience with NIST Publications, DOD Instructions/Directives, OMB Circulars/Memoranda, FISMA, PCI DSS, ISO/IEC 27000, 20000 series, and ITIL frameworks.\n\nSECURITY CLEARANCE:\nDoD Top Secret\n\nCERTIFICATIONS:\nProject Management Professional (PMP) - Number: 1333520 \nCertified Information Systems Security Professional (CISSP) \u2013 337761\nITIL V4\n\nEMPLOYMENT HISTORY:\n\nChief Technology/Information Security Officer, October 2015 - Present  \nEmployer: IBSS Corp\n\nFor the past five years, led the strategic vision for the IT services and capabilities. Developed a one-year, three-year, and five-year roadmap for IT capabilities and strategies across cybersecurity, enterprise infrastructure, and cloud computing verticals. This involves reviewing the latest technologies from industry leaders and engaging with clients to discuss their technical challenges and goals. Internally, led the establishment of departmental performance goals, objectives, and operating procedures with KPIs that produce measurable results within budgets and timeframes. Played a key role in standing up IBSS\u2019s PMO to provide oversight of project and quality management for a diverse set of federal clients in IT, Scientific Engineering and Management Consulting services. Co-led IBSS to achieve externally-certified ISO 9001, ISO 20000, ISO 27001, and CMMI-SVC/ML3 processes to deliver secure IT services across federal clients. Led IBSS in winning opportunities, providing technical solutions to client needs, capture management of key personnel, and proposal development for $34 million in IT services in the past ten years.\n\nProgram Manager/Enterprise Architect, October 2009 - 2019\nEmployer: IBSS Corp\n\nClient: DoD Education Activity (DoDEA) OCIO - 2009 \u2013 2019\nServed as a Technical Program Manager/Enterprise Architect and strategic advisor to DoDEA CIO, CTO, and CISO. I managed and provided leadership to a cross-functional team of network, system and security engineers with an operational budget of $35M with 35 direct reports and 42 indirect reports across the global enterprise in the Americas, Europe, and Pacific theaters. Responsibilities encompassed the development and maintenance of the agency\u2019s Information Security Program and technical oversight for 90,000+ users worldwide across Commercial, Non-Secure Internet Protocol Router Network (NIPRNet), and Secret Internet Protocol Router Network (SIPRNet) enclaves. \n\nDeveloped and presented overall strategy, mission, and key performance indicators for Cyber/Information Security Operations within DoDEA ecosystem in support of the CIO and CTO vision. Developed short-term and long-term strategic roadmaps providing leadership and direction for the enterprise cyber/information security operations/program. Maintained an in-depth understanding of current and emerging information security, regulatory, and compliance trends as they relate to the DoD and NIST technology services. Supervised the design, implementation, maintenance, and performance of information security controls for the organization's networks, information systems, and critical assets. Lead the development and implementation of effective and reasonable policies and practices to secure protected and sensitive data and ensure information security and compliance with relevant legislation and legal interpretations. Lead efforts to internally assess, evaluate, and make recommendations to senior executives regarding the adequacy of the organizations' information and technology systems' security controls. Worked with DoD leadership and relevant responsible compliance department leadership to effectively build cohesive security and compliance programs for the organization to address federal statutory and regulatory requirements.\n\nSuccessfully led 30 DoDEA information security efforts, from cloud security design and management, to policy auditing, to anti-virus solution and host-based intrusion prevention system design and implementation. My team integrated cutting edge security technologies that implemented a defense in depth architecture and continuously monitor and provide timely response to security events for over 100,000 global endpoints to protect the IT resources of 15,000 employees serving 73,000 students across 168 schools in 12 countries.\n\nSuccessfully lead the initiative to consolidate seven enterprise datacenters covering CONUS and OCONUS into a single Global Datacenter with an enhanced and optimized security stack located in Peachtree City, GA. \nLead DoDEA\u2019s transition to Risk Management Framework (RMF) from DoD Information Assurance Certification and Accreditation Process (DIACAP) based on DoDI 8510.01 guidance and through analyzing risks and recommending common controls; developing review processes and documentation templates that support RMF methodology, and developing and implementing a cybersecurity assessment plan for the DoDEA CIO. Assessed the status of DoDEA\u2019s DIACAP implementation, and migrated DoDEA\u2019s 25 Major Applications (MA) and General Support System (GSS) Enterprise Mission Assurance Support Service (eMASS) packages to RMF.\nLead the enhancement of DoDEA\u2019s defense-in-depth perimeter security architecture by migrating from Cisco 5520 ASAs to a highly available Palo Alto 5060s next-generation firewalls at each Internet Access Point (IAP) located in Georgia, Sembach and Okinawa. Standardized and configured SSL decryption, malware protection, intrusion prevention, URL Filtering, and WildFire sandboxing.\nProvided technical oversight and management on multi-tier architecture and deployment of enterprise monitoring solutions for 20,000+ network devices using SolarWinds NetFlow Traffic Analyzer & Network Performance Monitor to provide performance, availability, and fault monitoring capabilities. We developed dashboards that provide meaningful and simple-to-understand performance statistics to upper management and NOC/SOC engineers and analysts. Benefits resulted in reduced time spent on identifying, analyzing, and troubleshooting infrastructure issues. \nProvided technical oversight and management on standardizing DoDEA\u2019s MS Active Directory (AD) domain structure and architecture to enhance the security posture of the organization. DoDEA\u2019s single global forest has just under 500k AD objects, close 900 GPOs, and operates in a very large network of over 100 sites with over 100 domain controllers across the United States and other countries. Established a framework for consolidation of many domains into a few AD domains and a structured centralized AD management model, which offered the advantages of automatic trusts, common catalog, schema, and configuration changes applied once and synchronized throughout all domains that are part of the forest.\nProvided technical oversight and management on the implementation of McAfee ePolicy Orchestrator (ePO) architecture that was strategically deployed throughout the enterprise to ensure up-to-date Host-Based signatures protect the agency\u2019s assets. Utilized McAfee Host Intrusion Prevention System (HIPS) to detect and block known intrusion attempts and creates custom rules to mitigate against Zero-Day attacks. Threat data was compiled and analyzed by dedicated intrusion-analysis SME\u2019s who provide a weekly brief of enterprise security status. In addition, Policy Auditor (PA), Rogue System Detection (RSD), and Data Loss Prevention (DLP) modules were deployed across the enterprise for more than 100,000+ nodes to enhance DoDEA\u2019s cyber hygiene.\nProvided technical oversight and management on the implementation of McAfee Enterprise Security Manager (ESM) for real-time visibility into all activity on all systems, networks, databases, and applications for 10,000+ nodes (network devices and servers). Deployment of this solution resulted in streaming security operations, providing a centralized view of an organization\u2019s security posture, compliance status, and prioritized incidents that require investigation. \nLed the initiative to enhance DoDEA\u2019s vulnerability management program by the enterprise deployment of Tenable Network Security Named Assured Compliance Assessment Solution (ACAS). DoDEA\u2019s ACAS architecture is divided into four geographic areas \u2013 HQ, America, Europe, and Pacific.  Each area has a SecurityCenter managing some number of scan zones within its sphere of influence. The solution provided a centralized view for all network activity, including asset discovery, vulnerability detection, configuration and compliance auditing, and event management. \nManaged communication and stakeholder engagement to include; business intelligence; outreach to the stakeholder community; creation and updating of briefing packages and technical and operational guidance documents for upper management; on-going formulation, tracking, and reporting for internal and external data calls, e.g., CCRI audits, and CyberScope.\nLead the development and implementation of a DoDEA wide Policy Remediation program based on DoD, CNSS, NIST, and OMB compliant security policies to assist in the closure of CCRI and FISMA audit recommendations.\n\nClient: NOAA OCIO Network Operation Center Support \u2013 2010 - 2013 \nProvided project management for an average workload of 13 simultaneous infrastructure projects at remote locations throughout the US. \nDelivered processes included:  Change Management, Incident Management, Infrastructure Decommissioning, and Server and Firewall Requests Processes. \nConducted and documented numerous \"deep dive\" interviews with engineers, IT security analysts, project managers, and IT consultants to develop an in-depth understanding of current configurations and processes in preparation for data center process builds.\nReviewed blueprint engineering assessments, produced sizing analysis, project documents, coordinated project schedules, and ran virtual war rooms for infrastructure implementation. \n\nClient: NOAA OCIO Web Operation Center Support - 2010 \u2013 2013 \nUsing PMI/PMBOK principles, developed and managed project plan (scope, business case, risk plan, schedule/timelines, dependencies, constraints, budget, stakeholder's management plan, communications plan, RACI, prepared all project documentation; facilitated project meetings; prepared presentations; worked with cross-functional teams; managed onsite/external vendors; followed Change Control, IT Security and IT Compliance policies and procedures.\n\nSenior IT Consultant, March 2006 \u2013 October 2009\nEmployer: Booz | Allen | Hamilton\n\nClient: U.S. Air Force \nLead a team of developers and network engineers that were responsible for designing a Base Decision Support System (BDSS) for the United States Air Force under the Biometrics Task Force. The AF BDSS was an initiative to improve current identity management and security practices such that changes in suitability status can be acted upon in a timelier manner. \nDesigned and deployed Federation and Single Sign-On (SSO) technologies across two entities using different COTS products. \nEngaged with designing, installing, and configuring Microsoft (MS) Active Directory (AD), MS AD Federation Services (ADFS), CA SiteMinder Web Access Manager and CA Identity Manager. \n\nClient: U.S. Department of the Treasury \nSupported was an IdAM Concept of Operations (CONOPS) for the Department of Treasury. The CONOPS illustrated a 3-5-year operating model that enables the development of the IdAM future state vision and expectations into an operational concept that drives requirements across process, organization, and technology.  It articulated a Treasury IdAM business vision and provided the ability to develop a strategic plan. It also allows Treasury to communicate a shared IdAM vision to both internal and external customers and facilitates organizational buy-in and ownership for a Treasury IdAM program from its bureaus. \nDefined and documented the application security architecture for project solutions as part of the SDLC process and provided input and governance to the SDLC process.\nParticipated in IT strategy planning activities, bringing a current knowledge and future vision of security technology, and how they interact with the Treasury application portfolio and business goals and objectives.\nProvided hands-on experience in conducting C&A activities, which included developing security requirements, developing artifacts, conducting security tests and evaluations, developing risk assessments, and documenting the information system in system security plans through all accreditation activities.\nDeveloped and updated department-level IT security policies, procedures and plans such as PII/sensitive information handling, risk management, incident handling, contingency planning, secure backup and recovery, account management, personnel security, physical security and rules of behavior.\nPerformed collaborative reporting on operational security service levels, security event correlation strategy, intrusion prevention system enhancements, identification of security gaps, security enhancement planning, and project-based timeline metrics.\n\nClient: Defense Information Systems Agency \nManaged a technical team of 10 security personnel to create an integrated approach that provides data integrity, information confidentiality, and service availability.\nFrom a strategic perspective, implemented security best practices and helped drive compliance in a number of key business areas, including DOD/federal security compliance, industry regulation/standards, and compliance frameworks\nEnsured that risk analysis methods are embedded across architectural programs as new technologies and solutions are implemented.\nDesigned, analyzed, and implemented security essential practices and infrastructure at the data, application, service, operating system, and network levels to safeguard all data center assets and data.\nFocused on application security, intrusion detection, vulnerability assessment, proactive network monitoring and protection, and participated in systems development and deployment decisions from the perspective of security best practices.\nReviewed security measures including SIEM, DLP, IPS/IDS, AV/malware, DRP/BCP, MDM, endpoint protection, software updates, physical security, and all controls, policies, and procedures. Presented findings in presentations to executives and technical teams.\n\n\nSenior IT Engineer, June 2000 \u2013 Feb 2006\nEmployer: IBSS Corp\n\nClient: National Oceanic and Atmospheric Administration OCIO\nDesigned and implemented a MS Windows 2003 based Active Directory migration from Windows NT for 1000 users and over 60 servers that covered us of highly available Domain Controllers (DC), DNS, DHCP, Print, File Sharing and Web services architectures. \nCreated various complex and advanced group policies and organizational units to improve the security and administration of the organization. \nEstablished various IT policies and automation procedures which included the implementation of MS Operations Manager (MOM) 2005 for server monitoring and MS Systems Management Server (SMS) for software deployments, password synchronization, automated critical patch deployment and self-service password reset.\nAssisted with the modernization of a Wide Area Network (WAN) of 35 sites nation-wide utilizing Cisco routers (2500, 2600, 3600 and 3700) and MCI frame and relay cloud. \nConfigured and maintained varied Cisco equipment such as Catalyst 6500 Layer 3 core switches with Firewall Service Modules, Catalyst 4000 and 3500 access switches, 2600 and 3600 routers and VPN Concentrators. \nAssisted in the design, configuration, and implementation of Cisco Access Control List (ACL) on all WAN routers to secure the WAN and to protect the remote Local Area Networks (LAN)\u2019s from attacks. \nAssisted in headquarters LAN upgrade by designing and configuring Cisco Gigabit Ethernet as backbone using various Cisco Catalyst switches. Configured a new set of IP addressing schemes to create different Virtual LANs (VLANs) to provide security, bandwidth management and traffic control. \nInstalled and configured Multi Router Traffic Grapher (MRTG) to monitor the bandwidth usage for individual remote offices. In addition, installed and configured Big Brother software to monitor the operation of all devices and to notify the administrators if an outage occurs. \nDesigned and implemented Network-based Intrusion Detection Systems (NIDS) to detect Denial of Services and Prob&Scan attacks, both on testbed network and enterprise network. \nConstructed vulnerability analysis and penetration testing for both wired and wireless networks, providing security by Cisco firewall, NIDS, Snort, and TippingPoint IPS. More than 10 kinds of Distributed Denial of Service and probing & scanning attacks were used for evaluation. \nDesigned and implemented a novel authentication scheme (RSA-based), for a distributed system (wireless Mobile Ad hoc Networks) for two-factor authentication of network devices. \nConfigured IDS alerts correlation and analysis for finding cooperative scans and reducing false positives.\nReviewed all-source intelligence information and correlated it with data derived from various levels of perimeter defenses architecture (IDS, Firewalls, and Logs) to provide the customer with assessments and reports facilitating situational awareness and understanding of the current cyber threat.\n\nEDUCATION:\nUniversity of Maryland, College Park 2004\nB.S. Degree, Electrical Engineering\n\nUniversity of Maryland, University College 2018\nB.S. Degree, Cybersecurity\n\nKNOWLEDGE AND TECHNICAL SKILLS:\n\n",
          "confidence": 1.0,
          "method": "full_text",
          "structured_data": null
        },
        "tax_term": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "source_by": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "designation": {
          "value": "operations support",
          "confidence": 0.9,
          "method": "ner",
          "structured_data": null
        },
        "experience": {
          "value": "20",
          "confidence": 0.9,
          "method": "regex_total_experience_summary",
          "structured_data": null
        }
      }
    }
  ]
}