{
  "timestamp": "2025-06-15T07:41:30.147803",
  "summary": {
    "total_processed": 46,
    "successful_extractions": 46,
    "failed_extractions": 0,
    "success_rate": 100.0,
    "ocr_usage_percentage": 0.0,
    "avg_extraction_time": 165.69877584084222
  },
  "field_analysis": {
    "empty_fields": {
      "primary_email": 6,
      "secondary_email": 46,
      "phone": 18,
      "city": 18,
      "state": 18,
      "zip": 26,
      "work_authority": 40,
      "tax_term": 36,
      "source_by": 46,
      "experience": 22
    },
    "field_confidence": {
      "primary_email": [
        0.0,
        0.9,
        0.9,
        0.0,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.0,
        0.9,
        0.9,
        0.0,
        0.0,
        0.9,
        0.9,
        0.0,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9
      ],
      "secondary_email": [
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0
      ],
      "phone": [
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.9,
        0.0,
        0.9,
        0.9,
        0.0,
        0.9,
        0.0,
        0.9,
        0.9,
        0.0,
        0.9,
        0.9,
        0.0,
        0.0,
        0.9,
        0.0,
        0.0,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.0,
        0.9,
        0.9,
        0.0,
        0.0,
        0.9,
        0.9,
        0.0,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9
      ],
      "city": [
        0.0,
        0.0,
        0.7,
        0.0,
        0.0,
        0.7,
        0.9,
        0.7,
        0.7,
        0.9,
        0.7,
        0.7,
        0.0,
        0.7,
        0.7,
        0.0,
        0.7,
        0.7,
        0.7,
        0.0,
        0.7,
        0.7,
        0.0,
        0.7,
        0.7,
        0.7,
        0.7,
        0.7,
        0.7,
        0.0,
        0.7,
        0.7,
        0.0,
        0.7,
        0.0,
        0.7,
        0.0,
        0.0,
        0.7,
        0.0,
        0.0,
        0.7,
        0.0,
        0.0,
        0.7,
        0.0
      ],
      "state": [
        0.0,
        0.0,
        0.9,
        0.0,
        0.0,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.0,
        0.8,
        0.9,
        0.0,
        0.8,
        0.9,
        0.9,
        0.0,
        0.8,
        0.9,
        0.0,
        0.8,
        0.8,
        0.9,
        0.8,
        0.9,
        0.9,
        0.0,
        0.8,
        0.9,
        0.0,
        0.8,
        0.0,
        0.8,
        0.0,
        0.0,
        0.8,
        0.0,
        0.0,
        0.8,
        0.0,
        0.0,
        0.8,
        0.0
      ],
      "zip": [
        0.0,
        0.0,
        0.7,
        0.0,
        0.0,
        0.7,
        0.9,
        0.7,
        0.7,
        0.9,
        0.7,
        0.7,
        0.0,
        0.0,
        0.7,
        0.0,
        0.0,
        0.7,
        0.7,
        0.0,
        0.7,
        0.7,
        0.0,
        0.7,
        0.0,
        0.7,
        0.0,
        0.7,
        0.7,
        0.0,
        0.0,
        0.7,
        0.0,
        0.0,
        0.0,
        0.7,
        0.0,
        0.0,
        0.7,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0
      ],
      "work_authority": [
        0.0,
        0.0,
        0.8,
        0.0,
        0.0,
        0.8,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.8,
        0.0,
        0.8,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.8,
        0.9,
        0.0,
        0.8,
        0.9,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0
      ],
      "resume_link": [
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0
      ],
      "raw_resume": [
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0
      ],
      "tax_term": [
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.9,
        0.0,
        0.0,
        0.9,
        0.0,
        0.0,
        0.0,
        0.9,
        0.9,
        0.0,
        0.9,
        0.9,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.9,
        0.0,
        0.0,
        0.9,
        0.0,
        0.9,
        0.0,
        0.0,
        0.9,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0
      ],
      "source_by": [
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0
      ],
      "designation": [
        0.9,
        0.8,
        0.8,
        0.9,
        0.8,
        0.8,
        0.9,
        0.8,
        0.9,
        0.9,
        0.8,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.8,
        0.9,
        0.9,
        0.8,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.8,
        0.9,
        0.9,
        0.8,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.8,
        0.8,
        0.8,
        0.8,
        0.8,
        0.8
      ],
      "experience": [
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.9,
        0.0,
        0.9,
        0.9,
        0.0,
        0.0,
        0.0,
        0.9,
        0.0,
        0.0,
        0.9,
        0.0,
        0.9,
        0.0,
        0.0,
        0.9,
        0.0,
        0.9,
        0.0,
        0.9,
        0.0,
        0.0,
        0.0,
        0.9,
        0.0,
        0.0,
        0.9,
        0.0,
        0.9,
        0.0,
        0.0,
        0.9,
        0.0,
        0.9,
        0.0,
        0.9,
        0.9,
        0.0,
        0.9
      ]
    }
  },
  "skills_analysis": {
    "categories": {
      "technical_skills": {
        "count": 0,
        "skills": []
      },
      "business_skills": {
        "count": 0,
        "skills": []
      },
      "creative_skills": {
        "count": 0,
        "skills": []
      },
      "communication_skills": {
        "count": 0,
        "skills": []
      },
      "industry_skills": {
        "count": 0,
        "skills": []
      },
      "soft_skills": {
        "count": 0,
        "skills": []
      },
      "other_skills": {
        "count": 0,
        "skills": []
      }
    },
    "total_skills": 0,
    "unique_skills": 0
  },
  "resume_details": [
    {
      "resume_path": "data\\input\\.Net - TX - Hassan.pdf",
      "confidence_score": 0.27,
      "used_ocr": false,
      "extracted_fields": {
        "skills": {},
        "first_name": "Hassan",
        "last_name": "Mwase",
        "primary_email": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "secondary_email": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "phone": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "city": {
          "value": "",
          "confidence": 0.0,
          "method": "ner",
          "structured_data": null
        },
        "state": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "zip": {
          "value": "",
          "confidence": 0.0,
          "method": "regex",
          "structured_data": null
        },
        "work_authority": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "resume_link": {
          "value": "data\\input\\.Net - TX - Hassan.pdf",
          "confidence": 1.0,
          "method": "file_path",
          "structured_data": null
        },
        "raw_resume": {
          "value": "  \nHassan Mwase  \n \nProudly presented by Hassan Mwase  IT professional with 1 6+ years of experience as a software developer/solution architect  on multiple \nlarge complex  project s and production support for a variety of clients  including most recent \nexperience developing Web applications . Proficient in analysis, design, development, testing, \nintegration and troubleshooting skills.  Result oriented back -end and front -end developer with \nexceptional analytical skills, team oriented, dependable and very flexible. Experience with \nWaterfall and JIRA Agile methodologies.  \n \nTechnical Skills  \n\uf0b7 Experienced Software development of Web applications using  Azure DevOps, Cloud, Git, \nASP.NET Web Applicati on, ASP.NET Web Forms , ASP.NET MVC,  C#, .NET \nFramework, Kendo UI , JavaScript,  Razor,  Angular  JS, CSS 3, jQuery , bootstrap, Ajax, \ndependency injection,  JSON, LINQ, HTML5,  Microsoft Visual Studio enterprise 201 7, \nActive Server  Pages extended files,  Eclipse , Test Driven Development (TDD ), Selenium \nFramework, Web Services, Postman, Visual Studio code , DevExpress, SQL Server \nManagement Studio . \n\uf0b7 Proficient in analysis, design, development, testing, integration and troubleshooting skills.  \n\uf0b7 Experience with Waterfal l and JIRA Agile methodologies .  \n\uf0b7 Result oriented back -end and front -end developer with exceptional analytical skills, team \noriented, dependable and very flexible.  \n\uf0b7 Familiarity with REST/ HTML/CSS 3.  \n\uf0b7 Excellent relational database experience using Oracle and  SQL Server  including writing \nPL/SQL, SQL stored procedures, functions, triggers and views.  \n\uf0b7 Experience developing using Oracle Database,  Oracle Forms, Oracle Reports,  SQL*Plus, \nPL/SQL, UNIX, Pro*C . \n\uf0b7 Knowledge of Java/J2EE architecture/technologies,  WebSphere , Test Driven \nDevelopment TDD  and Oracle ADF , ODBC  \n\uf0b7 Writing unit tests with NUnit and Moq  \n\uf0b7 Familiar with Regular Expression and Pattern Matching.  \n\uf0b7 SQL Server \u2013 Microsoft SQL Server Management Studio v.18.8 , Visual Studio 2017, \nMicrosoft SQL Server To ols for Visual Studio, GraphiViz 2.38 \u2013 This is used in the PMEF \nNEDAT and new NSIGHT application as an API for creating bubble visualization images.  \nMicrosoft Internet Information Services  (IIS) version 10  web server . OleDB API \nUncovered errors in timeshe et contracts and Provided guidance in resolving major issues \nfrom past years. Applied fix to application code.  \n \n \nProfessional Experience  \n \nSecret Security Clearance  11/2020 to 11/2030                        May. 2020  \u2013 Present  \nSenior Software Programmer/Analyst Consultant /Agile developer  \nTools : .NET, ASP.NET C#, Web Forms .Net Framework ver.4.6, JavaScript, CSS3,  jQuery, \nbootstrap v4.0.0 , JSON, LINQ, HTML5, Visual Basic 201 7, Microsoft Visual Studio enterprise,  \nHassan Mwase           Page 2 \nProudly presented by Hassan Mwase  Developer, Web Services, Web Forms, Databases: SQL Server  and Oracle , Microsoft team TSF  \ndevelopment environment  \n\uf0b7 Develop web applications using C#,  ASP.NET and JavaScript . \n\uf0b7 As a senior developer, I develop, enhance  and Support critical federal Government  \nprojects.  \n\uf0b7 Develop and maintain multiple web applications written in ASP.NET, C#, JavaScript,  \nand Web Forms.  \n\uf0b7 Participate in project meetings with other technical staff, business owners and subject \nmatter experts.  \n\uf0b7 Troubleshooting and debugging Defects  \n\uf0b7 Write automated UI test script using ServiceNow ATF  Framework . \n \n \nFarmers Insurance  \u2013 USA                                                                      Sept. 2018  \u2013 Aug. 2020  \nSenior Programmer/Analyst Consultant /Agile developer  \nTools : .NET, ASP.NET C#, MVC, Web Forms  .Net Framework ver.4.6, JavaScript,  CSS3,  jQuery, \nbootstrap  v4.0.0, JSON, LINQ, HTML5, Visual Basic 201 7,  Microsoft Visual Studio enterprise,  \nDeveloper, Selenium Framework, Web Services, Web Forms, Databases: SQL Server  and DB2 ,  \nMicrosoft team TSF  development environment  \n\uf0b7 Develop web applications using C#, Azure DevOps, Git, ASP.NET MVC , JavaScript, \njQuery, Razor MVC . \n\uf0b7 Develop and maintain multiple customer web applications written in C# , JavaScript and \nWeb Forms.  \n\uf0b7 Participate in project meetings with other technical staff, business owners and subject \nmatter experts.  \n\uf0b7 Troubleshooting and debugging Defects  \n\uf0b7 Write automated UI test script and fix defects using Selenium Framework . \n\uf0b7 Write unit tests with NUnit and Moq  \n \nBoeing  Company  \u2013 USA                                                                              Feb. 2018  \u2013 Sept.2018  \nSenior Programmer/Analyst Consultant /Agile developer  \nTools : .NET, ASP.NET MVC ,C#, MVC,  Dependency Injection DI,  .Net Framework ver.4.6, \nJavaScript, Razor,  Angular  JS,  CSS3, jQuery, bootstrap , JSON, LINQ, HTML5 ,  Microsoft Visual Studio \nenterprise 201 7, PL/SQL, SQL D eveloper, Databases: Oracle 12C and SQL Server  , JIRA  Agile , Microsoft \nteam TSF development environment  \n\uf0b7 Develop se rver side C# pages, create entities in the relation object mapping to modernize \nBoeing data ERP application  using  ASP.NET MVC framework .  \n\uf0b7 Manage Agile task board .  \n\uf0b7 Design and develop back -end Database stored procedures, functions, triggers and views . \n\uf0b7 Code C# interfaces, Classes, view models, API controllers  and data services web resources.  \n\uf0b7 Develop server side XHML5 web pages leveraging angular JS  and jQuery. \n\uf0b7 Code C# Dependency Injection processes \u2013 DI Manager interfaces, DI Manager classes, \nDependency  Injection configuration and API Controllers .   \nHassan Mwase           Page 3 \nProudly presented by Hassan Mwase   \n \nCliffs Natural resources  - Cleveland , OH                                                    July 2016  \u2013 Dec. 2017  \nSenior Programmer/Analyst Consultant  \nTool s: .NET, C#, MVC,  .Net Framework ver.4.6, JavaScript, Angular JS, CSS, JSON, LINQ, HTML5,  \nMicrosoft Visual Studio enterprise 2015,  PL/SQL, Postman , SQL Developer , Toad,  Databases: Oracle  \n12C and SQL Server   \n\uf0b7 Re-designed the Oracle ERP application and convert oracle  forms and reports  web application \nusing  ASP.NET MVC framework . \n\uf0b7 Create d Technical specs based on user desired functionality changes.  \n\uf0b7 Design and develop back -end Database stored procedures, functions, triggers and views . \n\uf0b7 Coded  C# interfaces, Classes, view models, JavaScript controllers  and data services web \nresources.   \n\uf0b7 Develop front end web content XHML 5 web pages  leveraging angular  JS. \n\uf0b7 Code system enhancements, defect fixes  and production support .   \n\uf0b7 Commit and publish finished web application solutions.  \n\uf0b7 Support project and business users to Create  Unit Test, integration, and UAT test scenarios and \nscripts, as well as, performing system and integration testing.   \n\uf0b7 Design, build and implement client and server side applications  as needed , as well as \nsupporting and enhancing custom applications.  \n \nGap Inc. - Columbus , OH                                               July 2015 \u2013 Nov. 2015 \nSenior Programmer/Analyst Consultant /Agile developer  \nTools : Oracle 11G, PL/SQL, SQL Developer, Toad, JDeveloper,  Oracle 11G Web logic environment, \nEclipse, JavaScript,  Ruby , Eclipse, WebSphere, Tomcat,  JIRA Agile development environment.  \n\uf0b7 Onsite team lead working with product managers and business users to re-design the on -\nline order fulfillment process from batch processing to on -demand and transform oracle \ninterface based application to Web UI .  \n\uf0b7 Technical lead for on -site/offshore developers and QA.  \n\uf0b7 Convert legacy forms application to Web  UI using Java Script. \n\uf0b7 Coded supporting stored  procedure s, functions, packages  and views . \n\uf0b7 Assisted to test conver sion of  character based SSH screens for RF devices to a stateless \nHTML user interface, update to the tech stack from Oracle Forms to Java6/HTML5, \ndesigned to  extend the life of a legacy Warehouse Management System(RWMS)   \n\uf0b7 Create  test cases , run automated system tests and validate  output of Ruby automated testing \nfor process online customer orders.  \n\uf0b7 Support warehouse order processing and shipment message queues (MQ) .  \n\uf0b7 Create d Unit Test , integration, and UAT test scenarios and  scripts, as well as, performing \nsystem and integration testing. Write integration testing cases.   \nCliffs Natural Resources - Cleveland , OH  June 2011  \u2013 April 2015  \nSenior Programmer  Consultant  \nTools : Oracle 11G, SQL Server, Oracle Forms, Oracle Forms, .NET, C#, MVC,.Net Framework, \nJavaScript, HTML5,  Microsoft Visual Studio enterprise 2015,  PL/SQL, SQL Developer, Toad  \n\uf0b7 Support business functions through the development, implementation, and maintenance \nof applications systems.  \nHassan Mwase           Page 4 \nProudly presented by Hassan Mwase  \uf0b7 Responsible for Maintenance and support of Company ERP application, support various \nenterprise applications enhancements and code extensions, build and apply fixes.  \n\uf0b7 Maintained and coded database objects such as procedures, functions, packages and \ninterfac e that support ERP applications, ASP.NET MVC, .Net Framework and Object \nBusiness Intelligence  Enterprise Edition (OBIEE).  \n\uf0b7 Coded PL/SQL subprograms that were called by the front -end application written in C#  \nand ORM Entity framework .  \n\uf0b7 Worked with applicatio n managers to g ather requirements and build solutions based off of \nuser requirements and build enhancements . \n\uf0b7 Proactively assess opportunities, risks, challenges for technology and business, define \nscope, plan and schedule of custom enhancements.  \n\uf0b7 Consistent ly exceeded project requirements and expectations on -time delivery.  \n \nWalt Disney World  - Orlando , FL Feb.2008  \u2013 Aug.2010  \nLead  Senior Programmer/Analyst Consultant  \nTools:  Oracle 10G, Oracle Retail supply chain, Oracle Retail Merchandizing, Oracle Database, Oracle \nForms, Oracle Forms, PL/SQL, S QL Developer, Toad  \n\uf0b7 As project Stream lead, managed  full lifecycle implementation effort. Engaged client managers to \nreview project plans, timelines and deliverables.  \n\uf0b7 SME lead Oracle WMS/RMS implementation/integration contractor and client developer  team . \n\uf0b7 Responsible for analysis, design and final delivery of functioning and technical integrated multiple \nfacility warehouse. Setup and configure environments.  \n\uf0b7 Designed, wrote code  and tested application enhancements, mod ifications, and interfaces. \nSupported post go -live including system integration stabilization, inventory stock on hand \nimbalance resolution between RWMS13.x and RMS13.x issue resolution and knowledge transfer \nto Disney IT personnel.  \nCato Corporation, 8100 Denmark Road, Charlotte, NC   Dec. 2007 \u2013 Aug. 2008  \nProject  Lead - Senior Programmer/Analyst Consultant  \nLead technical consultant, to upgrade  RMS . Plan and recom mend most efficient upgrade strategy of all \ncurrent Oracle retail application modules  to be web -based.   \nTesco Central Europe Prague , Czech Republic  Aug.2007 \u2013 Oct.2007  \nLead Senior Programmer/Analyst Consultant  \nLead functional and technical RWMS Project lead consultant,  build and deploy Oracle Retail \nWarehouse Management System for both Prevov Fresh DC and Ambient DC . Review foundation and \ntransactional data to ensure successful build, test and implementation process  of Slovakian DC in \nPresov  \nIBM - Circuit City Richmond , VA Aug.2006 \u2013 July 2007  \nSenior Analyst Consultant  \nTools: RWMS 10.3 , UNIX, Pro*C, Oracle Database, SQL*Plus, PL/SQL, Oracle Reports, Oracle Forms, \nOracle, RIB  \nLead technical analyst to assist RMS11 implementation and integration . Worked on Purchase \nOrder , Cost and Deals team .  \n \nGAP Inc.  \u2013 Columbus , OH 2005 \u2013 2006  \nSenior Analyst Consultant  \nHassan Mwase           Page 5 \nProudly presented by Hassan Mwase  Tools: RWMS 10.3 , UNIX, Pro*C, Oracle Database, SQL*Plus, PL/SQL, Oracle Reports, Oracle Forms, \nOracle, RIB  \nDevelopment team lead developer on the WMS system - a highly customized version of Oracle \nRetail WMS.   \n \nRetek /Oracle  \u2013 Minneapoli s, MN  2003 \u2013 2005 \nSenior Developer  \nTools: RWMS 10.3 , UNIX, Pro*C, Oracle Database, SQL*Plus, PL/SQL, Oracle Reports, Oracle Forms, \nOracle, RIB  \n\uf0b7 Technical lead/supervisor of Retek WMS Developer and customer product support.  \n\uf0b7 Developed RWMS 10.3 /RMS 10.3 and RIB hospital application support interface. On -site DC \noperation s support and technical lead for RWMS/RIB  at the Navy Exchange Norfolk \nVirginia   .  \n \n \n \nEducation  \n \nBachelor of Commerce  \nUniversity of Calgary, Calgary, Alberta  \n \nComputer Programming  \nYork University, Toronto, Ontario  \n \nCertification  \nOracle Certified Professional \u2013 Certified Oracle DBA  \n \n \n",
          "confidence": 1.0,
          "method": "full_text",
          "structured_data": null
        },
        "tax_term": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "source_by": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "designation": {
          "value": "software developer",
          "confidence": 0.9,
          "method": "ner",
          "structured_data": null
        },
        "experience": {
          "value": "6+",
          "confidence": 0.9,
          "method": "regex_total_experience_summary",
          "structured_data": null
        }
      }
    },
    {
      "resume_path": "data\\input\\Affzal_SrDeveloperResumeS.pdf",
      "confidence_score": 0.48500000000000004,
      "used_ocr": false,
      "extracted_fields": {
        "skills": {},
        "first_name": "AFFZAL",
        "last_name": "ABDUL",
        "primary_email": {
          "value": "Affzal.sql@gmail.com",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "secondary_email": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "phone": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "city": {
          "value": "",
          "confidence": 0.0,
          "method": "ner",
          "structured_data": null
        },
        "state": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "zip": {
          "value": "",
          "confidence": 0.0,
          "method": "regex",
          "structured_data": null
        },
        "work_authority": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "resume_link": {
          "value": "data\\input\\Affzal_SrDeveloperResumeS.pdf",
          "confidence": 1.0,
          "method": "file_path",
          "structured_data": null
        },
        "raw_resume": {
          "value": " \nAFFZAL ABDUL  \nDeveloper  Sr. \nEmail:Affzal.sql@gmail.com  \nM: 361 -330-0666  \n \nPROFESSIONAL SUMMARY  \n \n\u2022 Over 9 years  of experience ranging software development, testing, assurance, and implementation and working in \ncomplex engagements.  Prominently has experience in data warehousing and Business Intelligence technologies using \nMS SQL Server 2000/2005/2008R2/2012/2014 &  2016 on Azure.  \n\u2022 Solid 8 years in depth experience  in MS SQL Server database development and administration  in enterprise \nenvironment including server installation, configuration, upgrade, maintenance, performance tuning , optimization, \nbackup/restore, recovery, migration, monitoring, security planning and trouble -shooting.  \n\u2022 Good understanding in database and data warehousing concepts (OLTP & OLAP)  and also data analysis  along with \nvisualizations using Tableau  and Sisense . \n\u2022 Experience in bo th building and utilizing DevOps  pipelines for the projects which started brand new and legacy systems .  \n\u2022 Leverage strong communication and interpersonal skills to cultivate strong working relationships with clients, vendors \nand offshore partners.  \n\u2022 Worked in dependent and as a team with strong critical thinking, time  management  and proven handling of \nresponsibilities and projects.  \n\u2022 Experience in Information Technology field as a n SQL server Developer with strong expertise in  SQL server \ndevelopment, Physical/Logical design, designing Stored -Procedures/T -SQL coding, Troubleshooting, Backups and \nRestore of databases,  SSIS/SSRS, OLTP and OLAP.  \n\u2022 Experience in creating visualizations on the data and created cosmetic clear dashboards  on top of the analyzed data  in \nboth Tableau and PowerBI . \n\u2022 Transformed data from one source  to other destinations  using tools like SSIS , Informatica  PowerCenter . \n\u2022 Experience in Optimizing  Code and Improving Efficiency in databases including  Re-indexing, Updating Statistics, \nRecompiling Stored Procedures  and performing other maintenance tasks and DBCC Commands.  \n\u2022 Good understanding of Microsoft Reporting  Service  (SSRS) with Report authoring , Report management , Report \nformatting, Report distribution, Report delivery and Report security.  \n\u2022 Extensive time spent on POC for Sisense  to be used in data visualization purposes and thereby  have to stick with using \ntableau for visualization purposes of the business data.  \n\u2022 Experience in enhancing and deploying the SSIS Packages from development server to production server.  Migrated  \nDTS packages into SSIS packages using upgrade advisor tool.  \n\u2022 Used Database Monitoring tools like profiler, performance monitor  and Event viewer.  Good understanding in SQL user \nlogins, database user accounts, server roles, database roles and its permissions . \n\u2022 Experience in integrating the SSRS reports into web applications using Report viewer control, URL methods and \ncreating data driven subscriptions to different users using Report Manager.  \n\u2022 Expertise in PowerShell  Scripting / Python  and making REST  API calls to pull data and also load it to databases.   \n\u2022 Expert ex perience in handling huge volume of data and transforming and cleansing of data and raw data.  Involved in \nhuge data migrations, transfers using BCP, SSIS and Informatica PowerCenter  from SQL Analysis server, Teradata, \nOracle . \n\u2022 Experience in Agile methodology in the SDLC process ( both scrum & Kanban).  \n \n \nEducation Background  \n \n\u2022 Bachelor of Engineering (Information Technology) - Osmania University, India. (2005 -2009)  \n\u2022 Master\u2019s in Information Technology and Management \u2013 IIT, Chicago. (2010 -2012)  \n \n \nTECHNICA L SKILLS  \n \nSQL Server Tools   : Enterprise Manager, Profiler, Query Analyser, Memory Optimization Advisor  \nExport & Import (SSIS), SSRS . \nRDBMS   : MS SQL Server 2005/2008 , 2012 , 2014 , 2016.MS  Access, Oracle  11g, Teradata . \nETL   : SSIS, Informatica  Power Center (9.5 & 10.2) . \nProgramming Languages  : SQL, T -SQL, PL-SQL, Core Java, C #, UNIX , PowerShell  and Vbs  \nInternet Technologies  : IIS, XML, HTML.  \nProductivity Applications  : MS Office Suite along with Outlook.  JIRA . \n \nOperating System  : Windows95/98, Windows 2000/NT/XP , 2003 , 7, MS DOS.  \nReporting Tools   : Crystal Reports 11, SSRS , ARMS reporting , Sisense, and Tableau . \nDesign Documentation              :           UML, MS Visio  \nOther  Tools    :            Tidal , Control -M, VSS, TFS, GIT,  SVN.  \n \n \nPROFESSIONAL EXPERIENCE  \n \nAnthem Inc. , Norfolk , VA                                                                         Mar 2017\u2013 Till date  \nDeveloper  Sr. \n \nAnthem Inc . is an American  health insurance company  founded in the 1940s, prior to 2014 known as  WellPoint, Inc. It is \nthe largest for -profit  managed health care  company in the  Blue Cross and Blue Shield Association . It covers 7 .7 million \nseniors, people with disabilities, low -income families and other state and federally sponsored beneficiaries, and federal \nemployees in 26 states, making it the nation\u2019s largest provider of health care for public programs . \n \nResponsibilities:  \n \n\u2022 Expertise in Provider  dealing with legacy system to load files from states into the FACETS application.  Expertise in \nProvider business domai n area and the flow of the FACETS  application. in-depth knowledge of both providers in \nFACETS.  \n\u2022 Apply the Business rules on the data coming from multiple operating markets. Maintain the business rules in a table for \nvisibility to all required processes.  \n\u2022 Actively worked in gathering requirements with SA and PO to convert them to technical documents for the deve lopment \nto initiate in Jira.  \n\u2022 Used JIRA, SVN Tortoise  & Bit bucket  for code repository and logged / tracked the activities in the team. Also CTU tool \nfor code deployments.  \n\u2022 Used  Splunk  for parsing complex files and  Splunk language ( SPQL ) to make the searches and created dashboards on \nthe data. Also configured notifications as requested by the business.  \n\u2022 Did POC  for DevOps tools and deeper research on both Sisense  and Tableau  tools for utilizing them as data \nvisualization purposes across the provider department.  \n\u2022 Have hands on experience in pulling data from JIRA using REST calls thru PowerShell scripting and load them to SQL \ndb. Also did dashboards on the loaded data and provi ded support for the created dashboards in tableau . \n\u2022 Used Python  to massage the data in REST calls using JIRA libraries and loading them to database. Built dashboards \non top the loaded data for project management team.  \n\u2022 Checked the configuration and performance of the existing databases including data file allocation, index, fill factor, \nfragmentation and the impact on system performance. Analyzed potential problems (response delay, locking, server \ndown time, etc.) to avo id and optimize.  \n\u2022 Actively involved  with production support ( 24*7) and Development environment.  \n\u2022 Used Premise  to trace the slow running queries and tried to Optimize SQL queries for improved performance and \navailability. Also used precise tool for reviewin g the performance.  \n\u2022 Actively involved in code reviews and mentoring the developers. And also involved in development activities.  \n\u2022 Excellent Team Building, Project Management, Analytical, Interpersonal & Communication Skills.  \n\u2022 Extensively worked and did resear ch on Redgate  suite tool belt and implemented them across the whole team.  \n\u2022 Have hands on experience in pulling data from JIRA using REST API calls thru PowerShell scripting and load them to \nSQL db. Also did dashboards on the loaded data and provided support for the created dashboards in tableau.  \n\u2022 Participated in Informatica  DVO  trainings and started extensive use of the tool as part of the DevOps roadmap.  Also \ndid POC  on icedq  and attended trainings.  \n\u2022 Built SQL pipelines using Redgate tool belt  in bamboo and deeper knowledge of branching strategy.  \n\u2022 Used both SonarQube  and Veracode  in the DevOps pipelines using Bamboo . \n\u2022 Built the automation framework for reducing the regression test cycle efforts.  \n\u2022 Used Informatica  PowerCenter  and SSIS  to both move and modify data from different environments and optimized \nworkflows.  \n\u2022 Proficient in SSIS ETL packages  & Informatica workflow  designing and development for various complex solutions \nincluding files processing, Incremental loads, historical data maintena nce. Extensively used Configurations, Logging, \nDebugging using breakpoints, Check points, Transactions, Error and Event Handling . \n\u2022 Assisted team in adding new data concepts and developing new data model development methodologies, principles, \nstandards and g overnance considering process optimization aspects.  \n\u2022 Worked on Tidal to schedule the jobs needed and configure them to receive the notifications on demand basis.  \n \n\u2022 Actively in Cloud  CoE trainings and completed AWS Cloud practitioner course.  \n\u2022 Creation of POC and doing architecture POC for upcoming AWS  engagements.  \n \n \n \nEnvironment : MS SQL Server 2016, 2014/2012/2008/2005/2000(SSMS), Visual Studio 2010/2012 ,2017 , TFS, VSS, \nReporting Services (SSRS), Integration Services (SSIS) , Informatica  PowerCenter , T-SQL,  PL-SQL,  .Net, Redgate tools, \nPowerShell Scripting,  python, Excel , Sisense, Tableau.  \n \n \n \n \nAccordant, A CVS Caremark Company, Greensboro, NC                                                                May 2015 \u2013 Mar 2017  \nSr. SQL  DBA  /Developer  \n \nAccordant Health Services , a CVS Caremark company, is a recognized leader in delivering disease management and case \nmanagement services for people with rare chronic conditions.  It has been providing value -added services on behalf of our \ncontracted clients such as health plans, emplo yers, and third -party  administrators (TPAs) for more than 14 years.  \n \nResponsibilities:  \n\u2022 Installation of MS SQL 2012  enterprise edition  on production, Test and Development environments . \n\u2022 Configured many alerts for sql server ongoing health checks and disk space alerts.  \n\u2022 Took part in design and implementation of Log shipping on few instances for the development and testing teams.  \n\u2022 Involved in Business requirement gathering, Technical Design Documents, Business use cases and Data mapping.  \n\u2022 Extensively worked on SSIS , designed and created mappings using various SSIS transformations like  \nOLEDB Command, Conditional Split, Lookup, Aggregator, Multicast, lookup and fuzzy logic.  \n\u2022 Developed SQL scripts to Insert/Update and Delete data in production Environment. Develop a nd implement policies \nand standards for preserving the integrity and security of data.  \n\u2022 Configure SSIS Package for run time Parameters and Configuration file.  Set the Standards for ETL development for the \nteam to follow on naming standards and best practice s for the meta -data, event handling and logging  \n\u2022 Strong expertise in writing stored procedures and performance optimization for the poor running queries. Good T -SQL \nProgramming skills.  \n\u2022 Worked with the development team to implement data strategies, build data flows and develop data models.  \n\u2022 Used Reporting Services (SSRS) to schedule reports to be generated on predetermined time.  \n\u2022 Generated on -demand and scheduled reports for business analysis or management decision using SQL Server \nReportin g Services (SSRS).  \n\u2022 Used Team Foundation Server and Visual Source Safe as a version control tool.  Migrated objects and solutions from \nvisual source safe to TFS. \n\u2022 Experience in deploying created reports in various sources like Web browser, XML and PDF.  \n\u2022 Used S QL Server Profiler to trace the slow running queries and tried to Optimize SQL queries for improved performance \nand availability.  Implement table partitioning  to avoid table level locking issues when different operating company\u2019s \ndata process is initiated at the same time by locking at partition level.  \n\u2022 Good understanding of Microsoft Reporting Service (SSRS) with Report authoring, Report management, Report \nformatting, Report distribution, Report delivery and Report security.  \n\u2022 Extensive experience in Capacity  planning, Performance Tuning, Disaster Recovery, Troubleshooting procedures . \nActively took part in disaster recovery planning and handled the real time issues.  \n\u2022 Also worked on ARMS  reporting application where I have hands -on experiencing in formatting reports, scheduling and \ndelivery of reports.  \n\u2022 Has good exposure to deploy the solutions in cloud platform and also storage usage.  \n\u2022 Involved in implementing business rules on client files  and membership files from the client and mock the data to \nrequired frequencies and also developed mechanism to alert when frequencies are not met on the fields in \nimplementation.  \n\u2022 Has hands -on experience in deploying the ssis packages to both file and serv er levels.  \n \n \nEnvironment : MS SQL Server 2014/2012/2008/2005/2000(SSMS), Visual Studio 2010/2012, TFS, VSS, Reporting \nServices (SSRS), Integration Services (SSIS), T -SQL, .Net, Excel.  \n \n \n \nFirstView Financial LLC, Atlanta, GA                                                         July 2011 \u2013 May 2015  \nSr. SQL DBA  /Developer  \n \nFirstView Financial LLC, Atlanta, GA provides prepaid debit card solutions enabling clients to enjoy all the benefits of \noperating a fully custom prepaid d ebit card program.  \n                                                                                                                                             \nResponsibilities  \n\u2022 Installation of MS SQL 2008 R2 servers on production, Test and Development boxes . \n\u2022 Developed ETL solutions using SQL Server Integration Services (SSIS) to import data based on requirements for \neasy/less maintenance and easy upgrade.  \n\u2022 Employed condition -based  notifications to let the user know the status of the Agent job s. \n\u2022 Developed complex SSIS packages  using proper control flow tasks  and data flow  transformations as per business \nrequirements.  \n\u2022 Migrated data from EXCEL and Flat files using SSIS packages to load data to the relational database in SQL Server \n2008R2 supporti ng BI solutions.  \n\u2022 Implemented Jaro -Winkler distance algorithm to calculate the similarity between strings in Sql server.  \n\u2022 Developed custom vb scripting for use in building custom functionality in job alerts.  \n\u2022 Developed dynamic SQL and dynamic store procedures, views, indexes, CTEs, cursors for business needs.  \n\u2022 Developed test scripts and environment of the developed SSIS solutions and the loaded data.  \n\u2022 Developed complex SQL scripts based on the business requirements in SQL Server databases.  \n\u2022 Implemented Disaster Recovery plans using Database Mirroring and Log Shipping.  Participated  in disaster recovery \ndrills and played important role in DR restoration.  \n\u2022 Controlling day to day activities of Production database and troubleshooting the issues.  \n\u2022 Researched and implemented SQL Server Auditing 2008 on SQL Servers for Auditing needs.  \n\u2022 Supported SSAS cubes as part of production support and monitored the loading process.  \n\u2022 Involved in requirement gathering, technical documentation Phases, supporting testing . \n\u2022 Created ETL jobs to load and clean gigabytes of data and also handled partitioning of the tables in ETL as a part of \nperformance tuning while handling huge volume of data.  \n\u2022 Provided production support and resolved production tickets.  \n\u2022 Handled  user requested issues through Service Tickets.  \n\u2022 Involved in Change Control Management is the discipline of systematically identifying and controlling change requests \nto a project from both internal and external sources.  \n\u2022 Migrated SQL Server 2005 database to  MS SQL Server 2008  \n\u2022 Designed and implemented comprehensive Backup plan and disaster recovery strategies Implemented.  \n\u2022 Created database objects like tables, views, indexes, stored -procedures, triggers, cursors   \n\u2022 Successfully implemented Database Mirroring in  SQL Server 2008  \n\u2022 Successfully Configured Snapshot, Transactional and Transactional with updatable subscription in Replication.  \n\u2022 Created Maintenance Plans for production servers (Full, Differential and Transactional Backups).  \n\u2022 Monitored and modified Performan ce using execution plans and Index tuning.  \n\u2022 Installation of 32 bit and 64 Oracle 10g client and applied DST Patches for Oracle Linked servers  \n\u2022 Co-coordinating with the programmer analyst for optimizing query, writing stored procedures.  \n\u2022 Conducting Root Cause Analysis of application availability and narrow down to issues related to coding practices, \ndatabase bottlenecks, or network latency  \n\u2022 Creating logins, groups, users, roles, database devices, databases, mirroring  devices, checking for database \nconsiste ncy, fixing DBCC errors, monitoring  error logs, database space allocations, transaction log space allocations , \nfine tuning SQL performance.  \n\u2022 Resolving Locking and Blocking issues by using various SQL  server internal commands  \n \nEnvironment  \nMS SQL Server 2008R2 Enterprise Edition, Oracle, SSMS, SSIS, SSRS , TOAD, SQL Plus, OLAP , OLTP, T -SQL, MS Excel, \nMS Access, Unix, MS Visual Studio.Net, VB, XP professional,2003, Crystal Reports XI.  \n",
          "confidence": 1.0,
          "method": "full_text",
          "structured_data": null
        },
        "tax_term": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "source_by": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "designation": {
          "value": "Developer",
          "confidence": 0.8,
          "method": "regex",
          "structured_data": null
        },
        "experience": {
          "value": "9",
          "confidence": 0.9,
          "method": "regex_total_experience_summary",
          "structured_data": null
        }
      }
    },
    {
      "resume_path": "data\\input\\Data Entry - NY - Lauren.pdf",
      "confidence_score": 0.645,
      "used_ocr": false,
      "extracted_fields": {
        "skills": {},
        "first_name": "Lauren",
        "last_name": "Dubin",
        "primary_email": {
          "value": "laurendbrnx2005@aol.com",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "secondary_email": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "phone": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "city": {
          "value": "Bronx",
          "confidence": 0.7,
          "method": "ner",
          "structured_data": null
        },
        "state": {
          "value": "NY",
          "confidence": 0.9,
          "method": "zip_database",
          "structured_data": null
        },
        "zip": {
          "value": "10468",
          "confidence": 0.7,
          "method": "regex",
          "structured_data": null
        },
        "work_authority": {
          "value": "S",
          "confidence": 0.8,
          "method": "regex",
          "structured_data": null
        },
        "resume_link": {
          "value": "data\\input\\Data Entry - NY - Lauren.pdf",
          "confidence": 1.0,
          "method": "file_path",
          "structured_data": null
        },
        "raw_resume": {
          "value": "Lauren Dubin  \n2835 WEBB AVENUE APT. 8G  \nBRONX, NY 10468  \nCELL:  917 -392-2569  \nPHONE/FAX (718) 549 -3568  \nlaurendbrnx2005@aol.com  \n \nPROFESSIONAL SUMMARY  \n \nA skille d and dedicated Administrative, Data Entry, and Customer Service professional with  10 years experience \ncoordinating, planning, and supporting daily operational functions. Excellent written and oral communication skills. \nExperience handling sensitive and confidential business. Superior organizational and multi -tasking skills. Ability to \nwork independently with minimal supervision, as well as in team environment. Highly motivated individual with \nprofessional demeanor.  \n \nSKILLS  \n \nData Entry, Typing (80+ WPM), Transcription, Medical Scribe, fast long -hand, familiarity with ICD -10 medical \ncoding, insurance, pre -certifications/authorizations, Windows 10, Microsoft Office 365 which includes Word  2019 , \nExcel  2019 , PowerPoint  2019 , Access  2019 , Outlook , and Publisher, Eudora, EPIC, NextGen, Athena, Mozilla \nFirefox & Thunderb ird, Adobe applications , Mac computers, electronic tablets, QuarkXPress, PC DOCS, medical \nterminology, Electronic Medical Records, most transcription systems including internet applications, and voice \nrecognition software such as PowerScribe, Legal word pr ocessing centers, recep tion including Call Centers, clerical, \nscanning, data entry, customer service, computer tech support/Help Desk (problem solving & troubleshooting  \nabilities) , maintaining filing and database systems . \n \nPROFESSIONAL EXPERIENCE  \n \nMetropol itan Transportation Authority     August 2017 -PRESENT  \nBronx, NY \nData Entry/Office assistant  \n\u2022 Comparing data entered with source documents.  \n\u2022 Correcting errors when appropriate.  \n\u2022 Maintaining required records of all completed work and compiling weekly reports.  \n\u2022 Assist in maintaining filing system by organizing, purging, maintaining and retrieving files.  \n\u2022 Clerical duties as assigned  \n \nAcacia Network, Bronx, NY       December 2016 \u2013 June 2017  \nMedical Scribe  \n\u2022 Anticipate physician needs to facilitate the flow of clinics.  \n\u2022 Accurately and thoroughly record patient medical history an d physical exam, procedures and treatments \nperformed by healthcare professionals, including nurses and physician assistants.  \n\u2022 Patient education and explanations of risks and benefits, physician -dictated diagnoses, prescriptions and \ninstructions for patient or family members for self -care and follow -up. \n\u2022 Prepare referral letters as directed by the physician.  \n\u2022 Collect, organize and catalog data for physician quality reporting system and other quality im provement \nefforts and format for submission.  \n\u2022 Attend training s on diverse subjects such as information technology, legal, HIPAA and regulatory \ncompliance, billing and coding.  Quickly assimilate new knowledge into process and procedures.  \n\u2022 Proofread and edit all the physician\u2019s medical documents for accuracy, spelling , punctuation and grammar.  \n\u2022 Utilizing NextGen E lectronic Medical Record system.  \n\u2022 Use of Microsoft Word 2016, Outlook 2016  \n \nTunstall Medical Alert \u2013 Long Island, NY      March 2016 \u2013 July 2016  \nData En try Operator, Customer Service Representative | Temporary As signm ent \n\u2022 Entering alphanumeric information into computer system in preparation for medical billing  \n\u2022 Assisting with authorizations for installation of medical alert equipment  \n\u2022 Electronic and paper filing \n\u2022 Customer service duties  \n\u2022 Clerical duties as assigned  \n \nMill Basin Radiology Services \u2013 New York, NY     October 2013 \u2013 November 2017  \nRemote Medical Transcriptionist /Medical data entry |Part-time and  Per Diem ) \n\u2022 Home based transcription utilizing Word 20 13 of time sensitive material in formatted medical radiology  \nreports, mainly sonograms on an as needed basis, often with deadlines  \n\u2022 Entry of medical data in radiology and medical procedures into Microsoft Word  \n\u2022 Use of Windows 10, medical terminology, Office  2013 including Outlook  \n \nPatient Care Associates \u2013 Mamaronec k, NY    November 2012 \u2013 June 2013  \nMedical Data Entry Operator and Transcriptionist, Office Assistant  \n\u2022 Entering of medical data of radiology and cardiology formats  for acute care facilities involvi ng deadlines and \ntime sensitive material, record keeping of same, projects and spreadsheets recording productivity  \n\u2022 Use of medical terminology, internet incorporated transcription and dictaphone applications  \n\u2022 Reception and clerical support as needed  \n \nMill B asin Radiology Services \u2013 New York, NY     November 2011 \u2013 October 2012  \nSenior In -House Medica l Transcriptionist, Secretary  \n\u2022 Entry of medical data in radiology and medical procedures into Microsoft Word 2003, often with deadlines , \ntime sensitive material fo rmatted  into medical radio logy reports, mainly sonograms  \n\u2022 Filing \n\u2022 Record keeping   \n\u2022 Problem solving and troubleshooting  \n\u2022 Reception  and clerical support as needed    \n\u2022 Use of medical terminology,  Windows XP, Mic rosoft Word 2003 and Excel 2003  \n \nWeill Cornell Medical  College \u2013 New York, NY     May 2009 \u2013 September 2011  \nMedical Administrative Assistant  \n\u2022 Performed administrative support services for the Director and Faculty of the Myeloproliferative Diseases \nprogram in the Department of Medicine/ Division of Hematology an d Medical Oncology.  Maintained \ncomputerized calendar   \n\u2022 Coord inated scheduling appointment process for long -term scheduling monthly, weekly, and daily views   \n\u2022 Coordinated all necessary paperwork for daily meetings and events  \n\u2022 Dealt with press and broadcast media to set up arrangements for the Director's public appear ances    \n\u2022 Handled travel arrangements including local, domestic, and foreign travel  \n\u2022 Handled confidential information and situations on a daily basis   \n\u2022 Screened all phone calls   \n\u2022 Assisted in prepa ration and typing of research grants and contract application s and protocol submissions  \n\u2022 Data entry of clinical material as dictated into Epic electronic medical record  \n\u2022 Performed other job -related duties as required. Use of medical terminology, Windows XP,  Microsoft Office \n2007, Eudora, Mozilla Firefox & Thunderbird , Adobe, Outlook, and Epic  \n \nJCW Enterprises Typing Service \u2013 New York, NY    November 2005 \u2013 December 2010  \nRemote Medical Transcriptionist  \n\u2022 Home based typing and transcription, work was on a contr act basis and commission and consisted of \nmedical transcripti on and data entry  of radiology and surgical reports  \n\u2022 Use of Windows XP, Microsoft Office 2003: Word and Excel, Centricity and internet applications; RTAS \ndictaphone  \n \nEDUCATION  \n \nThe Rhodes School \u2013 New York, NY  \nHigh School Diploma  \n \nMandl School \u2013 The Colleg e of Allied Health \u2013 New York, NY  \nMedical Secretary Science Certificate  \n",
          "confidence": 1.0,
          "method": "full_text",
          "structured_data": null
        },
        "tax_term": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "source_by": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "designation": {
          "value": "Data",
          "confidence": 0.8,
          "method": "regex",
          "structured_data": null
        },
        "experience": {
          "value": "10",
          "confidence": 0.9,
          "method": "regex_total_experience_summary",
          "structured_data": null
        }
      }
    },
    {
      "resume_path": "data\\input\\David Buchberger - Security - OH.pdf",
      "confidence_score": 0.5850000000000001,
      "used_ocr": false,
      "extracted_fields": {
        "skills": {},
        "first_name": "David",
        "last_name": "Buchberger",
        "primary_email": {
          "value": "david_buchberger@columbus.rr.com",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "secondary_email": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "phone": {
          "value": "6142029370",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "city": {
          "value": "Sugar Grove",
          "confidence": 0.9,
          "method": "address_pattern",
          "structured_data": null
        },
        "state": {
          "value": "OH",
          "confidence": 0.9,
          "method": "address_pattern",
          "structured_data": null
        },
        "zip": {
          "value": "43155",
          "confidence": 0.9,
          "method": "address_pattern",
          "structured_data": null
        },
        "work_authority": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "resume_link": {
          "value": "data\\input\\David Buchberger - Security - OH.pdf",
          "confidence": 1.0,
          "method": "file_path",
          "structured_data": null
        },
        "raw_resume": {
          "value": "Resume \u2013 David Buchberger \n 1 RR1 Box 592, Sugar Grove, OH 43155 - (614) 202-9370 -  david_buchberger@columbus.rr.com  PROFILE  Security professional and USAF veteran with 20+ years\u2019 IT experience (including at Battelle and State of Ohio), with 4+ years in security operations, seeks position that requires familiarity with security tools, ability to communicate with all levels of the organization, and experience with the implementation of cyber strategies from inception to execution.   EDUCATION  Embry Riddle Aeronautical University                                                                               B.S.  EMPLOYMENT HISTORY  August 2018 \u2013  February 2020 Infinite Contract State of Ohio \u2013 Office of Information Security and Privacy   Tier 1 Cyber Security Analyst \u2022 Respond to assigned tickets using ServiceNow \u2022 Remediate phishing attacks using Microsoft Defender Advanced Threat Protection, Office 365, and Symantec Management Center \u2022 Use Symantec Web Analytics to allow approved access to Web page URLS \u2022 Assist Tier 2 and Tier 3 with tickets as needed \u2022 Use McAffe ePO to detect malware on servers and desktops \u2022 Use Tanium to troubleshoot infected systems  2015 \u2013 2018 Battelle      Columbus, OH   Network Systems Analyst II (Security Operations Center/Vulnerability Management) \u2022 Scan Network with Nessus Manager and Professional \u2022 Create and monitor Scan schedule \u2022 Build Nessus Agent Groups \u2022 Perform credentialed, Non-credentialed and Agent scans on servers and workstations \u2022 Perform credentialed and Non-Credentialed WAS and DMZ scans \u2022 Perform internal and external Discovery scans \u2022 Work with clients to resolve Nessus plugin scan requirements \u2022 Prioritize network vulnerability threat with Kenna Security \u2022 Monitor Kenna Security Connector tunnel Nessus file upload integrity \u2022 Assign and track ServiceNow vulnerability remediation requests \u2022 Provide vulnerability remediation assistance to server Admins \u2022 Maintain Access Management CMDB \u2022 Create reports for upper management \u2022 Create detailed instructional documentation \u2022 SOC exposure to SIEM tools i.e. Carbon Black (Response and Defense), FireEye,   2013 \u2013 2015 Battelle      Columbus, OH   Network Systems Analyst II (Internal Network) \u2022 Maintain 2FA (RSA Token, Terminal Server and VMware) NIST FISMA and HIPAA environment \u2022 Verify VEEAM backup integrity, create reports and post to SharePoint for client review \u2022 Troubleshoot, install, and remove third party applications \u2022 Troubleshoot network connectivity issues \u2022 Troubleshoot printer issues \u2022 Install updates \u2022 Remediate vulnerabilities \u2022 Familiarity with Microsoft Group Policy objects as it applies to workstations \u2022 Experience with Microsoft Direct Access \u2022 Submit Firewall and Switch RFC \u2022 Assist remote Users \u2022 Create monthly internal network vulnerability report  2006 \u2013 2013 Battelle      Columbus, OH   Sr. Comm/Equip Spec III (20,000+ User environment)  \nResume \u2013 David Buchberger \n 2 2002 \u2013 2006  Arlington Computer Products (Discover Card)   Columbus, OH   PC Technician (3,500+ User environment) 2001 \u2013 2002  TEKsystems (Columbia Gas)    Columbus, OH   PC Technician (1,000+ User environment) 2000 -2001  National Board of BPVI    Columbus, OH   Technical Support Specialist (100+ User environment) 1998 \u2013 2000 United Parcel Service    Columbus, OH PC Technician (100+ User environment)   KEY SKILLS Communication: \u2022 Able to quickly resolve issues, strong communication skills, and able to maintain professional demeanour in stressful situations  IT: \u2022 Understand packet analysis and networking  \u2022 Familiar with incident response lifecycle  \u2022 Able to be part of on-call rotation  \u2022 Familiarity with Windows, and mobile operating systems  Cyber: \u2022 Previous Information Security experience  \u2022 Independent-thinker and self-starter \u2022 Team Player \u2022 Good follow-up skills and attention to detail  \u2022 Good customer service skills Department Interface Experience \u2022 Support Desk \u2022 Desktop and Infrastructure Support \u2022 Testing (Workstation/Server software/hardware configuration) \u2022 Account Management \u2022 Firewall Team \u2022 Security Operations Center (SOC) \u2022 Asset Management \u2022 Vulnerability Management \u2022 Security Engineering \u2022 Governance, Risk Management, and Compliance (GRC) \u2022 Information Security and Compliance Zone (Project Management) \u2022 Patch Management (SCCM) \u2022 Server Admins (Microsoft & Linux) \u2022 SharePoint \u2022 Data Backup/Storage \u2022 Third Party vendors Certifications: \u2022 ITIL v3, CompTIA A+, MCP, CompTIA Security+, CompTIA CySA+ (in progress) Other: \u2022 Microsoft Certified Professional \u2022 Microsoft Direct Access \u2022 Nessus Manager and Professional \u2022 McAfee Vulnerability Manager \u2022 SharePoint \n",
          "confidence": 1.0,
          "method": "full_text",
          "structured_data": null
        },
        "tax_term": {
          "value": "CONTRACT",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "source_by": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "designation": {
          "value": "Security Analyst",
          "confidence": 0.9,
          "method": "ner",
          "structured_data": null
        },
        "experience": {
          "value": "20+",
          "confidence": 0.9,
          "method": "regex_total_experience_summary",
          "structured_data": null
        }
      }
    },
    {
      "resume_path": "data\\input\\David Musia - Mainframe - TX.pdf",
      "confidence_score": 0.45500000000000007,
      "used_ocr": false,
      "extracted_fields": {
        "skills": {},
        "first_name": "David",
        "last_name": "Musia Denton",
        "primary_email": {
          "value": "davidmusia15@gmail.com",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "secondary_email": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "phone": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "city": {
          "value": "Denton",
          "confidence": 0.7,
          "method": "ner",
          "structured_data": null
        },
        "state": {
          "value": "TX",
          "confidence": 0.9,
          "method": "zip_database",
          "structured_data": null
        },
        "zip": {
          "value": "76210",
          "confidence": 0.7,
          "method": "regex",
          "structured_data": null
        },
        "work_authority": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "resume_link": {
          "value": "data\\input\\David Musia - Mainframe - TX.pdf",
          "confidence": 1.0,
          "method": "file_path",
          "structured_data": null
        },
        "raw_resume": {
          "value": "David Musia  \nDenton,  TX 76210        \ndavidmusia15@gmail.com  | https://www.linkedin.com/in/david -musia -bb1b04151/   \n \nOPERATOR and TECHNICIAN : \nCOMPUTER  OPERATOR  \n|\n MAINFRAME  OPERATOR  \n|\n BATCH  OPERATOR  \n|\n DATA CENTER  OPERATOR  \n \nPROFESSIONAL SUMMARY  \n \nMainframe, Batch,  Data Center and Computer Operator  with 20+ years\u2019 experience . Maintain ed 35+ mainframes consecutively. \nSuccessfully c ompleted and transitioned  with 3 corporate mergers.  Consistently identified  automation  opportunities  of \nmainframe processes to improve efficiencies.  Participated in interviewing, hiring, training and effectively managing personn el \nas needed by management.  Continually r eceive d accolades from customers and teammates for exceptional customer service.  \nMainframe industry experience includes Financial Services, Transportation , Card Services , Military, Oil, Medicare, Medicaid, \nSteel, Pharmaceutical, and U.S. Government . Strong decision -making skills and the ability to exercise composure and \nprofessionalism wh ile successfully managing all situations .   \n \nTECHNICAL EXPERTISE  \n \nAutomation Point  \nSRA  \nSM9  \nEON  \nTNG  \nCA-7 \nCA-11 \nSDSF  \nOPC  \nJOB MASTER  \nOPSMV S \nIMS \nCICS  \nDB2  \nMVS  \nJES2  \nJCS2  \nVirtual Tape  ATRM  \nHSC  \nSLS0  \nVM \nVSE  \nZEKE  \nEDSNET  \nTELLNET  \nSAM  \nWSF2  \nTSO  \nISPF  \nIMF  \nSYSVIEW  \nOMEGAMON  \nCAVIEW  \nNETVIEW  \nAPLMR  BOOLE and BABBAGE  \nJCL \nOffice Vision  \nCONTROL M  \nCONTROL R  \nVTAM  \nVPS  \nSAR  \nDOS  \nMS Offic e and Email  \nSkype  \nHP MyRoom  \nModify Commands  \nInfo Man  \nSTAR  \nVantive Digital Workflow / Service \nCenter  \n \nAREAS OF EXPERTISE  \n \nTeam Leader  \nManagement Support and Backup  \nMaintain 35+ Mainframe LPARS  \nIdentify  Automation  Opportunities  \nPerform IPL\u2019s  \nDisaster Recover Drills  \nDevelopment and Train New Employees  \nAnalyze, Identify,  Troubleshoot, Develop and Create Solutions  \nExceptional Customer Service  \nIdentify and Solve Hardware Issues  \nMonitor System Cycles  \nEmployee Interview and Hiring Decisions  ISO 9000, 9001 Certification  \nHigh Degree of Value on Accuracy and Detail  \nRisk, Controls and Compliance  \nSystems Migration  \nMonito r, Report, Document and Escalate  \nProject Administration  \nProblem Management  \nIncident Management  \nChange Management  \nQuality Management System  \nExce ptional  Planning  / Time Management  \nRoot Cause Analysis  \n \n \n \nEDUCATION  \n\n \nTexas State Technical College  \nComputer Science Technology and Programming  \nWaco , TX \n \nCAREER HIGHLIGHTS  \n \n\u2713 Identified numerous tasks to be automated to decrease operator intervention and increase streamlined operations.  \n\u2713 Relocated entire data center and trai ned Military personnel on operations.  \n\u2713 Facilitated training and development to offshore operators on outsource d mainframes.  \n\u2713 Monitored and supported 35+ mainframes.  \n\u2713 Analyze, troubleshoot and solve various system and customer mainframe issues.\n  \n\u2713 Create and proof QMS process documen ts.\n \n\u2713 Identify and troubleshoot operational issues, escalations and coordination of operational activities . \n\u2713 Ensur e the procedures o f change m anagement are followed for all change activity .\n \n\u2713 Monitor analyze and support  batch processing.\n  \n\u2713 Prioritize daily workload s to meet service level agreements.\n  \n \nWORK  EXPERIENCE  \n \nSenior Mainframe Operator           \nDXC Technology  (from EDS)          2000 \u2013 2019  \n Hewlett Packard Enterprises  (HPE) merged with DXC Technology 2017  \n Hewlett Packard (HP) merged with HPE 2015  \nElectronic Data Systems (EDS) merged with HP 2008  \n         \n \n \n",
          "confidence": 1.0,
          "method": "full_text",
          "structured_data": null
        },
        "tax_term": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "source_by": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "designation": {
          "value": "Project Administration",
          "confidence": 0.8,
          "method": "regex",
          "structured_data": null
        },
        "experience": {
          "value": "20+",
          "confidence": 0.9,
          "method": "regex_total_experience_summary",
          "structured_data": null
        }
      }
    },
    {
      "resume_path": "data\\input\\DEXTER K. NDENGABAGANIZI  - PM Program Manager - OH.pdf",
      "confidence_score": 0.555,
      "used_ocr": false,
      "extracted_fields": {
        "skills": {},
        "first_name": "PROFESSIONAL",
        "last_name": "EXPERIENCE C",
        "primary_email": {
          "value": "ndengaba@gmail.com",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "secondary_email": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "phone": {
          "value": "2693258811",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "city": {
          "value": "Pickerington",
          "confidence": 0.7,
          "method": "ner",
          "structured_data": null
        },
        "state": {
          "value": "OH",
          "confidence": 0.9,
          "method": "zip_database",
          "structured_data": null
        },
        "zip": {
          "value": "43147",
          "confidence": 0.7,
          "method": "regex",
          "structured_data": null
        },
        "work_authority": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "resume_link": {
          "value": "data\\input\\DEXTER K. NDENGABAGANIZI  - PM Program Manager - OH.pdf",
          "confidence": 1.0,
          "method": "file_path",
          "structured_data": null
        },
        "raw_resume": {
          "value": "DEXTER K. NDENGABAGANIZI  \n104 Thrush Circle  \uf06c  Pickerington, OH  43147  \uf06c (269) 325-8811  \uf06c ndengaba@gmail.com     \n \nEDUCATION  \nColumbia University : School of International and Public Affairs  New York, New York  \nGraduate Work Experience , International Finance and Economic Policy  Aug. 2017 \u2013 Jan. 2019  \n\uf0b7 Relevant Coursework : Finance, Public & Nonprofit Management,  Microeconomics, Macroeconomics, Statistics    \n \nMichigan State University: James M adison College  East Lansing, Michigan  \nBachelor of Arts in International Relations  May 2012\n \nPROFESSIONAL EXPERIENCE  \nCouncil on Foreign Relations  (CFR)  Washington, DC  \nAssistant Director, Washington Meetings Program  [May 2016 \u2013 Aug. 2017]  \nProgram Coordinator , Washington Meetings Program  [June 2015 \u2013 May 2016 ] \nProgram Associate, Washington Meetings Program  [Jan. 201 4 \u2013 June 2015 ] \nProgram Assistant , Washington Meetings Program  [Nov . 2012 \u2013 Jan. 201 4] \n\uf0b7 Conceptualize d, manage d, and coordinate d internal and external events ranging from roundtables to multi -day \n300+ daily attendee conferences with key policymakers across various stakeholder groups.  \n\uf0b7 Create d and m anage d relationships with  key domestic and international policymakers , subject matter experts, \nand business leaders to participate in CFR programming  and initiatives  for Council members.  \n\uf0b7 Research ed, monitor ed, and evaluate d international trends to  identify potential p rogramming for CFR \nmembers and the broader domestic and international political and policy community.  \n \nResults for Development Institute  Washington , DC  \nCenter for Health Market Innovations (CHMI) Intern  [May 2012 \u2013 Nov. 2012 ] \n\uf0b7 Assisted in the creation of s trategic plans to increase the visibility of dozens of health market innovations.  \n\uf0b7 Conducted needs synthesis used in the formulation of CHMI\u2019s Web and Facebook Outreach Strategy.  \n \nU.S. Embassy: Foreign Commercial Service  Singapore  \nForeign Commercial Service Intern  [Aug. 2011 \u2013 Jan. 2012 ] \n\uf0b7 Conduct ed trade promotion for 1,300+ American companies and products in Singapore . \n\uf0b7 Interviewed corporate and policy leaders, researched,  and produced  official U.S. Commercial Service Market \nResearch Report s used in U.S.-Singaporean business facilitation s. \n \nHouses of the Oireachtas: D\u00e1il \u00c9ireann (Irish National Parliament)  Dublin, Ireland  \nResearch Assistant , Fine Gael  [May 2011 \u2013 Aug. 2011 ] \n\uf0b7 Researched and produced reports on various European  economic policies , which were  used by Members of \nParliament in  nationally  televised  parliamentary address es.  \n \nADDITIONAL EDUCATION  \nUniversity of London  London, England [Summer 2009]  \nStudy Abroad Program - Economics of Law and Public Policy   \n \nUniversity of the Ryukyus  Okinawa, Japan [Sept. 2009 - Aug. 2010]  \nStudent Exchange Program \u2013 Study of Advanced Japanese and Japanese/Okinawan Culture  \n \nSKILLS  \nJapanese \u2013 Advanced -Low, Stata, R, Photoshop, Adobe Lightroom, Microsoft Access, Excel, Outlook, One Note  \n \nREPORTS AND PUBLICATIONS  \nSingapore: Aerospace Market  \u2013 U.S. Commercial Service: U.S. Department of Commerce (01/2012)  \nSingapore: Franchise Market  \u2013 U.S. Commercial Service: U.S. Department of Commerce  (12/2011)  \n",
          "confidence": 1.0,
          "method": "full_text",
          "structured_data": null
        },
        "tax_term": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "source_by": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "designation": {
          "value": "Program Coordinator",
          "confidence": 0.9,
          "method": "ner",
          "structured_data": null
        },
        "experience": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        }
      }
    },
    {
      "resume_path": "data\\input\\Enterprise Architect - LA - Vijay.pdf",
      "confidence_score": 0.405,
      "used_ocr": false,
      "extracted_fields": {
        "skills": {},
        "first_name": "Vijay",
        "last_name": "Kumar Kanamarlapudi",
        "primary_email": {
          "value": "vijay.kanamarlapudi@hotmail.com",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "secondary_email": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "phone": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "city": {
          "value": "",
          "confidence": 0.0,
          "method": "ner",
          "structured_data": null
        },
        "state": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "zip": {
          "value": "",
          "confidence": 0.0,
          "method": "regex",
          "structured_data": null
        },
        "work_authority": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "resume_link": {
          "value": "data\\input\\Enterprise Architect - LA - Vijay.pdf",
          "confidence": 1.0,
          "method": "file_path",
          "structured_data": null
        },
        "raw_resume": {
          "value": "Vijay Kumar Kanamarlapudi, TOGAF\u00ae, PMP\u00ae, CBAP\u00ae, ACP\u00ae, SAFe\u00ae4 POPM, SSM, PAHM, ITIL, SSBBP  1 \n VIJAY KUMAR KANAMARLAPUDI ,  \nTOGAF\u00ae, PMP\u00ae, CBAP\u00ae, PMI-ACP\u00ae, SAFe\u00ae 4 POPM, SSM , PAHM, ITIL\u00ae, SSBBP  \nEnterprise Business Architect / Project Manager/ Lead Business Analyst  \nvijay.kanamarlapudi@hotmail.com  | (860) 801 -0199  \n \nSummary  \nVijay has t hirteen  years of experience in healthcare as a  Business Consultant  with key expertise in \nBusiness Architecture, Project Management, Bu siness Analysis.  He has led teams in projects following \nAgile and Waterfall methodologies . \n \nCertifications  \n1. TOGAF\u00ae 9 Certified from The Open Group - 2019  \n2. Project Management Professional, PMP\u00ae  from Project Management Institute, PMI\u00ae - 2018  \n3. Certified Business Analysis Professional , CBAP\u00ae  from International Institute of Business \nAnalysis, IIBA\u00ae - 2014  \n4. Agile Certified Practitioner, PMI-ACP\u00ae  from Project Management Institute, PMI\u00ae -2017  \n5. Professional, Academy for Healthcare Management, PAHM  from America\u2019s Health Insurance \nPlans , AHIP - 2011 , 2017  \n6. SAFe \u00ae 4 Certified Product Owner/ Product Manager, SAFe \u00ae 4 POPM  from Scaled Agile \nFramework\u00ae , SAFe \u00ae \u2013 2017  \n7. SAFe \u00ae 4 Certified Scrum Master, SAFe \u00ae 4 SSM  from Scaled Agile Framework\u00ae, SAFe \u00ae- 2018  \n8. ITIL\u00ae  Foundation Certificate in IT Service Management from Axelos Global Best Practice - 2017  \n9. Six Sigma Black Belt Professional, SSBBP  from Management Strategy Institute - 2015  \n10. IBM Certified Database  Associate - DB2 Universal  Database V8.1 Family - DB2 Universal \nDatabase - 2007  \n \nKey Skills  \nEnterprise Business Architecture  Capability Modelling  \nValue Stream Mapping  Activity Mapping  \nBusiness Analysis  Planning and Monitoring  Project Management  \nRequirements  Management and Communication  Project Integration Management  \nElicitation  Project Risk Management  \nEnterprise Analysis  Project Scope Management  \nRequirements Analysis  Healthcare EDI Transactions  \nSolution Assessment and Validation  Waterfall Methodology  \nSCRUM Master  Agile Methodology  \nData Modeling  Project Quality Management  \nGap Analysis  Business Architecture  \nProcess Modelling  Use Case Modelling  \n \nTechnologies  \nVersion Controls: Clear Case, Visual SourceSafe, SharePoint  \nIBM Rational Tools: Requirements Composer, Requisite Pro, Clear Quest, Clear Case, Team Concert  \nSoftware: MS - Word, MS-Excel, Live m eeting, MS-Access, MS-Project, File -Aid, Cisco WebEx, TFS  \nProcess Modelling: Provision , Balsamic  \nLanguages: COBOL, JCL, SQL, XML  \nDatabases: IBM DB2, Oracle , SQL Server  \nVijay Kumar Kanamarlapudi, TOGAF\u00ae, PMP\u00ae, CBAP\u00ae, ACP\u00ae, SAFe\u00ae4 POPM, SSM, PAHM, ITIL, SSBBP  2 \n ERP: PeopleSoft  \nOther: Business Objects, nVision, Plan View, Cognos, Salesforce.com , Tableau , SSRS  \n \nEducation  \nMaster of Business Administration -MBA, May 2019 from Southeastern Louisiana University, Hammond, \nLA, USA  \nBach elor\u2019s in Electrical  and Electronics Engineering - B.Tech. , 2006 from Acharya Nagarjuna University, \nIndia  \n \nWork Experience  \nEnterprise Business Architect , Business Analyst Lead/ Senior Corporate Business Analyst  \u2022 11/2016- \nPresent  at Blue Cross and Blue Shield of Louisiana  \nPrincipal Business Analyst \u2022 09/2014 \u2013 10/2016 at Ultramatics Inc.  \nSenior Associate Consultant \u2022 04/2012 - 9/2014 at Infosys Ltd.  \nAssociate - Projects \u2022 09/2006 \u2013 04/2012 a t Cognizant Technology Solutions US Corp.  \n \nProfessional Experience  \n \nBlue Cross and Blue Shield of Louisiana , Baton Rouge, LA, US                             10/2018 - Present  \nROLE:  Enterprise Business Architect  \nSUMMARY:  \nWorked as a n Enterprise Business Architect analyzing enterprise level impacts on ideas and demands \nsubmitted by senior  management team and prepared business architecture for impacts at an enterprise \nlevel. Documented Business Capabilities; Value Streams; Processes th at are impacted for each idea and \nprepared Architecture Blue Prints for the Baseline and Target State Business Architecture.  \n \nRESPONSIBILITIES:  \n\u2022 Provide d strategic consultation to assigned line -of-business (LOB) or assigned functional area(s) of \nbusiness in  defining or designing business capabilities and processes, functions and organizational \nstructures.  \n\u2022 Participate d in divisional enterprise strategy development, including environmental analysis, \nopportunity identification, value cases and business innovati on portfolio development.  \n\u2022 Acted as an advocate for business needs while in parallel providing ongoing feedback on \ndevelopments and initiatives within IT  \n\u2022 Communicate d IT SLA metrics to respective areas of assigned business functions.  \n\u2022 Collaborate d, facilitat ed and consult ed with business stakeholders of the areas of responsibility in \nplanning, business case development and proposal of business initiatives in alignment with desired \nbusiness capabilities.  \n\u2022 Work ed with other peer business architects and EA architects to identify shared capabilities and \nprocesses and opportunities to create efficiencies across the enterprise and facilitate development \nof cross -functional solutions  \n\u2022 Provide d consultation to business a nalysts on an ongoing basis in communicating divisional \ncapability plans  \n\u2022 At the project execution level, collaborate d extensively with Business Analysts to ensure a tight \nalignment between the business capabilities and high -level use case scenarios identif ied in the \nSolution Blueprint document and corresponding detailed requirements identified in the Business \nRequirements Document (BRD) that support them.  \n\u2022 Participate d in enterprise architecture development, including business architecture, information \narchi tecture, application portfolio and technical architecture.  \n\u2022 Collaborate d with the enterprise program & project management office on reporting project status, \nissues, risks and benefits regarding areas of responsibility.  \nVijay Kumar Kanamarlapudi, TOGAF\u00ae, PMP\u00ae, CBAP\u00ae, ACP\u00ae, SAFe\u00ae4 POPM, SSM, PAHM, ITIL, SSBBP  3 \n \u2022 Explored  ways to apply new technology  to, and reuse existing technology for, business processes; \nresearch and provide information on technical trends and competitors' practices relevant to \nassigned LOB or assigned functional area(s) of business customers.  \n\u2022 Assess ed near -term needs, using struc tured interview processes, to establish business priorities; \nconsult with technical subject matter experts and develop alternative technical solutions; advise on \noptions, risks, costs vs. benefits, and impact on other business processes and system prioriti es. \n\u2022 Collaborate d with other IT functional areas to remain apprised of project status, and inform LOB \ncustomer management of progress; conversely, keep IT's technology and service managers aware of \nkey LOB customer issues, identifying and resolving potentia l problems and conflicts.  \n\u2022 Develop ed, motivate d and direct ed staff to create a team environment, and enable staff to fulfill the \nLOB customer operating objectives.  \n \nMY DELIVERABLES:  \nBusiness Architecture Blue print; Capability Mapping; Value Stream Mapping; Business Process Flow \nDiagram s; Business Need Analysis Document ; Data Requirements Specifications; User Interface \nSpecifications; Gap Analysis; Prototypes; Storyboarding; User Interface Screen Flow Diagram; Business \nRule Definitions . \n \nDELIVERABLES CONTRIBUTED TO:  \nNon -Functional Requirements Specification; Logical Data Model; Business Need Analysis Document; \nWork Breakdown Structure; Test Data Sheet; Test Plan; Test Scripts; Test  Strategy; Test Results \nSummary; Information Architecture; Project Deliverables List; Risk Register; Implementation \nPlan/Checkout activities ; Design Specifications; Technical Requirement Specifications ; SSO \nSpecifications.  \n \n \nBlue Cross and Blue Shi eld of Louisiana , Baton Rouge, LA, US                          10/2014 \u2013 10/2018  \nROLE:  Lead Business Analyst; Senior Corporate Business Analyst; Senior Enterprise Business \nAnalyst;  Scrum Master  \nSUMMARY:  \nWork ed as a program level business analyst overseeing the foundation to put together the operations \nof benefits administration of Medicare Advantage platform that Blue Cross Blue Shield of Louisiana is \nworking to put together.  Worked on various IT efforts to send data from BCBSLA to vendors and to \nreceive and sto re data from external vendors.  \n \nRESPONSIBILITIES:  \n\u2022 Worked closely with clients to identify business needs (requirements) and the costs and benefits of \nimplementing a proposed solution, as a project, in -order to produce an accurate business case for \nthe proj ect \n\u2022 Collaborated with business owners and subject matter experts to develop an understanding of \nbusiness processes and functions to aide in completing the standard project business case related to \na proposed project, analyzing business requirements and ide ntifying potential business solutions; \ncompletes appropriate documentation of business case, requirements and solutions.  \n\u2022 Participate d in divisional enterprise strategy development, including environmental analysis, \nopportunity identification, value cases and business innovation portfolio development.  \n\u2022 Performed project feasibility analysis, scopes project and helps client identify altern ative solutions; \nassist with the development of financial worksheets related to project costs and benefits  \n\u2022 Worked with clients to develop business process flows and to obtain a clear understanding of their \nbusiness models to identify potential pro cess improvements and efficiencies  \nVijay Kumar Kanamarlapudi, TOGAF\u00ae, PMP\u00ae, CBAP\u00ae, ACP\u00ae, SAFe\u00ae4 POPM, SSM, PAHM, ITIL, SSBBP  4 \n \u2022 Ensured adherence to BCBSLA project and program processes and lifecycles to ensure business and \nfunctional requirements are understood, agreed to, documented and can be traced to measurable \nproject success criteria.  \n\u2022 Analy zed business information and process flows, gather requirements and translate to Technical \nspecifications required for application programming and test strategy development.  \n\u2022 Assisted Project Manager in preparing an appropriate transition plan for projects prior to \nimplementation; assists with development and review of appropriate production documentation \nsuch as policies and procedures.  \n\u2022 Worked with clients and IT Quality Assurance to develop a test plan that can be executed via test \nscenarios and test cases  to ensure expected functionality, features, as well as traceability among \nbusiness requirements, technical specifications and testing are validated before releasing \napplication for User Acceptance Testing.  \n\u2022 Identified the functionality and features that should be tested and create test scenarios that validate \nperformance of the scenarios in the new application.  \n\u2022 Developed a test script that describes the details of the test scenarios  \n\u2022 Identified resources and functional areas that will be needed to perform testing  \n\u2022 Represented the Project Management Office in a positive, professional and enthusiastic manner \nwhen working with both internal and external clients to build and maintain the respect of the \nProject Management Office within the organization.  \n\u2022 Escalated  appropriate issues or problems to the project manager or Project Management Office \nManager, as appropriate, for discussion and evaluation  \n\u2022 Maintained professional demeanor with team members, business owners and fellow staff; provide \nconfidentiality for dis cussions regarding projects, position and salary information and other matters \nthat may be discussed in meetings  \n\u2022 Actively pursued personal leadership development to ensure professional growth; consistently \ndemonstrates a high level of quality results and m aintains expected productivity objectives.  \n\u2022 Served as Project Manager for selected effort when necessary.  \n\u2022 Adhered to organizational project management processes and lifecycles to ensure that project \nbusiness and functional requirements are understood, agree d to, can be traced to measurable \nproject success criteria and to drive quality among projects.  \n \nMY DELIVERABLES:  \nBusiness Process Flow Diagram; Data Requirements Specifications; User Interface Specifications; User \nStories; Gap Analysis; Deployment Communi cation Materials; End User Application Documentation; \nProduct Backlog; Issue Tracker; Risk Analysis; Prototypes; Storyboarding; User Interface Screen Flow \nDiagram; Business Rule Definitions; Requirements Traceability Matrix.  \n \nDELIVERABLES CONTRIBUTED TO:  \nNon-Functional Requirements Specification; Logical Data Model; Business Need Analysis Document; \nWork Breakdown Structure; Test Data Sheet; Test Plan; Test Scripts; Test Strategy; Test Results \nSummary; Information Architecture; Project Deliverables List; Ris k Register; Implementation \nPlan/Checkout activities.  \n \n \nCLIENT:  Aetna, Inc. , Hartford, CT, US                                                                     10/2013 - 10/2014               \nROLE:  Scrum Master; Lead Business Analyst  \nSUMMARY:  \nWorked as a Scrum  Master and Lead Business A nalyst for this project dealing not only with gathering \nand maintaining requirements but also guiding the team towards successful implementation of an \napplication with web -based  JAVA user interface functionalities in various  releases.  \n \nVijay Kumar Kanamarlapudi, TOGAF\u00ae, PMP\u00ae, CBAP\u00ae, ACP\u00ae, SAFe\u00ae4 POPM, SSM, PAHM, ITIL, SSBBP  5 \n RESPONSIBILITIES:  \n\u2022 Analysis of the existing business process and helping the product owners in determining the epics \naccording to the proposed business process  \n\u2022 Working with the product o wner to create and maintain product backlog  \n\u2022 Prepared user navigations flows and user interaction diagrams and requirement specifications for a \nweb based front end application.  \n\u2022 Documented business rules for storing the data entered by the user on the front JAVA user interface \n(web based UI) screens - into the back end DB2 tables.  \n\u2022 Planning the timelines for each of the sprints after categorizing the priority of the epics  \n\u2022 Facilitating productivity by removing impediments that obstructed the team\u2019s pursuit of its sprint \ngoals  \n\u2022 Ownership of requirements and also turning the requirements into working software  \n\u2022 Tracking and reporting the progress of the sprints and ensuring that the timelines are achieved \ncomfortably  \n\u2022 Conducting meetings for planning, daily scrum s tatus, scrum pre -planning, back log grooming, \nreview/demo and retrospective discussions  \n\u2022 Ensuring the application of standard processes in each and every stage of the sprints by enforcing \nthe rules of the agile process  \n\u2022 Documentation of the User stories, Use Case framework . \n\u2022 Preparation of Business Proce ss Flow Diagrams, User Interface Screen Flow Diagrams, User Interface \nMock Ups, Use Case Diagrams, Application Overview presentations  \n\u2022 Documentation of Deployment Communication Material and end user reference materials  \n\u2022 Ensuring that the scrum team works in a productive manner to deliver the goals of the sprints . \n \nMY DELIVERABLES:  \nBusiness Process Flow Diagram; Data Requirements Specifications; User Interface Specificatio ns; User \nStories; Gap Analysis; Deployment Communication Materials; End User Application Documentation; \nProduct Backlog; Issue Tracker; Risk Analysis ; Prototypes; Storyboarding ; User Interface Screen Flow \nDiagram ; Business Rule Definitions; Requirements Traceability Matrix.  \n \nDELIVERABLES CONTRIBUTED TO:  \nNon -Functional Requirements Specification; Logical Data Model; Business Need Analysis Document; \nWork Breakdown Structure; Test Data Sheet; Test Plan; Test Scripts; Test Strategy; Test Results \nSummary; Information Architec ture ; Project Deliverables List; Risk Register; Implementation \nPlan/Checkout activities.  \n \nCLIENT:  AETNA  INC., Hartford, CT, US                                                                   01/2013 \u2013 10/2013  \nROLE: Lead Business Analyst  \nSUMMARY : \nWorked as a Lead Business Analyst , managing the business analysts,  resolving i ssues to track to their \nclosure, ensuring  there are no scope -related concerns from stakeholders , guide and train  the Business \nAnalysts , and ensure  the requirements gathering, elicitation , and reviews are completed per plan.  \n \nRESPONSIBILITIES  \n\u2022 Leading a team of nine domain Business Analysts for all business analysis  activities  \n\u2022 Facilitated and lead requirement gathering sessions  \n\u2022 Problem s olving and issue resolution for scope related and business definition related topics  \n\u2022 Managing the requirement timelines and overcoming the challenges faced by individual domain \nlevel Business Analysts  \n\u2022 Coordinating all the tasks that need to be accomplished by all the Business Analysts in the project  \n\u2022 Busin ess analysis and requirement gathering  \nVijay Kumar Kanamarlapudi, TOGAF\u00ae, PMP\u00ae, CBAP\u00ae, ACP\u00ae, SAFe\u00ae4 POPM, SSM, PAHM, ITIL, SSBBP  6 \n \u2022 Conducting Joint Application Development (JAD) sessions  \n\u2022 Preparation of RTM \u2014Requirement Traceability Matrix  \n\u2022 Preparation of Functional Requirement Specification document  \n\u2022 Analysis of existing and nature of proposed proce ss \n\u2022 Developing process flow diagrams according to the proposed process and applications impacted  \n\u2022 Preparation of Use Case Narratives  \n\u2022 Reporting status of requirements progress for Level 1 management during project planning stage  \n\u2022 Support design, development, a nd testing t eams on clarifications for the requirements after \nrequirements phase  \n\u2022 Expertise in using various IBM Rational Tool : Rational Clear Case, Rational Clear Quest, Rational \nRequisite Pro  \n\u2022 Function Point Estimation  \n \nMY DELIVERABLES:  \nData Requirements Specification; Gap Analysis; User Interface Specification; Functional Requirements \nSpecification; Business Requirements Specification, Business Process Models; Use Case Model; Use Case \nNarratives; User Interface Screen Flow Diagram; Proto types ; Deployment Communication Material; Risk \nAnalysis; Issue Tracker; Decision Tables ; Coverage Matrix ; Requirements Traceability Matrix; Product \nSpecifications  \n \nDELIVERABLES CONTRIBUTED TO:  \nNon -Functional Requirements Specification; Logical Data Model; Source to Target Data Mapping \nDocuments; Business Need Analysis Document; Work Breakdown Structure; Test Data Sheet; Test Plan; \nTest Scripts; Test Strategy; Test Results Summary; Information Architecture; Project Delivera bles List; \nEstimation Work Book; Ri sk Register; Implementation Plan/ Checkout Activities.  \n \nCLIENT:  NORTHWESTERN MUTUAL LIFE INSURANCE , Milwaukee, WI, US         04/2012 - 01/2013  \nROLE:  Information Systems Business Consultant  \nSUMMARY : \nWorked as a business systems analyst in gathering requirements related to the proposed apart from \nunderstanding the nature of the current system. Played a key role in ensuring that the high -level design \ndeliverables aligned to the business needs enlisted by the business stakeholders.   \n \nRESPONSIBILITIES  \n\u2022 Analysis of existing system process functionality  \n\u2022 Analysis of the proposed business process flow and working on the proposed system process \nfunctionality based on the architecture guidelines  \n\u2022 Formulation of proposed process flows, use case diagrams and documentation of use cases.  \n\u2022 Documentation of the functional requirements for the proposed financial reports which vary across \nmultiple business areas  \n\u2022 Worked with the design teams to develop the technical specifications according to the reporting \nrequiremen ts \n\u2022 Helped the architectural and database design teams in migra ting the data from the old data base to \nthe new database  \n\u2022 Provided valuable contribution to the design of the new people soft accounting database (by \nproviding suggestions on the trees and nodes t o be created)  \n\u2022 Worked as an information system business consultant and helped to document the functional \nrequirements for the reports to be run from the new database  \n\u2022 Contributing in preparation of test strategy and test plan for the functionalities that are  expected \nfrom the new database  \nVijay Kumar Kanamarlapudi, TOGAF\u00ae, PMP\u00ae, CBAP\u00ae, ACP\u00ae, SAFe\u00ae4 POPM, SSM, PAHM, ITIL, SSBBP  7 \n \u2022 Preparation of high -level design documents with the help of the development team and conducted \nreview sessions with the business stakeholders  \n\u2022 Reporting the status of the functional specification and the high level design del iverables to the \nnecessary stakeholders  \n\u2022 Experience of working on nVision and Business Objects Reports  \n\u2022 Experience of working on reports based out of Cognos (for Business Intelligence Reporting)  \n \nMY DELIVERABLES:  \nReport Requirement Specifications; Data Requirements Specifications; High Level Design \nDocumentation; Business Process Models; Prototypes; Requirements Specifications; Deployment \nCommunication Material; End User Documentation; Gap Analysis; Issue Tracker; Risk Analysis; \nRequirements Traceability  Matrix  \n \nDELIVERABLES CONTRIBUTED TO:  \nNon -Functional Requirements Specification; Logical Data Model; Source to Target Data Mapping \nDocuments; Business Need Analysis Document; Work Breakdown Structure; Test Data Sheet; Test Plan; \nTest Scripts; Test Strategy ; Test Results Summary; Information Architecture; Project Delivera bles List; \nEstimation Work Book  \n \nCLIENT: AETNA INC., Hartford, CT, US                                                                 08 /2007 - 04/2012  \nROLE:  Business Analyst; Senior Business Analyst  \nSUMMARY:  \nWorked as a domain business analyst in all these projects and played a key role in analyzing the current \nprocess and working with the architecture team towards formulating a proposed process.  \nWorked wit h the appropriate stakeholders to deliver the requirements deliverables with good quality \nkeeping in mind the stringent timelines and compliance related constraints on the project.  \n \nRESPONSIBILITIES:  \n\u2022 Business Analysis and Requirement Gathering  \n\u2022 Preparation  of business process flow diagrams and use case model diagrams  \n\u2022 Conducting Joint Application Development (JAD) Sessions  \n\u2022 Preparation of RTM \u2014Requirement Traceability Matrix  \n\u2022 Preparation of Functional Requirement Specification Document  \n\u2022 Analysis of existing and nature of proposed process  \n\u2022 Developing process flow diagrams according to the proposed process and applications impacted  \n\u2022 Preparation of Use Case Specifications  \n\u2022 Reporting status of requirements progress for Level 1 management during project planning stage  \n\u2022 Support design, development, and testing teams on clarifications for the requirements after \nrequirements phase  \n\u2022 Expertise in using various IBM Rational Tools: Rational Clear Case, Rational Clear Quest, Rational \nRequisite Pro  \n\u2022 Worked on onsite/offshore model to  provide support to design, development and \nimplementation checkout activities.  \n \nMY DELIVERABLES:  \nRequirements Specification Document; Business Requirements Specification; Data Requirements \nSpecification; Prototypes; Issue Tracker; Use Case Narratives; Use  Case Models; Business Process \nModels; Risk Analysis; Gap Analysis; Data Scenarios; Decision Table; Business Rule Definitions; Data \nModels; Requirements Traceability Matrix; Function Point Estimation.  \n \nDELIVERABLES CONTRIBUTED TO:  \nVijay Kumar Kanamarlapudi, TOGAF\u00ae, PMP\u00ae, CBAP\u00ae, ACP\u00ae, SAFe\u00ae4 POPM, SSM, PAHM, ITIL, SSBBP  8 \n Non -Functional Requiremen ts Specification; Logical Data Model; Business Need Analysis Document; \nWork Breakdown Structure; Test Data Sheet; Test Plan; Test Scripts; Test Strategy; Test Results \nSummary; Information Architecture; Project Deliverables List, Risk Register  \n \nCLIENT:  AETNA INC. , Chennai, India                                                                         09/2006 \u2013 08/2007                  \nROLE:  Test Specialist  \nSUMMARY:  \nWorked as system integration tester, being the front runner in completing related testing, and made \nsure the deployed product is defect free. Identified design flaws and notified the project team of the \nimpacts and behavior of the system under flaws.   \n \nRESPONSIBILITIES  \n\u2022 Developed the various test artifacts, such as test plan, test scripts, test c ases, test execution logs, \ntest result summary  \n\u2022 Execution of application -oriented testing and recording the status in quality center  \n\u2022 Key resource with strong business knowledge in the corporate domain  \n\u2022 Performed complete analysis of the existing process and maintained proper repository for all the \nclarifications  \n\u2022 Conducted knowledge transfer session for explaining the design to the whole team  \n\u2022 Verification of coding for meeting the design in the given technology; performed the code review \naccording to the codin g standards  \n\u2022 Supported the team members for any technical challenges, as well as for the application challenges  \n\u2022 Well -versed in various tools used in the project such as IBM -DB2, Hummingbird Connectivity tool \nand monitoring jobs through Informatica  \n\u2022 Team management and effective team play  \n \nMY DELIVERABLES:  \nTest Data Sheet; Test Plans; Test Results Summary; Test Scripts; Test Strategy; SQA Testing Strategy and \nRequirements Summary; Coverage Matrix; Defect Tracker  \n \nDELIVERABLES CONTRIBUTED TO:  \nEstimation Wor kbook; Requirements Specification Document; Work Breakdown Structure; Use Case \nNarrative; Report Mockup  \n \n",
          "confidence": 1.0,
          "method": "full_text",
          "structured_data": null
        },
        "tax_term": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "source_by": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "designation": {
          "value": "Business Architect",
          "confidence": 0.9,
          "method": "ner",
          "structured_data": null
        },
        "experience": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        }
      }
    },
    {
      "resume_path": "data\\input\\Horace Royal - Resume.pdf",
      "confidence_score": 0.5700000000000001,
      "used_ocr": false,
      "extracted_fields": {
        "skills": {},
        "first_name": "Horace",
        "last_name": "Royal",
        "primary_email": {
          "value": "horaceroyaljr6_t3n@indeedemail.com",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "secondary_email": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "phone": {
          "value": "2402811264",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "city": {
          "value": "Washington",
          "confidence": 0.7,
          "method": "ner",
          "structured_data": null
        },
        "state": {
          "value": "DC",
          "confidence": 0.8,
          "method": "city_database",
          "structured_data": null
        },
        "zip": {
          "value": "",
          "confidence": 0.0,
          "method": "regex",
          "structured_data": null
        },
        "work_authority": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "resume_link": {
          "value": "data\\input\\Horace Royal - Resume.pdf",
          "confidence": 1.0,
          "method": "file_path",
          "structured_data": null
        },
        "raw_resume": {
          "value": "Horace Royal\nWashington, DC\nhoraceroyaljr6_t3n@indeedemail.com\n+1 240 281 1264\nWork Experience\nContract Closeout Specialist\nCTR Management\nNovember 2018 to Present\nContractor)\nEmployer: CTR Management\nClient: Department of Transportation (DOT)\n\u2022 Performs the full range of contract closeout activities pursuant to FAR 4.804, with a specialized focus on\nthe cognizant administration functions necessary to close flexibly-priced and Cost Accounting Standards\n(CAS) covered contracts.\n\u2022 Achieved closeout goals each month to efficiently and effectively eliminate the contract closeout\nbacklog.\n\u2022 Verify and coordinate closeout activities utilizing PRISM.\n\u2022 Supports the reconciliation of complex invoices.\n\u2022 Resolves issues related to de-obligated and excess funding.\n\u2022 Manages government contract billing/invoicing.\n\u2022 Closeout with a focus on Firm-Fixed Price (FFP), Labor Hour, and Time and Materials (T&M).\nContract Support Specialist\nCACI\nNovember 2018 to Present\nContractor)\nEmployer: CACI\nClient: Department of Health & Human Services (DHHS)\n\u2022 Performed closeouts of simplified acquisitions and various contract types (T&M, Labor, Cost Types).\n\u2022 Reviewed, prepared and submitted closeout documents. Input information into Contract\n\u2022 Management System. Retrieved information from various systems.\n\u2022 Processed Quick. Reviews and filled out all annual Prime Contractor Representations and\n\u2022 Worked jointly and coordinate with subsidiary company stakeholders to ensure all supporting policies\nand procedures are in-place to ensure compliance before making certification.\n\u2022 Collect, draft, review, maintain, and retained contract file documentation.\n\u2022 Draft acquisition/contract documentation for supported official(s) including requirements package\ndocumentation, Acquisition Strategy Panel (ASP) briefing slides, Acquisition Plan (AP), Business/Contract\nClearance form(s), Request for Proposal (RFP), and contract award document.\n\u2022 Pre-award functions include conducting market research, coordinating with stakeholders (e.g. Small\nBusiness Government Representative), publicizing contract actions, soliciting requirements, and drafting\nevaluation of offeror's proposal(s), and drafting awards.\n\u2022 Post-award functions include contract administration, contract actions (e.g. modification) and contract\ncloseout.\nContract Support Specialist\nPremier Management\nDecember 2016 to November 2018\nContractor)\nEmployer: Premier Management\nClient: Food drug Administration (FDA)\n\u2022 Performed all aspects of the Federal Government contract closeouts, some additional work in cradle-\nto-grave acquisitions.\n\u2022 Reconcile contract funding obligations and expenditures with final voucher amounts and determine\nde-obligated amounts.\n\u2022 Prepared contract closeout modifications using PRISM or like systems. Develops reports and prepares\nsupport documentation.\n\u2022 Performed audits of contract files ensuring adherence to policies, regulations, and identifying any\nduplication of efforts across contracts.\n\u2022 Reviewed documents for quality, clarity, and adherence to applicable regulations (FAR, agency\nregulations, etc.) and recommendation of revisions\n\u2022 Reconcile unbilled balances, excess funds, misaligned invoice payments, and related issues and\ncoordinate with the Finance Office to complete the reconciliation process for closure.\nContract Support Specialist\nCorporate Advance Auto\nDecember 2015 to March 2017\n\u2022 Managed complex contracts up to $100K for a variety of service contracts: Building & Facilities\nMaintenance, Lawn services/Grounds Maintenance, Janitorial Services,\n\u2022 Towing, and Police/Surveillance Services while performing in-depth technical market research and\nanalyzed past performance to prepare acquisition planning recommendations and cost analysis for\nDistrict Managers.\n\u2022 Drafted contracts and created, documented, and revised acquisition plans, policy procedures, and best\npractices on an ongoing basis.\n\u2022 Researched and analyzed federal contract laws and regulations.\n\u2022 Trained new personnel. Researched internally/externally to strategize and obtain the appropriate\nvendors, while performing market research to ensure vendors were given opportunity to bid. Negotiated\nwith vendors. Developed a price analysis.\nContract Support Specialist\nCavalier Consultants\nMarch 2015 to December 2015\nContractor)\nEmployer: Cavalier Consultants\nClient: Hillel Foundation\n\u2022 Researched the manufacturer of equipment or contractor.\n\u2022 Prepared provider monthly or annual service payments based on the terms of the maintenance\ncontract.\n\u2022 Negotiated bids for servicing of equipment and supply of labor.\n\u2022 Communicated with vendors as warranties were in the final phase of expiration dates for service\nof goods/ supplies Assisted Program Managers with technical issues with statement of objectives and\nperformance work statements.\n\u2022 Evaluated contract performance on previous contracts and meeting with the company representatives\nto discuss procurement needs, quality of items or services, current market prices, or delivery schedules.\nManaged several sales portfolio contract accounts totaled valued up to $250.000.00\nContract Support Specialist\nGate Gourmet\nJune 2012 to March 2015\n\u2022 Authorized and prepared with senior management staffing forecast schedules for annual budget for\ncredit cards, supplies, communications, travel and training new staff hires.\n\u2022 Developed and prepared inputs to briefings, planning documents, and policies for several airlines by\nconducting research and analysis of administrative information and data.\n\u2022 Provided sales financial analysis and support by designing graphs to present to Senior Management.\n\u2022 Completed and submitted all required documentation of existing contracts.\n\u2022 Acted as the contact person for execution of contracts.\n\u2022 Assisted in bidding sourcing teaming and subcontracting.\nContract Support Specialist\nDistrict Healthcare Inc\nJune 2006 to May 2012\n\u2022 Managed the evaluation process and for contract performance on previous contracts and organized\nkickoff- meetings with small and large businesses.\n\u2022 Created advertisements for bids, then reviewed and selected the bid most suitable to the timeframe\nand budget.\n\u2022 Researched the price of materials, labor cost, and overhead expenses to analyze the bids received.\n\u2022 Assessed whether contractors were adhering to contract parameters, laws, and regulations. Prepared\npurchase agreements, contracts, and leases to acquire the most cost-effective services and terms.\n\u2022 Compared prices, discounts, delivery dates, and handling charges. Negotiated prices and services.\nGrocery Manager\nShoppers Food Warehouse  - Clinton, MD\nMarch 1990 to January 2000\nI managed a store of 70 employees. I managed the shrink (theft,mis) lost of product. I also hired,trained\nemployees. I made the daily schedules. I ordered the products for the grocery department of dry goods.\nI did the monthly and quarterly/yearly inventory of the store. I stock the shelves as needed to keep\nproduct in store.\nEducation\nSTRAYER UNIVERSITY\n2015 to 2017\nAssociate in Business Management\nTemple Hills, MD\nSkills\n\u2022An organized professional with over 12 years of government contracting experience in both the\nfederal and private sectors. The focal-point of my expertise is in the realm of contract interpretation,\ncloseout, negotiation, and administration. In addition, I have extensive knowledge with the Federal\nAcquisitions Regulations (FAR) in which I have successfully executed complex multimillion-dollar\ncontracts for the federal government.\n\u2022Management Account Management, PSC ACQUISITIONS\n\u2022Contracting Writing System\n\u2022PD 2 Contracting Writing System\n\u2022SAM\n\u2022System for Award Management\n\u2022FPDS\n\u2022Federal Procurement Data System\n\u2022Comprizon, PD2 Contracting Writing System\n\u2022DCIS- Departmental Contracts Information System\n\u2022PRISM\n\u2022GLASS\n\u2022Contract Writing System for procurement\n\u2022DELPHI\n\u2022Finance Inventory (invoices, Billing data)\n\u2022Procurement\n\u2022Contracting\n\u2022Negotiation\n\u2022Outlook, windows 10, Microsoft,\n",
          "confidence": 1.0,
          "method": "full_text",
          "structured_data": null
        },
        "tax_term": {
          "value": "CONTRACT",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "source_by": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "designation": {
          "value": "Support Specialist",
          "confidence": 0.9,
          "method": "ner",
          "structured_data": null
        },
        "experience": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        }
      }
    },
    {
      "resume_path": "data\\input\\.Net - Data Lakes - Data Bricks - CA - Mehdi.doc",
      "confidence_score": 0.665,
      "used_ocr": false,
      "extracted_fields": {
        "skills": {},
        "first_name": "Mehdi",
        "last_name": "Haghdad",
        "primary_email": {
          "value": "mhaghdad2014@gmail.com",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "secondary_email": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "phone": {
          "value": "9493931150",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "city": {
          "value": "Newport Beach",
          "confidence": 0.7,
          "method": "ner",
          "structured_data": null
        },
        "state": {
          "value": "CA",
          "confidence": 0.9,
          "method": "zip_database",
          "structured_data": null
        },
        "zip": {
          "value": "92612",
          "confidence": 0.7,
          "method": "regex",
          "structured_data": null
        },
        "work_authority": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "resume_link": {
          "value": "data\\input\\.Net - Data Lakes - Data Bricks - CA - Mehdi.doc",
          "confidence": 1.0,
          "method": "file_path",
          "structured_data": null
        },
        "raw_resume": {
          "value": "\nMehdi Haghdad\n62 Parkcrest\nNewport Beach, CA 92612\nCell phone: (949) 393-1150\nmhaghdad2014@gmail.com\n\n                                  EDUCATION\nPHD, UCLA/UNIVERSITY OF CALIFORNIA DAVIS     OBTAINED ON 6/2003\nPhD in Electrical and computer engineering (in Smart Antenna Systems for\nLow Earth Orbit (LEO) Satellites)\n\nThe Royal Institute Of Technology, Stockholm, Sweden\nM.S. Degree: Telecommunications Engineering.\nB.S. Degree: Electrical Engineering\nB.S. Degree: Computer Science.\n\n                                   SUMMARY\nI HAVE ALMOST 20 YEARS OF EXPERIENCE IN ADVANCE ENGINEERING, 10 YEARS OF\nEXPERIENCE IN BIG DATA, 8 YEARS IN CLOUD, MULTI-CLOUD, AZURE, AWS, GCP, 7\nYEARS IN ARTIFICIAL INTELLIGENCE, ML DL CNN. I HAVE WORKED FOR SOME OF THE\nMOST PRESTIGIOUS COMPANIES IN SILICON VALLEY AND OTHER LOCATIONS SUCH AS\nLOCKHEED MARTIN, SPACE SYSTEMS LORAL, MICROSOFT (4 TIMES AS A SENIOR\nSOLUTION ARCHITECT FOR DIFFERENT MULTI-BILLION DOLLAR CLIENTS), HEWLETT\nPACKARD, TEXAS INSTRUMENTS, OPTUM, ACXIOM, CITIBANK, UNITED HEALTHCARE,\nERICSSON, ABB ATOM, ADAPTEC BROAD LOGIC, DELL, ARGONAUT TECHNOLOGIES,\nBAUSCH & LOMB, DARPA, DEPARTMENT OF DEFENSE (DOD) ETC.\nI have been responsible for the development of many systems and\napplications from the designing board to the commercial release.\n\n                                  OBJECTIVE\nPRIMARILY LOOKING FOR CONSULTING AND CONTRACT WORK BUT ALSO OPEN TO\nPERMANENT POSITIONS.\nSenior solution architect Hands On!!, Senior developer, Architect, DevOps\nfor advance engineering in Mulit-Cloud (Azure, AWS and GCP) Big Data,\nArtificial Intelligence. Can also help to build team and lead projects from\nconceptual design to commercial release.\n\nSenior Big Data and Cloud Architect Consultant / BI  Solutions  Architect  /\nData Management and Cloud Lead/ DevOps\n\n\n\n                                   SKILLS\n    \u00e2\u20ac\u00a2 18 YEARS OF SOLID WORKING EXPERIENCE WITH A PHD FROM UNIVERSITY OF\n      CALIFORNIA UCLA/DAVIS\n    \u00e2\u20ac\u00a2 17 years of experience as Hands-On Solution Architect, Team Lead,\n      Specialist, Developer, doing Senior Big Data and Cloud Architect\n      Consultant / BI Solutions Architect / Data Management and Cloud Lead\n    \u00e2\u20ac\u00a2 Cloud Architecture, Big Data Engineering, Machine Learning, Deep\n      Learning, Data Scientist, Business Intelligence (BI), Data Warehousing\n      System Engineering multi-tiered applications.\n    \u00e2\u20ac\u00a2 9+ years of experience in Big Data, Hadoop, Spark, MapR, Cloudera,\n      Hortonworks, Storm, Kafka Confluent, Hive, Impala, Flume, Sqoop,\n      MapReduce, Pig, HBase, NiFi, oozie, Tableau Power BI, Cloudera\n      visualization, QlikView Scala SBT (My Preference), Java Maven etc.\n    \u00e2\u20ac\u00a2 Expert in Big Data: HDFS, Hive, Spark, Scala, Java, Python, Hadoop,\n      Sqoop, Map/Reduce, Hortonworks, Cloudera, MapR, NoSQL HBase,\n      Cassandra, Kafka Confluent, Storm, Spark Streaming, Zeppelin, Kibana,\n      Spark MLib, Mahout, R, NiFi, Falcon, Oozie, Kylin, Atlas, Drill, Solr,\n      ElasticSearch, Ambari,Ranger, Flume, Impala, Pig, HDInsight, SBT etc.\n    \u00e2\u20ac\u00a2 8 years of Azure cloud and Multi-Cloud Hands on!! Extensive full cycle\n      Azure Cloud experience with full Big Data, Machine Learning Deep\n      Learning, Azure Machine Learning Studio, Azure Power BI, and Azure\n      Search. Comprehensive architecture, development and deployments of\n      massive Azure infrastructures for prestigious companies including 4\n      times for Microsoft as senior solution architect. Full cycle Data Lake\n      Design, Data Engineering and Pipelines, Data Streaming, Data Fabric,\n      Big Data and Artificial Intelligence design and implementation on\n      massive scale. Used Azure Data Factory (ADF Gen1 and Gen2), Azure\n      Datalake Storage (ADLS Gen1 and Gen2), SQL/NoSQL on Cloud, HDInsight,\n      Databricks, Databricks Delta Lake, Infrastructure as Code, Cloud\n      Governance, Azure CLI, Azure PowerShell, Python, DevOps, Kubernetes,\n      Docker, AKS Azure Kubernetes, CI/CD(CICD, CI CD,CI-CD) tools, MLOps,\n      Machine Learning, Deep Learning, Azure Machine Learning Studio, Azure\n      Search, and Elasticsearch, Azure DevOps with automated build and\n      release CI/CD(CICD, CI CD,CI-CD) pipelines utilizing ARM templates,\n      Terraform, Jenkins, Ansible, Azure CLI, Automations, Runbooks, Build\n      Tools, Azure Data Lake Store, Azure Data Lake Analytics, HDInsight\n      (Managed Hadoop), Databricks, Stream Analytics, Machine Learning\n      Studio, Azure Databricks\u00c2\u00b8 Azure Data Factory, Azure Data Flow, Azure\n      Data Warehouse, Azure Synapse, Azure SQL Data Warehouse (Azure SQL\n      DW), Azure Analysis Services, Event Hubs, Power BI, and Azure Data\n      Catalog, Atlas, Collibra Data Intelligence, Erwin Data Governance.\n      Azure security Azure Active Directory AAD, AD. AAD-DS. ESP, MFA, IR\n      Integration Runtime Gateway, Domain join, Apache Ranger, Ambari,\n      Advanced 4 pillar of security for HDInsight (perimeter VNET, Kerberos\n      AD ESP authentication, Hive policies Ranger and data encryption) Azure\n      Spark Hive HBase Hadoop Kafka Confluent Storm ML Services (R Server)\n      HDInsight clusters, Domain-joined HDInsight clusters, Azure Zeppelin\n      notebooks, Azure Jupyter notebooks, Azure SQL Data Warehouses, Azure\n      Databricks, Azure Data Lake, Azure Data Lake Factory, Azure Data Lake\n      Storage, Azure Data Lake Analytics, Azure Data Links, Azure\n      Integration Runtime (IR), Azure Data Gateway, Azure Kubernetes\n      Services, Azure Storage Blobs, Azure Active Directory, Azure Service\n      Principals, Azure Security Center, Azure Key Vaults, Azure Virtual\n      Network vnet, Azure Log Analytics, Azure Network Interfaces, Azure\n      Cosmos DB, Azure Cortana Intelligence Suite, Infrastructure as a\n      Service IaaS, Platform as a Service PaaS, Microsoft R Server, NLB, Key\n      phrase extraction Azure search, Unstructured text analytics, Event\n      hub, Streaming, Poly Base etc.\n    \u00e2\u20ac\u00a2 8 years of AWS Cloud Extensive full cycle Cloud AWS Redshift, RDS,\n      EMR, Kinesis, S3, Glue, DMS, Athena, EC2, Lambda, experience with full\n      Big Data, Amazon Elastic MapReduce (EMR), Hadoop, Spark, Hive, Pig,\n      Kafka Confluent, MSK, AWS Management Console, AWS CLI, Amazon EMR File\n      System (EMRFS); Comprehensive CDK and CloudFormation experience with\n      writing script in TypeScript, JavaScript, Python, Java, and C#.\n      Collaborative notebooks Apache Zeppelin, Jupyter, deep learning\n      frameworks like Apache MXNet, RDS Aurora MySQL PostgreSQL,\n      Elasticsearch and SOLR, Machine Learning and Deep Learning development\n      and deployment. AWS Compute E2C, FarGate, Lambda, VMware, AWS\n      Developer Tools, AWS Management Tools, Amazon Machine Learning, AWS\n      DeepLens, Amazon Deep Learning AIMs, Amazon TensorFlow on AWS and\n      other components etc.\n                    .\n    \u00e2\u20ac\u00a2 5 years of Google Cloud (GCP) Architected and implemented multiple\n      massive projects for different companies Hands On!! Including Big\n      Data, Artificial Intelligence, SOLR on Kubernetes indexing from\n      Dataproc directly. Used among others Cloud Dataproc, BigQuery, Cloud\n      Dataflow, Cloud Data Fusion, Cloud Dataprep, Data Catalog, Google\n      Kubernetes Engine (GKE), Kubernetes CLusters, Dataproc CLusters,\n      Container Registry, Deep Learning Containers, Cloud Bigtable, Cloud\n      SQL, Firebase Realtime Database. For artificial intelligence used AI\n      building blocks, Text-to-Speech, Speech-to-Text, AutoML, Vision AI,\n      Cloud Natural Language, Video AI, AI Platform, AI Hub and AI Platform\n      Deep Learning VM Image etc.\n    \u00e2\u20ac\u00a2 7 years of Machine Learning, Deep Learning and Artificial Intelligence\n      MLlib, TensorFlow, Keras, Weka Mahout, Multilayer perceptron\n      classifier (MLPC), the feedforward artificial neural network,\n      Convolutional Neural Network CNN, scikit-learn, Pandas,\n      Deeplearning4j, H2o, Sparkling Water ML, Caffe2, MxNet etc. Different\n      algorithms K-Means, Random Forest, Gradient Boosting algorithms (GBM,\n      XGBoost and CatBoost) etc.\n    \u00e2\u20ac\u00a2 9 years of search engines ELK Stack Elasticsearch, Logstash, Kibana,\n      Filebeat, SOLR from early versions until 8.4.2, SOLR on Kubernetes on\n      GCP!!, Lucene, Rsync, Tika. Also been involved with migration from\n      SOLR to Elasticsearch for at least three companies etc.\n    \u00e2\u20ac\u00a2 Expert in DevOps using Azure DevOps, VSTS, AWS-CodePipeline,\n      Terraforms, Jenkins, Ansible, Git, Maven, Cloudera Navigator, Data\n      Lineage, Kubernetes, Docker.\n    \u00e2\u20ac\u00a2 5 years of Solid Kubernetes, Orchestration and Micro Services\n      experience Distributed Container based Architecture, Docker, Docker\n      CLI, Kubernetes, Kubernetes CLI, kubeflow, Kube-scheduler, Pods, Pods\n      deployments. Service Deployments, Ingress, Helm Charts, Helm Charts\n      CLI, YAML etc. Successfully installed and deployed for several\n      companies huge Kubernetes Cluster (the latest with more than 2000\n      nodes 16,000 CPUs deployed in 20 minutes) with both Data Pipeline,\n      Data Lake (Raw Data Landing Zone, Processing Zone, Consumption Zone)\n      for Machine Learning, Convolutional Neural Network CNN, Spark on\n      Kubernetes, HDFS on Kubernetes, Kafka Confluent on Kubernetes,\n      Elasticsearch Logstash Kibana ELK on Kubernetes, NiFi on Kubernetes,\n      Hive HBase Jupyter Zeppelin with Scala and Python on Kubernetes. Used\n      Vagrant Terraform HashiCorp for deployment with 2000 nodes, 16,000\n      CPUs with Peta Bytes and 150TB / day capacity etc.\n    \u00e2\u20ac\u00a2 3 years of IBM Cloud and IBM Cloud Private (ICP) Distributed Container\n      based Architecture, Docker, Docker CLI, Kubernetes, Kubernetes CLI,\n      Pods, Pods deployments. Service Deployments, Ingress, Helm Charts,\n      Helm Charts CLI. Successfully installed and deployed an entire IBM\n      Cloud Private ICP Cluster then implemented and deployed ELK\n      Elasticsearch, Logstash, Kibana, Filebeat, Kafka Confluent, Zookeeper,\n      Cassandra, Curator on ICP IBM Private Cloud, Kubernetes, Pods using\n      Helm Charts, Scala SBT.\n    \u00e2\u20ac\u00a2 16 years of hands on .NET development, architecture and management\n      experience in application, real time, instrumentation, web, front end,\n      back end, full stack, multiple products out there multiple awards\n    \u00e2\u20ac\u00a2 16 years of hands on Java development, architecture, front end, back\n      end, full stack\n    \u00e2\u20ac\u00a2 6 years of Android mobile development and architecture with multiple\n      apps in the app store\n    \u00e2\u20ac\u00a2 16 years of experience in SQL 7-2016, MySQL, Oracle, and other\n      databases T-SQL, SSIS, SSRS, SSAS, OLTP, OLAP, Multidimensional Cube,\n      MDX, PowerPivot, Tabular Model, SharePoint, PerformancePoint.\n    \u00e2\u20ac\u00a2 Demonstrated experience and understanding of the best practices in all\n      aspects of data modeling, data warehousing (Inmon/Kimball approach).\n      Solid experience in Data Warehouse\n    \u00e2\u20ac\u00a2 Strong knowledge and proven results in Data Warehouse and Data Mart\n      design including Dimensional Modeling (Star & Snowflake Schemas), ER\n      Modeling, 3 Normal Forms, Normalization and Demoralization, Logical\n      Model and Physical Model, Fact/Dimension/Hierarchy identifications.\n    \u00e2\u20ac\u00a2 From Business Case to Data Visualization, I have designed and\n      developed solutions by combining Business Process with Information\n      Technology.\n    \u00e2\u20ac\u00a2 Firmware embedded programming, ARM, PIC, DSP, FPGA, RTOS Linux etc.\n    \u00e2\u20ac\u00a2 Significant management experience including 4 years as the VP of\n      engineering\n\n                                 EXPERIENCE\nMICROSOFT, NEW YORK, NEWPORT BEACH CALIFORNIA      3/2019-PRESENT\nThis was my 4th contract with Microsoft as a high level solution architect\nand expert. Please note I am always hands on, always in addition to doing\nthe architecture I do coding, do DevOps, and do my own developments and\nPOCs. This was a complex hybrid multi projects with Kubernetes, Big Data,\nMulti-cloud (Azue, AWS and GCP), MapR, Kafka, Confluent, SOLR,\nElasticsearch etc. It was for an important Microsoft client, a multibillion\ndollar company in New York with Petabytes of data, and 10 terabyte\nstreaming and data ingestion a day:\n\nI was involved with multiple Kubernetes projects, a hybrid of MapR and\nmulti-cloud system with a massive MapR cluster (with 1400 note 16000 CPUs)\nand Multi-Cloud project (Azure AWS and GCP) with cloud Migration, Kafka\nConfluent streaming using Kappa with massive amount of data in Petabytes\nwith real time streaming using Kappa architecture. This project among\nothers included a comprehensive Data Pipeline, multiple Data Lakes (Landing\nZone, Processing Zone and Consumption zone), high level of domain Join\nsecurity, Big Data Visualizations, Machine Learning, Deep Learning (CNN\nRNN). The architecture that we created is similar to Uber Gen-4\narchitecture with Hudi capable of processing tens of petabytes of streaming\ndata. Please note that due to confidentiality and sensitivity I cannot and\nwill not reveal specific technical details.\n\n    \u00e2\u20ac\u00a2 I lead, architected and helped developing multiple Kubernetes projects\n      on multiple large Kubernetes clusters. Initially we created POCs on\n      AWS (EKS) and Azure (AKS) but finally decided to use the Google\n      Kubernetes Engine (GKE) in GCP for a number of reasons (more than\n      welcome to ask me why in the interview). We created multiple clusters\n      and deployed HDFS Spark in Kubernetes (up to 1000 pods), Kafka\n      Confluent in Kubernetes (created both but decided on using pure Kafka\n      up to 30 pods brokers in Kubernetes), Zookeeper in Kubernetes (up to\n      30 pods ZK), a comprehensive SOLR cluster in Kubernetes (50 SOLR\n      pods), Elastic Search in Kubernetes (50 pods Elasticsearch) and other\n      orchestrations. Please note that due to the StatefulSet nature, these\n      projects were fairly complex using headless services for HDFS, Kafka,\n      Zookeeper, SOLR, Elasticsearch etc however after deployments they were\n      very reliable, scalable. For example with one replica command change\n      in Helm Chart it would go from 5 node to 30 brokers Kafka in minutes.\n      Could index from inside the dataproc into the SOLR kubernetes in a\n      lightning speed. Used other component of the GCP among others Cloud\n      Dataproc, BigQuery, Cloud Dataflow, Cloud Data Fusion, Cloud Dataprep,\n      Data Catalog, Google Kubernetes Engine (GKE), Kubernetes CLusters,\n      Dataproc CLusters, Container Registry, CI/CD (CICD, CI CD,CI-CD)\n      pipelines, Deep Learning Containers, Cloud Bigtable, Cloud SQL,\n      Firebase Realtime Database. For artificial intelligence used AI\n      building blocks, Text-to-Speech, Speech-to-Text, AutoML, Vision AI,\n      Cloud Natural Language, Video AI, AI Platform, AI Hub and AI Platform\n      Deep Learning VM Image etc.\n    \u00e2\u20ac\u00a2 I lead, architected and helped developing a massive MapR cluster with\n      1400 note 16000 CPUs. A comprehensive Data Pipeline with data\n      ingestions from variety of sources RDBMS, hardware logs, images,\n      streaming data, files, documents into the Data Lake. Using Confluent,\n      Kafka Confluent both inside Kubernetes and outside, Kappa architecture\n      with schema evolution, Avro, Parquet (similar to Uber Gen 4\n      architecture) we were able to ingest tens of terabytes of data a day\n      like a charm. The Big Data part used Hadoop, Spark, and Kafka\n      Confluent on mainly MapR but also interacting with Azure HDInsight,\n      AWS EMR, and Kubernetes Cluster. Used also, Spark, Spark Streaming,\n      Storm, Kafka Confluent, Hive, Pig, Impala, Flume, Sqoop, MapReduce,\n      Pig, HBase, NiFi, oozie, Tableau Power BI and, Oozie, QlikView etc.\n      Most of the code were written in Python, PySpark and Scala.\n    \u00e2\u20ac\u00a2 I lead, architected and helped developing a comprehensive Azure\n      infrastructure working with ADF Azure Data Factory gen2, ADLS Azure\n      Data Lake gen2, Data Catalog, Databricks, Delta Lake, Delta tables,\n      Databricks Staging and Merge! Kubernetes, HDInsight, HDInsight\n      Monitoring 4.0, ESP. AD. AAD-DS. ESP, MFA, IR Integration Runtime\n      Gateway, CI/CD (CICD, CI CD,CI-CD) pipelines, Domain join, Apache\n      Ranger, Ambari, Advanced 4 pillar of security for HDInsight (perimeter\n      VNET, Kerberos AD ESP authentication, Hive policies Ranger and data\n      encryption), Advanced ADLS data lake structures, advance Machine\n      learning and CNN, ARM templates, Azure Runbook, notebooks Zeppelin,\n      Jupyter,, Hadoop, Spark, Kafka Confluent, Hive, Hbase. For AI used\n      Azure Machine Learning Service, Azure Machine Learning Studio. For\n      monitoring used Azure Monitor, Azure DevOps, Azure CLI, Azure\n      PowerShell, and Azure Automation. For security I used Azure Active\n      Directory, Azure Role Based Access Control, Azure Subscription\n      Management Azure RBAC, Multi-Factor Authentication, Azure Active\n      Directory Domain Services AAD-DS, Azure Policy, Azure Service\n      Principal, Azure Keys and Key Vault, Enterprise Security Package Azure\n      ESP. For Kubernetes and Micros Services I used Azure Kubernetes\n      Service (AKS), Service Fabric Mesh, Azure Container Instances and\n      Azure Container Registry etc.\n    \u00e2\u20ac\u00a2 I lead, architected and helped developing a comprehensive AWS\n      infrastructure with: Redshift (also tested POC in Snowflake Data\n      Warehouse and Snowpipe), RDS, EMR, MSK (Kafka Confluent), S3 AWS\n      Compute E2C, Glue, DMS, Athena, EC2, RDS Aurora MySQL PostgreSQL,\n      Elasticsearch, Lambda projects with full Big Data, Amazon Elastic\n      MapReduce (EMR), CI/CD (CICD, CI CD,CI-CD) pipeline, CloudFormation\n      CFN,Spark, Kafka Confluent, Oozie, Sqoop, NiFi, Pig, Hive, Hbase, MSK,\n      AWS CLI, Amazon EMR File System (EMRFS). Comprehensive CDK and\n      CloudFormation experience with writing script in TypeScript,\n      JavaScript, Python, Java, and C#. Collaborative notebooks Zeppelin,\n      Jupyter. AWS networking used VPC, Private Subnet, Public Subnet,\n      Internet Gateway, Routing Security groups etc. Visualization and\n      analysis used QuickSight and CloudSearch. Also created a Kubernetes\n      cluster using Elastic Container Service for Kubernetes (EKS), App\n      Mesh, EC2 Container Service (ECS), FarGate. For AI used deep learning\n      frameworks like Apache MXNet Machine Learning and Deep Learning\n      development and deployment, VMware, AWS Developer Tools, AWS\n      Management Tools, Amazon Machine Learning, SageMaker, Alexa Skills\n      Kit, AWS DeepLens, Amazon Deep Learning AMIs, Amazon TensorFlow on AWS\n      and other components. For security used Identity and Access Management\n      (IAM), AWS Organizations, Multi-Factor Authentication. etc.\n    \u00e2\u20ac\u00a2 I lead, architected and helped developing multiple  CDK and\n      CloudFormation projects with writing script in TypeScript, JavaScript,\n      Python and Java.\n    \u00e2\u20ac\u00a2 I lead, architected and helped developing deep learning, machine\n      learning and Convolutional Neural Networks (CNN) using Python, R,\n      PySpark and Scala libraries like scikit-learn, Pandas, Deeplearning4j,\n      Sparkling Water ML, Caffe2, MxNet etc. Different algorithms were used\n      like K-Means, Random Forest, Gradient Boosting algorithms (GBM,\n      XGBoost, XGBoost and CatBoost). Also used GPUs especially with\n      Convolutional Neural Networks CNNs with TensorFlow, Keras.\n    \u00e2\u20ac\u00a2 Development in, Scala, SBT, Eclipse, Python, Pyspark, R, Zeppeline,\n      Jupyter\n\nEricsson, Santa Clara, California 8/2018-3/2019\nDesigned and developed data engineering solutions as a senior Hands-On\nSolution Architect Consultant, Big Data, Data Engineer, Data Scientist,\nsenior developer, Data Warehousing, Spark, HDFS, Kafka Confluent, BI,\nKubernetes, IBM Cloud and IBM Cloud Private (ICP), advanced Machine\nLearning Deep Learning and Convolutional Neural Networks CNN. I am one of\nthe main Architects of GAIA (Ericsson Global Artificial Intelligence\nAccelerator) a $75,000,000 / year new department intended to use Artificial\nIntelligence ML, DL, CNNs to revolutionize cellular communication\nespecially for the G5. Working as senior Solution Architect Consultant,\nsenior developer, senior data engineer, senior data scientist. Implemented\n2000 node 16,000 CPUs Kubernetes cluster and implemented variety of Machine\nLearning, Deep Learning and Convolutional Neural Network CNN. The\nKubernetes Cluster that I created had Data Pipeline, Data Lake (Raw Data\nLanding Zone, Processing Zone, Consumption Zone) with the capacity of Peta\nBytes 150TB /Day Data for Machine Learning, Convolutional Neural Network\nCNN, Spark, HDFS, Kafka Confluent, Elasticsearch Logstash Kibana ELK, NiFi,\nHive, HBase, Jupyter Zeppelin with Scala and Python. Also implemented\nadvanced Machine Learning, Deep Learning, and Convolutional Neural Networks\nCNN and deployed massive POCs on Azure Cloud, GCP, AWS and OpenStack with\nimpressive results.\n\n    \u00e2\u20ac\u00a2 Migrated and architected a Multi-Cloud solution using AWS and Azure\n      using resources such as Redshift, RDS, EMR, Kinesis, S3, Glue, DMS,\n      Athena, EC2, Lambda projects with full Big Data, CI/CD (CICD, CI CD,CI-\n      CD) pipelines, Amazon Elastic MapReduce (EMR), Hadoop, Spark, Hive,\n      Pig, Kafka Confluent, VPC, Subnet, Gateway, AWS Management Console,\n      AWS CLI, Amazon EMR File System (EMRFS), collaborative notebooks\n      Apache Zeppelin, Jupyter, deep learning frameworks like Apache MXNet,\n      Elasticsearch and SOLR, Machine Learning and Deep Learning development\n      and deployment. AWS Compute E2C, RDS Aurora MySQL PostgreSQL, FarGate,\n      Lambda, VMware, AWS Developer Tools, AWS Management Tools, Amazon\n      Machine Learning, AWS DeepLens, Amazon Deep Learning AIMs, Amazon\n      TensorFlow on AWS and other components. etc.\n\n   In a course of less than a year, I have architected and lead some of the\n      most sophisticated Big Data, Deep learning, Machine Learning, and\n      Convolutional Neural Network CNN in the nation for Ericsson Artificial\n      Intelligence Accelerator (Ericsson GAIA) in Santa Clara California.\n      For the first time the technologies were right to create a\n      comprehensive 2000 nodes 16,000 CPUs cluster on Kubernetes with all\n      necessary micro services with automatic orchestration with dynamic\n      deployment for a petabytes Artificial Intelligence system. Some of the\n      results were are impressive. Examples:\n\n\n    \u00e2\u20ac\u00a2 Machine Learning, Deep Learning, Convolutional Neural Network CNN,\n      Anomaly detection on massive amount of real time streaming 5G wireless\n      data with 150TB /day live with high accuracy using the 2000 nodes\n      16,000 CPUs Kubernetes cluster. This would have taken years of\n      processing in the past and is now feasible in minutes on live data.\n    \u00e2\u20ac\u00a2 High accuracy at detecting hack attack, security breach and data\n      breach on live data using CNNs\n    \u00e2\u20ac\u00a2 High accuracy at predicting system availability and reliability and\n      predicting anomalies.\n\n\n***Please note that due to the sensitivity and proprietary nature of these\nprojects and since I am one of the main architect I cannot and will not\nreveal and will not go into too much details!!!\n\n\n    \u00e2\u20ac\u00a2 Created massive Kubernetes clusters on OpenStak, Azure, GCP, AWS with\n      micro service Successfully installed and deployed clusters with more\n      than 2000 nodes 16,000 CPUs deployed in 20 minutes with both Data\n      Pipeline, Data Lake (Raw Data Landing Zone, Processing Zone,\n      Consumption Zone) for Machine Learning, Convolutional Neural Network\n      CNN, Spark on Kubernetes, HDFS on Kubernetes, Kafka Confluent on\n      Kubernetes, Elasticsearch Logstash Kibana ELK on Kubernetes, NiFi on\n      Kubernetes, Hive, HBase, kubeflow, Kube-scheduler, Jupyter Zeppelin\n      with Scala and Python on Kubernetes. Used Vagrant Terraform HashiCorp\n      for deployment with 2000 nodes, 16,000 CPUs with Peta Bytes and 150TB\n      / day capacity.\n    \u00e2\u20ac\u00a2 I lead, architected and helped developing deep learning, machine\n      learning and Convolutional Neural Networks (CNN) systems for 5G\n      wireless data anomaly detection, system availability, Hack attack,\n      Security breach, data breach detection and protection. For peak\n      performance a distributed architecture were created using 16,000 CPUs\n      with amazing results using Spark, HDFS, Scala, SBT, MLlib, TensorFlow,\n      Keras. Some of the development were based on Multilayer perceptron\n      classifier (MLPC) which is a classifier based on the feedforward\n      artificial neural network. Also created other prototypes using Python,\n      R, PySpark and Scala libraries like scikit-learn, Pandas,\n      Deeplearning4j, Sparkling Water ML, Caffe2, MxNet etc. Different\n      algorithms were used like K-Means, Random Forest, Gradient Boosting\n      algorithms (GBM, XGBoost, XGBoost and CatBoost). Also used GPUs\n      especially with Convolutional Neural Networks CNNs with TensorFlow,\n      Keras.\n    \u00e2\u20ac\u00a2 I used TensorFlow, Keras and create similar to YOLO type Convolutional\n      Neural Network on GPUs for detection of different type of anomalies on\n      wireless data. I used the conversion of data to pictures and run it\n      through the Neutral network with amazing results.\n    \u00e2\u20ac\u00a2 I used Acumos AI platform for certain type of ML and CNN projects with\n      cascading Convolutional Neural Networks. I attempted to create an\n      Artificial Intelligence AI environment to facilitate cascading\n      Convolutional Neural Network using.\n    \u00e2\u20ac\u00a2 Created Data Pipeline with real time, intermediate and permanent\n      repositories on Kubernetes. By using huge Kafka Confluent clusters\n      with huge partitions inside Kubernetes, the data was gathered in a\n      Round-robin fashion from variety of sources including real hardware,\n      routers, radios, etc. and was brought into the real time repositories.\n      Using Scala SBT Python Jupyter Zeppelin, Spark, Kafka Confluent, NiFi,\n      ELK, Kubeflow, Kube-scheduler etc. the capacity of the Data Pipeline\n      was 150TB /Day but could be easily extended by simple Kubernetes\n      Orchestration scripts.\n    \u00e2\u20ac\u00a2 Created a Petabytes Data Lake with Raw Data Landing Zone, Processing\n      Zone and Consumption Zone. The Data Lake was specifically designed for\n      ease of use for Artificial Intelligence and therefore the Data\n      Scientist were able to directly access different type of data from\n      real time to intermediate to permanent Data from the Data Lake\u00e2\u20ac\u2122s\n      Consumption Zone for different type of anomaly detection. The\n      processing in the Data Lake was using Delta and Flip to ensure that\n      the data is accessible at any time even during the processing.\n    \u00e2\u20ac\u00a2 Azure Cloud, I architected, led and did actual implementation of\n      massive Azure Cloud project with extensive full cycle Cloud Azure\n      experience covering the Data Ingestion, Data Transformation and Data\n      Consumption with Machine Learning Deep Learning. This was a massive\n      project on a large scale on Azure which covered a full range of areas\n      including but not limited to Big Data, Machine Learning Deep Learning,\n      Azure Machine Learning Studio, Azure Power BI, Azure Search, and\n      Elasticsearch on Azure development and deployment. Comprehensive\n      deployments of massive Azure infrastructures from inside the Visual\n      Studio 2017 using ARM Templates, Azure Runbook. Deployment of many\n      Azure projects for different companies and here are some of\n      practical!! Hands on!! Component that I have personally used, Azure\n      Spark Hive HBase Hadoop Kafka Confluent Storm ML Services (R Server)\n      HDInsight clusters, Domain-joined HDInsight clusters,, Azure Zeppelin\n      notebooks, Azure Jupyter notebooks, Azure SQL Data Warehouses, Azure\n      Databricks, Azure Data Lake, Azure Data Lake Factory CI/CD (CICD, CI\n      CD,CI-CD) pipelines, Azure Data Lake Storage, Azure Data Lake\n      Analytics, Azure Data Links, Azure Integration Runtime (IR), Azure\n      Data Gateway, Azure Kubernetes Services, Azure Storage Blobs, Azure\n      Active Directory, Azure Service Principals, Azure Security Center,\n      Azure Key Vaults, Azure Virtual Network vnet, Azure Log Analytics,\n      Azure Network Interfaces, Azure Cosmos DB, Azure Cortana Intelligence\n      Suite, Infrastructure as a Service IaaS, Platform as a Service PaaS,\n      Microsoft R Server, NLB, Key phrase extraction Azure search,\n      Unstructured text analytics, Event hub, Streaming, Poly Base etc..\n    \u00e2\u20ac\u00a2 The Big Data part used Hadoop, Spark on mainly MapR but also Cloudera\n      and Hortonworks, Storm, Kafka Confluent, Hive, Pig, Impala, Flume,\n      Sqoop, MapReduce, Pig, HBase, NiFi, oozie, Tableau Power BI and\n      Cloudera visualization, QlikView etc.\n    \u00e2\u20ac\u00a2 Development languages, Extensive Scala SBT (My Preference), Java\n      Maven, Eclipse Intellij (my preference), Python, R, Jupyter, PySpark,\n      Ruby and even some, Linux Shell Script, Shell Scripts\n\nMicrosoft, New York, California   3/2018-8/2018\nSenior Solution Architect Consultant, senior developer, Azure Big Data,\nETL, various databases, Data Warehousing, Spark, Databricks, HDInsight, BI,\nadvanced Machine Learning Deep Learning. I implemented, architected and a\nmassive Azure Cloud infrastructure with big data, machine learning, deep\nlearning, and Artificial Intelligence Neural Network\n\n   This was a project with Microsoft and I worked as Microsoft expert on\n      Azure in New York, other companies involved were Pragmatic Works and\n      Selective in New York. I was involved from the designing board all the\n      way to complete implementation and production release. Azure despite\n      simplicity has enormous amount of details. Many engineers that I\n      interviewed and some of whom that I worked with, may have known bits\n      and pieces but actually creating clusters and implementing systems on\n      Azure require substantial experience and know how, I have that.\n    \u00e2\u20ac\u00a2 Azure Cloud, I architected, led and did actual implementation of\n      massive Azure Cloud project with extensive full cycle Cloud Azure\n      experience covering the Data Ingestion, Data Transformation and Data\n      Consumption with Machine Learning Deep Learning. This was a massive\n      project on a large scale on Azure which covered a full range of areas\n      including but not limited to Big Data, Machine Learning Deep Learning,\n      Azure Machine Learning Studio, Azure Power BI, Azure Search, and\n      Elasticsearch on Azure development and deployment. Comprehensive\n      deployments of massive Azure infrastructures from inside the Visual\n      Studio 2017 using ARM Templates, Azure Runbook. Deployment of many\n      Azure projects for different companies and here are some of\n      practical!! Hands on!! Component that I have personally used, Azure\n      Spark Hive HBase Hadoop Kafka Confluent Storm ML Services (R Server)\n      HDInsight clusters, Domain-joined HDInsight clusters, Azure Zeppelin\n      notebooks, Azure Jupyter notebooks, Azure SQL Data Warehouses, Azure\n      Databricks, Azure Data Lake, Azure Data Lake Factory, CI/CD (CICD, CI\n      CD,CI-CD) pipelines, Azure Data Lake Storage, Azure Data Lake\n      Analytics, Azure Data Links, Azure Integration Runtime (IR), Azure\n      Data Gateway, Azure Kubernetes Services, Azure Storage Blobs, Azure\n      Active Directory, Azure Service Principals, Azure Security Center,\n      Azure Key Vaults, Azure Virtual Network vnet, Azure Log Analytics,\n      Azure Network Interfaces, Azure Cosmos DB, Azure Cortana Intelligence\n      Suite, Infrastructure as a Service IaaS, Platform as a Service PaaS,\n      Microsoft R Server, NLB, Key phrase extraction Azure search,\n      Unstructured text analytics, Event hub, Streaming, Poly Base etc.\n\n\n\nOptum United Healthcare, Santa Ana (partially in Minneapolis), California\n8/2016-3/2018\nLead, Senior Solution Architect Consultant, senior developer, Big Data,\nData Engineer, Data Scientist, Data Warehousing, Spark, HDFS, Kafka, BI,\nKubernetes, IBM Cloud and IBM Cloud Private (ICP), advanced Machine\nLearning Deep Learning on the cutting edge of the Genome, Berkeley Amplab\nAdam Genomics, GATK. Elasticsearch SOLR. Also implemented and deployed a\nmassive Azure Cloud infrastructure with big data, machine learning, deep\nlearning, and artificial intelligence convolutional neural network CNN.\n\n   In a course of one year, I have architected and lead some of the most\n      sophisticated Big Data, Deep learning and Machine Learning, Azure\n      Cloud projects in the nation for Optum in Minneapolis and California.\n      I have had access and utilized thousands of servers, 12,000 CPUs,\n      enormous amount of memories and the results has been astonishing\n      beyond even my own and everybody\u00e2\u20ac\u2122s expectations. Examples:\n\n\n    \u00e2\u20ac\u00a2 Genomic analysis for prediction of various cancers on 3000 known\n      samples from our genomic bank, 12,000 CPUs, enormous amount of\n      memories the processing time were reduced from 46.7 years to 22\n      minutes 47 seconds! with 99% prediction accuracy, this was recently\n      presented at a conference.\n    \u00e2\u20ac\u00a2 4 billion records with 53 pre and post processing queries, reduced\n      from days to under a minute!\n\n\n   Please note that due to the highly proprietary and sensitive nature of\n      these projects, I will not be able nor will I disclose the technical\n      details.\n\n\n    \u00e2\u20ac\u00a2 I lead, architected and helped developing deep learning and machine\n      learning systems for genetic analysis and prediction system for\n      occurring of different type of cancers with more than 99% accuracy.\n      For peak performance a distributed architecture were architected using\n      12000 CPUs with amazing results. Spark, HDFS, Scala, SBT, MLlib,\n      TensorFlow, Keras. Some of the development were based on Multilayer\n      perceptron classifier (MLPC) which is a classifier based on the\n      feedforward artificial neural network. Also created other prototypes\n      using Python, R, PySpark and Scala libraries like scikit-learn,\n      Pandas, Deeplearning4j, Sparkling Water ML, Caffe2, MxNet etc.\n      Different algorithms were used like K-Means, Random Forest, Gradient\n      Boosting algorithms (GBM, XGBoost, XGBoost and CatBoost)\n    \u00e2\u20ac\u00a2 For hardware processing of the Big Data, the Deep Learning and Machine\n      Learning different architectures were tested on IBM Neteeza, Teradata\n      and distributed architecture with 12000 CPUs and memory (Spark). The\n      result were absolutely clear, there is no comparison the distributed\n      architecture is far more superior and is the future!\n    \u00e2\u20ac\u00a2 The Big Data part used Hadoop, Spark on mainly MapR but also Cloudera\n      and Hortonworks, Storm, KafKa, Hive, Pig, Impala, Flume, Sqoop,\n      MapReduce, Pig, HBase, NiFi, oozie, Tableau Power BI and Cloudera\n      visualization, QlikView etc.\n    \u00e2\u20ac\u00a2 I lead, architected and helped developing a hybrid system of\n      Elasticsearch and HDFS using Logstash, Rsync and Kafka. The\n      Elasticsearch was sharded over 25 nodes but later deployed on AWS.\n      Three type of data were indexed and inputted into the Elasticsearch\n      (Kibana):\n    o The hardware and system logs for real time (Kafka) hardware and system\n      monitoring\n    o The patient and claim data from HDFS, Hive and HBase for search and\n      quick BI visualization in Kibana.\n    o Data Export files from MarkLogic\n    o For basic search used by different system via an API on top of the\n      Elasticsearch\n    \u00e2\u20ac\u00a2 I lead, architected and helped developing a massive data ingest system\n      from different providers with a permanent and real time pipeline\n      (Kafka) using Delta and Flip methods for an ongoing uninterrupted data\n      ingest. The source data were RDBMS, Hive, HBase, MarkLogic, flat data\n      and log files etc.\n    \u00e2\u20ac\u00a2 Azure Cloud, I architected, led and did actual implementation of\n      massive Azure Cloud project with extensive full cycle Cloud Azure\n      experience covering the Data Ingestion, Data Transformation and Data\n      Consumption with Machine Learning Deep Learning. This was a massive\n      project on a large scale on Azure which covered a full range of areas\n      including but not limited to Big Data, Machine Learning Deep Learning,\n      Azure Machine Learning Studio, Azure Power BI, Azure Search, and\n      Elasticsearch on Azure development and deployment. Comprehensive\n      deployments of massive Azure infrastructures from inside the Visual\n      Studio 2017 using ARM Templates, Azure Runbook. Deployment of many\n      Azure projects for different companies and here are some of\n      practical!! Hands on!! Component that I have personally used, Azure\n      Spark Hive HBase Hadoop Kafka Storm ML Services (R Server) HDInsight\n      clusters, Domain-joined HDInsight clusters, CI/CD (CICD, CI CD,CI-CD)\n      pipelines, Azure Zeppelin notebooks, Azure Jupyter notebooks, Azure\n      SQL Data Warehouses, Azure Databricks, Azure Data Lake, Azure Data\n      Lake Factory, Azure Data Lake Storage, Azure Data Lake Analytics,\n      Azure Data Links, Azure Integration Runtime (IR), Azure Data Gateway,\n      Azure Kubernetes Services, Azure Storage Blobs, Azure Active\n      Directory, Azure Service Principals, Azure Security Center, Azure Key\n      Vaults, Azure Virtual Network vnet, Azure Log Analytics, Azure Network\n      Interfaces, Azure Cosmos DB, Azure Cortana Intelligence Suite,\n      Infrastructure as a Service IaaS, Platform as a Service PaaS,\n      Microsoft R Server, NLB, Key phrase extraction Azure search,\n      Unstructured text analytics, Event hub, Streaming, Poly Base etc..\n    \u00e2\u20ac\u00a2 Genome Analysis Toolkit 4 (GATK4) from Broad Institute, ADAM Genomics\n      Berkeley AMPLab, BAM, SAM, VCF genome variant. Worked also with mango,\n      gnocchi, deca, avocado, quinine, cannoli etc\n    \u00e2\u20ac\u00a2 Using SOLR / Elasticsearch created a detail analytical graphical\n      dashboard in Kibana for Patient Data, Claim Data and Provider Data.\n    \u00e2\u20ac\u00a2 I architected and led multiple AWS projects, Redshift, RDS, EMR,\n      Kinesis, S3, Glue, DMS, Athena, EC2, Lambda, with full Big Data,\n      Amazon Elastic MapReduce (EMR), Hadoop, Spark, Hive, Pig, Kafka, AWS\n      Management Console, AWS CLI, Amazon EMR File System (EMRFS),\n      collaborative notebooks Apache Zeppelin, Jupyter, deep learning\n      frameworks like Apache MXNet, Elasticsearch and SOLR, Machine Learning\n      and Deep Learning development and deployment. AWS Compute E2C,\n      FarGate, Lambda, VMware, AWS Developer Tools, CI/CD (CICD, CI CD,CI-\n      CD) pipelines, AWS Management Tools, Amazon Machine Learning, AWS\n      DeepLens, Amazon Deep Learning AIMs, Amazon TensorFlow on AWS and\n      other components. etc.\n    \u00e2\u20ac\u00a2 Development languages, Extensive Scala SBT (My Preference), Java\n      Maven, Eclipse Intellij (my preference), Python, R, Jupyter, PySpark,\n      Ruby and even some, Linux Shell Script, Shell Scripts\n    \u00e2\u20ac\u00a2 Successfully installed and deployed an entire IBM Cloud Private ICP\n      Cluster then implemented and deployed ELK Elasticsearch, Logstash,\n      Kibana, Filebeat, Kafka, Zookeeper, Cassandra, Curator on ICP IBM\n      Private Cloud, Kubernetes, Pods using Helm Charts, Scala SBT. IBM\n      Cloud and IBM Cloud Private (ICP) is a Distributed Container based\n      Architecture, Docker, Docker CLI, Kubernetes, Kubernetes CLI, Pods,\n      Pods deployments. Service Deployments, Ingress, Helm Charts, Helm\n      Charts CLI.\n\n\nOneStop, El Segundo, California   2/2016-8/2016\nSenior lead, Senior Solution Architect Consultant, senior developer, Big\nData, Data Warehousing, BI, SOLR, Lucene, Elasticsearch, Mahout, Weka\nMachine Learning Lead.\n   I was initially hired at OneStop because of similar experience I had from\n      Dell and Microsoft in Big Data, SOLR Elasticsearch and Machine\n      Learning, taxonomy etc.\n    \u00e2\u20ac\u00a2 I lead, architected and helped developing a Big Data system with 256\n      nodes using Hadoop, Spark on Hortonworks and later migrated to\n      Cloudera, Storm, KafKa, Hive, Pig, Impala, Flume, Sqoop, MapReduce,\n      Pig, HBase, oozie, Tableau Power BI and Cloudera visualization,\n      QlikView\n    \u00e2\u20ac\u00a2 Development languages, extensive Scala SBT (my preference), Java\n      Maven, Eclipse Intellij, Python, R, PySpark, Ruby, C#, Unix Shell\n      Scripts, Linux Shell Scripts\n    \u00e2\u20ac\u00a2 I lead, architected and help developing the SOLR/Lucene Search system\n      which was later migrated to Elasticsearch with 15 nodes sharding. The\n      system was initially developed and maintained in house but later\n      deployed to AWS cloud and prototyped on Azure. The development and\n      test of the 15 node done on VirtualBox machines, physical machines\n      before deployment to the Cloud. The SOLR development was done in two\n      different phases, initially we did indexing directly on top of the\n      metadata extracted from various files with Apache Tika, Apache Flume\n      and scoop. I wrote a scheduler in Java that run delta indexing\n      periodically every few hours. We had customized faceting and then the\n      API would grab the top N results from the XML. The search worked\n      better than expected, the indexing was slow but the search was\n      extremely fast in fraction of a second. On the second phase we stored\n      all the raw documents in HDFS and create indexing and then use HBase\n      to store the index files in HDFS. Also an API was developed in JAVA\n      with a .NET wrapper with SOLR search calls into the SOLR engine.\n    \u00e2\u20ac\u00a2 I lead, architected and help developing an advanced Machine Learning\n      system initially in Spark MLlib then Weka and ultimately a Mahout\n      Machine learning and recommendation system using both ItemSimilarity\n      and UserNeighborhood. I personally favored and created porotypes using\n      Spark MLlib, TensorFlow, Keras, Python libraries like scikit-learn,\n      Pandas but in this case Mahout worked very well. The .NET API would\n      record every time a product was clicked or purchased. The data was\n      recorded in the database and then the metadata was created and the\n      mahout would create a scoring table (0-10) for product and region. The\n      .NET API would select top N highest score and would present it as\n      recommendations.\n    \u00e2\u20ac\u00a2 I lead, architected and helped developing a gigantic amount of data\n      extraction, data warehousing, Big Data. The data was gathered in\n      access of tens of terabytes from more than 40 top of the lines brands\n      Ecommerce sites partnered and operated by OneStop like FRYE, Juicy\n      couture, NYDJ, PAIGE, Splendid, Coffee Beans, Jones New York, Hudson\n      and many more. Used SSIS ETL for SQL to port data to the Data\n      Warehouse and then used Sqoop for extracting from RDBMS to the HDFS,\n      used Flume for extracting from logs files, FTP NAS files to the HDFS,\n      used Apache Tika and Java for extracting metadata from various files\n      into the HDFS, used Nutch for web crawling and for extraction metadata\n      into the HDFS, used SAPI, CMU Sphinx, Kaldi for customer service voice\n      to text conversion into the HDFS\n    \u00e2\u20ac\u00a2 I architected and implemented a real time and streaming component for\n      the Cloudera visualization using Apache Strom and Apache Kafka.\n    \u00e2\u20ac\u00a2 I lead, architected and help developing an elaborate real time\n      visualization using Tableau and Cloudera visualization for the big\n      data portion.\n    \u00e2\u20ac\u00a2 The Big data prototype was deployed both on AWS and Azure. For a\n      number reasons the final decision for the cloud deployment was made\n      for deployment into the AWS not Azure.\n    \u00e2\u20ac\u00a2 I lead and oversaw the conversion of part of the SOLR search project\n      to Elasticsearch and benchmarked the performance. Although I liked\n      working with JASON for various reason SOLR was preferred initially.\n    \u00e2\u20ac\u00a2 I lead, architected and help developing Kibana 4.5 visualization on\n      top of both SOLR and Elasticsearch.\n    \u00e2\u20ac\u00a2 Wrote and oversaw a development of combination of batch files, python\n      and Ruby scripts for SOLR/Lucene and Big Data deployment and\n      configurations. I have to add that I started the conversion of batch\n      file to Python but there were simply not enough time.\n    \u00e2\u20ac\u00a2 Did extensive prototyping and benchmarking and helped evaluating the\n      performance of the big data on Massively Parallel Processing (MPP) and\n      other  Data Warehouse Appliances such as IBM Netezza, Teradata, APS\n      (PDW), Oracle Exadata\n\n\nCanadian Tire, California / Toronto     8/2015-2/2016\nSenior Big Data, DW and BI Lead Solution Architect Consultant.\n    \u00e2\u20ac\u00a2 Led multiple large scale Big Data, Enterprise Data Warehouse EDW and\n      Business Intelligence BI projects on Teradata utilizing Spark, Hadoop,\n      Hortonworks, Cloudera, Hive, Impala, Flume, Sqoop, Map Reduce, Pig,\n      HDInsight, HBase, oozie, and facilitating the real-time data analysis\n      by the data scientist.\n    \u00e2\u20ac\u00a2 Led multiple EDW projects, prototyped and evaluated their performance\n      on the Massively Parallel Processing (MPP), other Data Warehouse\n      Appliances such as IBM Netezza, Teradata, APS (PDW), Oracle Exadata\n    \u00e2\u20ac\u00a2 Development languages, extensive Scala SBT (my preference), Java\n      Maven, Eclipse Intellij, Python, R, PySpark, Ruby, C#, Unix Shell\n      Script, Linux Shell Scripts\n    \u00e2\u20ac\u00a2 Leading the team, I designed architected and implemented the migrating\n      from legacy information warehouse to a modern high performance Big\n      Data and Data Warehouse running on multiple DW appliances. Drafted a\n      BI/DW prioritized implementation roadmap working with the business and\n      finance department.\n    \u00e2\u20ac\u00a2 Leading the team we migrated and deployed 5 projects to Azure Cloud. I\n      was personally involved in the full cycle of vendor selection,\n      requirement gathering, design, development and the deployment of these\n      projects. The migration included different aspects of the projects\n      from front, backend, and integration. We went through thorough\n      research before selecting the Azure cloud for this project and also\n      utilized cutting edge utilities to perform the migration and\n      deployment.\n    \u00e2\u20ac\u00a2 Drafted a BI/DW prioritized implementation roadmap while taking input\n      from internal divisional service plans, business and IT strategy\n      documentation, as well as corporate BI Strategy and the Financial\n      Planning and Reporting System\n    \u00e2\u20ac\u00a2 Designed Enterprise Information Management (EIM) solutions for retail\n      operation. Led technical teams and designed various BI solutions\n      including loyalty programs, card management, POS data management,\n      customer behavioral analysis, store dashboards, finance, ecommerce,\n      cyber security analytic.\n    \u00e2\u20ac\u00a2 Defined the data governance strategy, designed security patterns,\n      implemented data standards and procedures across the enterprise;\n      drafted business specific methodology to establish business\n      stakeholder-driven data stewardship through MDM\n    \u00e2\u20ac\u00a2 Conducted BI maturity assessment of the organization. Architected\n      DW&BI Program Structure, defined the role of DW&BI Program Steering\n      Committee, it's mission, objectives, roles and responsibilities,\n      monitored regular improvements to help manage risks, evaluate trends,\n      and develop capacity and capability to achieve the Program mission\n\nNovaWurks/DARPA, Los Alamitos, California    11/2014-8/2015\nSenior Big Data, DW and BI Lead Solution Architect Consultant, Java Android\nconsultant\nWorked as senior Big Data Solution Architect, team leader and core\ndeveloper on PHOENIX project, an advanced satellite system for DARPA\n(Defense Advanced Research Projects Agency), a network of small satellites\ndue to launch to orbit in 2015. Due to the sensitivity cannot go into too\nmuch details!\n    \u00e2\u20ac\u00a2 Led several Big Data projects on massive amount of transmitted and\n      logged data from the satellite network to the ground station. These\n      projects were developed utilizing Cloudera, Hadoop, Spark, Hive,\n      Impala, Flume, Sqoop, Storm, Pig, HDInsight, HBase, oozie. Due to the\n      real time nature of the project Apache Storm and Apache Kafka was used\n      for handling of the streaming and the real time data feed.\n    \u00e2\u20ac\u00a2 I led the team, designed, architected and implemented an elaborate\n      Data Warehouse and Data Mart using Dimensional Modelling Star Schema\n      for satellite data aggregation, data storage, data log, real time\n      operation status data and other needs.\n    \u00e2\u20ac\u00a2 Utilized the Cloudera Visualizations, Dashboards, and Reports to\n      monitor the operation of the satellites and any warning issues due to\n      any errors, miss functions or failures. Other visualization tools were\n      also created using Java and Android.\n    \u00e2\u20ac\u00a2 Development languages, extensive Scala SBT (my preference), Java\n      Maven, Eclipse Intellij, Python, R, PySpark, Ruby, C#, Shell Scripts\n    \u00e2\u20ac\u00a2 Led the team developed multiple real time Android Apps and middleware\n      using Android Studio and Eclipse, Android SDK and Java, RESTful APIs,\n      Retrofit, GSON, JSON, Regex, JGroups IP Multicast, Apache Thrift,\n      Python. Also used the following technologies and systems, Xilinx FPGA,\n      Verilog, TI DSP, ARM\u00c2\u00ae Cortex\u00c2\u00ae-A9 Cores: i.MX 6 Series Multicore\n      Processors etc.\n\nParamit, Morgan Hill, California  7/2013-11/2014\nSenior Big Data, DW and BI Lead Solution Architect, .NET Architect\nConsultant.\n    \u00e2\u20ac\u00a2 Led multiple large scale Big Data, Enterprise Data Warehouse EDW and\n      Business Intelligence BI projects utilizing, Hadoop, Cloudera, Hive,\n      Impala, Flume, Sqoop, Map Reduce, Pig, HDInsight, HBase, oozie, and\n      facilitating real-time data analysis by data scientist.\n    \u00e2\u20ac\u00a2 Leading the team, I designed architected and implemented the migration\n      from legacy normalized SQL, FoxPro, medical device manufacturing, ERP,\n      MRP, CRM, sales, finance and other information warehouses to a\n      consolidated modern high performance Big Data Warehouses running on\n      multiple DW appliances.\n    \u00e2\u20ac\u00a2 Development languages, extensive Scala SBT (my preference), Java\n      Maven, Eclipse Intellij, Python, R, PySpark, Ruby, C#, Shell Scripts\n    \u00e2\u20ac\u00a2 Leading the team we migrated and deployed multiple projects to Azure\n      Cloud. I was personally involved in the full cycle of vendor\n      selection, requirement gathering, design, development and the\n      deployment of these projects. The migration included different aspects\n      of the projects from front, backend, and integration. We went through\n      thorough research before selecting the Azure cloud for this project\n      and also utilized cutting edge utilities to perform the migration and\n      deployment.\n    \u00e2\u20ac\u00a2 Using a combination of WPF C# application GUI and the Cloudera\n      Visualizations, Dashboards, and Reports created advanced data\n      visualization and data entry tools for ERP, MRP, CRM, sales, finance\n      and other departments.\n    \u00e2\u20ac\u00a2 I lead, architected and help developing a SOLR/Lucene Search for the\n      huge amount of ERP, MRP and CRM. The SOLR project was later converted\n      to Elasticsearch. The Elasticsearch /Lucene system was architected\n      with 5 nodes sharding. It was developed and tested on 5 node\n      VirtualBox machines and then deployed to AWS cloud. Created an API in\n      C# .NET for calls to the search engine. Also a GUI was developed in C#\n      .NET for search calls to the Elasticsearch.\n    \u00e2\u20ac\u00a2 Developed a customized SOLR indexing scheduler in C# which would run\n      periodically to do the delta indexing.\n    \u00e2\u20ac\u00a2 Drafted a BI/DW prioritized implementation roadmap while taking input\n      from internal divisional service plans, business and IT strategy\n      documentation, as well as corporate BI Strategy and the Financial\n      Planning and Reporting System\n    \u00e2\u20ac\u00a2 Designed Enterprise Information Management (EIM) solutions for the\n      manufacturing process, customer support and retail operation. Led\n      technical teams and designed various BI solutions including medical\n      device manufacturing tracking process, component reliability analysis,\n      vendor analysis, customer behavioral analysis, finance, ecommerce,\n      cyber security analytic.\n    \u00e2\u20ac\u00a2 Conducted BI maturity assessment of the organization. Architected\n      DW&BI Program Structure, defined the role of DW&BI Program Steering\n      Committee, it's mission, objectives, roles and responsibilities,\n      monitored regular improvements to help manage risks, evaluate trends,\n      and develop capacity and capability to achieve the Program mission\n    \u00e2\u20ac\u00a2 Led the team and developed multiple applications including medical\n      device, ERP, MRP applications with big data architecture. Used NET\n      4.5, C#, WPF, WCF, WF, MVVM Light, Telrik, MVC 4 Razor Entity\n      Framework 6.0 TFS, SQL 2012.\n\nMicrosoft, Redmond WA  2/2013-7/2013\nSenior Big Data, DW and BI Lead Solution Architect, .NET Architect\nConsultant.\n    \u00e2\u20ac\u00a2 Led multiple Azure Cloud Big Data, NoSQL Riak, MongoDB SIP Trunk VOIP\n      projects doing analysis on massive amount of voice to text converted\n      data utilizing Hadoop/HDInsight, PDW, Map/Reduce jobs, Hive, and\n      Sqoop.\n    \u00e2\u20ac\u00a2 Development languages, extensive C++, Scala SBT (my preference), Java\n      Maven, Eclipse Intellij, Python, R, PySpark, Ruby, C#, Shell Scripts\n    \u00e2\u20ac\u00a2 Created real time multithreaded C# code using C++ Dubango Library,\n      SIP, TCP, UDP, RTP the VOIP telephony voice was recorded and using\n      SAPI converted to text. The text was then stored into key value and\n      document tables using Riak and MangoDB. The voice data gathered from\n      Cisco/IPCC telephone systems. Integrated with Cisco Verint for VOIP\n      call recording, quality monitoring (QM), and speech analytics.\n    \u00e2\u20ac\u00a2 Microsoft SQL Server Parallel Data Warehouse (SQL Server PDW) was\n      chosen as the main appliance for the Big Data processing due to its\n      Massively Parallel Processing (MPP) architecture designed for Big Data\n      Processing.\n    \u00e2\u20ac\u00a2 Microsoft Power BI in conjunction with a .NET application is used for\n      data visualization.\n    \u00e2\u20ac\u00a2 Led the design and development of the Workforce Management (WFM) data\n      warehouse and BI solution to optimize adherence and attendance in the\n      contact center. The predictive analytic component accurately forecasts\n      the number of CSRs needed in the call center to fulfill the services.\n    \u00e2\u20ac\u00a2 Led the design and development of an efficient BI auditing framework\n      that collects the data from packages being executed and used in data\n      flows, row counters, versioning, and error handling. The framework is\n      crucial for monitoring, timing, troubleshooting, and auditing. Also,\n      developed Stored Procedures, Views, and Functions for the framework to\n      automate logging the information and error handling in the packages.\n    \u00e2\u20ac\u00a2 Led the design and development of ETL processes and data mapping using\n      SQL server, Master Data Services (MDS), SSIS to extract data from\n      Lagan ECM and division data sources including SQL server and oracle\n      databases, flat files, and excel sheets. The data, then, is\n      transformed and loaded into a data warehouse for reporting.\n    \u00e2\u20ac\u00a2 Led the design and development of data quality ETL packages to correct\n      and cleanse the data and enhance the quality of consolidated data.\n      Wrote hundreds lines of .NET C# code, embedded in the packages, to\n      create a rules engine that loads business rules and apply them to the\n      data efficiently. In addition, the data quality issues are mapped for\n      reporting purposes.\n    \u00e2\u20ac\u00a2 Led the design and development of a SQL Server SSAS Analysis cube\n      utilizing star schema with complex MDX calculated measures, named sets\n      and KPIs to present an analytical view for the data and data quality\n      with multiple dimensions.\n    \u00e2\u20ac\u00a2 Led the design and development of map application and report using\n      ASP.NET/C# web application. The application loads the data from the\n      data warehouse, combines it with geographical information, and\n      displays the data on a map. The application communicates through\n      restful mapping services and uses client side scripts (JavaScript and\n      AJAX) to improve performance and user experience.\n\nDell, Austin, TX 6/2012-2/2013\nSenior Big Data, DW and BI Lead Solution Architect, .NET Architect\nConsultant.\n    \u00e2\u20ac\u00a2 Led a Big Data project on gigantic amount of taxonomy data and\n      customer portfolio using Hadoop,  Cloudera, Hive, Map Reduce, Pig,\n      HDInsight, and facilitating real-time data which was both analyzed and\n      also in real time restructured the Dell website on the demographic\n      portfolio of the customers.\n    \u00e2\u20ac\u00a2 I architected, worked and help developing the SOLR/Lucene Search\n      deployed to Azure. The indexing was done directly on top of the\n      metadata extracted from various files with customized Java code and\n      Apache Tika. Used customized faceting to overwrite the default search\n      criteria.\n    \u00e2\u20ac\u00a2 Development languages, extensive Java Maven, Eclipse Intellij, Python,\n      R, Scala SBT, PySpark, Ruby, C#, Unix Shell Scripts\n    \u00e2\u20ac\u00a2 Developed a customized SOLR indexing scheduler in C# which would run\n      periodically to do delta indexing.\n    \u00e2\u20ac\u00a2 Wrote variation of batch files, python for SOLR/Lucene deployment and\n      configurations\n    \u00e2\u20ac\u00a2 Leading the team, we designed architected and implemented the\n      migrating from legacy normalized SQL taxonomy data, customer portfolio\n      data and other data to a modern high performance Big Data Warehouses\n      running on multiple DW appliances.\n    \u00e2\u20ac\u00a2 Defined the data governance strategy, designed security patterns,\n      implemented data standards and procedures across the enterprise;\n      drafted business specific methodology to establish business\n      stakeholder-driven data stewardship through MDM\n    \u00e2\u20ac\u00a2 Led multiple EDW projects, prototyped and evaluated the performance on\n      Azure cloud, AWS Amazon Cloud, Massively Parallel Processing (MPP)\n      Data Warehouse Appliance\n    \u00e2\u20ac\u00a2 I wrote complicated taxonomy algorithm in C# to load, sort the\n      taxonomy data into huge multidimensional trees on the memory which\n      made the data processing supper fast.\n    \u00e2\u20ac\u00a2 Created Taxonomy data visualization using the Cloudera Visualizations,\n      Dashboards, and Reports to monitor customer profile, demography and\n      other useful data. Other visualization tools were also created using\n      C#.\n    \u00e2\u20ac\u00a2 Created data quality ETL packages to correct and cleanse the taxonomy\n      data and enhance the quality of consolidated data. The consolidated\n      taxonomy data then were segmented using Hadoop and Cloudera.\n    \u00e2\u20ac\u00a2 Led the design and development of a SQL Server SSAS Analysis cube\n      utilizing star schema with complex MDX calculated measures, named sets\n      and KPIs to present an analytical view for the data and data quality\n      with multiple dimensions.\n    \u00e2\u20ac\u00a2 Leading the team we migrated and deployed multiple projects to Azure\n      Cloud. I was involved in the full cycle of vendor selection,\n      requirement gathering, design, development and the deployment of these\n      projects. The migration included different aspects of the projects\n      from front, backend, and integration.\n    \u00e2\u20ac\u00a2 In conjunction with the Big Data I was involved in multiple projects\n      using variety of technologies including MVC 4 Razor, WPF, WF, WCF,\n      TPL, LINQ, SQL 2012, jQuery, Android, Java, J2EE, JRE, Ajax,\n      AngularJS, ExtJS, Entity Framework 5.0,.NET 4.5\n\nBEW / General Electric / 3 Gorges China, San Ramon, California      6/2011-\n6/2012\nTeam Leader, .NET Architect, Hands on Developer Consultant.\n    \u00e2\u20ac\u00a2 Worked as a system architect, core developer on a sophisticated\n      control system for generators and wind turbines lead the software\n      (WPF), hardware (Xilinx FPGA & TI DSP 6000) and firmware (C++\n      Verilog/VHDL) teams.\n    \u00e2\u20ac\u00a2 The high level software controlled a network of generators via TCP/IP.\n      The WPF C# project was architected using MVVM light, Entity Framework,\n      LINQ, WCF Services SQL etc. The Silverlight ASP .NET project was\n      architected using MVVM light, Entity Framework, LINQ, WCF RIA Services\n      Domain Service/Context. Developed equivalent Android application for\n      reading the generator\u00e2\u20ac\u2122s parameters like RPM, temperature, sensor\n      Voltages etc. Used Java programming and the Android Software\n      Development Kit, Eclipse using the Android Development Tools (ADT)\n      Plugin. Also worked on the firmware and FPGA DSP TMS320C6713\n      TMS320F28335 EMIF, I2C, MCBS, GPIO, RTC UART, Anybus CANbus, DM9000,\n      second level bootloader, EEPROM, code composer 3.3 etc FPGA Xilinx\n      Spartan 6, Xilinx ISE Design Suite 13.2, Verilog and VHDL.\n\nTexas Instruments, Dallas Texas   9/2010-6/2011\nTeam Leader, .NET Architect, Hands on Developer Consultant.\nWorked as the main architect, team leader, and core developer on a\nscientific highly multithreaded WPF C# application for emulation and design\nof advanced communication chips using scientific algorithms. I also worked\non an Android application for the PLL, Java programming using the Android\nSoftware Development Kit, Eclipse using the Android Development Tools (ADT)\nPlugin. The WPF application was architected using propriety MVVM\narchitecture. Utilizing advanced 3D objects the application was similar to\nOrCad and AutoCad. A smaller prototype version was also developed in\nSilverlight.\n\nMultibeam Corporation / Tokyo Electron (TEL), Santa Clara, California\n7/2008-9/2010\nTeam Leader, Architect, Hands on Developer Consultant.\n    \u00e2\u20ac\u00a2 Worked as the project lead, helped designing, architecting, and\n      implementing a revolutionary complex electron beam based instrument\n      for the next generations of semiconductor fabs. Advanced analog\n      digital boards, Embedded Linux, Xilinx & Altera FPGA, Quartus, NIOS,\n      ARM9, ARM11, C, C# .NET, WPF (MVVM), WCF etc. DSP TMS320C67x GPIO, RTC\n      UART, Modbus, DM9000, second level bootloader, EEPROM. Altera FPGA,\n      Stratix, Cyclone Series, Quartus II Nios II.\n\nDepartment of Defense Contract (DOD), Washington DC      3/2007-7/2008\nTeam Leader, Architect, Hands on Developer Consultant.\n    \u00e2\u20ac\u00a2 Architected, developed and led a highly sophisticated\n      hardware/firmware/software system. Due to the classified nature of the\n      project, I can only provide the following generic information: The\n      project involved advanced radio scanners, signal generators using GPS,\n      WCDMA, CDMA, GSM and other systems and protocols. The software\n      application controlling the instruments was a multi-tiered application\n      written in C#, .NET, Visual C++, MFC, CLR, Embedded Linux. It utilized\n      a very advanced multi-threading architecture with sophisticated\n      synchronization, message handling, logging system, serialization etc.\n      Specialized algorithms were devised to speed up the real-time\n      performance of hardware/software. Again because of the defense-related\n      nature of this project I cannot reveal any more details.\n\nPatton Design, Irvine, California 7/2000-3/2007\nVice President.\nWorked as the vice president of software and hardware. I led and developed\nthe software/hardware for a $140,000 instrument medical device - FDA.\nPlease check the website of Patton Design and Busch & Lomb to see this\naward winning instrument for cataract surgery. I designed, architected, led\nthe team and developed the software and also directed the hardware and\nfirmware developments. The software included a sophisticated multithreading\narchitecture, RS232 and TCP/IP communications, managed wrapper for firmware\ncalls, video streaming, voice recognition, database hierarchy encryption\netc. In addition to leading the team and acting as the vice president, I\npersonally wrote the complex core components in C# .NET. Due to the large\nscale of the medical device - FDA projects with hundreds of screens many of\nthe .NET C# libraries and objects had to be used. We also used legacy\nunmanaged code inside the managed code (wrappers). DirecX, DirectShow,\nWindows Communication foundation WCF, Windows Presentation Foundation WPF,\nWF, Silverlight, WCSF, SCSF, Enterprise Library, animation, video, audio\netc were also used.\nIn addition to the main control application I wrote and oversaw the\nfirmware in C++ Embedded Linux, C++ Round Robin, CodeWarrior. I also\noversaw and participated in the hardware development using OrCad 10.\n\n* Patton Design / Cameron Health: Developed the software and participated\nin the hardware design of the heart pacemaker medical device - FDA and the\ncontroller called Q-TECH\u00e2\u201e\u00a2 Programmer. medical device - FDA The heart\npacemaker is transplanted in the heart and controlled by the wireless\ncontroller via Bluetooth. Due to the FDA regulations I could not use the\n.NET framework but had to use Embedded Visual C++ 4.0 and MFC for windows\nCE. More than 140 screens! Very sophisticated programming involving memory\nmanagements, DirectX, DirectShow etc.\n\n* Patton Design / GoVideo: Worked as the Vice President/architect/team\nleader on a joint project between, GoVideo, Patton Design, Daewoo and MTK\nin Taiwan. I led the Patton Design team developed a TiVo style DVD/VCR\ncombo with hard drive recording capability. I was the vice president and\nthe team coordinator between the 4 companies overseeing hardware, software\nand Firmware (Embedded Linux), several patents were filed. The System was\npresented at the CES show in Las Vegas in 2007 and received tremendous\npositive recognitions.\n\n* Contract with usCalibration Inc.: Architected, developed and led a\nsophisticated web based application using C# .NET and Visual Studio 2005,\nSQL Server 2005 and SSRS. I wrote the core part of the application. The\napplication was successfully launched in 2006 for Calibration systems with\nadvanced security systems. Tens of thousands of lines of code with advanced\nnavigation systems with several pending patents.\nHewlett Packard (HP), Cupertino, California  4/2000-7/2000\nTeam Leader, Architect, Hands on Developer Consultant.\nWorked as senior developer/ technical lead on an advanced server client\nbased communication system for server diagnostics. The system was designed\nusing TCP/IP and SNMP protocols for monitoring hardware sensors like\nthermocouples, voltage and current monitoring sensors and other hardware\nsensors installed on HP servers. By reading these sensors, HP was able to\nremotely do detailed hardware/software diagnostics of the HP servers around\nthe globe.\nWorked on hardware, software and the overall system architecture. The\nsoftware had a server and client component and was written in visual C++,\nCOM (ATL), DCOM, ASP, Visual J++, XML, SNMP, MIB, SQL and InstallShield.\n\nBroad Logic, Milpitas, California 7/99-4/2000\nTeam Leader, Architect, Hands on Developer Consultant.\nI was brought to BroadLogic, Inc. by Paul Rudnick because of my expertise\nin satellite communication systems and my experience from Space Systems\nLoral and CyberStar. Prior to this, I had worked closely with Adaptec and\nBroadlogic on the development of the satellite receiver hardware while\nstill a senior manager at Space Systems Loral.\nI worked on the design and implementation of the next generation of two way\nsatellite Express PC transceiver cards, a high speed two way satellite\ncommunication system. I designed, simulated, researched, architected and\nled the project for the development of an advanced two way satellite\ncommunication system (satellite Express PC transceiver cards). Audio, video\ntransfer and high speed internet access over satellite. Using, frequency,\ntime, phase multiplexing. TDMA, CDMA, GMSK, Conditional Access. TCP/IP,\nUDP, DVB, SNMP, MIB and proprietary protocols. Using OQPSK modulation\nimplementation on the Texas Instrument DSP Chip. I have written several\ndocuments related to this system.\n\nHewlett Packard (HP), Mayfield, California   3/99-7/99\nTeam Leader, Architect, Hands on Developer Consultant.\nWorked as the senior architect, technical lead and senior developer on the\nHP Ecommerce site which later became the foundation of the HP website for\nPC and servers. The web application was developed in Visual InterDev 6.0\nusing Active Server Pages (ASP), Microsoft E-Commerce, SQL 7.0, XML, Visual\nC++ 6.0 and Visual Basic 6.0, Visual J++ 6.0, COM (ATL), DCOM, JavaScript\nand VB Script. The web server was Microsoft Internet Information Server\n(IIS), Microsoft site server 3.0, with Microsoft E-Commerce edition 3.0 and\nFrontPage extension running under the NT Server I have written several\ndocuments related to this application.\n\nHewlett Packard (HP), Cupertino, California  5/98-3/99\nTeam Leader, Architect, Hands on Developer Consultant.\nWorked as the senior architect, technical lead and senior developer on the\nHP servers configuration software which later became a major component and\nthe foundation of the HP website for PC and servers configuration. Stand\nalone and the web application in was developed in Visual InterDev 6.0 using\nActive Server Pages (ASP), Microsoft E-Commerce, SQL 6.5, Visual C++ 6.0\nand Visual Basic 6.0, COM (ATL), DCOM, JavaScript and VB Script. The web\nserver was Microsoft Internet Information Server (IIS), Microsoft site\nserver and FrontPage extension running under the NT Server. I have written\nseveral documents related to this application.\n\nSpace Systems Loral (CyberStar), Mountain View, California    11/96- 5/98\nTeam Leader, Architect, Hands on Developer Consultant.\nI was brought into Space Systems Loral from Lockheed Martin by Bob Lapin to\nhelp starting the CyberStar division at the Space Systems Loral. By the\ntime I left the CyberStar in 1998 to finish my PhD in Satellite\nCommunication, the CyberStar division had grown to more than 100 employees.\nI personally interviewed majority of those people.\nI was one of the main architects of the CyberStar project and oversaw the\ndesign, development and implementation of different aspects of hardware,\nsoftware, firmware and the satellite communication at CyberStar.\nI first established a complete satellite communication link both uplink and\ndownlink, using 3rd party modulators, demodulators, encoders, decoders,\ncryptography modules, conditional access, transmitter, receivers,\namplifiers, dampers, data aggregator, data parsers etc. Very soon we were\nable to transmit and receive from and to the satellite. We were primarily\nusing MIB and DVB protocols initially but I was one the first who managed\nin 1998 to implement TCP/IP and high speed internet access over satellite\nusing an ACK table!! (patents)\nTo develop the integrated transceiver hardware we started working with\nAdaptec and I personally was directly involved in the design and\nimplementation of the satellite receiver card hardware using OrCad. This\nlater led to the creation of BroadLogic from Adaptec. I was later hired by\nBroadLogic to continue the improvement of the two way satellite receiver /\ntransmitter.\n\nThis project was personally very important to me and made me understand and\nexperience the satellite communication in a very comprehensive way both\ntheoretically and practically. It helped me to get a PhD in Low Earth Orbit\nSatellite Communication from the University of California, one of very few\nwho did. I travelled extensively in both US and in Europe and came in\ncontact with some amazing people from NASA, Lockheed Martin, BroadLogic,\nEuropean Space Agency etc I wrote many documents in satellite communication\nduring this period for Space Systems Loral.\n\nLockheed Martin, Milpitas, California   6/96- 11/96\nTeam Leader, Architect, Hands on Developer Consultant.\nLockheed Martin at the time in 1996 had the most sophisticated high\nresolution CCDs (Farichild) in the world which were in use in a number of\nsensitive military applications, advanced high resolution digital satellite\nimaging, and few civilian applications.\nDue to the classified nature of some of these projects I cannot in detail\ndescribe what I did. However I was involved in the design and development\nof some of these advanced and sensitive projects. I worked as a senior\nengineer, designing and developing, systems, hardware, firmware and\nsoftware.\n* Hardware: We used OrCad for designing analog and digital circuits,\nfilters, amplifiers, Data collectors from CCDs, interfaces etc.\n*Firmware was written in C++ , flat file Round Robin, on Freescale HC and\nARM family CPUs.\n*Software: developed 32-bits, real time applications in Visual C++ 4.2\nusing MFC and SDKs under Windows 95 for control and testing of an advanced\ndigital camera with high resolution CCD. The GUI software is designed for\ndriving the special digital camera through parallel communication and\ntesting of IPS, ADP, CCD and different part of the system. The tests\nincluded advanced image processing and image quality tests. The project\ninvolved both 16-bits and 32-bits DLLs and VXDs (device drivers), Thunking\nand also conversion from and between 16-bits and 32-bits.\n\n\n                               OTHER PROJECTS\n\nERICSSON (ELLEMTEL), STOCKHOLM, SWEDEN\nTeam Leader, Architect, Hands on Developer.\nDeveloped and designed hardware and a control system for the new generation\nof AXE telephone systems, based on the FUTUREBUS+ bus technology, Using the\nVHDL programming language. I wrote the VHDL program on the SUN platform\n(SUN OS version 3.0).\nI documented the application in a detailed technical white paper entitled\n\"Verification Methods for Hardware Construction\". This paper was released\nto all programmers and hardware engineers at Ericsson and KTH. A copy is\navailable for your review.\n\nABB Atom AB, Vasteras, Sweden\nTeam Leader, Architect, Hands on Developer Consultant.\nElectrical and Computer Lab--section SLC3:\nDeveloped a series of utility programs / application in Quick BASIC\n(version 5.0) used for calibration of computer operated measurement\nequipment in the nuclear power plant reactors. Programs were run on the HP\n9000/300, and Intel 286 platforms.\n\nTECHNOLOGY\n    \u00e2\u20ac\u00a2 Data Warehouse, Data Mart, OLAP, OLTP Databases, Teradata, Netezza,\n      Oracle, Parallel Data Warehouse (PDW), SQL Server, MDM, MDS, Data\n      Quality (DQ), Spark, Hadoop, Hortonworks, Cloudera, Apache KafKa,\n      Hive, Impala, Flume, Sqoop, Map/Reduce, Pig, HDInsight, HBase, Storm,\n      oozie, Python, Scala, HDFS, StreamInsight, PolyBase, Microsoft SSIS,\n      SSAS, SSRS, ETL, BI, MDX, PL/SQL, TSQL, ERwin, Enterprise Architecture\n      (EA),  SQL Servere 2000/2005/2008/2012/2014, Power Query, Power Map,\n      PowerPivot, Power View, IBM Cognos, SPSS, InfoSphere DataStage,\n      Informatica PowerCenter, SAP BusinessObjects (BO), SAP HANA, Crystal\n      Reports, Hyperion, MicroStrategy, SharePoint 2007/2010/2013, Nintex,\n      SharePoint Social, Collaboration, Record Management, Search, Web\n      forms, InfoPath, Branding, CSOM, JSOM, PerformancePoint, Clustering,\n      Failover, Web Analytics, Google Visualization, .NET 1.1 to 4.5, C#,\n      WCF, Restful Services, WPF/Silverlight, WF, VB .NET, ASP .NET,\n      ADO.NET, LINQ, MVC, MVVM, MVP, AJAX, Visual Studio, Dashboard\n      Designer, SharePoint Designer, Visio, TFS, Cloud, Azure, PaaS, SaaS,\n      IaaS, HTML 5.0, DHTML, XML, XSL, WSDL, XSD, JSON, COM, DCOM, MFC, C,\n      Visual C++, Visual Basic, PowerShell scripts, and SDKs, DocXpress, BI\n      Documentation, Nintex, SharePoint Social, Collaboration, Record\n      Management, Search, Web forms, Branding.\n\n                                   SKILLS\n    \u00e2\u20ac\u00a2 BIG DATA, HADOOP, SPARK, CLOUDERA, HORTONWORKS, STORM, KAFKA, HIVE,\n      IMPALA, FLUME, SQOOP, MAPREDUCE, PIG, HDINSIGHT, HBASE, OOZIE, TABLEAU\n      POWER BI AND CLOUDERA VISUALIZATION\n    \u00e2\u20ac\u00a2 Cloud Azure, AWS, , HDInsight, Cortana Intelligence Suite, Data\n      Factory, Data Gateway, Infrastructure as a Service IaaS, Platform as a\n      Service PaaS, Microsoft R Server, NLB, Key phrase extraction Azure\n      search, Unstructured text analytics, Event hub, Streaming, Poly Base\n    \u00e2\u20ac\u00a2 Search engines Elasticsearch, SOLR, Lucene, Kibana. Logstash, Rsync,\n      Tika\n    \u00e2\u20ac\u00a2 Machine Learning and recommendation engines MLlib, TensorFlow, Keras,\n      Weka Mahout, Multilayer perceptron classifier (MLPC), the feedforward\n      artificial neural network, scikit-learn, Pandas, Deeplearning4j,\n      Sparkling Water ML, Caffe2, MxNet etc. Different algorithms K-Means,\n      Random Forest, Gradient Boosting algorithms (GBM, XGBoost, XGBoost and\n      CatBoost)\n    \u00e2\u20ac\u00a2 BI Framework: Strategy and Implementation Plans, Enterprise Metrics,\n      Integration Points, Gap Analysis, BI Portfolio, Performance Management\n      (PM), Analytic and PM Technologies, Defining Business and Decision\n      Process, Building Metadata and Services Centers, Establishing\n      Enterprise Information Management (EIM) Committees, Defining The Role\n      of DW and BI Program Steering Committee, It's Mission, Objectives,\n      Roles and Responsibilities, DAMA DMBOK\n    \u00e2\u20ac\u00a2 Architecture and Data Modeling: Initial Conceptual Solution, Solution\n      Blueprints, Technology Impact Analysis (TIA), Gap Analysis, Technology\n      Roadmap, Dimensional modelling, ER Modelling, Start Schema, Snowflake,\n      Fact, Dimension, Hierarchy, Inmon/ Kimball/ Imhoff, Data Marts, EDW,\n      ERWin 9.5/8.0/7.x, DeZign, Microsoft Visio, Enterprise Architecture\n      (EA), Service Oriented Architecture (SOA), UML, Zachman, TOGAF, Star &\n      Snowflake Schemas, 3 Normal Forms, Normalization and Demoralization,\n      Logical Model and Physical Model, Fact/Dimension/Hierarchy\n      identifications, Data Warehouse Development Lifecycle, Data Mapping,\n      Data Dictionaries\n    \u00e2\u20ac\u00a2 Data Governance: IBM InfoShere MDM, Informatica MDM, MDS, DQS,\n      Profisee Maestro, SAS MDM\n    \u00e2\u20ac\u00a2 Integration and ETL: SSIS/SSRS/SSAS, SQL Server\n      2014/2012/2008R2/2008/2005, Informatica PowerCenter, DataStage,\n      Cognos, ETL Mapping design, Data Profiling, Data Validation, Data\n      Migration, Data Cleansing, Data Structure, Data Quality Services\n      (DQS), BIDS, SQL Data Tools (SSDT), Auditing Framework, Execution\n      Plans, ETL Parallel Processing, Error Handling, Custom Scripting, IBM\n      Cognos, InfoSphere DataStage, Informatica PowerCenter, SAP\n      BusinessObjects (BO)\n    \u00e2\u20ac\u00a2 Data Warehousing and Analysis: OLAP/Cube/MDX/DAX, Dimensional\n      Modelling, Tabular Modelling, KPIs, KPPIs, Data Analysis, SPSS,\n      Predictive Analysis, Data Mining, Machine Learning, SAP HANA,\n      Statistical Analysis, SAS, SAS VA (Visual Analytics), R, XLSTAT,\n      Sentiment analysis, Speech analytics, Teradata. Netezza, Cloudera,\n      PDW, Aginity, Master Data Services (MDS), Master Data Management\n      (MDM), Data Quality (DQ), Analysis of Change (AOC), Metric Engine.\n    \u00e2\u20ac\u00a2 Reporting: Predefined Reports, Ad-hoc Reporting, Analytical Reports,\n      Custom Reporting with .NET/ Report Viewer, SQL Server Reporting\n      Services (SSRS), SharePoint 2013/2010/2007/2003, PerformancePoint,\n      PowerPivot, Power View, Crystal Reports, Hyperion, MicroStrategy,\n      Cognos Report Studio, Framework, Workspace Advanced, DMR, TM1\n    \u00e2\u20ac\u00a2 Data Visualization: Power Map, PowerPivot, Power View, SharePoint,\n      Liferay, PerformancePoint, Google Visualization, Esri's GIS\n      (geographic information systems) , mapping, SAP Lumira, QlikView,\n      Tableau, Data Mapping\n    \u00e2\u20ac\u00a2 Database: MS-SQL, Oracle, Oracle SQL Developer, TSQL, MDX, DMX,\n      PL/SQL, Stored Procedure, View, Function, Erwin Data Modeler, DB2,\n      PowerDesigner, MongoDB, Access, Excel, FoxPro, Informix, NoSQL, Big-\n      data, Hadoop, Spark, HBase, HDInsight, PDW, PolyBase, Hive, HQL,\n      Map/Reduce, HFS, Alert\n    \u00e2\u20ac\u00a2 Programing Languages: SQL, T-SQL, PL/SQL, C#, WCF, Restful Services,\n      WPF/Silverlight, WF, VB .NET, ASP .NET, ADO.NET, LINQ, MVC, MVVM, MVP,\n      AJAX, HTML 5.0, DHTML, XML, XSL, WSDL, XSD, JSON, Java Script,\n      PowerShell, COM, DCOM, VB Script, UNIX Shell Scripting\n    \u00e2\u20ac\u00a2 Others: Agile, Extreme Programing, RUP, Use Cases, SDLC, TCP/IP, CVS,\n      Microsoft Team Foundation Server (TFS), Tortoise SVN, SQL*Plus, TOAD,\n      WinSQL, SilverLight, LightSwitch, Kerberos, Single Sign-On, Datazen,\n      One-Key.\n    \u00e2\u20ac\u00a2 Architecture and Design: Enterprise Architecture (EA), Service\n      Oriented Architecture (SOA), Enterprise Service Bus (ESB), Top-Down/\n      Bottom-Up Design, Structured Design, Object Oriented Design, Multi-\n      tiered and Multi-threaded architecture, Rational Rose, UModel,\n      Patterns: Model/View/ViewModel (MVVM), MVC, MVP, Visio, UML, Zachman,\n      TOGAF, Federal Enterprise Architecture, Gartner Methodology\n    \u00e2\u20ac\u00a2 Business Optimization: Asset management, Information Technology\n      Infrastructure Library (ITIL), customer satisfaction, call center\n      management, service request enhancement, AODA compliance, fraud\n      detection, CRM and ERP optimization, improving marketing\n      effectiveness, portfolio optimization, governance, risk management,\n      compliance, healthcare patient records management, electronic medical\n      records (EMR), optimizing routes and schedules for logistics planning,\n      insurance risk assessment, optimizing manufacturing production.\n    \u00e2\u20ac\u00a2 Integration: Windows API, Biztalk, SOA, WCF, SSIS\n    \u00e2\u20ac\u00a2 Data Access: ADO.NET, LINQ, Entity Framework, Microsoft Enterprise\n      Library, OLE DB, Oracle Data Provider, MS OLAP, SQL Master Data\n      Services (MDS), StreamInsight\n    \u00e2\u20ac\u00a2 Software Development\n    \u00e2\u20ac\u00a2 Methodologies: Test driven programming, Agile software development,\n      Extreme Programming (XP) Microsoft .NET Framework (from 1.0 to 4.0),\n      C#, Visual Basic .NET, VB .NET, ADO .NET, WinFX including Windows\n      communication foundation (WCF), windows workflow (WF), windows\n      presentation foundation (WPF), XAML, XML, HTML, HTML5, Java J2EE,\n      Spring Framework, JavaScript, AJAX, RESTful services, Payment Card\n      Industry (PCI), Image Processing\n    \u00e2\u20ac\u00a2 Visual C++ (MFC, SDKs, COM, DCOM, ATL ActiveXs), VB, C++, Perl, VHDL,\n      Verilog, Shell, Skill, Ocean, SystemC\n    \u00e2\u20ac\u00a2 Scala SBT, Java Maven\n    \u00e2\u20ac\u00a2 Version control tools: Source Safe, Team Foundation Version Control,\n      (TFVC), Subversion Tortoise SVN\n    \u00e2\u20ac\u00a2 Code metrics: Simian, RSM\n    \u00e2\u20ac\u00a2 Type/ industry: financial, banking, biomedical, pharmaceutical,\n      engineering, telecommunication, semiconductor, logistics, health,\n      scientific, e-commerce, instrumental\n    \u00e2\u20ac\u00a2 Internet Development: ASP .NET, MVC, Sliverlight, HTML, DHTML, Web\n      services for marketing and financial applications, AJAX, ASP,\n      JavaScript and VB Script, XML, Microsoft Internet Information Server\n      (IIS), Microsoft E-Commerce, PHP, Webload\n    \u00e2\u20ac\u00a2 Cloud Computing\n    \u00e2\u20ac\u00a2 Windows Azure, Amazon AWS EC2\n    \u00e2\u20ac\u00a2 SharePoint: SharePoint 2013/2010/2007/2003, Multi-machine SharePoint\n      Farm Architecture, Setup, Configuration, Load Balancing, Clustering,\n      Backup Plans, Web Part and module development, Collaboration, Social,\n      Search, Web Content Management, Enterprise Content Management, App\n      Management, PerformancePoint and PowerPivot, PowerView, Application\n      Federation, Secure Store Application, Business Connectivity, Usage\n      Reports, SharePoint Designer, Dashboard Designer, PerformancePoint,\n      dashboard, charts, KPI, Scorecards, reports, filters, Excel Services,\n      PowerPivot Services, Web Analytics, Static Analysis, Hit Counters,\n      Custom Development, PowerShell, SharePoint API, Object Model, web\n      parts web services, workflows, Content Management, site\n      collections/structure\n    \u00e2\u20ac\u00a2 Mobile Development\n    \u00e2\u20ac\u00a2 Android, iOS, Windows\n    \u00e2\u20ac\u00a2 Operating System Used: Windows, UNIX, Windows Azure, Linux, Android,\n      iOS, Windows Mobile, MS-DOS\n    \u00e2\u20ac\u00a2 Hardware and Simulation: Matlab, Cadence Spectre, Spice, Eldo, ANSYS\n    \u00e2\u20ac\u00a2 Algorithms: Genetic algorithm, simulated annealing based algorithms,\n      heuristic search, binary search, quick sort\n    \u00e2\u20ac\u00a2 Automation and Scripting: VB, Perl, Unix Shell\n\n                                 LANGUAGES:\nENGLISH, SWEDISH, PERSIAN AND NORWEGIAN.\n",
          "confidence": 1.0,
          "method": "full_text",
          "structured_data": null
        },
        "tax_term": {
          "value": "CONTRACT",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "source_by": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "designation": {
          "value": "Cloud Architect",
          "confidence": 0.9,
          "method": "ner",
          "structured_data": null
        },
        "experience": {
          "value": "20",
          "confidence": 0.9,
          "method": "regex_total_experience_summary",
          "structured_data": null
        }
      }
    },
    {
      "resume_path": "data\\input\\Akin - PM  - OH.doc",
      "confidence_score": 0.48500000000000004,
      "used_ocr": false,
      "extracted_fields": {
        "skills": {},
        "first_name": "Akin",
        "last_name": "Saheeb",
        "primary_email": {
          "value": "asaheeb@Gmail.com",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "secondary_email": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "phone": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "city": {
          "value": "Westerville",
          "confidence": 0.7,
          "method": "ner",
          "structured_data": null
        },
        "state": {
          "value": "OH",
          "confidence": 0.9,
          "method": "zip_database",
          "structured_data": null
        },
        "zip": {
          "value": "43081",
          "confidence": 0.7,
          "method": "regex",
          "structured_data": null
        },
        "work_authority": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "resume_link": {
          "value": "data\\input\\Akin - PM  - OH.doc",
          "confidence": 1.0,
          "method": "file_path",
          "structured_data": null
        },
        "raw_resume": {
          "value": "\n                              Akin  Saheeb, MBA\n                           Westerville, Ohio 43081\n                              asaheeb@Gmail.com\n\n\n\nSummary of Qualifications:\n|Experienced Project Manager offering Retail and Financial Services domain|\n|expertise                                                                |\n|Extensive experience managing PAYMENTS projects                          |\n|Card Present Payment transaction                                         |\n|Card Not Present Payment transaction                                     |\n|Centralized Payments settlement                                          |\n|Apple Pay implementation                                                 |\n|Mobile wallet                                                            |\n|Integration of loyalty and offer products with payment solutions \u00e2\u20ac\u201c Pay,  |\n|Earn, Redeem, Save                                                       |\n|Created and managed project plan for additional functionality request to |\n|enable client to process an \u00e2\u20ac\u02dcabove site\u00e2\u20ac\u2122 payment that would allow user to|\n|settle their bill when dining in the restaurant by using their Mobile App|\n|or Mobile Internet site to check out with \u00e2\u20ac\u02dcPay at the Table\u00e2\u20ac\u2122 capabilities|\n|Project planning and execution, scope definition, change control, risk   |\n|mitigation, stakeholder engagement, and project close out                |\n|Budget management of up to $42M and teams of 25+                         |\n\nTechnical Skills Summary:\n \u00e2\u20ac\u00a2 Budget / Financial Management including Business Case completion,\n   Business Process Reengineering, Budget Planning, Enterprise Content\n   Management, CRM Systems, Cost Control, Schedule Management, Strategic\n   Planning, Earned Value Analysis, Quality Control, Process Improvement,\n   Risk Management, Scope Management, Change Management, Client Relations,\n   Business Analysis, Third Party Provider Risk Management,\n \u00e2\u20ac\u00a2 Clarity, Confluence \u00e2\u20ac\u201c Collaboration Software, Jira \u00e2\u20ac\u201c Workflow Management,\n   PlanView, OPPM, Microsoft Project, Project Workbench, MS Project\n   Portfolio Management, SAP, SharePoint, PeopleSoft Financial module,\n   Microsoft Visio, Access, PowerPoint, Microsoft Office Suite, Team\n   Foundation Server. Visual Studio\n \u00e2\u20ac\u00a2 PMBOK - PMI Processes, SDLC, RAD, JAD, Agile/Scrum, Waterfall, RUP, CRM,\n   EVA, and Six Sigma.\n \u00e2\u20ac\u00a2 Postman Collection tool \u00e2\u20ac\u201c from API point-of-view. Azure DeveOps,\n   Microsoft Office 365\n                             Experience Profile\n\nWendy\u00e2\u20ac\u2122s, Dublin, Ohio\n                                               10/2019 \u00e2\u20ac\u201c Present\nConsultant \u00e2\u20ac\u201c Payment Solutions Lead Project Manager\n                     Enterprise IT & Infrastructure Solutions\n \u00e2\u20ac\u00a2 Canada Card Present is payment solutions that spans multiple solutions;\n   On-Line Strategies Services Payments, LLC, GK Software, Moneris, GiveX ,\n   and Wendy\u00e2\u20ac\u2122s Payment Solutions.\n    Wendy's is seeking to implement an in-store payment processing solution\n   for their Canadian and US locations that will apply hardware-based Point-\n   to-Point Encryption (P2PE) of cardholder data (CHD) within the Ingenico\n   Lane 3000 PIN Encryption Devices (PED) that are integrated with their NCR\n   Aloha POS registers and Customer Self Order (CSO) kiosks.\n o Credit Card Processing\n o On-line Debit Card Processing\n o Gift Card Processing\n o Point-to-Point Encryption Services\n o End-of-Day Processing\n o Tokenization Services\n o Device Management Services\n o Store-to-Host Payment Processing Services\n o Network Connectivity\n\n\n \u00e2\u20ac\u00a2 Canada Card Not Present\n \u00e2\u20ac\u00a2 Centralized Payments Settlement\n\n\nFiserv, Dublin, Ohio\n                                               07/2018 \u00e2\u20ac\u201c 08/2019\nConsultant \u00e2\u20ac\u201c Biller Solutions Client Implementations Sr. Project Manager\n \u00e2\u20ac\u00a2 Electronic Billing and Payment Solutions.\n    o Electronic Bill drives paperless\n    o Mobile Wallet\n    o Remittance solution to receive and post payment details faster\n    o Team Size 10\n \u00e2\u20ac\u00a2 Bill Discovery and Alert also referred to as Challenger is an ePayments\n   initiative that spans multiple solutions; Fiserv Output Solutions,\n   ePayments, and Billler Solutions. The solution requires Fiserv to get pre-\n   loaded biller data to setup biller as payee. This project focuses on\n   developing an end to end process that will be followed to setup each\n   Biller, across the Biller and Payments Group (BPG).\n \u00e2\u20ac\u00a2 Designed business modelling, requirements modelling, and system analysis\n   to map out the desire workflow to understand it more completely.\n \u00e2\u20ac\u00a2 Engaged with customers and technical staff to create business and\n   technical requirements and other deliverables on projects.\n \u00e2\u20ac\u00a2 Developed and maintained documentation for assigned projects.\n \u00e2\u20ac\u00a2 Partnered with various IT departments to achieve alignment on the\n   supportability and architectural standards related to the technology\n   being implemented.\n \u00e2\u20ac\u00a2 Communicated project status internally and managed communication between\n   the project team and stakeholders. Utilized Clarity to maintain project\n   risks, action items/issues, milestones, and tracked team members\u00e2\u20ac\u2122 time.\n\nBill Discovery and Alert Projects:\n \u00e2\u20ac\u00a2 PennyMac Loan Services\n \u00e2\u20ac\u00a2 NISSAN/Infiniti\n \u00e2\u20ac\u00a2 TXU Energy\n \u00e2\u20ac\u00a2 AEP (American Electric Power)\n \u00e2\u20ac\u00a2 Frontier Communications\n \u00e2\u20ac\u00a2 AT&T\n \u00e2\u20ac\u00a2 Southwest Gas\n \u00e2\u20ac\u00a2 Jacksonville Electric Authority (JEA)\n\nOCP, San Mateo, CA\n                                               08/2017 \u00e2\u20ac\u201c 04/2018\nConsultant \u00e2\u20ac\u201c Remote Project Manager/Scrum Master - Software-as-a-Service\nProject\n \u00e2\u20ac\u00a2 Team Size 5 - 7\n \u00e2\u20ac\u00a2 Streamlined systems, and resolved issues for increased profitability, on-\n   time project delivery, and optimized customer satisfaction.\n \u00e2\u20ac\u00a2 Tracking multiple projects with multiple budgets.\n \u00e2\u20ac\u00a2 Effectively managing globally distributed model of delivering solution.\n \u00e2\u20ac\u00a2 Developed business processes and system solutions for various domains\n   from conceptualization to implementation.\n \u00e2\u20ac\u00a2 Managed project using Waterfall/Agile-hybrid framework.\n\nFirst Data Corporation, Hagerstown, MD\n07/2016 \u00e2\u20ac\u201c 06/2017\n(Digital Commerce Solutions)\nConsultant - Sr. Project Manager/Scrum Master\nAdvisory Services: Key tasks include client interaction, needs assessment\nanalysis, process improvements, and the ability to execute on key projects\nwithin anticipated schedules.\n \u00e2\u20ac\u00a2 In addition to client facing Project Management responsibilities, I have\n   provided consulting services on Digital Commerce, analysis of Mobile\n   Payment, and Reward programs. \u00c2\u00a0Designed and reviewed of various documents\n   including the Software Requirement Specifications\n   (SRS),\u00c2\u00a0Business\u00c2\u00a0requirements document (BRD), Use Case Specifications,\n   Functional Specifications (FSD), Systems Design Specification (SDS),\n   Requirement Traceability Matrix (RTM) and testing documents.\n \u00e2\u20ac\u00a2 Created and managed project plan for client solutioning project using --\n   Agile-Waterfall Hybrid methodology, as part of (Digital Commerce\n   Solutions).\n \u00e2\u20ac\u00a2 Developed the project charter, integrated project plan, resource plan,\n   contingency plan and related project management artifacts. Developed\n   communication plans for (day-to-day discussions, team meetings) and\n   written (status reports, change requests) form.\n \u00e2\u20ac\u00a2 Defined and tracked project milestones, maintained and reported on\n   overall integrated delivery plan.\n \u00e2\u20ac\u00a2 Coordinated and facilitated post deployment reviews for continual release\n   improvements.  Managed deployment of concurrent releases; communicated\n   release processes, timelines, requirements, and strategies to\n   stakeholders and project resources.\n \u00e2\u20ac\u00a2 Managed\u00c2\u00a0change\u00c2\u00a0management\u00c2\u00a0requests to ensure release contents were\n   compliant.\n \u00e2\u20ac\u00a2 Maintained communications with QA to ensure problem records were resolved\n   and closed prior to release launch.\u00c2\u00a0Created and distributed release\n   processes to cross functional teams.\u00c2\u00a0\n\nPay Now APIs Project\n \u00e2\u20ac\u00a2 Budget-450K, Team size- 10+\n \u00e2\u20ac\u00a2 Worked collaboratively with customers, as well as internal and external\n   stakeholders.\n \u00e2\u20ac\u00a2 Created and managed project plan for additional functionality request to\n   enable client to process an \u00e2\u20ac\u02dcabove site\u00e2\u20ac\u2122 payment that would allow user to\n   settle their bill when dining in the restaurant by using their Mobile App\n   or Mobile Internet site to check out with \u00e2\u20ac\u02dcPay at the Table\u00e2\u20ac\u2122\n   capabilities.\n\nCiti Retail Services Token Service Provider and Integration with Samsung\n \u00e2\u20ac\u00a2 Budget- 1M, Team size- 9-13\n \u00e2\u20ac\u00a2 Track First Data Integration with TCH for Cryptogram Verification\n   associated with Token.\n \u00e2\u20ac\u00a2 Manage connectivity with TCH which include Cryptogram Validation,\n   authorization Advice, and Detokenization.\n\nMasterCard Introduction of New 2 Series BIN Project\u00e2\u20ac\u201c (Compliance Project)\n \u00e2\u20ac\u00a2 Acquirers will need to upgrade their systems to be compatible with 2-\n   Series BINs.\n \u00e2\u20ac\u00a2 All merchants will need to be compliant with the 2-Series migration as of\n   January 1st. 2017.\n\nPetro Universal Commerce Enablement Project\n \u00e2\u20ac\u00a2 Budget-2.5M, Team size- 25+\n \u00e2\u20ac\u00a2 Mobile Application solutions developed to integrate loyalty and offer\n   products with payment solutions \u00e2\u20ac\u201c Pay, Earn, Redeem, Save.\n \u00e2\u20ac\u00a2 Managed mobile device authentication \u00e2\u20ac\u201c enhance the security of merchant,\n   commerce offering providing multiple layers of mobile device\n   authentication.\n\nValuelink TEST MIDs Project (Operational Efficiencies Project)\n \u00e2\u20ac\u00a2 Budget-100K, Team size- 9+\n \u00e2\u20ac\u00a2 The objective of this concept/idea is to create a series of TEST MIDs as\n   a result of anticipated growth; to be used exclusively in a sandbox/test\n   environment.\n \u00e2\u20ac\u00a2 Planned and managed requirements to allow quick boarding of potentially\n   new or standard Valuelink merchants on the uComm gateway and in some\n   cases the merchants are not existing Valuelink clients or are Valuelink\n   standard clients.\n\nDiscover Financial Services, New Albany, OH                         02/2015\n\u00e2\u20ac\u201c 12/2015\n(Common Payment Platform)\nConsultant - Sr. Program Manager / Scrum Master: Wallet Service Provider \u00e2\u20ac\u201c\nApple Pay\n \u00e2\u20ac\u00a2 Budget-5M, Team size- 25+\n \u00e2\u20ac\u00a2 Created and managed project plan for Apple Pay Implementation project\n   using Agile - Waterfall Hybrid methodology, as part of Common Payment\n   Platform (CPP). Created\u00c2\u00a0Business\u00c2\u00a0Requirement Document (BRD), Functional\n   Requirement Specification (FRS) document, User Requirement Specification\n   (URS) and Change Request (CR) document for system application\n   development.\u00c2\u00a0\n \u00e2\u20ac\u00a2 Collaborated with multiple functional areas to complete required\n   deliverables.\n \u00e2\u20ac\u00a2 Performed contract Gap Analysis.\n \u00e2\u20ac\u00a2 Performed Requirement Analysis and developed Use Cases, Activity Diagrams\n   using Visio diagraming. Worked with a wide variety of internal staff,\n   including senior level management, to ensure process flows and policies\n   are properly defined and documented according to company guidelines.\n \u00e2\u20ac\u00a2 Coordinated and facilitated post deployment reviews for continual release\n   improvements.  Managed deployment of concurrent releases; communicated\n   release processes, timelines, requirements, and strategies to\n   stakeholders and project resources.\n \u00e2\u20ac\u00a2 Maintained communications with QA to ensure problem records were resolved\n   and closed prior to release launch.\u00c2\u00a0Created and distributed release\n   processes to cross functional teams.\u00c2\u00a0\n\n\n3-D Secure Integration Project (ProtectBuy)\n \u00e2\u20ac\u00a2 ProtectBuy is an added layer of security to Discover card holder that\n   helps make online shopping experience more safe and secure.\n \u00e2\u20ac\u00a2 Defined boarding process and procedures\n \u00e2\u20ac\u00a2 Defined engagement process\n \u00e2\u20ac\u00a2 ACS Vendor Certification; MPI Vendor Certification; Issuing Participant\n   Certification\n \u00e2\u20ac\u00a2 Acquirer/Merchant Certification; Hosted Vendor \u00e2\u20ac\u201c OBO (On-behalf-Of);\n   Defined Support Service Model\n \u00e2\u20ac\u00a2 Anti-Money Laundering Project\n \u00e2\u20ac\u00a2 Performed a variety of AML/KYC money laundering activities in support of\n   Bank Secrecy Act (BSA) and /or Anti-Money Laundering regulations designed\n   to prevent financial-related crimes against the government while\n   decreasing client reputational risk.\n \u00e2\u20ac\u00a2 Review cases for adherence to EDD procedures and ensure that\n   recommendations are accurate based on all information available.\n\nCitigroup, Cincinnati, OH\n03/2014 \u00e2\u20ac\u201c 09/2014\n(Digital ATM)\nConsultant - Program Manager: ATM Windows 7 Upgrade\n \u00e2\u20ac\u00a2 Budget-42M, Team size- 50+\n \u00e2\u20ac\u00a2 Managed all forecasting and demand planning activities for E01 and E02\n   project size and effort estimates in excess of $41 Million dollars for\n   global initiatives.\n \u00e2\u20ac\u00a2 Managed Windows 7 32-bit software solution that is in compliance with\n   Citi CATE in 23 countries.\n \u00e2\u20ac\u00a2 Managed regional project managers.\n \u00e2\u20ac\u00a2 Utilized HP Quality Center and TFS to manage release plan.\n \u00e2\u20ac\u00a2 Managed Global release to support ATM Windows 7.\n \u00e2\u20ac\u00a2 Performed integration change control.\n \u00e2\u20ac\u00a2 Managed acceptance criteria for regional consumptions.\n\nOne Call Professionals, Columbus, OH\n02/2013 \u00e2\u20ac\u201c 11/2013\nConsultant - Scrum Master: eCommerce (Global Distribution System) CRM\n \u00e2\u20ac\u00a2 Worked on one workstreams of a large project to manage GDS - e-commerce\n   (Global Distribution System) implementations.\n \u00e2\u20ac\u00a2 Coached team in optimal time utilization through concepts of scrum and\n   agile management methodologies.\n \u00e2\u20ac\u00a2 Led sprint reviews and daily scrum meetings to touch base with whole team\n   and ensure that all members were performing satisfactorily.\n \u00e2\u20ac\u00a2 Trained team members as well as the rest of the company staff in\n   employing agile and scrum practices to improve work flow.\n \u00e2\u20ac\u00a2 Managed system integrations. Planned the release to ensure that sprints\n   did deliver increments with the right quality.\n\nJPMorgan Chase, Columbus, OH                                  02/2012 \u00e2\u20ac\u201c\n10/2012\nConsultant - IT Risk Project Manager:  (Data Management & Analytics)\n \u00e2\u20ac\u00a2 Managed multiple cross-functional projects with globally dispersed teams\n   to manage the risks, issues and change request to minimize its impact to\n   project scope, schedule and cost on the design and build of Integrated\n   Consumer Data Warehouse (ICDW); Enterprise Data Warehouse (EDW); and\n   Info1.\n \u00e2\u20ac\u00a2 Communicated the project status, risks and issues on regular basis to all\n   stakeholders.\n \u00e2\u20ac\u00a2 Collaborated with multiple functional areas to complete required\n   deliverables. Provided day to day direction to team members. Managed\n   Enterprise Security Manager (ESM) remediation review for Info1 Data\n   Management.\n \u00e2\u20ac\u00a2 Managed Guardium Logs Reporting on EDW (Enterprise Data Warehouse)\n   servers, to ensure that there is an audit trail of the changes being\n   performed by the DBA team with elevated access.\n \u00e2\u20ac\u00a2 Designed Business Impact Analysis process for ICDW (Integrated Customer\n   Data Warehouse), Information One, and EDW staffs.\n \u00e2\u20ac\u00a2 Managed ICDW (Integrated Customer Data Warehouse) Control Self-Assessment\n   to ensure that data back-up and retention requirements are defined and\n   agreed upon by key business stakeholders.\n \u00e2\u20ac\u00a2 Managed third Party Provider Oversight-Lifecycle to ensure that there is\n   a clear escalation/reporting process for issues arising (e.g. security\n   breaches, Service Level Agreement problems).\n\nStaffmark, Cincinnati, OH                                     01/2011\n\u00e2\u20ac\u201c10/2011\nConsultant - IT Project Manager\n \u00e2\u20ac\u00a2 Provided management with accurate, independent and timely assessments in\n   the area of risk, internal control weakness, information systems\n   security, ERP Systems (PeopleSoft \u00e2\u20ac\u201cCRM), database technologies, disaster\n   recovery and Business continuity planning etc.\n \u00e2\u20ac\u00a2 Managed resources during the entire project with strict deadlines and\n   budget constraint.\n \u00e2\u20ac\u00a2 Acted as a point of contact to all stakeholders by providing regular\n   project status updates to avoid communication breakdown and surprises.\n   Facilitated project meetings, change control, resource planning and\n   leveling, risk management and contingency planning.\n\n\n\n\n\nVeyance Technologies, Inc, Marysville, OH\n                                  04/2007 \u00e2\u20ac\u201c 08/2008\nConsultant - IT Project Manager /Scrum Master\n \u00e2\u20ac\u00a2 Plant infrastructure upgrade and SAP Implementation\n \u00e2\u20ac\u00a2 Create and execute Lean and Operational Excellence organizational\n   strategies, manage business program initiatives, and identify and\n   implement Best Practices.\u00c2\u00a0\n \u00e2\u20ac\u00a2 Led and managed the Lean Design for Innovation Committee for the\n   development of an \"Innovation Methodology Playbook\"\n \u00e2\u20ac\u00a2 Led cross-functional team to design and implement SAP ERP/CRM system for\n   the Manufacturing business using ASAP approach.\n \u00e2\u20ac\u00a2 Worked with and managed off-shore developer teams.\n \u00e2\u20ac\u00a2 Designed a process for managing new data Center applications and\n   initiatives, from concept to implementation.\n\nJPMorgan Chase, Columbus, OH                                  1991 - 2006\nIT Project Manager\n \u00e2\u20ac\u00a2 Managed several medium to large development projects focusing mostly on\n   call center infrastructure, data center applications and enterprise\n   telecommunication offerings with a project team of between 8 and 20, and\n   duration of between 3 months and 12 months: with the aid of MS Project\n   Portfolio Management.\n\n|Education & Certifications                                                 |\n\n\n \u00e2\u20ac\u00a2 Master of Business Administration, Information Technology Project\n   Management, Minor \u00e2\u20ac\u201c Finance, Ashland University, Ashland, OH\n\n\n \u00e2\u20ac\u00a2 Bachelor of Science, in Business Administration, Franklin University,\n   Columbus, OH\n \u00e2\u20ac\u00a2 Information Technology Project Management Certificate (MBA, Ashland\n   University) [pic][pic]\n-----------------------\n                           Program/Project Manager\n\n\n",
          "confidence": 1.0,
          "method": "full_text",
          "structured_data": null
        },
        "tax_term": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "source_by": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "designation": {
          "value": "Project Manager",
          "confidence": 0.9,
          "method": "ner",
          "structured_data": null
        },
        "experience": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        }
      }
    },
    {
      "resume_path": "data\\input\\Aleem Ahmed - resume.doc",
      "confidence_score": 0.37500000000000006,
      "used_ocr": false,
      "extracted_fields": {
        "skills": {},
        "first_name": "Aleem",
        "last_name": "Ahmed",
        "primary_email": {
          "value": "ahmed.8663167@gmail.com",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "secondary_email": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "phone": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "city": {
          "value": "",
          "confidence": 0.0,
          "method": "ner",
          "structured_data": null
        },
        "state": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "zip": {
          "value": "",
          "confidence": 0.0,
          "method": "regex",
          "structured_data": null
        },
        "work_authority": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "resume_link": {
          "value": "data\\input\\Aleem Ahmed - resume.doc",
          "confidence": 1.0,
          "method": "file_path",
          "structured_data": null
        },
        "raw_resume": {
          "value": "\n\nAleem Ahmed\n\n\n(608)\u00c2\u00a0909-1931\n\n\nahmed.8663167@gmail.com\n\n----------------------------------------------------------------------------\n             --------------------------------------------------\n\nProfessionally 14+ experience  in  Java  development  experience  of  Web/N-\nTier/Client Server/Distributed technologies (SOA,  Restful/SOAP,  WSDL,  Web\nServices, JMS, Spring, Hibernate,  EJBs,  Servlets,  JSP,  XML,  JavaScript,\nStruts, WebSphere Application Server, Rational  Application  Developer(RAD),\nWebLogic Application Server) and Client Server technologies (AWT, Swing)  on\nvarious platforms  like  Windows  2k/NT/XP/7/8,  Linux  and  Unix.  He  also\nexperienced in working on\u00c2\u00a0DevOps\u00c2\u00a0process and tools (Code review,  unit  test\nautomation, Build & Release automation, Environment, Service,  Incident  and\nChange  Management)  .He  has  developed  software  systems   for   Utility,\nInsurance and  Telecom  domains  with  responsibilities  in  all  phases  of\nsoftware development life cycle  including  Analysis,  Design,  Development,\nTesting, Implementation, Support and Training.\n\n\n \u00e2\u20ac\u00a2  Strong  experience  in  AWS\u00c2\u00a0Cloud  platform  and  its   features   which\n   includes\u00c2\u00a0EC2, VPC, EBS, Cloud  Watch,  Cloud  Trail,  CloudFormation  AWS\n   Config, Autoscaling,\u00c2\u00a0Cloud Front, IAM, S3. Drives Enterprise  Application\n   technical blueprints, business requirements & specifications, integration\n   of heterogeneous environments, API implementations. He is an  independent\n   worker  with   strong   problem   solving,   analytical,   interpersonal,\n   communication, presentation and testing skills.\n\n\n \u00e2\u20ac\u00a2 Expertise in developing  inter-operable  Web  Services  and  its  related\n   technologies like\u00c2\u00a0SOAP, WSDL,  UDDI,\u00c2\u00a0XML\u00c2\u00a0related  technologies/tools\u00c2\u00a0such\n   as  JAXB,  XSD,  XML\u00c2\u00a0with  good  understanding\u00c2\u00a0of\u00c2\u00a0JAX-WS,  JAX-RS,   JAX-\n   RPC\u00c2\u00a0inter-operable issue.\n\n\n \u00e2\u20ac\u00a2 Development experience using Java/J2EE ( JSF, JSP, Servlets,  JDBC,  EJB,\n   MDB, JTA, JPA, JNDI, JMS, IBM MQ, Web\u00c2\u00a0Services\u00c2\u00a0(Axis,  JAX-RPC,  JAX-WS),\n   MVC Struts frameworks, Hibernate , Spring  ,  Log4j,  JUnit4,  Maven  and\n   ANT).\n\n\nTechnical Skills:\n|Operating Systems   |Linux, Unix, Solaris, Windows /NT/2000/2003/XP      |\n|Version Control     |SVN, GIT, CVS, Visual Source Safe, PVCS, Clear Case,|\n|                    |Bit bucket                                          |\n|Automation/Build    |Jenkins, Docker, Chef, Puppet,\u00c2\u00a0Ansible, Kubernetes, |\n|Tools               |Harvest, ANT, Maven, Gradle, Artifactory            |\n|Virtualization      |Docker, Amazon AWS/EC2, VMWare, OpenShift           |\n|Language            |Java, J2EE, SQL                                     |\n|XML Technologies    |XML, WSDL, XSLT, SOAP, XML Schema, Web services.    |\n|Web Technologies    |J2EE, Spring, Hibernate, JSP, JNDI, LDAP, Servlets, |\n|                    |JMS, JDBC, Swing, HTML                              |\n|Web App & Web       |WebLogic, WebSphere, Tomcat, JBoss, JIRA, Service   |\n|Servers             |Now                                                 |\n|Scripting Languages |Java Script, Unix shell scripting, Python           |\n|Oracle Utilities    |Oracle Customer Care & Billing, Oracle MDM, OUAF    |\n|Database            |Oracle 10g/9i/11g                                   |\n|App Framework       |Spring MVC, Struts, Hibernate, Java Persistence API |\n|Data Integration    |Oracle Fusion, MQ, WMB 7.0                          |\n|IDE Tools           |Eclipse, RAD 8.5/7.0/6.1, JBuilder, JDeveloper,     |\n|                    |IntelliJ Idea                                       |\n\n\nProfessional Experience:\nAmerican Family Insurance, Madison WI                               (July\n2015 \u00e2\u20ac\u201c Feb 2020)\nSr. Java Developer\nResponsibilities:\n o Design, develop, and maintain web-based applications  using  Java,  J2EE,\n   Spring, Hibernate, and Oracle\n o  Involved  actively  in  the  complete  software  development  lifecycle,\n   including performance analysis, design, development, and testing\n o Participate in design refactoring and enhancement of critical component\n o Developed the application using spring  framework  that  leverages  model\n   view layer architecture, also configured Dependency Injection.\n o Extensively used Hibernate in data access  layer  to  access  and  update\n   information in the database.\n o Involved in the Development of Spring Framework Controllers\n o Developing application using Spring core module and worked  on  Messaging\n   service.\n o Developed user interface using JSP, JSP Tag libraries  JSTL,  HTML,  CSS,\n   JavaScript.\n o Design highly usable GUIs using Spring Framework, JSPs, Struts MVC, HTML,\n   JavaScript, and CSS\n o Worked  on  Single  Page  Applications  using  AngularJS  which  includes\n   Routing, Directives, and Templates.\n o Used JMS for the asynchronous exchange  of  critical  business  data  and\n   events amongJ2EE components and legacy system.\n o Implemented a fully automated CI build and deployment infrastructure  and\n   processes for multiple projects.\n o Involved in performing application deployment to AWS Elastic  Bean  Stack\n   environment.\n o Configured & deployed applications on AWS for a multitude of applications\n   utilizing the AWS stack, cloud formation.\n o Standalone system through SCCM and Docker containerization.\n o Performed  source  control  systems  to  automate  for  Build/deployment,\n   Software Configuration/Continuous Integration/Continuous Delivery/Release\n   Management related tasks in Java EE Environments using Maven and Jenkins\n o Batch/Shell scripting\n o Used Oracle as a transactional database for customer payments.\n o Participated in integration testing and unit testing.\n o Implemented TestNG framework for system integration testing.\n o Performed troubleshooting and fixed production defects.\n\n\n\n\nEnvironment:\u00c2\u00a0JAVA, J2ee, Spring  MVC,  Spring  Core,  Hibernate,  JSP,  XML,\nAngularJS, CSS3, JavaScript, AJAX, JSON, RESTful  APIs,Junit,  Oracle,  Git,\nJenkins,  Bitbucket,  Maven,  CVS,  AWS,  Shell  scripting,  Perl,   Python,\nGuidewire Applications, Apache Tomcat, JBoss, Puppet, CA Harvest, JIRA.\n\nDTE Energy, Detroit, MI                                           (May 2013\n\u00e2\u20ac\u201c May 2015)\nSenior Java Developer\nResponsibilities:\n o Worked on Oracle Utilities Customer Care and Billing Module.\n o Worked on Oracle Utilities Application Framework (OUAF) for usage data\n   management for bill print module.\n o Integrated meter data management with billing module using Oracle Fusion\n   ESB.\n o Implemented web services using JAX-WS, XML, SOAP, and WSDL.\n o Produced and consumed web services from external systems.\n o Implemented Spring security for payment gateway.\n o Performed source control systems to automate for Build/deployment,\n   Software Configuration/Continuous Integration/Continuous Delivery/Release\n   Management related tasks in JavaEE Environments using Maven and Jenkins\n o Developed UI using HTML, JavaScript, CSS3, and Java Server Pages (JSPs).\n o Business Logic was implemented using EJBs and DAOs.\n o Batch/Shell scripting\n o Used Oracle as a transactional database for customer payments.\n o Participated in integration testing and unit testing.\n o Implemented TestNG framework for system integration testing.\n o Performed troubleshooting and fixed defects.\n\nEnvironment: Oracle CCB 2.3.1, Java, J2EE,  JSP,  MVC  Struts,  Spring  MVC,\nSpring Core, Hibernate, Web Services, XML, SOAP, WSDL,  JavaScript,  jQuery,\nHTML5,  CSS3,  Oracle  11g,  Oracle  Fusion  ESB,  WebLogic  Server,  Maven,\nJenkins, ClearCase, UNIX, Windows 8.\n\n\n\nAAA Insurance, Dearborn, MI                            (October 2010 \u00e2\u20ac\u201c\nApril 2013)\nSenior Java Developer\nResponsibilities:\n o Developed SOA services using Agile methodology.\n o Designed the architecture for middleware integration with external\n   systems based upon business requirements using J2EE architecture\n   framework with special emphasis on SOA architecture and web services.\n o Implemented web services using JAX-WS and JAX-RS.\n o Developed SOA design using WS-Security, SOAP/HTTP, WSDL, and XML/XSD\n   (schemas).\n o Utilized elements of extreme programming approach (especially TDD, pair\n   programming, and code walkthrough\u00e2\u20ac\u2122s).\n o Developed Spring and Hibernate applications.\n o Developed Oracle PL/SQL packages, procedures, and functions.\n o Worked with messaging queue applications.\n o Developed web application using Struts.\n o Extensively used Maven for build and Jenkins for continuous integration.\n o Performed code reviews and suggested best coding practices.\n o Provided technical leadership for development team to implement the\n   system.\n o Developed applications using Agile Scrum methodology.\n\nEnvironment: Agile Methodology, Java, J2EE, SOA, Web  Services,  SOAP,  XML,\nWSDL, Struts, Spring, Hibernate, JMS, Maven, Jenkins, Oracle 11g, Linux  and\nWindows 2003/XP/7, Data Power, WebSphere Application Server 7.\n\nSyniverse, Tampa, FL                                       (January 2008 \u00e2\u20ac\u201c\nSeptember 2010)\nSenior Java Developer\nResponsibilities:\n o Worked on Direct Operator Billing SOA project and implemented web\n   services using JAX-WS.\n o Performed design, development, and testing of the web services\n   application.\n o Developed web services using service-oriented architecture (SOA).\n o Created and updated WSDLs for web services (produce and consume both).\n o Used SOAP protocol to access the web service - WSDL schemas and JAX-RPC\n   for remote.\n o Procedure calls of the third-party property leads (SOA).\n o Used XML technologies like XPath, XSLT, and JAXB for data exchange and\n   presentation.\n o Developed middleware components like Message Driven Beans for JMS/MQ\n   integration.\n o Built service-oriented platform using web services for client requests.\n o Developed code using spring and hibernate frameworks to access database.\n o Used CVS for source code maintenance.\n\nEnvironment: Java/J2EE, Spring, Hibernate, JPA, Web  Service  (XPath,  SOAP,\nWSDL, JAXB, JAX-RPC), MDB, SAML, JBoss, Windows 2003/XP, UNIX, Solaris.\n\nMindTree Consulting Pvt Ltd, Bangalore, India                   (June 2004\n\u00e2\u20ac\u201c December 2007)\nSenior Java Developer\nResponsibilities:\n o Analyzed, designed, and developed application based on Java/J2EE and\n   design patterns.\n o Used XML technologies for data exchange and presentation using XML, XSL,\n   and XSLT.\n o Developed various HTML, DHTML, and JSP pages for the presentation layer.\n o Implemented RUP methodology and designed application using UML design\n   patterns.\n o Visualized and designed use cases, sequence diagrams, and class diagrams.\n o Used J2EE design patterns and developed EJB and DAO components.\n o Used Struts framework for the development of J2EE components.\n o Created servlets for abstraction between the client and server layers\n   (using MVC Architecture).\n o Developed DAO (Data Access Objects) and called them from EJB.\n o Designed and developed the system business logic using Stateless Session\n   Beans.\n o Used XML templates to present the user data in different formats.\n o Created JAR, WAR and EAR files.\n o Deployed EAR file on WebLogic Application Server.\n o Used ClearCase for version control.\n o Created build scripts using ANT.\n o Provided support and maintenance.\n\n\nEnvironment: Java, J2EE, JDBC,  Servlets,  JSP,  EJB,  XML,  XSLT,  WebLogic\nApplication  Server  8.1/6.1,  Struts,  UML,  JavaScript,  HTML,   JBuilder,\nClearCase, SQL, SQL Server 2000/2005,  Oracle  10g/9i,  UNIX,  Sun  Solaris,\nRedhat Linux, Windows 2000.\n\n\n\nEducation: Master\u00e2\u20ac\u2122s Degree in Computer Applications Osmania University,\nIndia\n\n",
          "confidence": 1.0,
          "method": "full_text",
          "structured_data": null
        },
        "tax_term": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "source_by": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "designation": {
          "value": "Application Developer",
          "confidence": 0.8,
          "method": "regex",
          "structured_data": null
        },
        "experience": {
          "value": "14+",
          "confidence": 0.9,
          "method": "regex_total_experience_summary",
          "structured_data": null
        }
      }
    },
    {
      "resume_path": "data\\input\\Alexander Mai - SAS - VC.doc",
      "confidence_score": 0.5700000000000001,
      "used_ocr": false,
      "extracted_fields": {
        "skills": {},
        "first_name": "Alexander",
        "last_name": "T. Mai",
        "primary_email": {
          "value": "Alextmai@gmail.com",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "secondary_email": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "phone": {
          "value": "7742326198",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "city": {
          "value": "MA",
          "confidence": 0.7,
          "method": "ner",
          "structured_data": null
        },
        "state": {
          "value": "LA",
          "confidence": 0.8,
          "method": "city_database",
          "structured_data": null
        },
        "zip": {
          "value": "02445",
          "confidence": 0.7,
          "method": "regex",
          "structured_data": null
        },
        "work_authority": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "resume_link": {
          "value": "data\\input\\Alexander Mai - SAS - VC.doc",
          "confidence": 1.0,
          "method": "file_path",
          "structured_data": null
        },
        "raw_resume": {
          "value": "\n                              Alexander T. Mai\n\nAddress     7 University Road                                       Cell\n  (774) 232-6198\n                  Brookline, MA 02445\nEmail     Alextmai@gmail.com\n\nEducation\nNortheastern                                                      University\n                       Boston, MA\n    \u00e2\u2013\u00aa Graduated with Bachelor of Science Degree in  Business  Administration\n                                        May 2007\n    \u00e2\u2013\u00aa Majored in Dual Concentration: Management  Information  Systems  (MIS)\n      and Finance\n    \u00e2\u2013\u00aa  Completed  six  months  co-op  programs  at  Zoran  Corporation   and\n      Raytheon\n    \u00e2\u2013\u00aa Honors: Dean\u00e2\u20ac\u2122s List, Balfour Academy\u00e2\u20ac\u2122s Scholarship\n\n\nSkills\n    \u00e2\u2013\u00aa Applications: Advanced SAS, SQL, R,  Stata,  XML  Mapper,  SSIS,  SAP,\n      ArcGIS, TeamSite, Cognos\n    \u00e2\u2013\u00aa  Certifications:  Microsoft  Word  Certified,   SAS   Certified   Base\n      Programmer for SAS 9 Credential\n\nExperience\nHarvard                            Medical                            School\n       Boston,  MA                        Statistical  Programmer   &   Data\nAnalyst                                                                 June\n2016 \u00e2\u20ac\u201c Present\n    \u00e2\u2013\u00aa Design and develop statistical and data management programs\n      associated with grant funded projects on disability and health\n      insurance.\n    \u00e2\u2013\u00aa Perform in multiple types of large cross-sectional and longitudinal\n      data files. Some data stem from population-based surveys (e.g.,\n      Medical Expenditure Panel Survey, Health and Retirement Study). Other\n      data include health care and prescription drug claims from private and\n      public insurance plans (e.g., Medicare and/or Medicaid).\n    \u00e2\u2013\u00aa Develop high quality data sets to estimate statistical models and\n      conduct hypothesis testing.\n    \u00e2\u2013\u00aa Interpret research protocols and create programs and algorithms that\n      are then discussed with the PI.\n    \u00e2\u2013\u00aa Work independently and make decisions about how to implement\n      analysis and provide well written detailed reporting of results,\n      including determination of proper summary statistics, report formats\n      and other analysis considerations.\n    \u00e2\u2013\u00aa Responsible for successful completion of these tasks within\n      competing project timelines, and must ensure integrity of data\n      collection, data review, data compilation, and analysis techniques.\u00c2\u00a0\n\nSenior                             Whole                              Health\n                Cambridge,    MA                           HEDIS     Manager\n                                                       October  2014  \u00e2\u20ac\u201c  May\n2016\n    \u00e2\u2013\u00aa Responsible for all aspects of the HEDIS rates production process,\n      including working with internal data sources, external data sources,\n      supplemental data sources and vendor partners to produce and submit\n      the company's HEDIS rates on an annual basis.\n    \u00e2\u2013\u00aa Supported the integration of hybrid data, administrative data\n      refreshes and supporting HEDIS documentation in order to meet\n      mandatory NCQA and CMS STARS data reporting requirements.\n    \u00e2\u2013\u00aa Analyzed the HEDIS results, compares them to nationally identified\n      benchmarks, and advises the director on opportunities and strategies\n      to continue improving the rates.\n    \u00e2\u2013\u00aa Participated on teams working to understand the barriers to strong\n      performance, and identifying opportunities to resolve the barriers in\n      meeting all requirements of the HEDIS program, CAHPS and HOS as\n      needed.\n    \u00e2\u2013\u00aa Worked closely with relevant departments to assure understanding of\n      the financial implications of the HEDIS and Star Rating programs.\n    \u00e2\u2013\u00aa Utilized SAS and SQL programming to automate and transform multiple\n      reports, reconcile and transfer medical records by file types to\n      different locations, and validate XML files using XML Mapper\n\nCommonwealth of Massachusetts - Center for Health Information  and  Analysis\n(CHIA)                 Boston, MA                        Programmer  Analyst\n                                                            February 2013  \u00e2\u20ac\u201c\nOctober 2014\n    \u00e2\u2013\u00aa Conducted statistical programming and analysis on All Payer Claims\n      Database (APCD), Acute Hospital Case Mix Databases, Medicaid\n      Management Information System (MMIS), Centers for Medicare and\n      Medicaid Services (CMS), Census, and other data resources to support\n      the analytic and information needs of key audiences.\n    \u00e2\u2013\u00aa Worked with the Quality Assurance team to evaluate and standardize\n      APCD by applying statistical analyses related to health care service\n      utilization, health care costs, and quality measurement.\n    \u00e2\u2013\u00aa Supported the analytics for the state\u00e2\u20ac\u2122s consumer health care quality\n      and cost website by calculating cost metrics for providers in the\n      Commonwealth and annual Cost Trends reporting.\n    \u00e2\u2013\u00aa Consulted with Subject Matter Experts (SME) to determine appropriate\n      analysis plans and data extraction code specifications, coordinates\n      projects involving organization of data, text and graphics, designing\n      study methodologies; and assisting with the development of new areas\n      of data collecting and policy reporting for the agency.\n    \u00e2\u2013\u00aa Applied HEDIS measures to analyze health care quality and\n      utilization data to identify best practices, care delivery trends and\n      clinical practice variation within hospital, provider group and\n      individual physician sites of care\n    \u00e2\u2013\u00aa Trained other analysts on SAS, SQL, ArcGIS and helped troubleshoot\n      production reporting issues, identified root causes and implement\n      necessary coding updates\n\nCVS                                                                 Caremark\nWoonsocket,  RI                        Manager  &   Senior   Consulting   of\nAccounting and  Financial  Reporting                            May  2012  \u00e2\u20ac\u201c\nJanuary 2013\n    \u00e2\u2013\u00aa Responsible for the creation and delivery of multiple Medicare Part\n      D reports that summarize results, identify trends, and develop\n      forecasts for: Beneficiary Premiums, Drug Utilization, Incurred but\n      not Recorded Claims, Risk Scores, Actuarial Metrics and Member\n      Eligibility.\n    \u00e2\u2013\u00aa Utilized large databases management in an AIX environment, and\n      applied basic Unix language, SAS Data Step and Proc SQL to turn\n      million of records of raw data from multiple tables/files into summary\n      reports.\n    \u00e2\u2013\u00aa Automated majority of the reports used by the accountants to book\n      journal entries. Other reports will be delivering to external business\n      clients as part of their monthly financial package.\n    \u00e2\u2013\u00aa Managed raw data from both external and internal by performing data\n      reconciliation and reports variance. External are the government\n      (Center for Medicare and Medicaid Services) and third party vendors.\n      Internal are the Enterprise Data Warehouse and monthly data extracts\n      from AS400.\n    \u00e2\u2013\u00aa Interacted effectively with members of accounting, finance,\n      actuarial, forecasting, billing, account management, and Medicare\n      operations groups. In addition, reviewed and assisted junior team\n      members in their workload.\n\nMIB  Solutions,  Risk  Management  Solutions  for  the  Insurance   Industry\n                           Braintree,   MA                          Database\nProgrammer                                                           Analyst\n          December 2010 \u00e2\u20ac\u201c April 2012\n    \u00e2\u2013\u00aa Performed logic and syntax checks, and standardizes data submitted\n      from various sources. Appropriately documented procedures in support\n      of projects as assigned. Managed interns and worked on multiple\n      projects at the same time.\n    \u00e2\u2013\u00aa Streamlined previously written computer code and used or modified to\n      meet the current project objectives. Participated in skills growth\n      path to promote achievement of corporate goals as recommended by the\n      Department Manager.\n    \u00e2\u2013\u00aa Interacted with Database Programmer Analysts, Actuaries, and\n      Developers to develop software applications for data analysis.\n      Participated in the evaluation, construction and transition into\n      production of software.\n    \u00e2\u2013\u00aa Migrated software applications done in SAS and the legacy\n      environment to SQL Server based applications. Complied with all\n      applicable corporate and departmental software policies, standards,\n      and practices including all standards relating to Security and\n      Disaster Recovery.\n\nBlue       Cross       Blue       Shield        of        MA        (BCBSMA)\n                                   Boston, MA\nData                  Analyst                  &                  Programmer\n                                July 2007 \u00e2\u20ac\u201c February 2010\n    \u00e2\u2013\u00aa Managed production databases and web/online  content  development  and\n      organization.\n    \u00e2\u2013\u00aa Designed/developed, programmed, maintained and  published  operational\n      reports.\n    \u00e2\u2013\u00aa Delivered production ready reporting with graphs, charts  and  dynamic\n      drill through capabilities.\n    \u00e2\u2013\u00aa  Performed  adhoc  and  customer   reports   for   Informatics/analyst\n      community.\n    \u00e2\u2013\u00aa  Developed  tools  using  SAS,  SQL,  and  Cognos,  to  minimize  data\n      manipulation required by informatics analysts.\n    \u00e2\u2013\u00aa  Created  value  by  integrating  current   business   Excel,   Access\n      processes into DW production process.\n    \u00e2\u2013\u00aa Trained interns  and  prepared  presentation,  documentation  and  key\n      management reports.\n\n\n\n\n",
          "confidence": 1.0,
          "method": "full_text",
          "structured_data": null
        },
        "tax_term": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "source_by": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "designation": {
          "value": "Data Analyst",
          "confidence": 0.9,
          "method": "ner",
          "structured_data": null
        },
        "experience": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        }
      }
    },
    {
      "resume_path": "data\\input\\Enterprise Architect - NJ - Madan.doc",
      "confidence_score": 0.66,
      "used_ocr": false,
      "extracted_fields": {
        "skills": {},
        "first_name": "Madan",
        "last_name": "Vemurie",
        "primary_email": {
          "value": "madanvemurie78@gmail.com",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "secondary_email": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "phone": {
          "value": "8563694341",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "city": {
          "value": "Jersey",
          "confidence": 0.7,
          "method": "ner",
          "structured_data": null
        },
        "state": {
          "value": "OH",
          "confidence": 0.8,
          "method": "city_database",
          "structured_data": null
        },
        "zip": {
          "value": "",
          "confidence": 0.0,
          "method": "regex",
          "structured_data": null
        },
        "work_authority": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "resume_link": {
          "value": "data\\input\\Enterprise Architect - NJ - Madan.doc",
          "confidence": 1.0,
          "method": "file_path",
          "structured_data": null
        },
        "raw_resume": {
          "value": "\n\n\n                                Madan Vemurie\n                              Phone: 8563694341\n                     Email Id:  madanvemurie78@gmail.com\n        LinkedIn: http://www.linkedin.com/pub/madan-vemurie/8/441/599\n\n\nObjective:\n\n      Highly industrious and self-motivated, creative individual seeking a\n      challenging position in Managing and Designing/Delivering client\n      solutions that best utilizes my past expertise in Technical Sales and\n      Management. Experienced technical product manager, technical project\n      lead, and business analyst with over 16 years of experience in the\n      health and fitness, excellent communication skills and a track record\n      of successful product releases working in cross - functional teams.\n\nSummary:\n\n    \u00e2\u2013\u00aa 16 plus years of experience in  application  design,  development  and\n      testing\n    \u00e2\u2013\u00aa Extensive experience on Spring Boot Microservice design and  hands  on\n      development.\n    \u00e2\u2013\u00aa Worked on NodeJS Microservice design and development.\n    \u00e2\u2013\u00aa Working as an Enterprise Architect in recent assignments\n    \u00e2\u2013\u00aa Worked as an Architect with BPM Architecture.\n    \u00e2\u2013\u00aa Worked on Swagger  API  Design  model  and  WS02  for  publishing  the\n      API\u00e2\u20ac\u2122s.\n    \u00e2\u2013\u00aa Worked as a Release Manager and Operations Manager during  my  Comcast\n      Project.\n    \u00e2\u2013\u00aa Strong Command on Weblogic 11.03, Websphere 8,  Tomcat,  Jboss  7  and\n      other Application Servers, Open Shift\n    \u00e2\u2013\u00aa  Extensive  experience   in   developing   applications   using   J2EE\n      technologies such as JPA, Servlets, EJB,  Spring  Boot,  Struts,  JMS,\n      JDBC, Hibernate.\n    \u00e2\u2013\u00aa Experience in REST and SOAP Web services.\n    \u00e2\u2013\u00aa Experience with  Mockito  unit  testing  framework  and  Cucumber  for\n      automation testing and Integration testing.\n    \u00e2\u2013\u00aa Experience in  MVC  (Model  View  Controller)  architecture  by  using\n      Struts Frameworks, Spring Framework, Spring Integration, Spring  Data,\n      Spring Boot, Service Oriented Architecture, Design Patterns and UML.\n    \u00e2\u2013\u00aa Extensive knowledge of XML, XPATH, XSLT, SOAP, WSDL,  SOA,  REST  API,\n      JASON, Jersey.\n    \u00e2\u2013\u00aa Expert in BPM 10.2 and BPM 5.7.\n    \u00e2\u2013\u00aa Experience in Pivotal Cloud Foundry  installation  of  micro  services\n      and testing them.\n    \u00e2\u2013\u00aa Experience in AWS EC2 Cloud set up for micro services.\n    \u00e2\u2013\u00aa Expert in Spring Framework, Spring Integration,  Spring  Data,  Spring\n      Boot.\n    \u00e2\u2013\u00aa Expert knowledge in Core Java and Multi Threading.\n    \u00e2\u2013\u00aa Experience in Unix Shell Scripting.\n    \u00e2\u2013\u00aa Knowledge in all phases of  Software  Development  Life  Cycle  (SDLC)\n      and AGILE methodology.\n    \u00e2\u2013\u00aa Experience in SVN, GIT Repository, CVS, ClearCase, MS Visual Source.\n    \u00e2\u2013\u00aa Experience in UML.\n    \u00e2\u2013\u00aa Team player with very good communication  skills  and  inter  personal\n      skills.\nEducation:\n    \u00e2\u2013\u00aa Masters of Science, Computer  and  Information  Sciences  Jan  03\u00e2\u20ac\u2122-Dec\n      04\u00e2\u20ac\u2122\n      North Illinois University, Dekalb, IL USA\n    \u00e2\u2013\u00aa Bachelors of Engineering, Electronics and Communication Engineering\n      Dr. Ambedkar Institute of Engineering, Bangalore, India\n\n\n\n\nTechnical Skills:\n\n|Operating     |:|Windows 95 / 98 / 2000 / NT/XP/ 7/8, Sun Solaris8.0,     |\n|System        | |Linux, Red Hat                                           |\n|Database      |:|Oracle 11, MS-Access, SQL, PL/SQL, DB2.                  |\n|Languages     |:|JAVA, C, C++, J2EE, EJB, Applets, Servlets, JDBC, AWT,   |\n|              | |UML, Swings, Java Beans, HTML, XML,                      |\n|Oracle        |:|BPM 10g, JBpm, Oracle DB 10g, Weblogic 11.03             |\n|Products      | |                                                         |\n|IDE/Version   |:| Eclipse, RAD, IntelliJ, MS Visio, MS VSS, CVS, SVN, GIT.|\n|Controller    | |                                                         |\n|Web           |:|Spring Boot, Spring Integration, Spring Data, Struts,    |\n|Applications  | |MVC, Design Patterns, EJB, JSP, Java Servlets, XML, SOAP,|\n|              | |REST API, XSLT, XPATH, JDBC, SOA, Hibernate.             |\n|Micro Services|:|Sprint Boot, Node Js                                     |\n|Web Servers   |:|BEA Weblogic, Java Web Server, Apache TomCat, Websphere  |\n|              | |V4, IIS.                                                 |\n|Cloud         |:|Pivotal Cloud Foundry PaaS(Platform as a Service),       |\n|Infrastructure| |Kubernetes, Mesos, Docker, AWS                           |\n|CI/CD         |:|Jenkins, Puppet                                          |\n\n\n\n\n\nProfessional Experience:\n\n\n\n      Client Royal Caribbean Cruise Line\n                                         Manager (Enterprise Architect), PA\n                                                          Jan 2019-Till Date\n\n\n      Leading the project Architecture and project delivery in the Advisory\n      and Architecture group for Royal Caribbean Cruise on Employee track. I\n      am leading the initiative to improve Employee efficiency and\n      experience and ease of doing business on cruise.\n\n\n      EMPLOYEE Track\n      Working on design of 3 major initiative\n     1. Scheduling and ILO compliance Architecture and Design.\n        I am working on design and complete Architecture which will be used\n        by delivery and executives to bring in and integrate with out of\n        box tool.\n        This will automate all the crew members Scheduling, Time Keeping\n        and Compliance rules engine on the ships and help to ease the\n        employee experience\n     2. Stateroom Attendant App Architecture and Design.\n        I am working on a design and Architecture of an app which will be\n        developed by RCCL team. This app will be used to assign to assign\n        guest rooms and areas of the attendants for cleaning. It provides\n        them with checklist to ensure that all areas are covered. And once\n        attendant completes the work these are send to floor supervisors\n        for inspection.\n        Supervisor has the ability update the status of inspection and may\n        be reject it and reassign with comments.\n     3. JDE HCM replacement with SAP HCM initiative and working on cut over\n        strategy of all the Ship and Shore products which are dependent on\n        HCM. And planning the gradual systems cut over with data synching\n        strategy, risk analysis and roll back strategy.\n\n\n             Innovation and Emerging Technology Track\n       1.  Started working with Innovation Team on initiative for Kids\n          tracking on ship during voyage. This is very important for kids\n          safety and kids will wear a WOW band and they can use the band\n          which has a Bluetooth beepers which can be tracked by Auruba/Cisco\n          listeners on ship.\n          The location is compiled and tracked on guest app.\n       2. Mustering 2.0 was an initiative to remove the guest pain point by\n          removing the safety mustering at one time and giving them\n          provisions to view videos on app or stateroom TV.\n\n\n      Environment: UML, draw.io, Spring Boot, REST Web Services, SOAP Web\n      Services, GSON/JSON, XSD, AWS, VMware, Kafka, TIBCO, Kubernetes, Mesos\n      Docker.\n\n\n\n    \u00e2\u017e\u00a2 Prudential (Cognizant Technology Solutions)\n                                                                        Sr.\n      Manager/Delivery Executive\n                  Nov 2017- Dec 2018\n      Working as Technical Delivery and Sr. Architect with hands on\n      experience for a project to convert Web Method Based Services to\n      Spring Boot services and deploying on Docker AWS.\n      My team has delivered a very fast pacing project where we have\n      converted 100 plus Documentum Services to Spring Boot Micro Services.\n      Introduced Swagger model to write swagger Jason file and convert it to\n      Java Code and Publishing Swagger Jason file on WS02 for testing and\n      viewing and discovering.\n      This whole project is converted to spring boot Micro Services with all\n      the security, common feature and adding Apigee API Gateway based\n      security with a very small team.\n      \u00e2\u20ac\u00a2 Involved in Design, Planning, and Co-ordination with offshore team\n        and helped in Coding of the application common features in Service\n        layer environment.\n      \u00e2\u20ac\u00a2 Involved in Design and coding involving core Spring Technologies.\n    \u00e2\u20ac\u00a2 Involved in REST Web services development and design\n      \u00e2\u20ac\u00a2 Trained the team with Mockito unit testing framework and Cucumber\n        for automation testing and integration testing and implement the\n        testing frame work with 85% of test coverage.\n      \u00e2\u20ac\u00a2 Experience in TDD environment and created unit test cases before\n        writing the business logic using Mockito. Used Sonar with Jenkins\n        for code coverage.\n      \u00e2\u20ac\u00a2 Used Spring Boot framework for development and spring annotations.\n      \u00e2\u20ac\u00a2 Created Custom Annotations and used Lombok Annotations for POJO\u00e2\u20ac\u2122s.\n      \u00e2\u20ac\u00a2 CI/CD builds with Jenkins\n      \u00e2\u20ac\u00a2 Worked with Dev Ops team for Dockerization of services and\n        deploying on Kubernetes.\n\n\n      Environment: UML, Core Java, J2EE, Spring Boot, REST Web Services,\n      SOAP Web Services, GSON/JSON, XSD, Tomcat, GIT, STS IDE, Swagger IO,\n      APIGEE, UNIX, CI/CD, Kubernetes\n\n\n\n\n    \u00e2\u017e\u00a2 TD Ameritrade (Cognizant Technology Solutions)\n                                                                        Sr.\n      Architect/Sr. Manager\n                        May 2017- Nov 2017\n\n\n      Working as API Designer and Development lead for a Rest API Pilot\n      project in BOS Service layer team in TD Ameritrade. Worked on a Pilot\n      project to initiate API design standards and Spring framework in the\n      project. Introduced Swagger model to write swagger Jason file and\n      convert it to Java Code and Publishing Swagger Jason file on WS02 for\n      testing and viewing and discovering.\n      This was a Pilot project to set up the Spring Framework for REST API\n      development and introduce the team with Agile Principals.\n      \u00e2\u20ac\u00a2 Involved in Design, Coding of the application in Service layer\n        environment.\n      \u00e2\u20ac\u00a2 Involved in Design and coding involving core Spring Technologies.\n    \u00e2\u20ac\u00a2 Involved in REST Web services development and design\n      \u00e2\u20ac\u00a2 Experience with Mockito unit testing framework and Cucumber for\n        automation testing and integration testing.\n      \u00e2\u20ac\u00a2 Experience in TDD environment and created unit test cases before\n        writing the business logic using Mockito. Used Sonar with Jenkins\n        for code coverage.\n      \u00e2\u20ac\u00a2 Created and updated stories worked on stories on Jira tracker.\n      \u00e2\u20ac\u00a2 Used Spring Boot framework for development and spring annotations.\n      \u00e2\u20ac\u00a2 Created Custom Annotations and used Lombok Annotations for POJO\u00e2\u20ac\u2122s.\n      \u00e2\u20ac\u00a2 Environment: UML, Core Java, J2EE, Spring Boot, REST Web Services,\n        SOAP Web Services, GSON/JSON, XSD, Tomcat, GIT, STS , Swagger IO,\n        WS02,UNIX\n\n\n\n    \u00e2\u017e\u00a2 FDC (Cognizant Technology Solutions)\n      Sr. Architect/Sr. Manager\n                           Aug 2016- April 2017\n\n\n      Solution design and Design work for integration with Fraud monitoring\n      Solution with a third Party Machine Learning and Monitoring Response\n      System Feedzai. Design micro services which are currently in\n      Production.\n      Also integration with another Smart Phone device Fraud alert system\n      InAuth Service.\n      The client which this products are used are mainly from Exxon Mobile,\n      Chickfilae, Tacobell.\n\n\n      This new integration is designed completely as an independent micro\n      services which are deployed separately and have their own purpose.\n      These services are also currently on PCF (Pivotal Cloud Foundry) PaaS\n      as a test services and intention was to move completely to PCF in\n      future.\n   Responsibilities: -\n      \u00e2\u20ac\u00a2 Involved in Design, Coding of the application in Service layer\n        environment.\n      \u00e2\u20ac\u00a2 Involved in Design and coding involving core Spring Technologies.\n    \u00e2\u20ac\u00a2 Involved in REST Web services development and design\n      \u00e2\u20ac\u00a2 Experience with Mockito unit testing framework and Cucumber for\n        automation testing and integration testing.\n      \u00e2\u20ac\u00a2 Experience in TDD environment and created unit test cases before\n        writing the business logic using Mockito. Used Sonar with Jenkins\n        for code coverage.\n      \u00e2\u20ac\u00a2 Developing micro API\u00e2\u20ac\u2122s and REST web services for the businesses and\n        vendors to use.\n      \u00e2\u20ac\u00a2 Created and updated stories worked on stories on Jira tracker.\n      \u00e2\u20ac\u00a2 Used Spring Boot framework for development and spring annotations.\n      \u00e2\u20ac\u00a2 Created Custom Annotations and used Lombok Annotations for POJO\u00e2\u20ac\u2122s.\n      \u00e2\u20ac\u00a2 Worked on POC for Pivotal Cloud Foundry PaaS as for deployments of\n        applications on Cloud Infrastructure.\n      \u00e2\u20ac\u00a2 Worked on POC for API Gateway Apigee on PCF.\n      \u00e2\u20ac\u00a2 Used JPA (Java Persistence API) Spring Data for DB design and\n        development.\n\n\n   Environment: UML, Core Java, J2EE, Spring Boot, JPA, REST Web Services,\n   SOAP Web Services, GSON/JSON, XML, XSL, XSD, Web Sphere, GIT, STS,\n   Mocito, Apigee , UNIX\n\n\n\n    \u00e2\u017e\u00a2 United Health Care, Horsham, PA\n\n\n         Sr. Architect/Software Engineer\n                       Mar 2015- June 2016\n\n\n      Digital Service Medicare and Retirement\n\n\n            Solution Summary document for a new project was developed.\n      Provider Web is a web site residing within the SHIP Comprehensive\n      Policy Administration System (COMPAS) with self-service features for\n      providers.  When it was built 11 years ago, no enterprise-wide\n      provider portal existed, so it was built into the Policy\n      Administration system.  Today, there are better options, specifically,\n      leveraging the UHC-wide Provider Portal, also known as Optum Provider\n      Touch Point (PTP). Our recommendation is to align to the enterprise\n      standard by retiring the Policy Administration system Provider Web in\n      favor of leveraging Optum PTP.\n           In addition, the Policy Administration system has an outstanding\n      policy exception (PEX) with UHC Corporate IT for accessing the United\n      Claim Processing System (UCPS) system directly in order to maintain\n      information in the UCPS Provider Database. To resolve the outstanding\n      PEX issue, the Policy Administration system will integrate with ACES\n      provider maintenance services in order to pass information between\n      UCPS and the Policy Administration system.\n\n\n           Hands on a development of micro API Services and Restful Web\n      Service, which provides services and to business and other teams for\n      looking up claims, Provider Demographics, plans, eligibilities,\n      preferences, fulfillments. Used Spring Boot framework with extensive\n      unit testing using mockito framework, automation testing using\n      Cucumber and integration testing.\n\n\n\n\n\n\n   Responsibilities: -\n      \u00e2\u20ac\u00a2 Involved in Design, Coding of the application in SOA environment.\n      \u00e2\u20ac\u00a2 Involved in Design and coding involving core Java technologies.\n    \u00e2\u20ac\u00a2 Worked on REST Web services.\n      \u00e2\u20ac\u00a2 Worked on writing unit test cases with Mockito framework and\n        Cucumber for automation testing and integration testing.\n      \u00e2\u20ac\u00a2 Developing micro API\u00e2\u20ac\u2122s and REST web services for the businesses and\n        vendors to use.\n      \u00e2\u20ac\u00a2 Created and updated stories worked on stories on Pivotal tracker.\n      \u00e2\u20ac\u00a2 Used Spring Boot framework for development and spring annotations.\n      \u00e2\u20ac\u00a2 Created Custom Annotations and used Lombok Annotations for POJO\u00e2\u20ac\u2122s.\n      \u00e2\u20ac\u00a2 Worked on POC for Openshift automatic deployments of applications\n        on Cloud Infrastructure.\n      \u00e2\u20ac\u00a2 Used JPA (Java Persistence API) for DB queries\n\n\n   Environment: UML, Core Java, J2EE, Spring Boot, JPA, REST Web Services,\n   SOAP Web Services, GSON/JSON, XML, XSL, XSD, Web Sphere, GIT, IntelliJ ,\n   UNIX\n\n\n\n    Bank Of America, Newark\n    Sr. Architect/ Technical Lead\n                     Jan 2013-Feb 2015\n\n\n        I am currently working as a Sr. Architect and Technical Lead in Bank\n   of America in Card Technologies and Common Services team. I work closely\n   with Business team and other teams on quarterly releases. I have worked\n   on Fraud Activity Verification project where Business is moving the Fraud\n   Verification service in house. CTCS team is mainly an Orchestration Layer\n   between different teams and we provide Wed Services, which are consumed\n   by different teams.\n   I have also worked on Batch processes project, which is used mainly for\n   our Mailers and Agreement printing services for customers.\n\n\n   Responsibilities: -\n      \u00e2\u20ac\u00a2 Involved in Design, Coding of the application in SOA environment.\n      \u00e2\u20ac\u00a2 Involved in Business requirements meeting to collect BRD\n        requirements.\n      \u00e2\u20ac\u00a2 Involved in Integration testing with all the teams in SOA\n        environment.\n      \u00e2\u20ac\u00a2 Involved in Design and coding involving core Java technologies.\n      \u00e2\u20ac\u00a2 Worked on JBPM 6 projects for Card fraud project.\n\n\n   Environment: UML ,Core Java, J2EE, Web Services, XML, XSL, XSD, Web\n   Sphere, RAD 7.5, JBpm 6, RTC, UNIX.\n\n\n\nComcast, Mount Laurel, NJ\n Sr. Architect/ Technical Lead\n                               Jan 2007-Dec 2012\n\n\n  I have started working in Comcast from Jan 2007 and from very first week\nthe work was exciting. I have worked on lot of Major project development\nand releases for Commercial Work Bench Portal. Sales team to create and\nProvision Business customers with Comcast Line of Businesses products use\nthis product. I worked on all the major Product release in Commercial\nWorkbench both in designing and development phase. We have also done lot of\nupgrades to our projects from Weblogic 8.1 to Weblogic 10.01. Also last\nyear we have done a major upgrade from Aqua-logic BPM 5.7.3 to Oracle BPM\n10.\nWorked as a Release Manager, Enterprise Architect and Technical Lead for\nall development release.\nSome of my responsibilities in the application:-\n    \u00e2\u2013\u00aa Integration with LDAP server for security authentication for  SSO  and\n      also for SalesForce integration. Also for one of our  external  client\n      we have used Public Key and Private Key authentication for getting the\n      Login Tokens.\n    \u00e2\u2013\u00aa Involved  in  Requirements  gathering,  Requirement  analysis,  Design\n      analysis, Integration and deployment for enhancements and changes.\n    \u00e2\u2013\u00aa Involved High Level design documents and Detail design  documents  for\n      our projects in every monthly release.\n    \u00e2\u2013\u00aa  Performance  evaluation  with  the  performance  team  before   every\n      release.\n    \u00e2\u2013\u00aa Involved in  coding  and  Integration  testing  with  clients  of  the\n      application in SOA environment. We have different vendor  clients  who\n      are consuming out  services  like  Salesforce,  IBM  CPQ  (Evolution),\n      Century 2.1.\n    \u00e2\u2013\u00aa Application uses Spring frame work and Struts framework  for  existing\n      services and we have Oracle BPM 10g as a middleware.\n    \u00e2\u2013\u00aa Integration our application with external  Business  Class  Voice  and\n      Business Class Trunking provisioning system.\n    \u00e2\u2013\u00aa Created UI page for Trunking product using GWT\n    \u00e2\u2013\u00aa Integration with Enterprise services  like  billing  systems,  account\n      services, offer management services, order management services.\n    \u00e2\u2013\u00aa Used multi threading and core Java for parallel processing.\n\n\n\n   Environment: UML, Java, J2EE, BPM 10.3, BPM 5.7, Spring, Struts, BEA\n   Weblogic 10.03, BEA Weblogic 10.1 Portal, Oracle 10G2, JavaScript, GWT,\n   DWR, AJAX, Web Services , XML, XSL, XSD, JavaScript, Hibernate,  Apache\n   Tomcat,  UNIX.\n\n\n\n \u00e2\u017e\u00a2 Pfizer Inc. New London, CT\n      Senior Java Developer/Architect\n                       April 2006- Dec 2006\n   1.   Data Standard Catalog         Team Lead\n   Working on a Data Standards Catalog project and this application is used\n   all over the world Pfizer end users and study teams to post issues on\n   Core data standards and efficacy standards. This application is\n   interfaced with an extra view product for reporting the issues and\n   tracking issues. This application is deployed in Weblogic 8.1.\n\n\n       \u00e2\u2013\u00aa  Directly involved with business team to maintain the whole\n         application. First point of contact for application support and\n         upgrades.\n       \u00e2\u2013\u00aa  Involved in coding, interfacing and testing with the extra view\n         team.\n       \u00e2\u2013\u00aa  Written stored procedures and packages for the versioning.\n       \u00e2\u2013\u00aa  Documenting and maintaining the requirements and the\n         application documents.\n       \u00e2\u2013\u00aa Maintaining the production and staging application server boxes\n         and directly co-ordination with the middle tier team for new\n         releases.\n\n\n   2. Dictionary extract project\n      Developed a java application using Live Link API, which is used to\n   upload files from the clinical trials servers every day to the Pfizer\n   Live Link servers, which are used by the business and end user for\n   review.\n      \u00e2\u2013\u00aa Involved in coding, interfacing and testing of the project.\n      \u00e2\u2013\u00aa Written stored procedures and packages.\n      \u00e2\u2013\u00aa Documenting and maintaining the requirements and the application\n        documents.\n      \u00e2\u2013\u00aa Implemented Service oriented Architecture for web service\n        communications with the different Pfizer clients.\n      \u00e2\u2013\u00aa Maintaining the production and staging application server boxes\n        and directly co-ordination with the middle tier team for new\n        releases.\n\n\n   3. WHO- Drug Dictionary and TMS servers front end web interface\n      Developed web front end using JSP and Struts frame work for a front\n      end interface were a business person can come and login using Pfizer\n      NT account username and password and upload the spreadsheets or\n      download the spread sheet for the dictionary terms.\n\n\n       \u00e2\u2013\u00aa Involved in designing, coding, interfacing and testing of the\n         project.\n       \u00e2\u2013\u00aa  Documenting and maintaining the requirements and the\n         application documents.\n       \u00e2\u2013\u00aa   Used Adobe photo shop for look and feel images.\n       \u00e2\u2013\u00aa Maintaining the production and staging application server boxes\n         and directly co-ordination with the middle tier team for new\n         releases.\n       \u00e2\u2013\u00aa Used Hibernate for mapping.\n   Environment: Core Java,Rational Rose, UML, WingNut, J2EE, Struts,\n   JavaScript, XML, XSL, XSLT, VB scripting, JavaScript, Hibernate,  Apache\n   Tomcat,  Adobe Photoshop, UNIX, CVS for version controlling.\n\n\n\n\n   FUNDTech, Norcross, GA\n   Senior Java Developer\n                              Nov 2005- April 2006\n   Working on a Banking project and our main job is Maintenance and\n   production support. So we are dealing with small failures and code\n   Enhancements. The project is both web based and swing based so I am\n   dealing with both the sides issue on client side.\n\n\n    \u00e2\u2013\u00aa  Involved in  Requirements  gathering,  Requirement  analysis,  Design\n      analysis, Integration and deployment for enhancements and changes.\n    \u00e2\u2013\u00aa Involved in  coding  and  Integration  testing  with  clients  of  the\n      application\n    \u00e2\u2013\u00aa  The  application  uses  the  Swing  and  Servlets.  The   views   are\n      programmed using XML and XSLT pages which are converted into HTML. And\n      use AJAX for changing the Html page without reloading the page.\n    \u00e2\u2013\u00aa  Used XSLT for transforming the XML content into HTML\n    \u00e2\u2013\u00aa Used Starteam for version control.\n    \u00e2\u2013\u00aa Used JavaScript as the scripting language for the View.\n\n\n      Environment: Core Java, J2EE, Struts, JMS, JavaScript, JBuilder,  XML,\n      XSL,  XSLT,   JavaScript,   Apache  Tomcat,  UNIX,  SOA,   Star   Team\n      Enterprise.\n\n\n   Cingular Wireless, Alpharetta, GA\n                                               Senior Analysts(Java\n   Developer)\n      Dec 2004- Nov 2005\n\n\n   1. EDD (Enterprise Data Documentation), EDD Resend Web  Application,  EDD\n      Reporting Tool. In Production from August 2005\n      Designed and developed along with my team members  an  automated  tool\n      which can receive customer payment information from clients queue in a\n      bean format and convert it in XML string and put it in the local queue\n      and store it LDAP database.  After  that  sending  a  notification  of\n      payment to the customer via SMS, Email or Post Card according  to  the\n      preference.\n      Designed and  developed  a  web  application  which  is  used  by  CSR\n      (Customer Service Representatives) to resend the payments  information\n      a customer for past two years. This web  application  takes  customers\n      information and displays the records of all the previous payments  and\n      then the customer can choose one or more  than  one  record.  And  the\n      message is again sent via SMS, Email or Post Card which ever  customer\n      prefers.\n      Developed  and  developed  web  application  which  is  used  by   the\n      production support team and the  administration  to  view  the  totals\n      records of each client and the resend records. It will also  tell  how\n      many records have been successfully sent and how many failed.\n\n\n   2. Corporate Email System\n      Designed and developed a web application which is going to be used  by\n      Cingular agents or Marketing people to send emails to  individuals  or\n      group of people or to a Category of people. The agents  can  create  a\n      new template or use the  existing  template  to  send  emails  to  the\n      clients.\n\n\n   Responsibilities:\n\n\n    \u00e2\u2013\u00aa Involved  in  Requirements  gathering,  Requirement  analysis,  Design\n      analysis, Integration and deployment\n    \u00e2\u2013\u00aa Involved in  coding  and  Integration  testing  with  clients  of  the\n      application\n    \u00e2\u2013\u00aa  Responsible  for  the  design  and  development  of  the  application\n      framework\n    \u00e2\u2013\u00aa Designed and Developed UI\u00e2\u20ac\u2122s using MVC architecture\n    \u00e2\u2013\u00aa The application uses the STRUTS framework. The  views  are  programmed\n      using JSP pages with the struts tag library. Model is a combination of\n      EJB, DAO and Java classes.\n    \u00e2\u2013\u00aa Used XSLT for transforming the XML content  into  HTML  and  used  SOA\n      for web service security.\n    \u00e2\u2013\u00aa Used JavaScript for much functionality of the web  pages.  Used  JUnit\n      for unit testing of the system and Log4J for logging\n    \u00e2\u2013\u00aa Used EJB and DAO a middleware in designing  and  developing  a  three-\n      tier distributed application\n    \u00e2\u2013\u00aa Used web services with  session  bean  for  Synchronous  communication\n      with the client and even used a vendor tool for web service  security.\n\n\n    \u00e2\u2013\u00aa Used Hiberate as a mapping tool\n    \u00e2\u2013\u00aa All the queries are written and tested on TOAD for this application.\n    \u00e2\u2013\u00aa The Java Message Service  (JMS)  API  is  used  to  allow  application\n      components to create, send, receive, and read messages\n    \u00e2\u2013\u00aa Installed Web Logic for the application is UNIX environment.\n    \u00e2\u2013\u00aa Used ClearCase for version control.\n    \u00e2\u2013\u00aa Used AJAX to modify the page asynchronously.\n    \u00e2\u2013\u00aa Used JavaScript as the scripting language for the View.\n\n\n   Environment: Core Java, J2EE, Struts, JSP, EJB, Servlets, JMS, JDBC, Java\n   (JDK 1.4), JavaScript, JBuilder, Rational Rose, Hibernate DAO, XML,  XSL,\n   XSLT, AJAX, PL/SQL, HTML, JavaScript, WebLogic 8.1 SP4, Web  Sphere  4.1,\n   JBOSS, Apache Tomcat, Oracle 9i/8i, UNIX (HP)\n\n\n \u00e2\u017e\u00a2 Sonata Software Ltd., Bangalore, India     Software Developer\n   Nov\u00e2\u20ac\u212201 \u00e2\u20ac\u201c Jan\u00e2\u20ac\u212202\n\n      For the design and simulation  of  the  Electronic  Control  Units  at\n   Continental Teves during their R&D phase, Sonata proposed and developed a\n   web-based system with  a  central  Oracle  database.  Web  interface  and\n   interface  to  other  existing  systems  were  provided.   Offline   data\n   manipulation for field engineers and  overseas  database  synchronization\n   was also implemented. MVC architecture was used with JSP,  Servlets,  and\n   JavaBeans to build the interface for the database.\n   Responsibility:\n    \u00e2\u2013\u00aa Designed and developed session and entity beans\n    \u00e2\u2013\u00aa Developed Java Servlets used for the server end\n    \u00e2\u2013\u00aa Installed and maintained Oracle Server\n    \u00e2\u2013\u00aa  Involved  in  unit  testing,  module  testing,  product  testing  and\n      development of test cases.\n    \u00e2\u2013\u00aa Involved in the web interface development using HTML, XML,  Java  AWT,\n      Swing\n   Environment: Java, JSP, C++, C#, Servlets, JDBC, JavaBeans, Oracle, HTML,\n   XML, Jawa AWT, Swing Windows NT\n\nReferences:  Available  or  please  check  out  my  LinkedIn   profile   for\nreferences.\n\n\n\n\n",
          "confidence": 1.0,
          "method": "full_text",
          "structured_data": null
        },
        "tax_term": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "source_by": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "designation": {
          "value": "product manager",
          "confidence": 0.9,
          "method": "ner",
          "structured_data": null
        },
        "experience": {
          "value": "16",
          "confidence": 0.9,
          "method": "regex_total_experience_summary",
          "structured_data": null
        }
      }
    },
    {
      "resume_path": "data\\input\\Finance Manager - CA - Stepfan.doc",
      "confidence_score": 0.655,
      "used_ocr": false,
      "extracted_fields": {
        "skills": {},
        "first_name": "Stepfan",
        "last_name": "Jiles",
        "primary_email": {
          "value": "Stepfanjiles@hotmail.com",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "secondary_email": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "phone": {
          "value": "9016908956",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "city": {
          "value": "San Ramon",
          "confidence": 0.7,
          "method": "ner",
          "structured_data": null
        },
        "state": {
          "value": "CA",
          "confidence": 0.9,
          "method": "zip_database",
          "structured_data": null
        },
        "zip": {
          "value": "94583",
          "confidence": 0.7,
          "method": "regex",
          "structured_data": null
        },
        "work_authority": {
          "value": "Meetings",
          "confidence": 0.8,
          "method": "regex",
          "structured_data": null
        },
        "resume_link": {
          "value": "data\\input\\Finance Manager - CA - Stepfan.doc",
          "confidence": 1.0,
          "method": "file_path",
          "structured_data": null
        },
        "raw_resume": {
          "value": "\n                                Stepfan Jiles\n                         2218 Canyon Village Circle\n                             San Ramon, CA 94583\n                          Stepfanjiles@hotmail.com\n                                (901)690-8956\n\nOBJECTIVE\nTo obtain a position with a company that seeks an assertive, energetic\nfinance professional resource. The ideal position will allow me to utilize\nmy leadership and strong financial planning & analysis skills to have a\npositive impact on profitability and productivity.\n\u00c2\u00a0\u00c2\u00a0\nEDUCATION\nChamplain College\n       Burlington, VT\nCost Accountant Certification (CCA)\n                      July, 2018\nFlorida Institute of Technology\n\n   Melbourne, FL\nExecutive MBA Finance & Accounting\n\n   June, 2011\nBryan College\u00c2\u00a0\u00c2\u00a0\u00c2\u00a0\u00c2\u00a0\n\n                      Dayton, TN\nBachelors of Science in Business Administration\u00c2\u00a0\u00c2\u00a0\u00c2\u00a0\u00c2\u00a0\n\nMay, 1998\nMajor: Management\nWentworth Military Academy \u00c2\u00a0\u00c2\u00a0\u00c2\u00a0\u00c2\u00a0  \u00c2\u00a0\u00c2\u00a0\n\nLexington, MO\nLiberal Arts Associate Degree\u00c2\u00a0\u00c2\u00a0\u00c2\u00a0     \u00c2\u00a0 \u00c2\u00a0\u00c2\u00a0\n\n              May, 1993\nMajor: Accounting\n\u00c2\u00a0\u00c2\u00a0\nEXPERIENCE\nPacific, Gas & Electric (PG&E) Utilities\nInsight Global Inc.- Finance Consultant\n                                                           January 2020 -\nPresent\nSr. Project Control\u00e2\u20ac\u2122s Analyst\n                                           San Ramon, Ca\nSchedule Development and Analysis:\n \u00e2\u20ac\u00a2 In coordination with Project Manager develop project schedule and cash\n   flow/forecast plans with functional department input.\n \u00e2\u20ac\u00a2 Attend Project and Job kickoff and walk down meetings.\n \u00e2\u20ac\u00a2 Create, maintain and update schedules.\n \u00e2\u20ac\u00a2 Document, monitor and communicate project milestones and risks with\n   appropriate stakeholders.\n \u00e2\u20ac\u00a2 Participate in project status meetings, collect progress data and revise\n   project plan as needed.\n \u00e2\u20ac\u00a2 Monitor financial progress and maintain Project Manager\u00e2\u20ac\u2122s order group.\n   Develop, maintain and control project schedule plans using appropriate\n   software (for ex: SAP Project System, SAP Work Management System, and MS\n   Project).\nCost Plan Development and Analysis:\n \u00e2\u20ac\u00a2 Use the cost and schedule plan to determine if project objectives are\n   achievable.  Create, maintain, and update monthly forecast budgets.\n   Prepare various cost reports and maintain forecast accuracy. Prepare\n   project performance analysis, cost, and schedule status reports. Identify\n   cost and schedule variances from objectives and recommend corrective\n   action.\n \u00e2\u20ac\u00a2 Assess and report on project performance using established industry\n   standards.\n \u00e2\u20ac\u00a2 Ensure that responses to project budget, and accruals\u00e2\u20ac\u2122 cost requests\n   reflect accurate and current project cost information and stakeholders\n   are in concurrence.\n \u00e2\u20ac\u00a2 Provide regular communication on project cost, schedule and risk status\n   to project team members, stakeholders and public.\n \u00e2\u20ac\u00a2 Interface with design and planning resources and software systems.\n \u00e2\u20ac\u00a2 Review and assist in the preparation of Advance Authorizations, Job\n   Estimates and Re-Authorizations and assist PM in routing for approval.\n \u00e2\u20ac\u00a2 Prepare journal entries and coordinate or process goods receipts in\n   current software system.\n \u00e2\u20ac\u00a2 Ensure that responses to budget and cost requests reflect correct project\n   cost information and stakeholders are in concurrence.\n \u00e2\u20ac\u00a2 Input data into various programs and prepare various cost and forecasting\n   reports.\nProject Documentation & Reporting:\n \u00e2\u20ac\u00a2 Verify that project cost and schedule milestones were/were not attained\n   and provide input to identify future process or business improvements and\n   work with responsible parties to implement.\n \u00e2\u20ac\u00a2 Manage orders from inception through completion and all required\n   documentation is entered in current software system (Ex: EDRS).\n \u00e2\u20ac\u00a2 Resolve all open items, ensure compliance requirements are met and\n   settlement rules are entered, and close out order.\n \u00e2\u20ac\u00a2 Maintain scope change, contingency release, change order, and journal\n   entry logs.\n \u00e2\u20ac\u00a2 Maintain written and electronic project documentation and records for\n   required aspects of the project: Maintain project files in accordance\n   with established guidelines and requirements (Ex: utilizing the\n   electronic document management system or EDMS).\n \u00e2\u20ac\u00a2 Document change order requests, project status, key issues, risks and\n   resolution, priority changes and approvals.\n \u00e2\u20ac\u00a2 Provide Project Manager with monthly report of project costs and/or\n   schedule information including variance analysis according to an agreed\n   upon level of detail and prioritization.\n \u00e2\u20ac\u00a2 Provide Project Team members with current status report containing\n   schedule and cost information. Comply with Utility Operations Policies,\n   Standards and Guidelines.\n \u00e2\u20ac\u00a2 Assist PM with Post Job Critique.\nCentral Data Management Functions - Quality and Analysis.\n \u00e2\u20ac\u00a2 Prepare various monthly and weekly project management reports and report\n   performance reports to management group including scorecards.\n \u00e2\u20ac\u00a2 Ensure quality of reports for the department, to report out to\n   stakeholders.\n \u00e2\u20ac\u00a2 Validate weekly and monthly data sets.\n \u00e2\u20ac\u00a2 May provide support to project manager(s) as necessary.\n \u00e2\u20ac\u00a2 Fulfill ad-hoc requests for cost-related data analysis.\n \u00e2\u20ac\u00a2 Participate on (or lead) various process improvement initiatives within\n   Project Management.\nCentral Financial Analysis Functions - Project Budget & Cycle Forecast\nMonitoring/Reporting\n \u00e2\u20ac\u00a2 Prepare various monthly and weekly project performance reports - budget\n   and cycle forecast reports; and finance status reports, utilizing the\n   financial tracking software (e.g. SAP/BW system and EPM).\n \u00e2\u20ac\u00a2 Coordinate and work with all stakeholders, on creating new reports, that\n   best measure and support good business decisions, to meet the Year End\n   Annual Budget target.\n \u00e2\u20ac\u00a2 Coordinate and summarize project cycle forecast variance explanations for\n   the department, to report out to stakeholders.\n \u00e2\u20ac\u00a2 Ensure that all responses to budget, quarterly cycle requests reflect\n   accurate and current project cost information and stakeholders are in\n   concurrence.\n \u00e2\u20ac\u00a2 Post and update all the project performance; budget and cycle forecast;\n   and status reports on the Project Management website.\nFinancial and Performance Management\n \u00e2\u20ac\u00a2 Monitor, analyze and report out on the Department\u00e2\u20ac\u2122s Forecasting\n   Performance, with weekly performance reports, utilizing the financial\n   tracking tools (e.g. SAP/BW).\n \u00e2\u20ac\u00a2 Analyze financial results on a monthly basis, providing explanations of\n   significant cost drivers to PM Leadership \u00e2\u20ac\u201c reports such as the Green-Red\n   Scorecard and Lessons Learned Reports.\n\nCordoba Corp.- Finance Consultant\nBART/ Santa Clara Valley Transit Authority Phase II Extension Project\n        December 2017 \u00e2\u20ac\u201c January 2020\nProject Finance Consultant\n                          San Jose, Ca\n \u00e2\u20ac\u00a2 Lead the tracking of the capital needs of the BART/ Santa Clara Valley\n   Transit Authority Phase II Extension Project, including project\n   commitments, project spend, project budgets, and overhead.\n \u00e2\u20ac\u00a2 Maintain and update annual GAAP and cash operating forecast models.\n \u00e2\u20ac\u00a2 Support the strategic planning process for the Project Controls\u00e2\u20ac\u2122\n   division.\n \u00e2\u20ac\u00a2 Support the Budget process and monitor the capital needs of the BART/\n   Santa Clara Valley Transit Authority Phase II Extension Project.\n \u00e2\u20ac\u00a2 Lead and track KPI's and financial metrics to inform senior leader\n   decision making, measure divisional performance, and drive\n   accountability.\n \u00e2\u20ac\u00a2 Lead Project FP&A to roll-up and map task and sub-task division models to\n   corporate consolidated models.\n \u00e2\u20ac\u00a2 Prepare monthly project scope reviews and track actuals to budget;\n   present and communicate to senior management.\n \u00e2\u20ac\u00a2 Perform various other financial analyses and industry related research,\n   tracking and compiling industry trends and competitive analyses, to\n   inform senior management strategic decision-making; effectively\n   communicate findings to team/colleagues through presentations, memos, and\n   other deliverables.\n \u00e2\u20ac\u00a2 Manage and assist in the preparation of annual project business reviews\n   and variance analyses.\n \u00e2\u20ac\u00a2 Manage sub contract level ultimate\u00e2\u20ac\u2122s, tracking against actual activity\n   from third party reporting.\n \u00e2\u20ac\u00a2 Track invoice rights and availabilities for project, supporting\n   comprehensive agreement strategy.\n \u00e2\u20ac\u00a2 Support Program Management Team (PMT) Strategy and Analysis in the setup\n   of the processes and procedures infrastructure for ongoing deal analysis\n   for new third party venders and ongoing contracts.\n \u00e2\u20ac\u00a2 Perform finance/accounting, cost economics, customer and market research\n   analysis. Analyzing, interpreting and presenting data related to\n   transportations and infrastructure market operations.\n\n\nCalifornia High Speed Rail Project\n        April, 2017 \u00e2\u20ac\u201c November 2017\nSr. Financial Analyst\n              Sacramento, Ca\n\u00e2\u20ac\u00a2     Accounting duties - Performs complex accounting functions for High\nSpeed Rail Finance and Budget\n       department, internal office departments, and program. Analyzes and\nassigns transaction codes in accordance\n       with state reporting and local management requirements. Reviews and\nreconciles account and fund balances;\n        reviews documents and claims for accuracy, completeness, and\nuniformity to rules, regulations, and laws;\n       coordinates, prepares and reviews year-end closing process and\ndocuments. Performs periodic review of\n       financial reports and verifies accuracy and project solvency.\n\u00e2\u20ac\u00a2     Budget Monitoring - Support the development of budgets for internal\nand external customer; support the\n       creation of new budget coding; provides information and assists\nstaff and administrators in budget preparation,\n       implementation, and control; prepares, balances, compiles, and\nenters budget data. Support the preparation of\n       quarterly revisions and billings; in accordance with Federal\nRailroad Administration (FRA) guidelines, monitors\n       and evaluates federal and state fund budgets and cash flow for\nappropriateness to ensure solvency and\n       accountability for compliance with state law.\n\u00e2\u20ac\u00a2     Financial Reporting - Prepares complex financial reports as mandated\nby the administration, California High\n       Speed Rail Authority, and other agencies; assists internal and\nexternal business customers in compiling data and\n       interpreting legal reporting requirements and regulations. Monitors\ntimelines to meet strict deadlines in\n       reporting regulations; reviews grant letters, funding and\nentitlement reports; reviews and verifies statistical and\n       financial information including independent reports and long-term\ndebt documents.\n\u00e2\u20ac\u00a2     Research and Data Analysis - Researches and analyzes financial data\nfor internal and external clients; creates,\n       organizes, and maintains files using database and spreadsheet\nprograms; retrieves and organizes data into\n       required reporting formats; collects, retrieves and organizes data\nto identify financial discrepancies and resolve\n       client inquiries; recommends solutions to clients and staff ensuring\nthat corrections or changes are implemented\n       properly.\n\u00e2\u20ac\u00a2     Communications and Technical Support - Maintains communication with\ninternal and external clients regarding\n       financial matters; provides information and technical support in the\ndevelopment and revision of policies and\n       regulations; assists in the development and documentation of office\nprocess updates and revisions to procedures;\n      reviews proposed contracts for adherence to rules and regulations.\nSupport procedures for monitoring grants.\n\nCBRE                                                         August, 2015 \u00e2\u20ac\u201c\nJan., 2017\nSr. Project Analyst/Sr. Financial Analyst\n San Francisco, Ca\n\u00e2\u20ac\u00a2      Manages a broad range of complex financial analysis and/or financial\nreporting activities to measure\n        profitability for a region, line of business or large, complex\nclient. Includes the management of such functions as\n        budgeting forecasting, financial reporting, strategic planning and\nmanagement reporting processes and work\n        product.\n\u00e2\u20ac\u00a2      Manages one or all of the following: capital budgeting process,\ncapital forecasting process, capital strategic\n        planning process, and standard management reporting. Establishes\noverall departmental priorities and ensures\n        that all deadlines are met.\n\u00e2\u20ac\u00a2      Supervises finance staff including training. Reviews and approves\ncertain HFM monthly journal entries.\n        Conducts other special financial and business studies and other\nduties as assigned by management.\n\u00e2\u20ac\u00a2     Produces a variety of routine and ad hoc financial reports, packages\nand pro forma analyses for senior\n       management and planning unit. Participates in the research,\ndevelopment and preparation of accounting policy\n       and procedures, as required. May act as consultant to management on\nfinancial policies, procedures, and\n       applications. Manages special projects and prepares presentations\nfor senior management. Performs other duties\n       as assigned.\n\u00e2\u20ac\u00a2    Ability to comprehend, analyze, and interpret complex financial\ninformation and transactions and accounting\n      principles. Ability to independently research complex accounting\ntransactions/issues. Ability to problem-solve,\n      both independently and working as a team. Perform advanced analytical\nand quantitative skills. Draws upon the\n      analysis of others and makes recommendations that have a direct\nimpact on the company.\n\nSiemens, Inc.\nNovember, 2013 \u00e2\u20ac\u201c May, 2015\nCommercial Program Manager\nSacramento, Ca\n\u00e2\u20ac\u00a2     Directly lead the financial planning & analysis of the U.S.\noperations lite rail and locomotive bogie\n       manufacturing team, supporting my technical bogies\u00e2\u20ac\u2122 program manager\nin relation to all commercial and legal\n       issues. Performing sales and expense forecasting and budgeting for\nall Bogie projects.\n\u00e2\u20ac\u00a2     Perform weekly & monthly program reporting/project status\ndiscussions/milestone reviews. Collaboration on the\n       project completion report and summarizing the lessons learned with\nfeedback to the organization executive\n       management staff. Collaborating with internal & external customers\nin negotiations and also formulating and\n       implementing contracts in the program.\n\u00e2\u20ac\u00a2     Analyzing and assessing complex, possibly international suppliers,\ncontract agreements\u00e2\u20ac\u2122. Managing claim and\n       change order management: Asserting own claims and warding off\nunjustified engineering claims.\n\u00e2\u20ac\u00a2     Maintain contractual changes with regard to the scope of delivery and\nservices, prices, deadlines or other\n       contractual agreements, as well as forecast opportunity and risk\nmanagement: Identifying and financially\n       assessing opportunities and risks, defining and implementing\nsuitable measures for reducing risks or realizing\n       opportunities and taking precautions for remaining risks.\n\u00e2\u20ac\u00a2     Drawing up the order receipts, concurrent and final costing\ncalculations for quotes and bids for one off sales\n       request. Performing asset management, correcting assignment and\nmonitoring of cost. Drawing up invoices and\n       tracking claims. Maintaining internal project controlling\n(deadlines, costs and quality).\nFinancial Analyst III\n\u00e2\u20ac\u00a2     Performs complex financial and operational analysis to support short\nterm and long range strategic plans and\n       operating budgets. Act as a key partner to the Engineering, UT\nAssembly Production & Scheduling cost centers\n       for financial planning and controlling activities. I am the\nHyperion/Essbase/Smartview lead for this company.\n\u00e2\u20ac\u00a2     Conducts work order variance analyses on actual expenditures to\nbudget estimates. Evaluates data, prepares\n       forecasts, and analyzes trends in operations, manufacturing, general\nbusiness conditions and other areas.\n\u00e2\u20ac\u00a2     Verifies labor cost and workload planning by comparing plan vs.\nactual costs. Identifies voucher-\n       related or other transactions mistakenly applied to the projects\nthat need to be removed.  Coordinates the\n       correction through the EZ Suite feeder system or as a journal entry.\n\u00e2\u20ac\u00a2    Works closely with executive management to assist in hourly rate\ninternal calculations. Provide timely and\n       accurate invoices and billing information to selected outside\ncustomers. Monitor projects for Project Status EAC\n       vs. actual related issues. Ensures appropriate action is taken to\nupdate statuses and prepares required accounting\n       entries. I am the site Hyperion, Essbase, Smartview lead support for\nthe OBA department.\n\u00e2\u20ac\u00a2    Responds to various ad hoc requests as well as requests for additional\ninformation from customers both Internal\n       and External.\u00c2\u00a0 Coordinates the development of additional cost\ncontrol reports. Perform monthly budgeting,\n       forecasting, ETC/EAC monitoring, revenue recognition and variance\nanalysis.\n\u00e2\u20ac\u00a2    Assists Department Managers in the timely and accurate preparation of\nannual capital budgets and mid-month\n      forecasting. (ERP JD Edwards, Hyperion, Essbase and Smart View\nretrieve software).\n\nUnited Technology Corporation\nAugust, 2012 \u00e2\u20ac\u201c July, 2013\nFinancial Analyst II\nFairfield, CA\n\u00e2\u20ac\u00a2     Develop and maintain financial models and metrics to measure\n      performance for company sales mix that\n       accurately predict business performance and highlight key issues for\n      senior management. Complete key\n       financial reports including but not limited to: Monthly performance\n      recap, weekly sales analysis, monthly gross\n       margin and other key metrics. Provide ad hoc analytical support for\n      Sales & Inventory Operations Planning\n       (SIOP) to influence decision making with financial perspective.\n\u00e2\u20ac\u00a2     Support monthly A/P & A/R invoicing for SBU and direct labor\n      analysis. Manage and prepare monthly account\n        commission journal entry and accrual, process check request and\n      maintain account analysis for reporting.\n        Manage and update a percentage of our prepaid and accrual sub-\n      ledger. Reconcile all general ledger accounts\n        and prepare/update general ledger account analysis.\n\u00e2\u20ac\u00a2      Build and maintain sales and cost schedules to track actual vs.\nbudget variance on a MTD and YTD basics.\n        Support budgeting and forecasting processes by performing trends\nanalysis via moving average schedule and\n        models.  Audits Capital Projects (e.g. construction applications,\nconstruction invoices, legal services invoices,\n        etc.) for the purpose of ensuring proper account codes,\nmathematical correctness and availability of\n        funds. Perform revenue recognition analysis for monthly sale\nreporting under GAAP. Maintain\n        lease register for all lease agreements and assets on site.\n\u00e2\u20ac\u00a2      Provide financial support to Program Managers and Pricing Analyst by\nmonitoring program financial\n        performance and developing cost effective strategies to meet budget\ncost performance. Establish and\n        maintain cost of goods sold schedule to determine estimate at\ncompletion for sales order programs.  Databases &\n        software being utilized daily Hyperion Financial Management(HFM),\nSAP, eSOX compliance module, Auto\n        time, SAP BO/BW, SmartView, Microsoft office.\n\nParamount Staffing\nMenlo- Nike Golf                                               January,\n2011- April, 2012\nCost Accountant/Financial Analyst\n      Memphis, TN\n\u00e2\u20ac\u00a2     Support off-site corporate Controller in all accounting duties, i.e.\n      month end closings, accruals, cost accounting\n       analysis, analysis of actual against budget & forecast time\n      period(s) revenues and cost of materials, standard cost\n       vs. actual cost variance of manufacturing production cost. Analyzes\n      financial and manufacturing cost data on\n       local level to produce relevant decision making tools and provide\n      recommendation to senior management to\n       make sound business decisions. Support the on- site General Manager\n      & Site Production Manager of the golf\n       manufacturing operations in financial analysis, inventory accounting\n      and financial decision making as directed.\n\u00e2\u20ac\u00a2\u00c2\u00a0\u00c2\u00a0\u00c2\u00a0\u00c2\u00a0 Ensure accounting transactions are in compliance with Nike Golf\n      accounting policies and US GAAP.  Assist in\n       the maintenance of complete and accurate standard operating\n      procedures and SOX related internal control\n       documentation.  Assist in the maintenance of domestic and/or\n      international ledgers including the preparation of\n       A/P related journal entries and reconciliations.  Decision support\n      analysis of financial transactions to determine\n       present and future financial performance. Perform bookkeeping to G/L\n      for month end closing using SAP, HFM,\n       Smartview.\n\nRobert Half & Associates\u00c2\u00a0\u00c2\u00a0\u00c2\u00a0\u00c2\u00a0\n                                                                      Nov,\n2005 \u00e2\u20ac\u201c Jan, 2014\nContract Financial Consultant\n                                                 Memphis, TN / San\nFrancisco, Ca\n\u00e2\u20ac\u00a2    Analyzes transaction data for specific account codes for the purpose\nof identifying potential budget variances,\n      compiling statistical information, developing procedures, and\nconforming to established financial practices and\n      regulatory requirements. Maintains spreadsheets for the purpose of\nensuring cash balances are managed on a\n      weekly basis, reconciling monthly expenditures and revenues and\ngeneral ledger cash balances.\n\u00e2\u20ac\u00a2    Responds to inquiries from a wide variety of internal and external\nsources (e.g. staff, supplier companies,\n      government agencies, etc.) for the purpose of providing information,\ndirection and/or appropriate referrals.\n      Review vendor activity to ensure all monthly invoices have been\nreceived and processed. Review contract\n      obligations to ensure all contract invoices have been received and\nprocessed. Perform the operating cash account\n      reconciliation and related journal entries. Perform various detailed\nmonth-end allocations for fringe benefits,\n      shared costs and indirect cost allocations. Record the release from\nrestrictions revenue journal entry.  Perform\n      various account reconciliations and work to resolve any\ndiscrepancies. Other duties / special projects as assigned.\n\nFedEx Trade Networks Transport & Brokerage, Inc.\n                                               April, 2008 \u00e2\u20ac\u201c Jan, 2012\nSr. Express Clearance Operations (ECO) Import Agent\n                                                         Memphis, TN\n\u00e2\u20ac\u00a2    Serves as the primary link between FedEx Trade Networks Transport &\n      Brokerage, Inc. and FedEx Express for\n      account and shipment specific issues. Sorts and assigns departmental\n      incoming work and monitor daily workflow\n      to ensure regulatory timeframes are met. Failure to meet these\n      timeframes could cost the company and our\n      customer dollars in potential liquidated damages and additional duty.\n      Refer potential major problems to the\n      manager.\n\u00e2\u20ac\u00a2    Maintains uniform business practices and procedures to ensure\n      compliance with U.S. customs and other\n      government agency (DEA, FDA, DOD, F&WA) laws and regulations and\n      according to FedEx Express\n      expectations. Analyzes documentation provided to determine that\n      harmonized tariff number and whether the\n      shipment qualifies for special tariff treatment, NAFTA, reduced duty\n      rates and ensures all government agency\n      regulations are followed. Also, ensures that the required information\n      is submitted or transmitted electronically.\n\u00e2\u20ac\u00a2    Completes the classification of imported merchandise by determining\n      the proper tariff classification and associate\n      duty rate, and calculating the entered value using t/b and FedEx\n      systems.\n\nMemphis Marriott East\u00c2\u00a0\u00c2\u00a0\u00c2\u00a0\u00c2\u00a0\u00c2\u00a0\u00c2\u00a0\u00c2\u00a0\u00c2\u00a0\u00c2\u00a0\u00c2\u00a0\u00c2\u00a0\u00c2\u00a0\u00c2\u00a0\u00c2\u00a0\u00c2\u00a0\u00c2\u00a0\u00c2\u00a0\n                                                                  June,\n2003 - Sept, 2005\nNight Auditor Supervisor/Night Manager.\u00c2\u00a0\u00c2\u00a0\u00c2\u00a0\u00c2\u00a0\n\nMemphis, TN\n\u00e2\u20ac\u00a2\u00c2\u00a0\u00c2\u00a0\u00c2\u00a0\u00c2\u00a0 Responsible for reconciling all hotel cashier transactions;\nreviewing, organizing and compiling management\n        reports on a timely basis and ensuring the accuracy of guest\nbillings and hotel ledger transactions.  I was also\n        responsible for effectively communicating concerns and/or related\nissues to all levels of management and\n        performing guest service agent duties as required, including check-\nin, check-out, switchboard operation and\n        reservations.\u00c2\u00a0\u00c2\u00a0\u00c2\u00a0\u00c2\u00a0\n\u00e2\u20ac\u00a2\u00c2\u00a0\u00c2\u00a0\u00c2\u00a0 \u00c2\u00a0Oversaw the overall operation of a 14 floor 319 room hotel property\nduring the evening and/or night shift to\n        ensure guest satisfaction and safety. I had full hotel authority in\nthe absence of the General Manager during the\n        night shift. Responsible for following up and making decisions\naffecting guest and team member issues.\n        Maintain direct supervision over all night shift team members.\n\u00e2\u20ac\u00a2\u00c2\u00a0\u00c2\u00a0\u00c2\u00a0\u00c2\u00a0Supervised and trained night Guest Services Agents to ensure guests\n      are satisfied with their stay and reports are\n      accurately completed. Immediately address any security issues found\n      on property. Ensuring the property is well\n      maintained and free from any safety hazards. Consistently walking\n      through all departments to ensure that all\n      staff members are in proper uniform and present in work areas.\n.\nCOMPUTER SKILLS\nAs a Leader, I have fostered a team approach on consistently executing a\nsolid partnership by developing strong relationships with internal and\nexternal clients with an objective of meeting all clients\u00e2\u20ac\u2122 financial\nexpectations from a simplified approach. Excellent critical thinking,\nanalytical and spreadsheet modeling skills. Demonstrate the ability to\ndocument relevant facts and information to support testing and conclusions\nso other reviewers can follow the auditor\u00e2\u20ac\u2122s logic and methodology.\nProficient in Ariba, SAP analysis for office (AO), Primavera P6, Oracle,\nSAP, SAP BW/BO experience, Hyperion (HFM retrieve), Smart View, JD Edwards,\nPower BI, QuickBooks, advance Microsoft Excel (Pivot tables, V-lookup,\nSUMIFS, AVERAGEIFS, COUNTIFS).\n\u00c2\u00a0\u00c2\u00a0\nADDITIONAL SKILLS\nMy leadership in the past 10 years in Finance has allowed me to acquire\noutstanding qualities throughout the process of leading the delivery of\nfinancial solutions and services to meet the client's financial budgets. As\na Leader, it\u00e2\u20ac\u2122s imperative that I am effective at collaborating with all\nlevels of individuals to ensure the delivery of a customer centric\nenvironment.\n\n\n",
          "confidence": 1.0,
          "method": "full_text",
          "structured_data": null
        },
        "tax_term": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "source_by": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "designation": {
          "value": "Project Manager",
          "confidence": 0.9,
          "method": "ner",
          "structured_data": null
        },
        "experience": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        }
      }
    },
    {
      "resume_path": "data\\input\\Grant Compliance Manager - MA - Joan.doc",
      "confidence_score": 0.5650000000000001,
      "used_ocr": false,
      "extracted_fields": {
        "skills": {},
        "first_name": "Joan",
        "last_name": "E. Eline",
        "primary_email": {
          "value": "Joan.e.eline@gmail.com",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "secondary_email": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "phone": {
          "value": "4013392244",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "city": {
          "value": "Fall River",
          "confidence": 0.7,
          "method": "ner",
          "structured_data": null
        },
        "state": {
          "value": "RI",
          "confidence": 0.9,
          "method": "zip_database",
          "structured_data": null
        },
        "zip": {
          "value": "02871",
          "confidence": 0.7,
          "method": "regex",
          "structured_data": null
        },
        "work_authority": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "resume_link": {
          "value": "data\\input\\Grant Compliance Manager - MA - Joan.doc",
          "confidence": 1.0,
          "method": "file_path",
          "structured_data": null
        },
        "raw_resume": {
          "value": "\n\n                                Joan E. Eline\n\n                               Fall River, MA\n                                (401) 339-2244\n                           Joan.e.eline@gmail.com\n\n\n\nEXPERIENCE\n\n\n\n\n\n\n              Primrose Center, Inc\n                                  2015-Present\n\n              Orlando, FL\n\n              Accounting and Grant Compliance Manager\n\n               \u00e2\u2013\u00aa Accounts Receivable, including invoicing and collections\n               \u00e2\u2013\u00aa Posting of payments to customer accounts\n               \u00e2\u2013\u00aa Processing Credits\n               \u00e2\u2013\u00aa Payroll & Benefit processing\n               \u00e2\u2013\u00aa Supervision of Accounts Payable and Reception\n               \u00e2\u2013\u00aa Journal Entries as needed\n               \u00e2\u2013\u00aa Preparation of YE Audit Workpapers\n               \u00e2\u2013\u00aa Grant Compliance, Federal, State and City Level\n               \u00e2\u2013\u00aa Projects as needed\n\n\n\n\n\n              Airsports, LLC & Kong USA, LLC\n                                  2014-2015\n\n              Bristol, RI\n\n              Accountant/Customer Service\n\n               \u00e2\u2013\u00aa Full Charge Bookkeeping for multiple entities\n               \u00e2\u2013\u00aa Processing of all payables and receivables, including\n                 invoicing\n               \u00e2\u2013\u00aa Posting of payments to customer accounts\n               \u00e2\u2013\u00aa Compiled monthly financial statements, including balance\n                 sheets, income statements, general ledger details and cash\n                 flow\n               \u00e2\u2013\u00aa Payroll & Benefit processing\n               \u00e2\u2013\u00aa Reconciling of bank accounts\n               \u00e2\u2013\u00aa Processing of monthly and quarterly Payroll & Business\n                 Taxes\n               \u00e2\u2013\u00aa Projects as needed\n\n\n\n\n\n              Island Carpet & Floor Covering, LLC\n                                  2004-2015\n\n              Middletown, RI\n\n              Office Manager/Bookkeeper\n\n               \u00e2\u2013\u00aa Processing of all payables and receivables, including\n                 invoicing\n               \u00e2\u2013\u00aa Posting of payments to customer accounts\n               \u00e2\u2013\u00aa Compiled monthly financial statements, including balance\n                 sheets, income statements, general ledger details and cash\n                 flow\n               \u00e2\u2013\u00aa Payroll & Benefit processing\n               \u00e2\u2013\u00aa Reconciling of bank accounts\n               \u00e2\u2013\u00aa Processing of monthly sales tax\n               \u00e2\u2013\u00aa Worked with certified public accountants on tax issues\n\n\n                    Promptus Communications, INC\n                       1996 - 2004\n\n              Portsmouth, RI\n\n              Accounting Manager\n\n               \u00e2\u2013\u00aa Processing of all payables and receivables, including\n                 invoicing\n               \u00e2\u2013\u00aa Posting of payments to customer accounts\n               \u00e2\u2013\u00aa Compiled monthly financial statements, including balance\n                 sheets, income statements, general ledger details and cash\n                 flow\n               \u00e2\u2013\u00aa Payroll & Benefits\n               \u00e2\u2013\u00aa Fixed Asset Accounting, including depreciation schedules\n               \u00e2\u2013\u00aa Worked with certified public accountants on tax issues\n               \u00e2\u2013\u00aa All duties associated with Human Resources\n\n\n\n\n\n\nJoan E. Eline\nPage 2\n\n\n\n\nEDUCATION\n\n\n                   BACHELOR OF SCIENCE, MANAGEMENT\n                      2003\n\n                   Fisher College, Boston, MA\n\n                                    ASSOCIATES OF SCIENCE, ACCOUNTING\n                            1994\n                    Fisher College, Boston, MA\n\nSKILLS\n              Microsoft Excel, Word, Outlook, Internet Explorer, QuickBooks\n              Premier, Quickbooks Enterprise and Microsoft Dynamics\n\n\n\nREFERENCES\n\n            Paul Fredette                                     401-683-6100\n            215 Ethel Dr\n            Portsmouth, RI  02871\n\n            John Dunn                                    401-253-3759\n            62A Ballou Blvd\n            Bristol, RI  02809\n\n            Kenneth W Fain                                    401-465-0693\n            28 Greaton Dr\n            Providence, RI  02906\n",
          "confidence": 1.0,
          "method": "full_text",
          "structured_data": null
        },
        "tax_term": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "source_by": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "designation": {
          "value": "cco",
          "confidence": 0.8,
          "method": "regex",
          "structured_data": null
        },
        "experience": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        }
      }
    },
    {
      "resume_path": "data\\input\\HD - VA - Ovais.doc",
      "confidence_score": 0.49500000000000005,
      "used_ocr": false,
      "extracted_fields": {
        "skills": {},
        "first_name": "Ovais",
        "last_name": "Khan",
        "primary_email": {
          "value": "Ovais.kit@gmail.com",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "secondary_email": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "phone": {
          "value": "7035052053",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "city": {
          "value": "",
          "confidence": 0.0,
          "method": "ner",
          "structured_data": null
        },
        "state": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "zip": {
          "value": "",
          "confidence": 0.0,
          "method": "regex",
          "structured_data": null
        },
        "work_authority": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "resume_link": {
          "value": "data\\input\\HD - VA - Ovais.doc",
          "confidence": 1.0,
          "method": "file_path",
          "structured_data": null
        },
        "raw_resume": {
          "value": "\n                                 Ovais Khan\n                                703-505-2053\n                             Ovais.kit@gmail.com\n\nSummary:\n    \u00e2\u20ac\u00a2 Verified and supported hard disk issues\n    \u00e2\u20ac\u00a2 Identified browser and connectivity issues\n    \u00e2\u20ac\u00a2 Configured local administrator user account and network connections.\n    \u00e2\u20ac\u00a2  Managed  to  identify  Apple  devices  connection  issues  and  acted\n      accordingly.\n    \u00e2\u20ac\u00a2 Administered Software Update utility\n    \u00e2\u20ac\u00a2 Utilized the appropriate purpose and format of Internet Protocol  (IP)\n      addresses and subnet masks.\n    \u00e2\u20ac\u00a2 Utilized Access Network services  for  email,  Internet,  and  instant\n      messaging.\n    \u00e2\u20ac\u00a2 Resolved accessing services problems.\n\nSkill                                        Years Used           Last Used\nInstalling and imaging new computers and loading of\nappropriate   software    for    customers.                               16\n        2021\nMigrating   data   and   user   profiles                                  16\n                             2021\nStrong  Microsoft  productivity  software  skills                         10\n                                   2021\nExperience  providing   end   user   system   support                     16\n                             2021\nInstalling and imaging new computers and loading of\nappropriate   software   for    customers.                                10\n                             2021\nExperience  troubleshooting  browser  dependency  issues                  10\n                                   2021\nExperience  with  providing   direct   customer   service                 16\n                             2021\nExperience     with     bug     documentation     and     bug      lifecycle\n  10                                       2021\nExperience            with            workstation            troubleshooting\n     10                        2021\nExcellent   analytical   and   logical   skills                           16\n        2021\nExtensive experience  with  windows  operating  systems                   16\n                                   2021\nExperience      with      MAC       OS                                     3\n                  2021\nExperience  with  Android  OS  for   mobile   device                       4\n                             2021\nWorking knowledge of  application  and  desktop  security  concepts        4\n                                   2021\nExperience  with  remote  desktop   control   tools                       10\n        2021\nExperience   writing   SQL   Queries                                       5\n        2021\nITIL Certification                                 Course in Progress\n\nProfessional Experience:\n\nWashington       Radiology       \u00e2\u20ac\u201c       FairFax,       Virginia        USA.\n     Aug 2021 \u00e2\u20ac\u201c Current\nHelpdesk Technical Support\n    \u00e2\u20ac\u00a2 Generates top results in terms of resolving calls &  tickets  quickly,\n      efficiently, and effectively.\n    \u00e2\u20ac\u00a2 Unlocking accounts and resetting user password in Active Directory\n    \u00e2\u20ac\u00a2 Installing and  imaging  new  computers  and  loading  of  appropriate\n      software for customers.\n    \u00e2\u20ac\u00a2 Respond to user\u00e2\u20ac\u2122s tickets, e-mails and assist in resolving Office 365\n      related issues.\n    \u00e2\u20ac\u00a2 Answering around 50 calls in timely manner and logging accurately.\n    \u00e2\u20ac\u00a2 Closed 95% of trouble tickets on the first call without escalation.\n    \u00e2\u20ac\u00a2 Escalating technical issues correctly and effectively to right area of\n      support\n    \u00e2\u20ac\u00a2 Work within a team to provide technology support to Radiologists\n    \u00e2\u20ac\u00a2 Flexible working hours to accommodate a 24/7 shift, including rotating\n      weekends and holidays\n\nConsult     America     Inc     \u00e2\u20ac\u201c     Great     Falls,     Virginia     USA.\n     Nov 2018 \u00e2\u20ac\u201c Apr 2021\nNIH Bethesda MD\nTechnical Support Engineer\n    \u00e2\u20ac\u00a2 Deploying, installing, and configuring PCs, printers, mobile  devices,\n      and other peripheral equipment\n    \u00e2\u20ac\u00a2 Deployed new hardware and  software,  leveraging  automated  processes\n      established.\n    \u00e2\u20ac\u00a2 Installing and  imaging  new  computers  and  loading  of  appropriate\n      software for customers.\n    \u00e2\u20ac\u00a2 Migrating data and user profiles\n    \u00e2\u20ac\u00a2 Successful pc windows migration completed\n    \u00e2\u20ac\u00a2 Working within customer SLA guidelines\n    \u00e2\u20ac\u00a2 Commended for quickly resolving complex issues  including  application\n      issues, system  crashes,  network  slowdowns,  connectivity  problems,\n      security breaches, virus infections and more.\n    \u00e2\u20ac\u00a2 Consistently logged  and  monitored  ticket  status  to  ensure  fast,\n      quality resolution of every issue.\n    \u00e2\u20ac\u00a2 Maintained bug  status  reports  and  drove  to  resolution  including\n      verification of fixes.\n    \u00e2\u20ac\u00a2 Provide assistance for ongoing maintenance, backups, system health  of\n      local servers and network gear.\n\nSaapa      Consulting      Technology      Abu       Dhabi       \u00e2\u20ac\u201c       UAE\nOct 2016 \u00e2\u20ac\u201c Aug 2018\nTechnical Engineer\n    \u00e2\u20ac\u00a2 Provide a variety of highly visible, complex,  and  valuable  Security\n      consulting services in  support  of  the  SaapaTech  Projects  (Bolden\n      James, Digital Guardian, Fidelis, Secunia, Nutanix, and  Tenable)  and\n      Business Development pursuits.\n    \u00e2\u20ac\u00a2 Focus on providing added value  to  Saapa,  pursuits,  proposals,  and\n      projects with Security IT solutions.\n    \u00e2\u20ac\u00a2 Provide technical delivery to support Saapa Responses to  RFIs,  RFQs,\n      and RFPs.\n    \u00e2\u20ac\u00a2 Prioritized & coordinated multiple projects to maximize  efficiency  &\n      achieve critical timelines.\n    \u00e2\u20ac\u00a2 Extensively used ServiceNow for ticketing.\n    \u00e2\u20ac\u00a2 Provide IT Security technical qualifications and solutions as part  of\n      SaapaTech Proposal content.\n    \u00e2\u20ac\u00a2 Lead the Business  Analysis,  Design,  Development,  and  Delivery  of\n      technical  solutions  (incl.  Applications,  IT   Infrastructure)   to\n      Projects.\n    \u00e2\u20ac\u00a2 Analyze  business  needs,  recommend  solutions,  facilitate  solution\n      delivery, and work with the project teams to ensure Saapa  ability  to\n      meet client deliverables.\n    \u00e2\u20ac\u00a2 Designed and populated specific table  for  collection,  tracking  and\n      reporting of SQL queries data.\n    \u00e2\u20ac\u00a2 Upon Project  award,  work  on  IT  mobilization  activities.  Manage,\n      execute, monitor, and control Projects.\n    \u00e2\u20ac\u00a2 Manage Project Scope, Budget, and Schedule.  Engage  team  members  as\n      needed.\n    \u00e2\u20ac\u00a2 Documented dead date  test  case  (UAT)  while  working  in  an  agile\n      software development life cycle.\n    \u00e2\u20ac\u00a2 Build client relationships while managing  multiple  initiatives  with\n      competing priorities.\n    \u00e2\u20ac\u00a2 Maintained bug  status  reports  and  drove  to  resolution  including\n      verification of fixes.\n    \u00e2\u20ac\u00a2 Ensure  the  delivery  of  support  services  necessary  for  projects\n      successful  execution  while   also   promoting   standardization   of\n      applications and systems to the extent possible.\n\nComputer       Sciences       Corporation       -       CSC        Australia\n      Jun 2015 \u00e2\u20ac\u201c Sep 2016\nTechnical Analyst\n    \u00e2\u20ac\u00a2 Maintaining full accountability for handling all aspects of day-to-day\n      activities as the  primary  point  of  contact  for  users  to  report\n      technical issues and incidents for pioneer  accounts  for  clients  as\n      BHP, AMP Bank, BMC Coal.\n    \u00e2\u20ac\u00a2 Contribute technical expertise  in  evaluating,  troubleshooting,  and\n      resolving routine computer incidents for internal and external users.\n    \u00e2\u20ac\u00a2 Successfully handled all facets  of  day-to-day  incident  and  change\n      management  for  routine  PC,  server,  mainframe   applications   and\n      hardware.\n    \u00e2\u20ac\u00a2 Prioritized & coordinated multiple projects to maximize  efficiency  &\n      achieve critical timelines.\n    \u00e2\u20ac\u00a2 Demonstrated a strong proficiency in assessing upcoming  releases  and\n      identifying support gaps, risks, and opportunities.\n    \u00e2\u20ac\u00a2 Steered a wide array of different IT initiatives, including escalating\n      complex IT issues, evaluating and tracking interactions, incidents and\n      requests within the IT Service Management tool,  and  providing  users\n      with incident status and outages.\n    \u00e2\u20ac\u00a2 Apply strategic planning,  problem  solving,  and  project  management\n      skills  toward  consistently  achieving   critical   deadlines   while\n      maintaining high quality standards.\n\nDepartment     of      Family      and      Community      Services      NSW\nMar 2015 - May 2015\nClient Service Officer\n    \u00e2\u20ac\u00a2 Providing Technical support to over 25,000 users across NSW  per  ITIL\n      framework. I performed the following tasks:\n    \u00e2\u20ac\u00a2 Managing client expectations by ensuring applications are supported to\n      agreed service levels.\n    \u00e2\u20ac\u00a2 Create user accounts and assign group rights  using  Microsoft  server\n      tools (ADS) Active Directory\n    \u00e2\u20ac\u00a2 Manage Citrix Applications access through group policies.\n    \u00e2\u20ac\u00a2 Answering 30-40 ticket calls in timely manner and logging accurately.\n    \u00e2\u20ac\u00a2 Support MS Office Suite 2010 related issues.\n    \u00e2\u20ac\u00a2  Responsible  for  obtaining   required   information   from   service\n      requesters,   querying   clients   to    ensure    accurate    product\n      identification,  and  logging  the  information   into   proper   call\n      management system.\n    \u00e2\u20ac\u00a2 Escalating calls to correct department.\n\nINTA            serve             -             Sydney             Australia\n  Jul 2012 - Feb 2015\nTechnical Support Engineer\n    \u00e2\u20ac\u00a2 INTA serve is Australia's premiere  hosting  and  domain  specialists.\n      Providing Shared, Co-Location  and  dedicated  hosting  to  Australian\n      business and domains. I performed the following tasks:\n    \u00e2\u20ac\u00a2 Managing technical issues on the DNR (Domain  Name  Registration)  and\n      Shared Hosting servers.\n    \u00e2\u20ac\u00a2 Maintaining daily, weekly and monthly data  backups  as  per  schedule\n      given by management.\n    \u00e2\u20ac\u00a2 Deploying maintenance routine job including data backup  and  restore,\n      upgrade and patch.\n    \u00e2\u20ac\u00a2 Installing Antivirus server/client machine.\n    \u00e2\u20ac\u00a2 Management of mission critical email servers (IMAIL, @MAIL),  DNS  and\n      web server (IIS).\n    \u00e2\u20ac\u00a2 Installation and administration  of  all  software  used  to  run  web\n      services including Microsoft IIS.\n    \u00e2\u20ac\u00a2 Installation & configuration of DNS, Mail and WWW services\n    \u00e2\u20ac\u00a2 Provide technical support to  wide  range  of  dedicated  server,  Co-\n      Location Clients and end users using different means of communication,\n      remote access, telephone and in person support methods.\n    \u00e2\u20ac\u00a2 Diagnosis & resolve  client\u00e2\u20ac\u2122s  queries  in  specific  time  frame  for\n      hardware, software and peripherals.\n    \u00e2\u20ac\u00a2 Work closely with other groups within company  like  server,  printer,\n      desktop and remote resolution group  to  maintain  smooth  running  of\n      network system.\n    \u00e2\u20ac\u00a2 Document incident and resolution  information  in  multiple  ticketing\n      systems.\n    \u00e2\u20ac\u00a2 MS SQL, Linux control panels (c-panel, Plesk)\n    \u00e2\u20ac\u00a2 Implementation, management & maintenance of all departmental desktop &\n      servers.\n    \u00e2\u20ac\u00a2 Designed and populated specific table  for  collection,  tracking  and\n      reporting of SQL queries data.\n    \u00e2\u20ac\u00a2 Installation & configuration of Windows 2000, 2003 and 2008 servers.\n    \u00e2\u20ac\u00a2 Installing servers (IBM, DELL, HP) in racks in Data center.\n    \u00e2\u20ac\u00a2 Ensuring the implementation  of  safe  systems  appropriate  to  their\n      operational responsibility.\n    \u00e2\u20ac\u00a2 Maintenance and troubleshooting of servers & workstations  security  &\n      identifying & solving the hardware, software issues\n    \u00e2\u20ac\u00a2 Documented dead date  test  case  (UAT)  while  working  in  an  agile\n      software development life cycle.\n    \u00e2\u20ac\u00a2 Coordination with team members to ensure  efficiency  and  quality  of\n      service.\n    \u00e2\u20ac\u00a2 Major achievements include:\n    \u00e2\u20ac\u00a2 Worked on the Data  Centre  Migration  Project  as  a  Junior  Project\n      Officer.\n    \u00e2\u20ac\u00a2 Dramatically improved problem-solving skills  through  troubleshooting\n      and customer enquiries\n    \u00e2\u20ac\u00a2  Providing  technical  support  to  users  using  different  means  of\n      communication.\n\nHostway              Australia\u00e2\u20ac\u201c              Sydney,              Australia.\n       Apr 2007 - Jun 2012\nTechnical Service Specialist\n    \u00e2\u20ac\u00a2 Hostway is one  of  the  biggest  client  service  providers  covering\n      Australia and countries of Far East. Working as client service manager\n      my main duties included project deployment and technical expertise  to\n      various teams. I reported directly to head office in Chicago,  USA.  I\n      performed the following tasks:\n    \u00e2\u20ac\u00a2 Performing key account management duties that apply to over 80 managed\n      hosting clients.\n    \u00e2\u20ac\u00a2 Purchase new hardware from vendors IBM & Dell & sourcing new  partners\n      to achieve cost reduction.\n    \u00e2\u20ac\u00a2 Managing aspects of service delivery - making sure  that  hardware  is\n      sourced, technical team receives hardware and installation is complete\n      before the client is notified.\n    \u00e2\u20ac\u00a2 Providing technical assistance to Technical Support team.\n    \u00e2\u20ac\u00a2 Managing Microsoft License and  submitting  monthly  reports  to  Head\n      Office in Chicago- USA\n    \u00e2\u20ac\u00a2 Training new technical staff with  procedures  &  providing  technical\n      documentation.\n    \u00e2\u20ac\u00a2 Day-to-day operations  involving  technical  aspect  are  carried  out\n      smoothly.\n    \u00e2\u20ac\u00a2 Bug status reports and drove to resolution including  verification  of\n      fixes were documented.\n    \u00e2\u20ac\u00a2 Document incident and resolution  information  in  multiple  ticketing\n      systems.\n    \u00e2\u20ac\u00a2 Maintaining &  Developing  client  support  procedures,  policies  and\n      standards for IT department.\n    \u00e2\u20ac\u00a2 Point of contact for Client technical complaints and resolve technical\n      issues in a smooth manner.\nMajor achievements include:\n    \u00e2\u20ac\u00a2  Able  to  produce  and  maintain  high  quality  accurate   technical\n      documentation.\n    \u00e2\u20ac\u00a2 Able to utilize technical knowledge in the continuous  improvement  of\n      procedures and policy.\n    \u00e2\u20ac\u00a2 Gained Analytical and structured problem-solving skills and  now  able\n      to analyze, diagnose and troubleshoot root cause  technical  problems.\n\n\n    \u00e2\u20ac\u00a2 Delivered high-quality technical solutions to  client  with  excellent\n      feedbacks\n\nIndigo  Solutions  \u00e2\u20ac\u201c  Sydney,  Australia.                                Feb\n2006 -Mar 2007\nTechnical Support Engineer\n    \u00e2\u20ac\u00a2 Indigo is an Australian hosting company serving the Enterprise clients\n      hosting needs in dedicated servers and cloud  computing.  I  performed\n      the following tasks:\n    \u00e2\u20ac\u00a2 System Maintenance and builds for: Dell Productions Servers in an  IDC\n      environment.\n    \u00e2\u20ac\u00a2  Server  Configuration:  IBM  servers  and  OEM  servers  for  an  IDC\n      Environment.\n    \u00e2\u20ac\u00a2 System Maintenance and builds for: IIS Web server, Linux Web  servers,\n      DNS, CSR and SSL Certificates,\n    \u00e2\u20ac\u00a2 Data Centre Support: 24/7 Pager Alert Monitoring on Enterprise Servers\n      & Backup Monitoring.\n    \u00e2\u20ac\u00a2 Remote Power Rail Management and Remote Server Management.\n    \u00e2\u20ac\u00a2 Documentation of IP, Subnet, NAT and Rack diagrams\nMajor achievements:\n    \u00e2\u20ac\u00a2 Produced & maintained high quality accurate technical documentation.\n    \u00e2\u20ac\u00a2  Updated,  modified,  and  created  users  training,   technical   and\n      administration manuals.\n\nMax            SI             \u00e2\u20ac\u201c             Brisbane,             Australia.\n          Jan 2004 - Dec 2005\nSystem Analyst\n    \u00e2\u20ac\u00a2 Max SI is a  software  development  company  which  developed  systems\n      according to client\u00e2\u20ac\u2122s requirement  and  offered  software  testing  to\n      client\u00e2\u20ac\u2122s software. I performed the following tasks:\n    \u00e2\u20ac\u00a2  Documentation  of  C3M  (Customer  Call  Centre  Management)   system\n      developed by MAX-SI.\n    \u00e2\u20ac\u00a2 Script writing for regression test critical program components.\n    \u00e2\u20ac\u00a2 Preparing Requirements Document  for  purpose  of  system  integration\n      using UML (Unified Modeling\n    \u00e2\u20ac\u00a2  Language).  Workflow  modeling/process  mapping  of  the  C3M  system\n      (Microsoft Visio)\n    \u00e2\u20ac\u00a2 Successful implementation of Unified  Modeling  Language  (UML),  Data\n      Feed Diagrams (DFD), Entity\n    \u00e2\u20ac\u00a2 Relationship Diagrams (ERD) and Program  Evaluation  Review  Technique\n      (PERT).\n    \u00e2\u20ac\u00a2 Updating,  modifying,  and  creating  users  training,  technical  and\n      administration manual.\n    \u00e2\u20ac\u00a2 Debugging, problem solving  and  analyzing  user  interface  form  for\n      better friendly usage.\nMajor achievements include:\n    \u00e2\u20ac\u00a2 Confident application of UML for system mapping (various modules)\n    \u00e2\u20ac\u00a2 Part of developer team to launch MAX-SI\u00e2\u20ac\u2122s first successful  in  house-\n      built software C3M (Call Centre\n    \u00e2\u20ac\u00a2 Customer Management) etc.\n\nNet          Technologies          Solutions          \u00e2\u20ac\u201c           Australia.\nOct 2002 - Sep 2003\nWeb Developer/Tester:\n    \u00e2\u20ac\u00a2 Net Technologies is a Gold Coast  based  web  application  development\n      company, designing & developing  local  business  static  and  dynamic\n      websites. I performed the following tasks\n    \u00e2\u20ac\u00a2 Development of web based, component based and database application.\n    \u00e2\u20ac\u00a2 Database designing, management, administration, and optimization.\n    \u00e2\u20ac\u00a2   Software   design,   development,   and   technical    specification\n      documentation.\n    \u00e2\u20ac\u00a2 Software Proposal and time estimates preparation.\n    \u00e2\u20ac\u00a2 Code review and application performance improvement.\n    \u00e2\u20ac\u00a2 Test designing, development, and execution of manual test cases.\n    \u00e2\u20ac\u00a2 Participation in Software Process Improvement activities.\n    \u00e2\u20ac\u00a2 Coordination with development teams and project managers and  client\u00e2\u20ac\u2122s\n      coordination.\nMajor achievements include:\n    \u00e2\u20ac\u00a2 Understanding Team, Project managers & Clients coordination.\n    \u00e2\u20ac\u00a2 Development and execution of manual test cases.\n    \u00e2\u20ac\u00a2 Demonstration of Database designing, management skills.\n\nEducation:\n    \u00e2\u20ac\u00a2  MIT.  Master  of  Information  Technology  -   Griffith   University.\n      Australia.\n    \u00e2\u20ac\u00a2 Certifications:\n    \u00e2\u20ac\u00a2 ITIL. V4 Foundation - Course in Progress\n\n\n",
          "confidence": 1.0,
          "method": "full_text",
          "structured_data": null
        },
        "tax_term": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "source_by": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "designation": {
          "value": "system support",
          "confidence": 0.9,
          "method": "ner",
          "structured_data": null
        },
        "experience": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        }
      }
    },
    {
      "resume_path": "data\\input\\HealthRules - configuration  - GA - Violet Jones.doc",
      "confidence_score": 0.34500000000000003,
      "used_ocr": false,
      "extracted_fields": {
        "skills": {},
        "first_name": "Violet",
        "last_name": "Jones",
        "primary_email": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "secondary_email": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "phone": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "city": {
          "value": "Quincy",
          "confidence": 0.7,
          "method": "ner",
          "structured_data": null
        },
        "state": {
          "value": "MA",
          "confidence": 0.8,
          "method": "city_database",
          "structured_data": null
        },
        "zip": {
          "value": "",
          "confidence": 0.0,
          "method": "regex",
          "structured_data": null
        },
        "work_authority": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "resume_link": {
          "value": "data\\input\\HealthRules - configuration  - GA - Violet Jones.doc",
          "confidence": 1.0,
          "method": "file_path",
          "structured_data": null
        },
        "raw_resume": {
          "value": "\nViolet Jones\n\nSummary:\n\n    \u00e2\u20ac\u00a2 15 years Healthcare Payer Experience\n    \u00e2\u20ac\u00a2 10+ years Facets and NetworX Pricer experience (versions 2.96 to 5.10,\n      both front and back end expertise for configuration and testing)\n    \u00e2\u20ac\u00a2 10+ year experience in HSD Diamond 950, Xcelys,  Amysis, Amysis\n      Advance, QNXT and Health Rules\n    \u00e2\u20ac\u00a2 Comprehensive experience in healthcare data analysis encompassing\n      systems reporting and testing. Superior claims adjudication and\n      auditing expertise resulting in excellent performance evaluations.\n\n\nExperience:\n\n10/2014-Present (Traveling Position) Jacobson Staffing; HCSC-Blue\nCross/Blue Shield Of Texas Medicare Contract Office; Facets Benefits\nConsultant, UAT Testing Analyst, Configuration Analyst\n\n    \u00e2\u20ac\u00a2 Ensure updates of CPT, HCPCS, DRG, and ICD-10 codes as well as\n      pricing, billing rates, analysis, data entry, testing and\n      documentation of configuration set up for all Facets applications.\n    \u00e2\u20ac\u00a2 Translates business rules into effective and efficient Facets\n      configuration\n    \u00e2\u20ac\u00a2 Gathering and analyzing data in support of business cases, proposed\n      projects and system requirements.\n    \u00e2\u20ac\u00a2 Prepare and deliver reports, recommendations or alternatives in an\n      effort to streamline processes and improve operational efficiencies\n    \u00e2\u20ac\u00a2 Define and deploy operational requirements and system goals based off\n      of conducted analysis and agreed upon recommendations\n\n\n7/2015- 9/2015 (Traveling Position) Catalyst Solutions- Lumeris Essence\nHealthcare; Facets Benefits Consultant, Testing Analyst, Sr. Facets\nConfiguration Analyst\n\n    \u00e2\u20ac\u00a2 Set up & configure ITS; Synchronize ITS Home- Facets Configuration\n      Analyst; Configure MQ series\n    \u00e2\u20ac\u00a2 Configure SA/Security; ITS Product Security; ITS User Security\n    \u00e2\u20ac\u00a2 Facets to ITS code Conversions; ITS COB Method Conversion; ITS Codes\n      Description; ITS Error codes description\n    \u00e2\u20ac\u00a2 ITS Home Administrative Rules Application; ITS HRA Administrative\n      Rules\n    \u00e2\u20ac\u00a2 ITS Home Products; ITS Proxy provider applications\n\n\n\n\n\n3/2011\u00e2\u20ac\u201c8/2014 (Traveling Position) Dell Staffing, Harvard Pilgrim Health\nQuincy, MA.\nBusiness Analyst , Amysis  Advance & Facets Configuration Analyst, UAT\nTesting Analyst\n\n    \u00e2\u20ac\u00a2 Provides oversight of business system requirement/ specification\n      analyst & maintenance in accordance with customer deliverables.\n    \u00e2\u20ac\u00a2 Document and test  Facets test scenarios, cases, and step-by-step\n      instructions based on Harvard pilgrim health business functions.\n    \u00e2\u20ac\u00a2 Developed and maintained business system configuration solutions in\n      accordance with provider/employer contracts and customer requirements.\n    \u00e2\u20ac\u00a2 Conceiving and delivering Claims Mass re-Adjudication (CMA)\n    \u00e2\u20ac\u00a2 Administered the data/ configuration maintenance of systems\n      parameters, specifications and data linkages to other systems involved\n      in SOA component based architecture.\n    \u00e2\u20ac\u00a2 Query and analyze data backend table data from Facets and NetworX\n      tables.\n    \u00e2\u20ac\u00a2 Developed, maintained, and disseminated internal and external system\n      documentation, including status updates; prepared procedural and\n      customer documentation when required.\n    \u00e2\u20ac\u00a2 Responsible for data maintenance in Facets Tables which includes but\n      not limited to backend tables, writing queries through data dictionary\n      and stored procedures\n    \u00e2\u20ac\u00a2 Test and quality analysis validation of configuration building test\n      claims\n    \u00e2\u20ac\u00a2 Able to convert CORE Facets pricing to NetworX pricing\n\n\n\n9/2009\u00e2\u20ac\u201c3/2011(Traveling Position) Dell Staffing   Universal America\nHouston, TX\n        Facets Configuration Analyst, Facets Testing Analyst, Senior\nAppeals and Grievance Specialist\n\n    \u00e2\u20ac\u00a2 Implementation of Medicare Line of Business into Facets 5.01\n    \u00e2\u20ac\u00a2 Performed Facets front and back end configuration and inquiries\n      activities using SQL\n    \u00e2\u20ac\u00a2 Responsible for timely and accurate configuration build and test\n      activities related to Benefits, Payments, Pricing, Claims, Provider\n      contracts & Membership/Enrollment, including ITS.\n    \u00e2\u20ac\u00a2 Creation and maintenance of documentation related to build, test,\n      policies and processes related to implementation efforts\n    \u00e2\u20ac\u00a2 Provided status reports on weekly basis\n    \u00e2\u20ac\u00a2 Responsible for monitoring effectuations of all resolutions as a\n      result of appeal or grievance.\n\n3/2007-9/2009; 5/2013-3/2014 (Work @ Home) Jacobson Staffing    MVP\nRochester, NY\n          Senior Facets Claims Specialist and Adjustment Representative\n\n    \u00e2\u20ac\u00a2 Researched, analyzed and adjusted claims in accordance with the plans\n      provision and provider contracts\n    \u00e2\u20ac\u00a2 Produced high productivity and accuracy rate processing claims using\n      Facets version 4.81\n    \u00e2\u20ac\u00a2 Adjudicated and adjusted claims using Facets Medicare, Medicaid,\n      Manage Care and PPO claims\n    \u00e2\u20ac\u00a2 Strong problem solving techniques with excellent verbal and written\n      communication skills\n    \u00e2\u20ac\u00a2 Outstanding organizational skills and ability to meet deadlines\n\n\n\n3/2005-3/2007(Traveling Position) Jacobson Staffing    Lovelace Health Plan\n   Alb, NM\n       Facets Testing and Configuration Analyst, Appeals and Grievance\nRep.; Senior Claims Processor\n    \u00e2\u20ac\u00a2 Followed established requirements and design for Facets configuration,\n      testing and debugging, provided build and release project/product\n      components\n    \u00e2\u20ac\u00a2 Facets  membership (enrollment) and adjudication of test claims for\n      eligibility and benefit configuration validation\n    \u00e2\u20ac\u00a2 Responsible for identifying necessary steps and processing claims of\n      various complexity levels for Medicare, Medicaid, HMO, PPO, SNF, Rehab\n      etc.\n    \u00e2\u20ac\u00a2 High productivity and accuracy rate processing overturned appeals;\n      processed and adjusted claims in accordance with the plans guidelines\n\n\n10/2003-3/2005: Southern Crescent Brain Injury Hospital Stockbridge, GA.\nSenior Biller and Collections Specialist for Medicare, Medicaid and\nCommercial Insurance\n\n    \u00e2\u20ac\u00a2 Providing appropriate coding on the patient\u00e2\u20ac\u2122s medical history,\n      diagnosis, tests and treatments\n    \u00e2\u20ac\u00a2 Using the coded medical records to create invoices that are sent to\n      the patients and insurance companies.\n    \u00e2\u20ac\u00a2 Responsible for submitting and following up on all claims which\n      includes electronically filing Medicare, Medicaid, Commercial claims\n    \u00e2\u20ac\u00a2 Liaison to insurance companies, accepting and posting payments\n    \u00e2\u20ac\u00a2 Accurately recording the patient\u00e2\u20ac\u2122s and insurance company details into\n      the system.\n    \u00e2\u20ac\u00a2 Facets patient enrollment, Billing, case management and follow-up\n\n\n\n\n6/2000-10/2003: The Shepherd Center Spinal and Brain Injury Hospital\nAtlanta, Ga.\nBiller and Collection Specialist for Medicare and Medicaid\n\n    \u00e2\u20ac\u00a2 Used coded data to produce and submit claims to insurance companies\n    \u00e2\u20ac\u00a2 Worked directly with the insurance companies, healthcare providers,\n      and patients to get claims processed and paid\n    \u00e2\u20ac\u00a2 Reviewed and appealed unpaid or denied claims; handle collections on\n      unpaid accounts\n    \u00e2\u20ac\u00a2 Manage the facility\u00e2\u20ac\u2122s accounts receivable reports; answer patients\n      billing questions\n\n\nEducation:\nGeorgia State University Atlanta, Georgia\nA.S., Math\nGraduated Cum Laude\n\nSystem Experience:\nHIX, Oracle, RFT/QTP functional automation tool, Soap UI, Soap UI Pro,\nAMISYS, NASCO, Facets (2.96 through 5.10), Rims, Xcelys, NextGen, QNXT,\nDiamond, TOPPS, SQL, CSC, Macess, EMDON, Electron and Navicure clearing\nhouse, Florida Shared Systems, Mutual of Omaha Electronic Claims, Trizetto\nNextworX Pricer, Care Planner, Citrix, ICD-9, ICD-10, HIPAA Compliance,\nProficient in Microsoft Word, Excel, PowerPoint, Outlook E-mail, Database\nManagement & Intermediate computer skills.\n\n\nCertification:\n\n    \u00e2\u20ac\u00a2 CPAR, 2005\n\n\n\n\n",
          "confidence": 1.0,
          "method": "full_text",
          "structured_data": null
        },
        "tax_term": {
          "value": "CONTRACT",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "source_by": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "designation": {
          "value": "Business Analyst",
          "confidence": 0.9,
          "method": "ner",
          "structured_data": null
        },
        "experience": {
          "value": "10+",
          "confidence": 0.9,
          "method": "regex_total_experience_summary",
          "structured_data": null
        }
      }
    },
    {
      "resume_path": "data\\input\\HealthRules - Data Analyst - MI - John.doc",
      "confidence_score": 0.25000000000000006,
      "used_ocr": false,
      "extracted_fields": {
        "skills": {},
        "first_name": "PROFESSIONAL",
        "last_name": "SUMMARY",
        "primary_email": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "secondary_email": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "phone": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "city": {
          "value": "",
          "confidence": 0.0,
          "method": "ner",
          "structured_data": null
        },
        "state": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "zip": {
          "value": "",
          "confidence": 0.0,
          "method": "regex",
          "structured_data": null
        },
        "work_authority": {
          "value": "",
          "confidence": 0.8,
          "method": "regex",
          "structured_data": null
        },
        "resume_link": {
          "value": "data\\input\\HealthRules - Data Analyst - MI - John.doc",
          "confidence": 1.0,
          "method": "file_path",
          "structured_data": null
        },
        "raw_resume": {
          "value": "\nPROFESSIONAL SUMMARY:\n \u00e2\u20ac\u00a2  8+  years\u00e2\u20ac\u2122of  diverse  experience  as  Data  Analyst   across   multiple\n   organizations.\n \u00e2\u20ac\u00a2 Experience with mapping and validating various EDI  files  such  as  834,\n   820, 835, 837, etc.\n \u00e2\u20ac\u00a2 Strong knowledge of  software  development  methodologies  including  the\n   agile methodology, Waterfall, and RAD Methodology.\n \u00e2\u20ac\u00a2  Knowledge  and  experience  working  with  FACETS  and   FACETS   claims\n   processing.\n \u00e2\u20ac\u00a2 Skilled in bug reporting and tracking using  ALM  (Quality  Center),  IMB\n   Clear quest and JIRA.\n \u00e2\u20ac\u00a2 Experience in developing, trouble shooting and customizing  test  scripts\n   using\u00c2\u00a0Quick Test  Professional\u00c2\u00a0QTP\u00c2\u00a0/HP  UFT\u00c2\u00a0for\u00c2\u00a0Functional\u00c2\u00a0and\u00c2\u00a0Regression\n   Testing.\n \u00e2\u20ac\u00a2 Experienced in  using  management  tools  such  as  HP  ALM,  JIRA-Zephyr\n   combination, CA Agile Central (Rally) for writing the  test  cases,  test\n   data, executing the test cases, tracking, logging and  reporting  defects\n   as well as developing Test Analysis Report (TAR).\n \u00e2\u20ac\u00a2 Performed backend database testing by writing SQL and PL/SQL  scripts  to\n   verify data integrity.\n \u00e2\u20ac\u00a2  Tested  the  HIPAA  EDI  transactions  834,  837/835,  276/277,  270/271\n   according to the requirement test scenarios.\n \u00e2\u20ac\u00a2 Assist with managing\u00c2\u00a0FACETS of  project  life  cycle,  including  design,\n   development, testing, and deployment.\n \u00e2\u20ac\u00a2 Good working knowledge of HL7 standards and implementation.\n \u00e2\u20ac\u00a2 Worked with Microsoft SQL Server. Thorough knowledge and understanding of\n   Dimensional Data Modeling, Star schema, Snow-Flake  schema,  creation  of\n   Fact  and  Dimension  Tables,  OLAP,  OLTP  and  other   Data-Warehousing\n   concepts.\n \u00e2\u20ac\u00a2     Solid     understanding     of\u00c2\u00a0Membership,     Claims     Processing,\n   Benefit/Eligibility, COB, Authorization /Referrals,\u00c2\u00a0and  have  experience\n   in\u00c2\u00a0HIPAA\u00c2\u00a0standards and corresponding\u00c2\u00a0EDI\u00c2\u00a0transactions.\n \u00e2\u20ac\u00a2 Hands on experience with  solid  understanding  of  Business  Requirement\n   gathering,  Business  Process  flow,  Business   Process   Modeling   and\n   database/data warehouse experience.\n \u00e2\u20ac\u00a2 Excellent communication and organizational skills  with  the  ability  to\n   adapt to a new environment.\n \u00e2\u20ac\u00a2 Experience in facilitating meetings with clients to discuss and  sign-off\n   on the document.\n \u00e2\u20ac\u00a2 Good experience in Back-End Testing using SQL queries on Oracle,  MS  SQL\n   Server to validate the consistency of data.\n \u00e2\u20ac\u00a2 Expertise in  Unit  testing,  Integration  testing,  Regression  testing,\n   System testing, User Acceptance testing, and Implementation testing.\n \u00e2\u20ac\u00a2 Extensive working experience in Functionality, System, Integration, Front-\n   End, GUI, Back-End, Recovery Testing,  Compatibility  Testing,  Usability\n   Testing, Regression, Tracking Bug Reports using Bug tracking  Tools  like\n   Quality Center.\n \u00e2\u20ac\u00a2 Experience in usage of defect tracking tools  Quality  Center/ALM,  Clear\n   Quest, Rally, JIRA for logging the defects,  generation  of  reports  and\n   analyzes the reports.\n \u00e2\u20ac\u00a2 Used Agile-testing methodology for achieving deadlines in UAT.\n \u00e2\u20ac\u00a2 Performed planning and development of Test Plans, Test  Strategies,  Test\n   Cases and Test Scenario to meet product\u00e2\u20ac\u2122s business requirement\n \u00e2\u20ac\u00a2 Good knowledge of SQL and experience at conducting backend testing.\n \u00e2\u20ac\u00a2 Expertise in Testing of Client/Server and Web based applications.\n \u00e2\u20ac\u00a2 Experience in Functional Testing, System Integration  Testing,  Back  End\n   Testing, GUI Testing, configuration Testing, User Acceptance Testing.\n \u00e2\u20ac\u00a2 Tested web applications in Agile, waterfall development processes.\n \u00e2\u20ac\u00a2 Strong experience in Front-end Testing and Database  (back-end)  Testing,\n   GUI testing black box testing, white  box  testing,  gray  box  and  User\n   acceptance testing.\n \u00e2\u20ac\u00a2 Possess strong problem-solving skills with the ability to adapt to a  new\n   environment and meet deadlines.\n \u00e2\u20ac\u00a2 Motivated self-starter with exceptional team  building,  leadership,  and\n   interpersonal skills. Good team player with the ability to work  in  time\n   sensitive environments.\n \u00e2\u20ac\u00a2 Experience  in  creating  and  maintaining  the  Requirements  definition\n   documents   that   included   Business   requirements   and    Functional\n   requirements.\n \u00e2\u20ac\u00a2 Solid experience in creating and validating TCS letters  through  various\n   FACETS  Applications  (Utilization  Management,  Claim   Processing   and\n   Customer Service Applications) in the QAE1 environment.\n\n\nTECHNICAL SKILLS:\n\n Operating Systems:          Windows\n Languages:            Java, SQL, HTML, XML\n Testing Tools:              Selenium, SOAP UI\n Bug Reporting Tools:                 Bugzilla, JIRA, HP ALM/ Quality Center\n\n Project Methodologies:      Agile Scrum, Waterfall\n Database:             MS SQL server\n ETL Tools:            MS SQL server integration Services\n\n\n\n\nPROFESSIONAL EXPERIENCE:\n\nUnited            Health            Group,            Phoenix             AZ\n                  Sep 2018 - Present\nData Analyst\nI have worked on the project for Health Claims Scanning and Data  Extraction\n(OCR).  I developed and managed needs analysis, requirements gathering,  gap\nanalysis,  creation  of  vendor  disaster  recovery  plan,  design,  layout,\nbusiness rule development and associated software interface development  and\ntest  documents,  development,  and  EDI   implementation   insuring   HIPAA\ncompliance.   The  project  had  a  budget  of  $800K  and  the  system  was\nanticipated to have an annual volume 1.5M claims.\nResponsibilities:\n \u00e2\u20ac\u00a2 Assisted the project manager in the creation of  the  project  charter  &\n   vision document during the inception phase of the project.\n \u00e2\u20ac\u00a2 Used\u00c2\u00a0Informatica\u00c2\u00a0Power Center to extract, transform and  load  data  from\n   various source systems to staging and target Data warehouse.\n \u00e2\u20ac\u00a2 Tuned the performance of mappings by following\u00c2\u00a0Informatica\u00c2\u00a0best practices\n   and applied several methods to get best performance by decreasing the run\n   time of workflow.\n \u00e2\u20ac\u00a2 Imported Source/Target Tables from the respective databases  and  created\n   reusable  transformations  (Joiner,  Routers,  Lookups,   Rank,   Filter,\n   Expression, and Aggregator) in a Mapplet and created new  mappings  using\n   Designer module of Informatica.\n \u00e2\u20ac\u00a2  Developed Test Cases for Testing the Facets Model.\n \u00e2\u20ac\u00a2 Analyzed  business  Processes,  Subscribers  \u00e2\u20ac\u201c  group  \u00e2\u20ac\u201c  plan  \u00e2\u20ac\u201c  county\n   structure, current processes, Facets configurations  and  FACETS  backend\n   processes.\n \u00e2\u20ac\u00a2 Coordinated with the developers and IT architects to design the interface\n   of the new system according to the X12 (270, 276, 278, 834,  837  (I,P,D)\n   and 820) standards.\n \u00e2\u20ac\u00a2 Performed daily tasks including backup and restore by  using  SQL  Server\n   2008 R2 tools like SQL Server Management Studio, SQL Server Profiler, SQL\n   Server Agent, and Database Engine Tuning Advisor.\n \u00e2\u20ac\u00a2  Worked  with  Facets,  e-Billing  and  EDI  HIPAA  Claims  (837/835/834)\n   processing\n \u00e2\u20ac\u00a2 Develop, design & implement department plan to operationalize new  FACETS\n   integrated processing system, to include but not  limited  to,  workflow,\n   management oversight and performance analysis.\n \u00e2\u20ac\u00a2 Translated  business  requirements  into  functional  specifications  and\n   documented the work processes and information flows of the organization.\n \u00e2\u20ac\u00a2 Extensive use of OLAP Function as well as complex joins to  extract  data\n   for reporting purposes.\n \u00e2\u20ac\u00a2 Designed the overall ETL solution including analyzing  data,  preparation\n   of high level, detailed  design  documents,  test  plans  and  deployment\n   strategy.\n \u00e2\u20ac\u00a2 Implemented ETL process to load and extract data.\n \u00e2\u20ac\u00a2 Written Test Cases for ETL to compare Source and Target database systems.\n \u00e2\u20ac\u00a2 Created ETL execution scripts for automating jobs.\n \u00e2\u20ac\u00a2 Created test cases and test procedures for  various  stages  of  the  ETL\n   process including source to extract, source to  staging  and  staging  to\n   target using SQL, Quality Center 10.00 and TOAD for Oracle 10g.\n \u00e2\u20ac\u00a2 Administration of HL7 Interfaces  (XML/CCD,  Demographics,  Appointments,\n   Orders and Results, Billing, HIE) in various environments.\n \u00e2\u20ac\u00a2 Worked in all phases of SDLC in Waterfall  and  Agile  environments  with\n   business stakeholders, platform leads, and test leads  to  define  scope,\n   gather user stories and requirements.\n \u00e2\u20ac\u00a2 HL7 v3 interface engineering to process  ambulatory  and  acute  EMR  HL7\n   messages.\n \u00e2\u20ac\u00a2 Implemented HL7 interfaces to process EMR messages of medical POC  (Point\n   of Care) devices.\u00c2\u00a0\n \u00e2\u20ac\u00a2 Created views to make data available on need to know basis.\n \u00e2\u20ac\u00a2 Created queries to  fetch  data  from  different  tables,  using  derived\n   tables, Sub  queries,  as  well  as  Correlated  queries  on  SQL  Server\n   platform.\n \u00e2\u20ac\u00a2 Performed numerous SQL queries that were highly tuned using concepts like\n   Explain, Stats and CAST.\n \u00e2\u20ac\u00a2 Used SAS to import and export data from and into SQL Server platform.\n \u00e2\u20ac\u00a2 Used UML Diagrams to graphically capture system functionalities.\n \u00e2\u20ac\u00a2  Periodically  Designed  Conceptual,  Logical  and  Physical  Models   to\n   effectively improve Business processes.\n \u00e2\u20ac\u00a2 Modified various data sources,  writing  complex  new  queries  based  on\n   business needs.\n \u00e2\u20ac\u00a2 Maintained departmental reports on daily basis  and  managed  performance\n   issues.\n\nCareSource,                            Dayton,                            OH\n    Oct 2016 \u00e2\u20ac\u201c Jul 2018\nDATA  ANALYST\nI worked as a Data Analyst on Provider  information  delivery  team,  Health\ncare application Dashboard and involved  in  troubleshooting  and  resolving\nerrors in 834 and  820  transactions  for  health  insurance  exchanges  and\nperforming root cause analysis.\nResponsibilities:\n \u00e2\u20ac\u00a2 Analyzed the source data coming from  Legacy  system,  Oracle,  DB2,  SQL\n   server and flat files. Worked with  Data  Warehouse  team  in  developing\n   Dimensional Data Model.\n \u00e2\u20ac\u00a2 Worked on solving the errors of EDI  834  load  to  Facets  through  MMS.\n   Created keyword files to have member data bulk  loaded  into  the  FACETS\n   system through the MMS batch\n \u00e2\u20ac\u00a2 Built a\u00c2\u00a0python\u00c2\u00a0class where the objects were batch jobs depending on their\n   severity.\n \u00e2\u20ac\u00a2 Worked with\u00c2\u00a0data\u00c2\u00a0investigation, discovery and mapping tools to scan every\n   single\u00c2\u00a0data\u00c2\u00a0record from many sources.\n \u00e2\u20ac\u00a2 Optimized the performance of SSIS packages by filtering  data  at  source\n   level, using fast load options and tuning embedded SQL queries.\n \u00e2\u20ac\u00a2 Worked extensively on EDI transactions 837 and 835  Involved  in  writing\n   test cases for different LOB\u00e2\u20ac\u2122s (ITS, FEP and Regular) for  SIT,  Parallel\n   and UAT.\n \u00e2\u20ac\u00a2 Set Claim processing data for different FACETS Module.\n \u00e2\u20ac\u00a2 Analyzing  Medicare,  Medicaid  healthcare  programs  and  performs  data\n   transformations, data manipulations, data validations using SQL,  Access,\n   VLOOKUP, and Index Matches for  dealing  with  data  inconsistencies  and\n   maintain data systems with accurate data.\n \u00e2\u20ac\u00a2 Converted syntax of diverse queries from Oracle to MS SQL Server in order\n   to reach the business requirements\n \u00e2\u20ac\u00a2 Accomplished the data mapping documentations for database updates\n \u00e2\u20ac\u00a2 Experienced in full life cycle MDM development including  data  analysis,\n   database design, data mapping and data load in batch.\n \u00e2\u20ac\u00a2 The data was integrated with the Medicaid data after extensive analysis.\n \u00e2\u20ac\u00a2  Involved  in  Data\u00c2\u00a0Analysis,  Data  Cleansing,  Requirements  gathering,\n   Business Analysis,  Entity  Relationship  diagrams  (ERD),  Architectural\n   design docs, Functional and  Technical  design  docs,  and  Process  Flow\n   diagrams\n \u00e2\u20ac\u00a2 Handled and maintained model versioning using Erwin.\n \u00e2\u20ac\u00a2  Created  entity  /  process  association  matrices,  entity-relationship\n   diagrams, functional decomposition diagrams and data flow diagrams.\n \u00e2\u20ac\u00a2 Analyzing the reference of data which is extract from  source  table  and\n   transform to the target table by using the SQL query.\n \u00e2\u20ac\u00a2 Experiences in Designing, modeling, and creating database and Normalizing\n   or de-normalizing data according to business  requirements  and  Creating\n   Star and snowflake schemas.\n \u00e2\u20ac\u00a2 Designed Enterprise reports using SQL Server Reporting Services (SSRS)\n \u00e2\u20ac\u00a2 Did Transformation and data cleansing activities use various Control flow\n   and data flow tasks in SSIS packages during data migration\n \u00e2\u20ac\u00a2 Created and managed  Event  Handlers,  Package  Configurations,  Logging,\n   System and User-defined Variables for SSIS Packages.\n \u00e2\u20ac\u00a2 Analyzed the FACETS Requirements and conducted gap analysis.\n \u00e2\u20ac\u00a2 Responsible for Medicaid Claims Resolution/Reimbursement for peach  state\n   health plan using MMIS.\n \u00e2\u20ac\u00a2 Used SSIS  and  T-SQL  stored  procedures  to  transfer  data  from  OLTP\n   databases to staging area  and  finally  transfer  into  data  marts  and\n   performed action in XML.\n \u00e2\u20ac\u00a2  Participated  in   Complete   Formal   Design   process   from   initial\n   specifications and requirement. Involved  in  creating  technical  design\n   documentation.\n \u00e2\u20ac\u00a2 Worked with data compliance teams, Data governance team to maintain  data\n   models, Metadata, Data Dictionaries.\n \u00e2\u20ac\u00a2 Analyzed results and EDI ANSI X12 file mapping and reported  on  standard\n   analysis spreadsheet. Reviewed EDI companion guides  for  all  payers  to\n   ensure compliance, edit integrity and maintain up-to-date list  of  payer\n   contacts. Acted as a liaison between client and payer/intermediary.\n \u00e2\u20ac\u00a2 Tested SOAP request and response in XML format using SOAP UI.\n \u00e2\u20ac\u00a2 Extracted data from a data source and performed ad-hoc queries.\n \u00e2\u20ac\u00a2 Extracted data  from  existing  data  stores,  Developing  and  executing\n   departmental reports for performance and response purposes by  using  SQL\n   Server procedures, packages, functions, database triggers\n \u00e2\u20ac\u00a2 Created numerous views with complex SQL statements to retrieve data  from\n   the Tables.\n \u00e2\u20ac\u00a2 Devising a planned approach  to  maintain  the  month  long  segmentation\n   process.\n \u00e2\u20ac\u00a2  Extensively  used  ERWIN  to  design   Logical/Physical   Data   Models,\n   forward/reverse engineering, publishing data model to acrobat  files  and\n   Data Cleansing.\n \u00e2\u20ac\u00a2 Validated the positions and the characters after the data gets translated\n   through TIBCO.\n \u00e2\u20ac\u00a2 Analyze test cases and defects being loaded in HP  ALM  by  QA  teams  to\n   ensure the link entities and accuracy of data.\n \u00e2\u20ac\u00a2  Involved  in  designing,  development  and  testing  of  Interfaces   to\n   communicate with third party data.\n \u00e2\u20ac\u00a2   Prepared   technical   design/specifications   for   data   Extraction,\n   Transformation and Loading.\n\nTUFTS            HEALTH             PLAN,             BOSTON,             MA\nMay 2014 \u00e2\u20ac\u201c Jul 2016\nDATA ANALYST\nTHP is going to build a new system called Health Rules Payer where our  team\nis working on the transformation/conversion of historic data.\nResponsibilities:\n \u00e2\u20ac\u00a2 Worked on the Data Warehouse team analyzing data files  that  had  to  be\n   compiled from disparate  non-production  sources  and  readied  them  for\n   production. Tasks included: comparing data to requirements documentation,\n   creation of data layouts and data dictionary.\n \u00e2\u20ac\u00a2 Coordinates HIE\u00c2\u00a0maintenance events with application  owners  and  partner\n   systems; executes maintenance routines in the system\n \u00e2\u20ac\u00a2 Developed  various  T-SQL  objects  such  as  Functions,  Tables,  Views,\n   Triggers, Indexes, Constraints, Stored Procedures and Queries to be  used\n   in the project.\n \u00e2\u20ac\u00a2 Optimized the performance of queries with modification in T-SQL  queries,\n   creating sub queries, establishing joins and  creating  indexes  wherever\n   necessary.\n \u00e2\u20ac\u00a2  Lead  client  discussions  relating  to  the\u00c2\u00a0Facets\u00c2\u00a0configuration  of  a\n   standard Medicare Advantage plan.\n \u00e2\u20ac\u00a2 Supported new business requirements by extending the functionality of the\n   core Facets system using the Facets extensibility architecture feature.\n \u00e2\u20ac\u00a2 Involved in Facets Implementation, including end to end testing of Facets\n   Billing, Claim Processing and Subscriber/Member module.\n \u00e2\u20ac\u00a2 Receives reviews and enters  data  and  source  documents  into  Medicaid\n   billing system\n \u00e2\u20ac\u00a2 Participated as member of a project  team  performing  various  lifecycle\n   tasks  for  an  intranet  application  with  SQL  Server  2012  database.\n   Developed logical  and  physical  database  models  and  data  dictionary\n   documentation.\n \u00e2\u20ac\u00a2 Created Stored Procedure and Views, Indexes, SQL joins and Sub queries\n \u00e2\u20ac\u00a2 Worked with integration services for transferring and reviewing data from\n   different sources like (Flat file, Excel, CSV)\n \u00e2\u20ac\u00a2 Responsible for acquisition of Medicaid  data  from  MDW  (Medicaid  Data\n   warehouse)\n \u00e2\u20ac\u00a2 Generated Daily, Weekly, Monthly reports for the analysis of managers and\n   end users by using SQL Server Reporting Services.\n \u00e2\u20ac\u00a2 Co-ordinate with different application team to  develop  and  standardize\n   enterprise wide data model, created dimensional data  model  using  Agile\n   Methodology,  Designed  and  developed  dimension  tables,  fact  tables,\n   conformed fact and dimension tables.\n \u00e2\u20ac\u00a2  Analyzed  the  member  eligibility,  plans  including\u00c2\u00a0Medicaid/Medicare,\n   refill, pricing and drug edits.\n \u00e2\u20ac\u00a2 Participate in  weekly  data  analyst  meeting  and  submit  weekly  data\n   governance status\n \u00e2\u20ac\u00a2 Examined existing XML with TIBCO Resources to solve any  existing  issues\n   or provide new direction\n \u00e2\u20ac\u00a2 Data profiling and Data  quality  checks  along  with  integration  rules\n   determination and documentation.\n \u00e2\u20ac\u00a2  Experience   with\u00c2\u00a0data   mining,\u00c2\u00a0data   cleansing,\u00c2\u00a0data   modeling,\u00c2\u00a0data\n   manipulation,\u00c2\u00a0record matching,\u00c2\u00a0table linking,\u00c2\u00a0table structures,\u00c2\u00a0importing\n   and exporting.\u00c2\u00a0\n \u00e2\u20ac\u00a2 Created ETL documentation such as EDI X12  837(P,I),  834  and  835  Data\n   Mapping, Transformation logic for Main Frame Layout, Updating  Meta  data\n   documents for new Platform.\n \u00e2\u20ac\u00a2 Extracted raw data from an Oracle database and used\u00c2\u00a0SSIS to read  it  and\n   run statistical analysis using\u00c2\u00a0SSIS.\u00c2\u00a0\n \u00e2\u20ac\u00a2 Normalization to 3NF/de-normalization techniques for optimum  performance\n   in relational and dimensional database environments.\n \u00e2\u20ac\u00a2 Wrote Test Plans, Test Scenarios, Test Cases and the Test Matrix.\n \u00e2\u20ac\u00a2 Participated in the tasks of data migration from legacy to  new  database\n   system\n \u00e2\u20ac\u00a2 Responsible for creating and modifying the PL/SQL  procedures,  functions\n   according to the business requirement.\u00c2\u00a0\n \u00e2\u20ac\u00a2  Re-engineered  and  captured  EDI  transactions  with   legacy   systems\n   [Enrollment -834, Eligibility Transaction (270/271), Claims (837),  Claim\n   Status Request and Response (276/277), Remittance (835)].\n \u00e2\u20ac\u00a2 Created SQL Server Reports, handling sub-reports and writing  queries  to\n   perform drill down, drill-through operations in SSRS.\n \u00e2\u20ac\u00a2 Used shell commands to create serialized C# class files  from  XML  based\n   schemas Involved in performing the  unit  testing,  Regression  and  User\n   Acceptance Testing.\n\nPremera            Blue            Cross,            Seattle,             WA\n                                                      Mar 2013 \u00e2\u20ac\u201c Mar 2014\nData Analyst\nThe project deals with Mainframe Applications that has  different  types  of\nTransactions that supports the HC - Claims Adjudication  Process  System.  I\nworked as a Sr.  Data  Analyst  for  the  Meredian-Accums  Project  for  the\nconversion of claims to FACETS.\nResponsibilities:\n \u00e2\u20ac\u00a2 Involved in Planning, Defining and  Designing  database  using  Erwin  on\n   business requirement and provided documentation.\n \u00e2\u20ac\u00a2 Installed SQL Server 2008 R2 and Management tools using SQL Server  Setup\n   Program.\n \u00e2\u20ac\u00a2 Created SSIS packages to extract data  from  OLTP  to  OLAP  systems  and\n   scheduled Jobs to call the packages and Stored Procedures.\n \u00e2\u20ac\u00a2 Applied various data  transformations  like  Slowly  Changing  Dimension,\n   Aggregate, Sort, Multicasting, Conditional Split, Derived column.\n \u00e2\u20ac\u00a2 Developed SSIS packages for ETL\u00c2\u00a0to migrate data  from  different  sources\n   including  insurance  groups  etc  to  the  data   Involved   in   FACETS\n   configuration, Customization, reporting,  analysis  and  enhancement  and\n   worked on membership, claim module, Batch Processing, Pricing Module.\n \u00e2\u20ac\u00a2 Coordinated with the developers and IT architects to design the interface\n   of the new system according to the X12 (270,  276,  278,  834,  835,  837\n   (I,P,D) and 820) standards.\n \u00e2\u20ac\u00a2 Performed  end  to  end  analysis,  design,testing,  implementation,  and\n   support for enhancements of complex systems utilizing Waterfall SDLC.\n \u00e2\u20ac\u00a2 Detailed Analysis of the HIPAA 4010, 5010 along with the 6020 version  of\n   the\u00c2\u00a0834\u00c2\u00a0and\u00c2\u00a0820Companion guides.\u00c2\u00a0\n \u00e2\u20ac\u00a2 Created the logic for flat  files  and  837I,  837D,  837P  according  to\n   business requirements and documented in FRD.\n \u00e2\u20ac\u00a2 Supported all healthcare workflows through HL7.\n \u00e2\u20ac\u00a2 Performed  manual  testing,  including  validation/smoke  testing  of\u00c2\u00a0HL7\n   interface messages on each new build before  delivering  to  the  quality\n   assurance team.\n \u00e2\u20ac\u00a2 Perform the system testing to see whether FACETS is passing the data thru\n   Application Enablement to Provider Portal or Member Portal as per need or\n   not.\n \u00e2\u20ac\u00a2 Validated that the 270/271 generated  is  in  accordance  with  the  5010\n   implementation guide.\n \u00e2\u20ac\u00a2 Wrote T-SQL queries, Stored Procedures and used them to build packages.\n \u00e2\u20ac\u00a2 Handled slowly changing dimensions to maintain the history of the data.\n \u00e2\u20ac\u00a2 Followed HL7 standards to match the  International  Standard.  Documented\n   the Functional Specifications Document using Standards of HL7.\n \u00e2\u20ac\u00a2 Involved in FACETS configuration, Customization, reporting, analysis  and\n   enhancement  and  also  worked  on  membership,   claim   module,   Batch\n   Processing, Pricing Module.\n \u00e2\u20ac\u00a2 Have done analysis on HL7 that establishes criteria for representing  and\n   communicating data associated with health care.\n \u00e2\u20ac\u00a2 Coordinated the efforts  to  implement  appropriate  HL7  changes  on  an\n   ongoing basis.\n \u00e2\u20ac\u00a2 Analyzed the change  detection  process  on  FACETS  database  tables  to\n   capture  the  daily  changes  done  by  Users   through   Online   FACETS\n   Application.\n \u00e2\u20ac\u00a2 Responsible for optimizing all indexes, SQL queries, stored procedures to\n   improve the quality of software.\n \u00e2\u20ac\u00a2 Designed, reviewed, and created primary objects such  as  views,  indexes\n   based  on  logical  design  models,  user   requirements   and   physical\n   constraints.\n \u00e2\u20ac\u00a2 Designed, Developed and Deployed reports in  MS  SQL  Server  environment\n   using SSRS-2008 R2.\n \u00e2\u20ac\u00a2 Generated Sub-Reports, Drill down  reports,  Drill  through  reports  and\n   Parameterized reports using SSRS.\n \u00e2\u20ac\u00a2 Created reports to retrieve data  using  Stored  Procedures  that  accept\n   parameters.\n \u00e2\u20ac\u00a2 Used SQL Server profiler for auditing  and  analyzing  the  events  which\n   occurred during a particular time horizon and stored in script.\n \u00e2\u20ac\u00a2 Planned a complete backup on the  database  and  restored  database  from\n   disaster recovery.\n\n\n",
          "confidence": 1.0,
          "method": "full_text",
          "structured_data": null
        },
        "tax_term": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "source_by": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "designation": {
          "value": "Data Analyst",
          "confidence": 0.9,
          "method": "ner",
          "structured_data": null
        },
        "experience": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        }
      }
    },
    {
      "resume_path": "data\\input\\Help Desk - VA - Andre Potts.doc",
      "confidence_score": 0.7500000000000001,
      "used_ocr": false,
      "extracted_fields": {
        "skills": {},
        "first_name": "Andre",
        "last_name": "L. Potts",
        "primary_email": {
          "value": "andrepotts1234@aol.com",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "secondary_email": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "phone": {
          "value": "5716068186",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "city": {
          "value": "Woodbridge",
          "confidence": 0.7,
          "method": "ner",
          "structured_data": null
        },
        "state": {
          "value": "VA",
          "confidence": 0.8,
          "method": "city_database",
          "structured_data": null
        },
        "zip": {
          "value": "16756",
          "confidence": 0.7,
          "method": "regex",
          "structured_data": null
        },
        "work_authority": {
          "value": "US Citizen",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "resume_link": {
          "value": "data\\input\\Help Desk - VA - Andre Potts.doc",
          "confidence": 1.0,
          "method": "file_path",
          "structured_data": null
        },
        "raw_resume": {
          "value": "\n\n                             Andre L. Potts, MCP\n                   16756 Flotilla Way Woodbridge, VA 22191\n                           Cellular:\u00c2\u00a0 571.606.8186\n                       Email:\u00c2\u00a0 andrepotts1234@aol.com\n\n                                 Clearance:\n                             DOD Top Secret \u00e2\u20ac\u201c June 28, 2019\n                  DOD Top Secret \\ SCI \u00e2\u20ac\u201c Granted July 2019\n\nFull legal name: Andre Potts\nCurrent Location (with zip code): 2O774\nWork Authorization: US Citizen\nCurrently working on Contract/Fulltime:\nCurrent project end date: Oct 2021\nExpected per Hr. pay Rate:\nBest time to reach:\nVaccination - Yes\n\n $30/hour on 1099 (30 Min commute)\n\nCareer Summary\n\nOver 20 years of experience as a Network, Desktop, Helpdesk and Migration\nLead.\u00c2\u00a0 Additional experience includes over 8 years as a System\nAdministrator with an extensive background in networking technology and\ncustomer relations.\u00c2\u00a0 Experienced in planning, designing, implementation and\nmanagement of Windows 7, 8 and 10. Windows Technology XP and 2003, NetWare\n3.x/4x networks.\n\nTechnical Skills\n\nConfiguration of Vista, Windows XP, 2000, NT, Windows 7, 8 and 10; MS\nOffice 2003 and MS Outlook; AT&T 7290 Blackberry; MS VPN; Remedy 5.0;\nWindows; Active Directory; HP Jet Direct; Symantec Ghost image 7.5; Net IQ;\nNetWare 3.x, 4.x, 5.0; Dell Server and Workstations 2003 & 2008; Compaq\nServer and workstations; RAS, PAL and VPN, VM Ware IPad\n\nCertifications\n\nITIL v3, MCP (Workstation and Server) A+, Security+ - 12-2020 (Dell,\nCertification on Dimension, Optaplex and Latitude Professional Workstation.\n\nEmployment History\n\nMetronome \\ State Department \u00e2\u20ac\u201c Senior Computer Administrator      Jan. 2020\n\u00e2\u20ac\u201c  Oct 2021\n\n    \u00e2\u20ac\u00a2 Created user and admin accounts with the aid of Active Directory\n    \u00e2\u20ac\u00a2 Assist teleworking clients with connection and general computer\n      problem.\n    \u00e2\u20ac\u00a2 Assist client at the shop with classnet \\ jwic\n    \u00e2\u20ac\u00a2 Support client with computer need with major application\n    \u00e2\u20ac\u00a2 Created clients email accounts in exchange for all 3 networks\n    \u00e2\u20ac\u00a2 Utilize ITIL best practices to enhance and optimize the services\n      provided to end users\n\n\n\n\n\n\nSSI \\ JSP DOD \u00e2\u20ac\u201c Senior System Engineer Support          Oct. 2018 \u00e2\u20ac\u201c Jan\n2020\n\n    \u00e2\u20ac\u00a2 Lead a team of Deskside Support Technicians providing technical\n      support for system users.\n    \u00e2\u20ac\u00a2 Assist in Creating work schedules, updating the front-end voicemail\n      when necessary, complete the daily attendance and shift handoff\n      reports, and provide ideas for improvement in daily operations of the\n      client Service Center.\n    \u00e2\u20ac\u00a2 Assist in the development of performance measurements\n      (goals/metrics/reports) utilizing the current service desk platform\n      and facilitate a feedback system to team members on issues such as\n      customer service, communication, and technical skills, in order to\n      enhance the quality of support delivered.\n    \u00e2\u20ac\u00a2 Utilize ITIL best practices to enhance and optimize the services\n      provided to end users\n    \u00e2\u20ac\u00a2 Complete and provide regular Service Desk metrics reports\n    \u00e2\u20ac\u00a2 Expert knowledge in the implementation of an enterprise knowledge base\n      and knowledge management best practices\n    \u00e2\u20ac\u00a2 Coordinate with IT support teams to resolve customer\n      incidents/problems, fulfill service requests, and institute continuous\n      service improvement initiatives and methods.\n    \u00e2\u20ac\u00a2 Resolve customer issues effectively or escalate them to appropriate\n      support tiers.\n    \u00e2\u20ac\u00a2 Work with management in assessing staff performance/reviews/changes.\n    \u00e2\u20ac\u00a2 Assist in the management of Service Desk resources for optimal\n      performance.\u00c2\u00a0\n    \u00e2\u20ac\u00a2 Assist in the professional and technical development of the team\n    \u00e2\u20ac\u00a2 Meet customer requirements for Service Desk performance.\n\n\n\n\nSAIC / State Department \u00e2\u20ac\u201c IT Operations Watch Office Analyst        Sept.\n2015 \u00e2\u20ac\u201c Oct. 2018\n\n    \u00e2\u20ac\u00a2 Monitor network and server performance; assess potential problems\n         o Networks, Applications, Hosting, Voice, and security\n         o Work with Service Operations team staff during the installation,\n           upgrade, or decommissioning of infrastructure equipment or\n           software.\n    \u00e2\u20ac\u00a2 Monitors and analyzes critical incident alerts regarding outages or\n      degradation in service affecting the enterprise using monitoring\n      tools.\n         o NeuralStar\n         o vCenter\n         o SCOM\n         o Other Enterprise Available Tools\n    \u00e2\u20ac\u00a2 Assist with the development and maintenance of standard operation\n      monitoring procedure\n    \u00e2\u20ac\u00a2 Provide in-depth analysis of nework performance alerts on the\n      messaging and management\n    \u00e2\u20ac\u00a2 Networks and well as associated servers and applications\n    \u00e2\u20ac\u00a2 Assist with Major Incident Severity Level calls (standing up\n      conference call, tracking down service line resources, creating a\n      timeline of events)\n    \u00e2\u20ac\u00a2 Preparing, updating, and reporting Situational Awareness to the IT\n      Operations Center Watch Officer\n\nClearbridge /AT&T/State Department    Network and Port Validation\nMarch 2014 \u00e2\u20ac\u201c Sept. 2015\n \u00e2\u20ac\u00a2 Responsible for providing on-site assistance.\n \u00e2\u20ac\u00a2 Participate in the assessment, inventory, and circuit tracing and desktop\n   support.\n \u00e2\u20ac\u00a2 Facilitate office and network distribution closet equipment.\n \u00e2\u20ac\u00a2 Physical installation of desktop experience, office systems and network\n   equipment including all telecom and network,  cabling cat 5 and 6.\n \u00e2\u20ac\u00a2 Inventory tracking and management for initial survey and when equipment\n   is  added or removed.\n \u00e2\u20ac\u00a2 Create and/or update as-built site documentation and inventories.\n \u00e2\u20ac\u00a2 Configure and patch management of servers, switches, routers and other\n   network devices with remote assistance from engineers.\n\n\n\nAvenion / USMC    OCIO VIP Support Analyst April 2012 \u00e2\u20ac\u201c October 2013\n\n1. Participate in system implementation and all team related activities.\n2. Interface with the federal client on base daily.\nResponsible for conducting a preliminary survey before each tech refresh.\n4. Effectively communicate any issues that could impede the success of the\n   refresh.\n5. Work with VIP\u00e2\u20ac\u2122s and contractor PM\u00e2\u20ac\u2122s before beginning the work.\n6. Responsive to all calls and other activities surrounding the Help Desk\n   (described in detail below).\n7. Facilitate a resolution to any problems.\n8. Track and process system file interfaces for external stakeholder\n   entities.\n9. Distribute strategic correspondences to internal and external\n   stakeholders to communicate system outages and other significant issues.\n10. Update and distribute weekly minutes and agenda for the Operations\n   Status meetings (occurs 3 times per week).\n11. Quality control and data entry for all new/modified user access\n   request.\n12. Monitor operations email inbox.\n13. Create, update and maintain Help Desk tickets, report status of open\n   Help Desk Tickets.\n14. Archive completed user access forms on a weekly basis.\n15. Interface with the team Database Administrators.\n\n16. Provide Tier II support per request from various constituencies.\u00c2\u00a0\n   Investigate and troubleshoot issues.\n\n17. Perform daily system monitoring, verifying the integrity and\n   availability of all hardware, server resources, systems and key\n   processes, reviewing system and application logs, and verifying\n   completion of scheduled jobs such as backups.\n\nRemote into system using Borgate, configure VPN, IPAD\u00e2\u20ac\u2122s and Blackberry\nServe as 2nd tier support on the Service Desk team which is responsible for\nthe triage and resolution of a wide variety of IT issues and requests.\n\nResponsible for monitoring enterprise performance, conducting audits and\nresolving complex problems related to network and desktop equipment and\nsoftware applications.\n\nTroubleshoot and utilize problem solving skills and previous experience\nmaintaining and supporting complex systems that are deployed nationally.\n\nWyle/Insight Global/Pentagon - Windows Refresh Analyst   September 2011 \u00e2\u20ac\u201c\nFebruary 2012 (temp assignment)\n21. Performed basic system administrator duties.\n22. Created user accounts.\n23. Disabled accounts in AD (Active Directory).\n24. Placed user in the correct OU (Organizational Unit).\n25. Removed workstation in and out of the correct containers.\n26. Closed, updated and produced Remedy Daily Report.\n27. Added user on and off the domain.\n28. Managed exchange mailboxes, configured mailboxes on user workstations.\n29. Created user mailboxes for Outlook 2010.\n30. Reemerged Windows 7 Laptops using the Ghost 7.5 system from XP, Windows\n   2000.\n31. Back-up workstation using Windows Migration by retrieving data off the\n   machines and storing in a container on the server.\n32. Migrate from Windows XP to Windows 7, configured the user desktop.\n33. Troubleshot tickets using Remedy Ticketing System and new issues with\n   re-image Migrate 7 migration tool.\n\nATR/General Dynamics/FBI System Installer and Network Engineering (3\nmonths) tasks March 2011 \u00e2\u20ac\u201c September 2011\n\nResponsible for conducting tech refresh and run preliminary survey report\nbefore each.\n35. Effectively communicate any issues that may harm the success of the\n   refresh.\n36. Work with contractor PM\u00e2\u20ac\u2122s before beginning the work.\n37. Made sure replacement components are in place before replacing CISCO\n   switches and routers.\n38. Installed all upgraded components and switch routers to ensure a smooth\n   transition with the taclane.\n39. Performed installs of Dell Desktop, Laptops and office Devices.\n40. Installed workstation in SIPRNET & NIPRNET environment.\n41. Troubleshot system for any malfunction.\n42. Install and update patches and upgrade of Share Point.\n43. Update and data entry in Share point.\n44. Administrator Share Point install, configure and monitor.\n45. Ran trouble tickets from help desk Remedy ticketing system.\n46. Back up user data for deployment.\n47. Supported trouble tickets from helpdesk Tier II and Tier III.\n\nFYI/General Dynamics/Coast Guard    System Engineer   June 2010 \u00e2\u20ac\u201c March\n2011 Transfer\n\n48. Perform Active Directory function in a SIPRNET & NIPRNET environment.\n49. Support user on base of over 2000 user and active DOJ Employees.\n50. Help Desk environment Tier I and Tier II tech using Remedy ticket\n   system.\n51. Resolved user issues from a Tier II \u00e2\u20ac\u201c Tier III aspect ticketing system.\n\n\n52. Testing application, Run Gold Disk and run report on all pre-installed\n   application.\n53. Administer Vista and XP desktop OS for users and test lab, Administer\n   Server 2003, 2007 and 2008 OS.\n54. Execute patches and updates on Windows enterprise environment via WSUS.\n55. Ran complete and full report of results of updates and patches every\n   month.\n56. Document work performed and disseminate to Administrator and Engineers.\n57. Administer a SIPR environment in a VMWare ESX network.\n58. Execution of application management utilizing WISE.\n59. Submit and execute all applications through SMS, package application in\n   Win-zip and deploy thought SMS.\n\nTech-System / Lockheed \u00e2\u20ac\u201c Martin/Air Force\u00c2\u00a0System Engineer    June 2009 \u00e2\u20ac\u201c\nJune 2010\n\n       Environmental, Vulnerability and Mitigation Engineer (EVM) Team\n60. Executed patches and update on Window environment enterprise.\n61. Monitored and created tickets thought Remedy ticketing system.\n62. Assisted with Information Assurance Vulnerability Assessment (IAVA)\n   efforts.\n63. Administer Vista, XP and Windows 7 desktop OS.\n64. Performed Active Directory function and remote connection through SMS\n   in a Siprnet & Niprnet environment.\n65. Planned TCNO implementation strategy, packaged and deployed TCNO\n   patches.\n66. Implemented TCNO instructions throughout the enterprise, documented\n   TCNO implementation IAW AFI 33-138 and advised VAS when implementation\n   has been completed.\n67. Tested and evaluated TCNO for effect on network.\n68. Assigned resources to perform patch application management.\n69. Provided updates during weekly status meeting as a key contributor.\n\nPerot Systems; /BTA/DFAS System Administrator II/Blackberry Enterprise\n   Administrator: January 2007 \u00e2\u20ac\u201c June 2009\n70. Served as 2nd tier support on the Service Desk responsible for the\n   resolution of a wide variety of IT issues, requests and operating system\n   upgrades for the Business Transformation Agency (BTA), a DOD entity.\n71. Responsible for monitoring enterprise performance, conducting audits\n   and resolving complex problems related to network and desktop equipment\n   and software applications.\n72. Performed daily check of the backup system (Symantec) and\n   troubleshooting any failed scheduled backup.\n73. Served as a System Administrator in a test lab environment (DOD\n   SIPRNET& NIPRNET environment).\n74. Assisted with Information Assurance Vulnerability Assessment (IAVA)\n   efforts.\n75. Responded with technical assistance to requests from walk-ins, callers\n   and remote site users as required.\n76. Maintained data files and control procedures for a system of 20\n   networked servers with a Windows, UNIX, Linux and VMware mix.\n77. Monitored use of resources, back-up files and responded to management\n   requests for information.\n78. Simulated or re-created user problems to resolves use of problems\n   management database and helpdesk systems.\n79. Provided configuration support of laptops, blackberry\u00e2\u20ac\u2122s, cell phones\n   and VPN clients for on/remote site users as required.\n80. Responsible for the in-house network and communication environment;\n   controlled the system inventory of all equipment, software and licenses\n   to all on and off site employees.\n81. Provided server and system support on a Wireless network and configured\n   all equipment issued clients with Vista operation system as the standard\n   also configured VMware.\n82. Developed standard weekly reports as required regarding progress and\n   attended weekly meeting via phone.\n83. Installed and maintained DOD software applications while utilizing\n   appropriate IT procedures, standards, and guidelines for all tasks\n   performed.\n84. Responsible for 24 hour technical support via a DOD provided pager on a\n   rotational schedule.\n85. Managed the IT Service Center Call Tracking System, (REMEDY ver. 5.0)\n   by removing in user system with the use of SMS Client.\n86. Installed existing or new systems, including initial software loading\n   and configuration.\n87. Diagnosed issues related to the operating system, modems, printers,\n   network interfaces, software applications and other system level\n   problems.\n88. Coordinated with network technicians and self-maintenance groups to\n   install, trouble shoot and relocate computer related equipment and\n   peripherals.\n89. Oversaw maintenance of User Accounts as applicable.\n90. Supported local networking, telephone, and voicemail equipment to\n   include additions, moves and changes for remote BTA locations.\n91. Responsible for the Oversight and execution of LAN/Server migrations.\n92. Configured and loaded the software on a 6400 Cisco router\n93. Active Directory was used to create and managed user accounts.\n94. Managed mail accounts using Exchange 2003\n\nEducation\nColorado Technical University, CO (On-line)  February 2011 - Present\nUniversity of Phoenix, Alexandria, VA,  October 2004 \u00e2\u20ac\u201c 2006\nLiming Computer School Stafford, VA.\n                          June 2000 \u00e2\u20ac\u201c 2001\nTEST Technology College, Hyattsville MD,\n                  September 1986 \u00e2\u20ac\u201c October 1988\n\n\n\nTraining\nBasic Program Management, CBT, Unisys University\nEffective Communication, CBT, Unisys University\nNovell 3.x, 4.x Operation and Networking Essentials, Crystal City, VA\n\nSecurity + Certification 12- 2020\n\nSkill\nPossess ability to troubleshoot and utilize problem solving skills, with\nproficient experience in maintaining and supporting complex systems that\nare deployed nationally.  Possess working knowledge of Helpdesk systems,\nand a working knowledge of accounting practices.  1-3 years\u00e2\u20ac\u2122 experience\nwith the latest MS Office suite 2010, 2007.  Excellent communication\nskills, including strong writing skills, and experience writing emails and\nupdating technical documentation with expert attention to detail.\n\nREFERENCES AVAILABLE UPON REQUEST\n\n",
          "confidence": 1.0,
          "method": "full_text",
          "structured_data": null
        },
        "tax_term": {
          "value": "1099",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "source_by": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "designation": {
          "value": "Senior System Engineer",
          "confidence": 0.9,
          "method": "ner",
          "structured_data": null
        },
        "experience": {
          "value": "20",
          "confidence": 0.9,
          "method": "regex_total_experience_summary",
          "structured_data": null
        }
      }
    },
    {
      "resume_path": "data\\input\\.Net  Architect - DE - Muthu.docx",
      "confidence_score": 0.31500000000000006,
      "used_ocr": false,
      "extracted_fields": {
        "skills": {},
        "first_name": "",
        "last_name": "",
        "primary_email": {
          "value": "muthukumar@consultant.com",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "secondary_email": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "phone": {
          "value": "4433161103",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "city": {
          "value": "",
          "confidence": 0.0,
          "method": "ner",
          "structured_data": null
        },
        "state": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "zip": {
          "value": "",
          "confidence": 0.0,
          "method": "regex",
          "structured_data": null
        },
        "work_authority": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "resume_link": {
          "value": "data\\input\\.Net  Architect - DE - Muthu.docx",
          "confidence": 1.0,
          "method": "file_path",
          "structured_data": null
        },
        "raw_resume": {
          "value": "Introduction\nKumar is an Enterprise and Solution Architect / Senior System Architect / Certified Solution consultant (SME) with, around 20 years of architectural, systems analysis and development experience in Enterprise & Solutions Architectures, Technical Evaluations and Solutions using technologies such as products and enterprise wide solutions using Java, J2EE, Python, API Gateways, Visual C# /.NET / in Unix, Windows, IBM, Mobile iOS, Android, Windows CE, Linux, Hadoop & Cloud such as AWS and Azure Cloud environments. \n\nHas extensive experiences in AS-IS analysis, Transition Architectures and Future State architectures using TOGAF and FEAF\nSuccessfully worked on CBA, TCOs, Budgets, TBM, Asset Managements and Cost modellings\nInvolved in Technical Proposals, Pricing, Acquisitions, RFIs and RFP activities\nWorked extensively on Technical evaluations, Solutions proposals, Implementations planning and design artifacts\nWorked on CCBs, Risk Assessments, Controls, Oversight on SDLC, Agile and SAFe 4.0 & 4.5\nWorked on Data-center operations including Cloud and Government Datacenters and focused on networking operations, monitoring systems and applications\nHave good working knowledge on BI tools within the Datacenter using analytics, logger functions and enterprise services management\n Applied Architectural principles and produced successful solutions with, design pattern modeling, analysis, & programming SME for real time in enterprise and public facing production systems on varied local and international projects.  \nWell-versed in programming API\u2019s and module implementation to support mobile, wireless programming and communications with Prototyping\nHas extensive work in .NET, Java, MS C#, VC++, NET Components, jQuery, Test Driven Development, Pre-Sales, JSON, HTML5, MVC, MVVM, COM/COM+, ADO (.NET), BizTalk, OLEDB, MQ Series, DCOM, MFC, ATL, MTS, IIS, UML, Rabbit MQ, Spring Boot, Puppet, Chef, ADO/DAO, SOA, OLE Automation, MS Agent (Speech API), Virtual machines, Docker, Synchronize / A-synchronize Programming, XML, PL/SQL, ETL, Micro Services, Data Modeling, Ruby, Meta data, Object Oriented Analysis and Object Oriented Design using Visio / Rational Rose Products.\nWith better communication and leadership skills, could handle any project in N-tier, multi-tier, SOA, Domain and Window DNA or Complex architectures. \n\nEducational Qualifications and Certifications\nMaster\u2019s in Business Administration, UMUC, MD\nMasters in Computers, Bharathidasan University, India\nHonors Diploma in Systems Management, NCC, UK\nCertified Application Developer, MCAD, Microsoft, 2003\nProject Management Professional (PMP), PMI, 2008\nInformation Technology Infrastructure Library v3 Foundation (ITIL), 2011\nAWS Certified Solutions Architect - Associate (2013)\nAWS Certified Solutions Architect \u2013 Professional (2018)\nAWS Business Professional Accreditation (2013)\nAWS Technical Professional Accreditation (2013)\nTOGAF Certified Enterprise Architect (2016)\nAzure Associate MCP (2017)\n\nDevelopment Toolset:\nMicrosoft Visual C# 2013, 2012, 2010, 2008, 2005, Microsoft Visual C++ 2008, 6.0, 5.0, 4.0,1.52, Visual Basic 6.0, 7.0, Android 1.x, 2.x, 3.x, 4.x, iPhone, iPad, iPod Touch, Azure, Amazon Web Services (AWS), Active Server Pages (ASP 2, 3), BizTalk 2004, 2006, MTS, Business Objects 5.x, MS Share Point, MFC/ATL, COM / COM+/Serviced Components, Managed Services and Components, Threads, Sockets, Events, OLE Automations, OCX, ActiveX, Active Reports, Crystal Reports, Microsoft Agent, Managed Controls, SDK, J-Script/JSP, AJAX, Server-lets, J2EE, J2Me, MOSS, WSS, SharePoint, VBScript, HTML, XML, XSD, XSLT, XPath, WebSphere, UML, C, C++, Java, PKI, Digital Certs, RSA, SHA128, html5, jQuery, JavaScript, JSON, Arc , ESRI Maps\n\nDatabases:\nOracle (7.3, 8i, 9.0, 10g, 11g), Sybase (11.x, 12), SQL Server 6.5, 7.0, 2000, 2008, 2012, Informix 9.0, ActiveX Data Objects (ADO 2.5, 2.6), ADO.net, SQL Plus and MS Access with ODBC.\n\nDevelopment Platforms:\nMicrosoft Windows XP Professional, Windows Vista, Windows 7, 8.1, 10, 2008, 10 Servers, 2000 Professional, NT 4.0/98/95, Windows CE, iOS 5, Windows Tablet PC,  Unix [HP-UX (10.x, 11), Solaris (8x), AIX (4.3.x) SVR4, Linux (2.4.x), SCO (3.2.x)], VMS, Windows 3.11 Workgroup, DOS and OS/2, Internet Information Server (IIS), Apache/Tomcat, WebSphere, Netscape Enterprise, IBM MQ Series\n\nDesign Tools & Methodologies:\nVisual Studio UML, SAFe 3.x, 4.5, TOGAF, Zachman, Visio, Rational Rose, Rational Clear Case, OOA, OOD, OOP, Rational ClearQuest, RequisitePro, Visual Modeler, Visio, UML, Booch, WinRunner, LoadRunner, Sheridan, Infragistics Controls, Bounds Checker, Clear case, Install Shield, PVCS, VSS, RCS, SCCS, Tivoli, Python, Perl, Verisign, WLBS, UDP, LDAP, TCP/IP, 802.11b, CDPD, Toad, ISO, CMMI process methodology, NLB, IIB, Layer 7,ISO, MVC (model-view-controller), Agile (XP, Kanban, SCRUM), TDD, Continuous Integration, Continuous Delivery \n\nIndustry Experience:\n-Financial Management\t\t\t- Security Management\t   -Payroll\n-Claims Management/Insurance Industry\t- Manufacturing\t  \t   -Hotel Management\n-Hospital Management\t\t\t- Communication/Mobile     -Federal Government \n\nPROFESSIONAL EXPERIENCE\n\nGSA, Treasury, USDA, HUD Coe\u2019s \tJul 14 \u2013 Till Now\nEnterprise Solutions Architect, SME Consultant\n\nAs a Chief Enterprise Architect, responsible for enterprise portfolio of applications.  I have worked on multiple systems and integrated many applications with technical systems such as Enterprise, Mobile, PEGA, JAVA, J2EE, Spring Framework & Technologies including SharePoint 2010-2013, API Gateway, Python, Enterprise Service Bus, .net 4.0, 4.5, 4.8, .net core 1.8 and 2.0 Visual Studio 2010, 2015, 2019 MS-SQL Server 2008, XML, JSON, REST API, JavaScript and Objective C, Business Objects. I worked independently and as a part of the team that builds and integrates interactive Web sites, applications, and services for both internal and public sites.  Worked on the latest version of the mobile and deployed using Samsung Knox security suite for Androids.  My technical domain includes the ability to architect websites, designing data-driven applications, and developing efficient client-server solutions & systems administrations in a cloud and hybrid environment.\n\nCloud Centers of Excellence \u2013 Cloud CoE \u2013 Senior Solutions Architect\nWorked as Senior Solutions Architect for the GSA Centers of Excellence\nWorked in all aspects of Phase I of Cloud Adoptions in AWS and MS Azure Platforms\nCreated and implemented TBM as part of GSA CoE.\nWritten multiple proofs of concepts in Azure, AWS and Serverless Architectures\nWorked on Hadoop Stack, EMR, HDInsight big data architectures\nWorked on HDP platform using Java, Python, HDFS, HBase, Hive, Nifi, Spark, Solr, Impala\nEnterprise Architect \u2013 EA Role\nPerformed extensive work on As-Is analysis, Transition architecture and Future-State architecture\nWorked on Cost-Benefit analysis (CBA), TCO, budgeting and cost modelling\nWorked on Technical recommendations, evaluations and proposals\nWorked on Risk management, Control implementations and Change control boards\nArchitected and Recommended solutions and implementation plans for mobile and desktop cross device cross platform browser-based application architecture using iPhone/iPad/iPod touch iOS devices and android devices and .NET windows nodes.\nAWS, Azure, SOA, PEGA & Admin Role:\nWorked on the team that helped RATB in moving enterprise systems to Amazon cloud technology (AWS) solutions to improve its security and agility, consolidate systems, and streamline IT infrastructure and overhead costs. Worked on Service Oriented Architecture using REST and SOAP methods.  Worked on end-points using Mule ESB and worked on various Enterprise Integration Patterns.  Used various conversions methods using J2EE and .NET to various conversion formats such as XML, JSON, CSV etc., Worked on agency\u2019s cloud design includes leveraging Microsoft Office 365 for messaging services, and Amazon\u2019s virtual private cloud (VPC) for all other enterprise services. \nWork on the mobile product and architect them for the app store\nWorked on CA API Gateway Design/Architecture\nWorked on Python, J2EE/.NET application Architected, designed and developed solution with Micro services\nWork on mobile, web trends and cloud integration using Java/.NET Suites\nHands-on on PEGA PRPC, Data Pages, UI/UX with Services and Agents\nWorked on multiple Share Point Projects, services to deploy web application and Migration\nWork on the mobile product and deploy them on the app store\nExperience in ESB and message integration techniques, like aggregation, transformation, dynamic routing, content-based routing, re-sequencer, etc., using Mule ESB\nExperience in scalable Mule flows, error handling etc.\nExperience in the design and implementation of Mule transactions, Async flows, message filters, message translators, content enricher and integration patterns\nExperience in the development of testing suites using Mule MUnit and JMeter to test regular mule flow\nExperience with CI/CD tools such as Maven, Jenkins, Gradle, Artifactory, Github, etc.\nWorked on Share Point Projects, services to deploy web application and Migration\nExperience with version control systems such as SVN, CVS, Harvest, and IBM ClearCase\nExpertise working with API Gateway (AWS, Ax-way and CA APIs), Runtime Manager, etc.\nWork on mobile and cloud integration\nInstall and maintain Microsoft Windows 2008 R2, IIS, SP2010, Project Server 2010, Site Collections and COTS Software regular upgrades and applying patches\nInstall and maintain DNS Services (Primary and Secondary DC) and manage Windows 2008 R2 Server admin accounts\nMaintaining and managing firewall rules on servers running on the Amazon EC2\nSystems and applications monitoring IIS Logs, SQL Logs, alerts notification, issues identification and corrective actions \nSalesforce.com\nDeveloped custom code, user interface, business applications, shared rules on Salesforce.com platform to facilitate reporting and enhanced security\nDesigned and implemented dashboards, dynamic dashboards and tailored reports for sales, governor, DevOps, and finance departments\nLeveraged clients existing IT framework/CRM system for better data visibility and system integration.\nWorked on controllers, messaging, service facets of the Apex\nAssisted in developing Project Scope, Business Requirements and Functional Requirements with internal team and clients.\nFacilitated the creation of test scenarios and test scripts and worked with data maintenance\nEnvironment: \nMicrosoft .NET Framework 3.5, 4.0, Java (Eclipse) J2EE, PEGA 7.x, J2Me, Python, TOGAF, Microsoft Visual Studio 2010-2019, SOA, Microsoft SharePoint 2010, Windows Web development with C#, C++, Docker, Objective C, SQL Server 2008, CSS, JavaScript, JSON, jQuery, dojo, XML, XHTML, XPATH, Adobe Flash, , ASP.net 4.0, ADO.net, Puppet, Chef, Rabbit MQ, Knox, Amazon Web Services (AWS), EC2, VPC, VPN, F5, Jenkins, Salesforce.com, ArcGIS, ESRI GIS Integration, OS X (Lion, Mountain Lion), iPad, iPod Touch and iPhone, iOS 4.x, 5.0, 5.1, Windows 2008, Java, C#, JavaScript, Kafka, Hadoop, HBase, Hive, Nifi, Solr, Spark, HDInsight, Apache Tomcat, Android, iPad, iPod Touch and iPhone iOS 4.x, 5.x, 6.x, CRM, Knox Security, Knockout, MVVM, WCF, Objective C\n\nRecovery.gov, Treasury.gov, USASpending.gov, \tJun 12 \u2013 Jul 14\nSolutions Architect, Integrator, Consultant\n\nAs a technical Solutions consultant, have worked on the systems and integration in Mule Soft ESB, Mobile, JAVA, J2EE, Spring Framework, HA Hadoop, Amazon Web Services (AWS) and Open Stack cloud & Microsoft Technologies including SharePoint 2010-2013, .net 3.5, 4.0, Visual Studio 2010, MS-SQL Server 2008, XML, JSON, REST API, JavaScript and Objective C, Business Objects.  I worked independently and as a part of the team that builds and integrates interactive Web sites, applications, and services for both internal and public sites.  Worked on the latest version of Recovery.gov app in Objective \u201cC\u201d and deployed to the Apple\u2019s app store.  My technical domain includes the ability to architect websites, designing data-driven applications, and developing efficient client-server solutions & systems administrations in a cloud and hybrid environment.\n\nMule Soft and SOA Architecture Role:\nWorked on Service Oriented Architecture using REST and SOAP methods.  Worked on end-points using Mule ESB and worked on various Enterprise Integration Patterns.  Used various conversions methods using J2EE and .NET to various conversion formats such as XML, JSON, CSV etc.,\n\nAWS EC2, SharePoint/CMS & Architectural Admin Role:\nWorked on the team that helped RATB in moving enterprise systems to Amazon cloud technology (AWS) solutions to improve its security and agility, consolidate systems, and streamline IT infrastructure and overhead costs. Worked on agency\u2019s cloud design includes leveraging Microsoft Office 365 for messaging services, and Amazon\u2019s virtual private cloud (VPC) for all other enterprise services. \n\nWorked on Technical Evaluation, Recommendations and Solution Implementation Plans\nWorked on Cost Benefit and TCO calculations\nWork on the mobile product and deploy them on the app store\nWorked on .NET application Architected, designed and developed solution \nWorked on CRM/SharePoint 2010 application design pattern and architectures \nWork on mobile and cloud integration\nWorked on multiple CRM/Share Point Projects, services to deploy web application and Migration\nDesigning and Documenting Servers, Systems, Applications configurations using Visio Modeler\nSQL Server database backup and restore on EBS Volumes\nVirtual Servers backup and restore, NLB, Layer 7\nManaging on-site and off-site backup storage\nWorked on Website using Ruby on Rails\nInstall and maintain Microsoft Windows 2008 R2, IIS, SP2010, Project Server 2010, Site Collections and COTS Software regular upgrades and applying patches with Jenkins and Puppet\nInstall and maintain DNS Services (Primary and Secondary DC) and manage Windows 2008 R2 Server admin accounts\nMaintaining and managing firewall rules on servers running on the Amazon EC2\nElastic-Search\nArchitected solution for Elastic Search solution for USA spending\nImplemented solution on Elastic search using multiple nodes\nCompared for the accuracy of results between previous search engine and Elasticsearch\nEnvironment: \nMicrosoft .NET Framework 3.5, 4.0, ASP.net 4.0,  TOGAF, Microsoft Visual Studio 2010, SOA, Microsoft SharePoint 2010 (CMS), Windows Web development with C#, C++, Objective C, SQL Server 2008, CSS, JavaScript, JSON, jQuery, dojo, XML, XHTML, XPATH, Adobe Flash, ADO.net, Amazon Web Services (AWS), EC2, VPC, VPN, ArcGIS, ESRI GIS Integration, OS X (Lion, Mountain Lion), Apache Tomcat, Jenkins, Chef, Puppet, iPad, iPod Touch and iPhone, iOS 4.x, 5.0, 5.1, Windows 2008, Java, C#, JavaScript, ETL, Elasticsearch, Android, iPad, iPod Touch and iPhone iOS 4.x, 5.x, 6.x, CRM, ETL, MVVM, WCF, Objective C\n\nUSDA - FS, VA, Mar 08 \u2013 Apr 12\nMobile Technology Consultant / Mobility Architect/ Lead\nDescription:\nArchitected and developed mobile cross device cross platform browser-based application for iPhone/iPad/iPod touch iOS devices and android devices with Visual studio model view controller architecture, HTML5, jQuery and JavaScript.\n\nThe Integrated Mobile Applications (IMA) suite provides applications that allow the user to enter data while in the field, and upload the data to NRM when they return to the office and connect to the network. Some front-desk applications also require the ability to work in disconnected mode, in order to continue serving public customers and users in cases where field office networks are very slow or unstable. IMA applications run on a variety of mobile devices, including notebooks and tablet computers running Windows, and handheld devices running Pocket PC, Android and Windows Mobile.  Mobile Workflow software in an integrated shared platform connected via windows shared services.\n\nThe mobile devices utilize a common Mobile Data Builder application for downloading data from I-Web and for uploading data back into the database. Used Service orient Architecture (SOA) in check-out and check-in process is defined for each mobile application in the Integration Mobile Framework, a database schema containing metadata describing the data required by each application, and a series of procedures for data validation and loading.  Project is CMMI Level 2 certified.\n\nEnvironment: \nMicrosoft .NET Framework 2.0, 3.5, 4.0, Microsoft Visual Studio 2008, 2010, J2ME, J2EE, SOA, MOSS Enterprise 2007, WSS 3.0, Windows Mobile 5.6/6.0, Windows Forms development with C#, C++, Windows Mobile development, Oracle 9.x, 10.x, 11g, xml, ADO.net, ODP.net, Windows Azure (Cloud Services), Amazon Web Services (AWS), ArcMaps, ESRI GIS mobile Integration, IBM Work light, IBM MQ Series, Salesforce Lightning Connect, Apex, Sencha Touch, Windows CE, iPad, iPod Touch and iPhone, iOS 4.x, 5.0, Android 1.x, 2.x, 3.x, 4.x.\n\nIRS, DC & MD, July 05 \u2013 Feb 08\nCI & CDE, Business / Technology Consultant\nDescription:\nCDE and CI both are. Net-based application implemented for Federal government.  I was involved in CI Architecture, from the design & developmental stage to implementation into production stage.  In CDE project, I was involved from the beginning of the project till the implementation into production. XML is the preferred language to construct the messages and XSD is used to validate the structure.  This interfaces with the SQL server engine that triggers the process to write the messages into the local and remote queues.  The service at the other end of the channel constantly listens for the messages and dispatches to the respective end user application\n\nRole: \nAs Senior Consultant and SDM, Designed and Developed C# Business objects, SQL Stored Procedures and MQ for middle tier and interacted with Site minder  components using predominantly C#\nResponsible for Design, Development and Testing XML / XSD validation Request / response structure of the Business and client message communications. \nWorked on BizTalk middle tier\nDeveloped C# components using XmlDb, Data set and XML Web Service\nUI for Windows \nMQ Messaging interfaces to interact with various queues and host systems\nCommunications With SQL Server .net and Access using messages\n\nEnvironment: \nVisual C# 7.0, BizTalk 2004 & 2006, ADO (.NET), ASP. Net, COM+, Windows 2000, 2003, Windows XP Prof., Windows NT, Windows CE, Python, Windows Tablet PC, MS Access, UML, XML, IIS, JSON, CSS, HTTP, XML/XML Schema DOM, IBM MQ-Series, UML, JavaScript, VBScript, Oracle 8.x, 9.x, SQL Server 2000, DB Mining and Engineer, Visual Source safe, Version Manager, Rational Rose\n\n\nUnited Parcel Service Company, MD Feb 05 \u2013 Jun 05\nPackage Operations, Consultant\nDescription:\nThis project is core MQ programming using C#, C++ on Windows platform. Programmatic approach and using MQ Calls have been used to create local queues, remote queues, Xmit queues and channels.  Encoding and decoding the data packets between communications is the heart of this core engine.  Unicode and multi-bytes encoding support is provided.\n\nAt one layer above, we should able to put and get messages using the regular programming methods.  XML is the preferred language to construct the messages and XSD is used to validate the structure.  This interfaces with the SQL server engine that triggers the process to write the messages into the local and remote queues.  The service at the other end of the channel constantly listens for the messages and dispatches to the respective end user application.\n\nRole: \nAs a Senior Consultant, Designed and Developed Business objects and MQ Objects for middle tier and Web sphere components using C# and VC++\nResponsible for Design and Testing XML / XSD validation Request / response structure of the WebSphere MQ Server and Message client communications. \nDeveloped C#, MC++, VC++ components using Data set, Record set and XML Web Service\nUI for Windows CE using windows CE and CE Emulation 4.2 .net\nMQ Messaging interfaces to interact with various queues and host systems\nCommunications With SQL Server .net and Access using messages\n\nEnvironment: \nVC++ 7.0, Visual C# 7.0, MC++, ADO (.NET), ASP. Net, COM+,  Windows 2000, Windows XP Prof., Windows NT, Windows CE, Windows Tablet PC, MS Access, UML, XML, ETL, HTTP, XML/XML Schema DOM, IBM MQ-Series, Sibel,  Oracle 8.x, 9.x, SQL Server 2000, Visual Source safe, Version Manager, Rational Rose,  MS SharePoint\n\nBaltimore Gas & Electric, MD\tFeb 03 \u2013 Feb 05\nField Resource Management, Consultant\nDescription:\nWireless work order dispatch system between Wireless Mobile PC\u2019s and Dispatcher Desktops.  Works with PCAD and CDPD systems.  The work orders are dispatched to the work engineers from the dispatcher workstations using MDS Dispatcher/Mobile Systems.\n\nMiddle layer is managed components written in Visual C# and COM Components with using Visual Basic.  Oracle database is used as the backend database.  Wireless mobile notebooks use MS Access as their local database and this is updated by wireless radio.  COM and serviced components have been used.  UI for windows CE has been created using CE .net emulation software.\n\nFor messaging, MQ Messaging Server used between communication ends.  Worked on Queue poll and pull.  Local message server encapsulates the MQ Messaging server and communicated via Message clients.  XML DOM architecture is heavily used for sending messages.\n\nRole: \nAs a Senior Consultant, Designed and Developed Business objects in Middle tier COM+ components using VB .NET and C#\nResponsible for design and Testing XML Request / response structure of the MQ Server and Message client communications. \nDeveloped C#, VB.NET components using Data set, Record set and XML Web Service\nUI for Windows CE using windows CE and CE Emulation 4.2 .net\nMQ Messaging interfaces to interact with various queues and host systems\nCommunications With Oracle and Access using messages\n\nEnvironment: \nVB.NET 6.0, Visual C # 7.0, ADO (.NET), ASP. Net, COM+,  Windows 2000, Windows XP Prof., Windows NT, Windows CE, Windows Tablet PC, MS Access, UML, XML, HTTP, XML/XML Schema DOM, IBM MQ-Series, RS6000 AIX 4.3.3 with Oracle 8.x, 9.x, Visual Source safe, Rational Rose, Pragma COM, Business Objects 5.x, MS SharePoint\n\nAMS - Asset Management Company, PA\tJan 02 \u2013 Jan 03\nGlobal plus, Global plus Messaging\nDescription:\nAsset Management using Global plus software that makes buy and sell of securities, multi currencies, multi-lingual software to manage the asset owned by individuals, corporations and banks.  Manages the security brokers, fixed assets, liquid assets with user friendly GUI.  Makes trades up to trillion dollars.  Execution Tickler and Transaction Tickler handling is done using XML on outbound message on the MQ-Series using MHH Monitor.\n\nRole: \nAs a Senior Developer, Designed and Developed the Middle tier VC++/C components. \nDeveloped and debugged modules of Inbound/Outbound messaging in IBM MQ-Series\nResponsible for Design and Testing of Global plus in AIX and G-plus Messaging using AIX MQ-Series\nDeveloped interface using XML/XML Schema for Outbound Messaging\nUI for Windows CE and Emulations\nDeveloped application in VC++/VB for unit/integration testing of the services.\nPortability testing performed using Oracle 8.x\n\nEnvironment: \nVisual C# 7.0, VC++ 7.0, ADO(.Net), ASP. Net, COM+, Windows 2000, Windows CE, VMS, Unix, SQL Server 7.0, UML, XML, HTTP, .net WEB Service C++/C, GNU C++, GDB, XML/XML Schema, IBM MQ-Series, RS6000 AIX 4.3.3 with Oracle 8.x, e-ChangeMan, Customer First, X-Windows CDE, Global Plus, ClearCase, PL/SQL\n\nLegg Mason, Financial Brokerage Company, MD \t\tFeb 00 to Dec 01\nSecurity Lite / PerfMon /Security Related Products, Consultant, Project Manager\n\nDescription:\nSecurity related product, written in VC++/C++ that authenticates and validates its user whoever login to the application after domain login or direct application login.  Perform variety of tasks like has access to various applications, has access to Branch, Division, Region, Firm and Super firm.  The different modules are LMADO, LMCrypt, LMDBLite, PerfMon, Sec2000, and Data Security Form.\nLMADO / LM Crypt / LMDB Lite: \nData service layer for the above components, written using ADO connectionless objects. Encryption components written in MTS, using RC4, 128 bit.  Uses default Microsoft security context for encryption/decryption. Work with stream/block algorithms conversion switch.  Initially implemented with 1/1 cipher. Digital certificates using Veri-sign and thawte another MTS component used to retrieve asset management set of data from the Sybase database.  Has variety of calls with respect to Asset management.  All calls are sync.\nPerf-Mon / Sec 2000 / Data Security Form\nSec2000 is the visual basic front end/VC middle tier application that updates the database thru forms and grid controls.  Retrieves/ Stores values from / to the database.   Has the User interface for values to be stored in the repository. Does auto authentication based on the NT user login and validates against the security-lite software component.\n\nRole: \nAs a Lead/Senior Developer, Designed and Developed the Middle tier COM components for SecurityLite/PerfMon/Sec2000/Cryptography using PK Infrastructure. \nDesigned architecture document using Visio UML\nDeveloped the front-end ASP/HTML file for Data Security Form using ATL components and user interaction and optimized the implementation.\nImplemented the Stored procedure to run MTS Components interacting with Sybase / Oracle multiple data connections.\nResearched and implemented PKI RSA128 Bit encryption using RC128 /SHA128 and certificates using Verisign\nInvolved in Database design and SQL, PL-SQL/Stored Procedure scripts.\nDeveloped prototype for MSMQ message queue\nDeveloped Perfmon integration of components using system Perf-Mon utility\nDeveloped Test applications in VB for Unit Testing and System testing and Win Runner\n\nEnvironment: \nVC++ 6.0/C++, ATL/MFC/SDK, COM/COM+, ADO, MTS, IIS, VB 6.0, ASP, MSMQ, Windows NT, Sybase 11.x/12, XML, HTTP, HP 10.2x, AIX, Win-Runner, UML, RSA, Certificates, VSS, SQL Server 7.0\n\nInternet Dynamics, Internet Security Company, CA\t Nov 99 - Feb 00\nISAPI/NSAPI Filter, Consultant\n\nAs a Senior Team Member, developed/Debug ISAPI filter using IIS/Apache/Netscape web servers interacting with Oracle Virtual DB\nInteracted with the Oracle Database using Stored Procedures, Triggers and SQL-PL/SQL Scripts.\n\nEnvironment:  VC++/MFC, IIS, Netscape Enterprise Server/Apache, SQL Server 7.0, Oracle 7.3 & Windows NT, Windows 2000, Linux, Python.\n\n\nWORK AUTHORIZATION\nUS Citizen\n\nCLEARANCE\nMBI\nActive Public Trust\n\nCONTACT\nEmail-ID  muthukumar@consultant.com  & mk3787037@gmail.com  \nHerndon, VA\nPrimary Phone: 443-316-1103\n",
          "confidence": 1.0,
          "method": "full_text",
          "structured_data": null
        },
        "tax_term": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "source_by": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "designation": {
          "value": "Senior System Architect",
          "confidence": 0.9,
          "method": "ner",
          "structured_data": null
        },
        "experience": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        }
      }
    },
    {
      "resume_path": "data\\input\\.Net - MD - Raj.docx",
      "confidence_score": 0.48500000000000004,
      "used_ocr": false,
      "extracted_fields": {
        "skills": {},
        "first_name": "Kallidumbil",
        "last_name": "Raj Rajnarayanan",
        "primary_email": {
          "value": "k.rajnarayanan@gmail.com",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "secondary_email": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "phone": {
          "value": "3016933175",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "city": {
          "value": "",
          "confidence": 0.0,
          "method": "ner",
          "structured_data": null
        },
        "state": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "zip": {
          "value": "",
          "confidence": 0.0,
          "method": "regex",
          "structured_data": null
        },
        "work_authority": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "resume_link": {
          "value": "data\\input\\.Net - MD - Raj.docx",
          "confidence": 1.0,
          "method": "file_path",
          "structured_data": null
        },
        "raw_resume": {
          "value": "Kallidumbil \u201cRaj\u201d Rajnarayanan\nPhone: 301 693 3175\nEmail: k.rajnarayanan@gmail.com\n\nSUMMARY\t\n\n13+ years of IT experience in Web and Client Server Development and Management.\nStrong at Object Oriented \u201cAnalysis and Design\u201d and Programming skills along with ability to learn new emerging technologies and adapt quickly \nExpertise in JavaScript development using Node, jQuery, Angular, React and Backbone\nWebsite development using HTML5, CSS3, AJAX, JSON, LESS and SASS\nExperience in XML, XPATH, XQUERY, XSL, WSDL, SOAP, REST and XSLT\nExperience in web development using Drupal\nExtensive experience in Shell Scripting, Cron job scheduling in both Linux and Unix\nExperience in database development using DB2, Oracle, SQL Server, MySQL and MS Access\nExperience in NOSQL databases MongoDB, Dynamodb, Solr and ElasticSearch\nExperience in CDN configuration and management\nExperience in implementing enterprise search using Solr, ElasticSearch and RDBMS Full-Text\nExpertise in .NET forms, MVC, API and Core\nExperience in Core and JEE Platform development\nExperience in JSP, Servlets\nExpertise in JPA and Hibernate\nExperience in Spring MVC, Spring Boot, Maven and Apache Ant\nExpertise in ETL Development using Informatica and SSIS\nExperience in Data warehousing and Master Data Management\nExperience in using a variety of reporting tools such as Oracle Reports, Crystal Reports, Microsoft Reports, SSRS and Active Reports\nExcellent Interpersonal, Analytical and Communication skills\nKnowledge in various states of System Development Life Cycle (SDLC)\nWell versed in Agile Development Methodologies \nHigh proficiency in working with users to gather requirements, analyze them, and subsequently use design tools to model the requirements\n\nTECHNICAL SKILLS\n\nLanguages: \tJavaScript, Java, Python, VB.NET, C#, Scala, TypeScript\nWeb Technologies: \tHTML5, CSS3, Nodejs, ASP.NET, .NET Core, Spring Framework, Flask, Django, Drupal, REST, SOAP, XSL, XML, XPATH, Web Services, jQuery, Angular, React, Backbone, AJAX, LESS, SASS\nMobile:\tXamarin, ionic.\nCloud:\tAWS, AWS Lambda, Azure and Azure Functions\nDatabase Tools: \tSQL Enterprise Manager, SQL Profiler, Query Analyzer, TOAD, SQL Developer, phpMyAdmin\nORM Frameworks:\tHibernate, JPA, NHibernate, LINQ, Entity Framework\nRDBMS: \t\t\tIBM DB2, MS SQL Server, Oracle, MySQL, SQLite, Access, Postgres \nBigData/NOSQL:\t\tApache Spark, Mongodb, Dynamodb, \nSearch Technologies:\t\tSolr, Elastic\nOperating Systems: \t\tWindows, Mac OS X, Linux, Unix, IBM z/OS, Android and IOS\nTechnology: \tCOM+, CORBA, MSMQ, Web Services, WebDAV, Win Forms, WCF, WPF, WF, Servlets, Enterprise Java Beans, Struts, Spring, JPA, Flex\nDatabase Scripting: \t\tTSQL, PL/SQL\nScripting Languages:\tJ\tavaScript, VB Script, Action Script, Shell Scripting\nDatabase Connectivity: \tADO.NET, OLEDB, ODBC, JET, ADO, JDBC\nSource Control: \tGit, Visual SourceSafe, Visual Studio, Team Foundation, Subversion\nReporting Tools: \t\tSQL Server Reporting, Crystal Reports, Active Reports, SSRS\nVirtualization: \t\tVMware, Virtual PC, VirtualBox\n\nSECURITY CLEARANCE\nPublic Trust\nEDUCATION\nCurrently pursuing MS in Computer Science from Hood College, Frederick, MD\nMSc Business Information Technology, Middlesex University, London UK \nDiploma in Advanced Computing: CDAC Pune, India\nBSc Mathematics, University of Calicut, Kerala, India\n\nPROFESSIONAL EXPERIENCE\nKTBSonline, Sparks,MD\nSenior Software Developer\t\t\t\t\t\t\tJun 2019 - Present\n\nKTBSonline is a fully customizable web portal that facilitates the enrollment and management of employee benefits for employers of all sizes. Since 1998, KTBSonline has been renowned for its proven efficiencies and exceptional employer/employee experience, as well as the critical data it provides and uncompromising security.\n\nKTBSonline provides a benefits management system for HR professionals and a web portal for employees. Through KTBSonline, employers reduce benefit spend, increase the value of their health care plans, and become more efficient and productive \u2014 while increasing compliance, data security, and employee engagement. Simply put, it helps increase the efficiency of any Human Resources department. It is a fully customizable web portal that facilitates the enrollment and management of employee benefits and payroll for businesses of all sizes.\nResponsibilities:\nDevelop ASP.net WebForms based application\nPort WebForms to Razor\nPort WCF services to GRPC\nAzure PaaS\nImplement and consume WCF and GRPC based web services\nImplement DevExtreme and Vue based user interface components\nImplement MSMQ and NServiceBus based Message queue\nSQL Server 2017 Programming\n\nEnvironment:\nC#, TypeScript, Vuejs, NodeJs, Dotnet Core, ASP.NET MVC, SQL Server 2017, LINQ, Entity Framework,CSS3, HTML5, jQuery, Visual Studio 2019, DevExtreme UI, , Agile, Scrum, MongoDB, NServiceBus, MSMQ, GRPC, WCF\n\nHarmonia Inc, Tysons Corner VA\nSenior Software Developer\t\n\t\t\t\t\t\tSep 2017 - Jun 2019\nFSIS APM\nAPM is the Product recall module that intends to replace the current paper based process with a web based portal that will be integrated with FSIS.\nResponsibilities:\nDevelop ASP.net Core based application\nDevelop Xamarin based mobile application for Android and IOS\nImplement and consume REST based services using ASP.NET Core 2.0\nParticipate in architecture design process\nIdentify and solve complex problems using design patterns such as factory, singleton, Strategy, observer  and builder patterns\nImplement Telerik Kendo UI based user interface components\nImplement Anguklar7.0 based UI\nImplement IText7 based PDF generation and signing functionality\nSQL Server 2017 Programming\n\nEnvironment:\nC#, TypeScript, Angular, NodeJs, Dotnet Core, ASP.NET MVC, SQL Server 2017, LINQ, Entity Framework,CSS3, HTML5, jQuery,4.5, Visual Studio 2017, Telerik Kendo UI, IText 7, Agile, Scrum, Xamarin forms for Android, TDD, BOD\n\n\nFSIS Application Portal\nFSIS implemented the Public Health Information System (PHIS) as part of our effort to collect,\nconsolidate, and analyze data. PHIS is a user-friendly, web-based application that replaces many\nof FSIS\u2019 existing systems\nResponsibilities:\nDevelop ASP.net WebForms based application\nAzure PaaS\nImplement and consume WCF based web services\nImplement Telerik based user interface components\nImplement RabbitMQ and Masstransit based Message queue\nSQL Server 2014 Programming\n\nEnvironment:\nC#, WCF, ASP.NET MVC, ASP.NET Web Forms, SQL Server 2014, LINQ, Entity Framework, CSS3, HTML5, jQuery,4.5, Visual Studio 2015, Telerik, RabbitMQ, Masstransit, Agile, Scrum,m Azure\n\n\nLHNCB at National Library of Medicine, NIH Bethesda, MD\t \nSenior Web Developer\t\t\t\t\t\t\tSep 2015- Sep 2017\n\nPersonnel Management System\nThe project Personnel Management System help manage Researchers, Interns and Associates at the National Library of Medicine at NIH. It tracks their academic achievements such as degrees and awards and helps the NLM management track their career development at NLM. The system was originally developed in ASP and is in use for over 15 years. Support for some components such as Crystal Reports and Windows 2003 are no longer available. The system lacked user interface improvements due to the proliferation JavaScript and lacked important data security measures such as data encryption and secure login. \n\nResponsibilities:\nConvert ActiveX Based Crystal Reports to .NET\nImplement sensitive data encryption in the database\nImplement LDAP based authentication\nImplement XSS and SQL injection prevention throughout the website\nAdd search and data download functionalities\n\nEnvironment:\nC#, ASP.NET MVC, ASP.NET Web Forms, Classic ASP, Crystal Reports, SQL Server 2012, CSS3, HTML5, jQuery, COM+, VB Script, .NET 4.5, Visual Studio 2015. \n\nLforms\nLForms is a lightweight, feature-rich, open-source widget that creates input forms for Web-based medical applications or to integrate into electronic health records (EHRs), personal health records (PHRs), and mobile health apps. LForms is a completely free widget toolkit that leverages LOINC's rich content model for panels, forms, and survey instruments. Developed by LHNCBC in collaboration with the Regenstrief Institute, LForms can read any of the 1,700+ panels defined in LOINC\u00ae and render a powerful data entry form. It works across all the domains (laboratory panels, survey instruments, etc.) included in LOINC. LForms power comes through its support of detailed form attributes, including: data type, cardinality, default value, units of measure (if numeric), answer lists (if multiple choice), ability to make multiple choice answer lists function as \"select one\" or \"select all that apply,\" relationship (in a nested hierarchy) to other questions, default value settings, validation checks, skip logic and help messages.\n\nResponsibilities:\nDevelop node js based REST services that serves data stored in the elastic index to the user. The rest services can be customized to return specific fields only\nDevelop ElasticSearch based indexes. These indexes are tuned to maximize response time and handle high volume traffic\nImplement Elasticsearch based search on the Gene Index.\nCreate a Shell script using tools such as AWK, sed, sort and such to de-duplicate, group, truncate and sort raw data\nCreate Jasmine and Mocha Tests based test for LForms\n\nEnvironment:\nNodejs, Angular 2.0, Elasticsearch, REST, Mocha, Jasmine, Grunt, AWK, Shell Script, Red hat Linux, Oracle 11g, ASP.NET MVC, ASP.NET Web Forms, ASP, SQL Server 2012, SQL Server 2012, Jira, BitBucket\n\n\n\n\nSocial Security Administration, Gwynn Oak MD\t\t\t\t\t\nApplication Developer\t\t\t\t\t\t\tJul 2014- Jul 2015\n\nThe purpose of this Project is to provide support to the Office of the Chief Strategic Officer (OSCO) at the Social Security Administration in connection with OCSO\u2019s executive information site and all related data tables and reports. LOCKHEED MARTIN will seamlessly update and transition Executive and Management Information System (EMIS) from the current version to a new one by suggesting improvements including intranet web interfaces, database design, report design, and user tools.\n\nResponsibilities:\nDevelop tools that allow OSCO staff to update raw data\nCreate new reports when necessary\nDevelop a system that allows modifications to report structure, data elements, and charts/graphs without hard coding or new programming, through user-friendly interfaces\nDevelop new program(s) with the flexibility for direct database access\nDevelop interactive graphical user interfaces) (GUI) for the executive information site on EMIS, Agency Priority Goals (APG) and Strategic Objective data where, through data visualization (i.e., dashboards), users can easily identify overall progress and clearly see those areas or timeframes where additional information and analysis is required\nProvide business analyst support to document code, maintain, and archive program development and requirements changes by producing a data dictionary and a user manual\nProvide data analytics support as requested\n\nEnvironment:\nASP.NET MVC, ASP.NET Web Forms, ASP, WCF, Web Services, SQL Server 2008, SQL Server 2012, SSIS, jQuery, Highcharts, Bootstrap, D3.js Library, JavaScript, CSS 3, HTML 5 C#, REST, ASP.NET Web API, Java, Spring 3.0, WebSphere, 508 Compliance. \n\n\nThe Great Courses, Chantilly VA\t\t\t\t\t\t\t\nSenior Web Developer\t\t\t\t\t\t\tFeb 2013- Jul 2014\nThe Great Courses is a series of college-level audio and video courses produced and distributed by The Teaching Company, Chantilly, Virginia company, via mobile apps, CD, DVD, or MP3 and MPEG-4 download formats\n\nResponsibilities:\nMaintain existing website and making frequent updates upon business requirements\nWork closely with the marketing team to implement new requirements and bug fixes\nDeploy website across dev, stage and pre-production environments\nDevelop SQL stored procedures as needed for implementation\nDesign, implement and maintain code that supports all browsers\nImplement tagging needed for tracking user behavior and conversion rates\nImplement split version of the website for testing purposes\nImplement user Interface using JavaScript and HTML5\nCreate and maintain SSIS packages for the MIS Data Mart\nMaintain and enhance Apache Solr Search Platform and integrate it with the Website\nDevelop and integrate email campaign using Strong Mail\nDevelop REST API for mobile integration\n\nEnvironment:\nASP.NET MVC, ASP.NET Web Forms, ASP, Visual Basic 6, Windows Froms, SQL Server, 2008, SQL Server 2012, SSIS, Solr, jQuery, JavaScript, CSS3, HTML5, WCF, C#, VB.NET, VB Script, REST, ASP.NET Web API\n\nMaricom, a CSC Company, Windsor Mill MD\t\t\t\t\t\nETL Developer\t\t\t\t\t\t\t\tAug 2012-Feb 2013\nMaricom, a CSC company, is an IT solutions firm, specializing in Data Management, Software Engineering, Business Intelligence, and Contact Center solutions. Located within the Baltimore-Washington corridor Maricom has worked successfully with Government, Healthcare, and Commercial sectors, providing management, maintenance, and support of large and complex data environments.\n\n\nResponsibilities\nDeveloped Mappings and Sessions based on user requirements and business rules to load data from source flat files and RDBMS tables to target tables.\nInvolved into design the logical data model from the technical design documents and translating to a physical data model using ERWIN\nDesigned the ETL processes using Informatica to load data from DB2, Oracle, SQL Server, Flat Files, XML Files and Excel files to target Oracle Data Warehouse database.\nIdentified and loaded sources and targets metadata into repository\nCreated Shell Scripts to control and initiate Informatica workflows, Process flat files and email status messages\nDeveloped Mappings and Sessions based on user requirements and business rules to load data from source flat files and RDBMS tables to target tables\nDesigned the ETL processes using Informatica to load data from DB2, Oracle, Flat Files, XML Files and Excel files to target DB2 Database.\nCreate SOAP Java web services using Apache Axis 2.\n\nEnvironment:\nInformatica Powercenter 9.1.0, Oracle 11g, 10g/9i, DB2 10 for z/OS, UNIX Shell Scripting, Windows NT/2000, Unix, Sun Solaris, Eclipse, Java, Apache Axis 2. \n\nHertz Rent2Buy Park Ridge, NJ\nAnalyst/Programmer\t\t\t\t\t\t\t\tFeb 2012- Aug 2012 \nHertz Global Holdings Inc.\u00a0is an American\u00a0car rental\u00a0company with international locations in 145 countries worldwide. Rent2Buy is an innovative program launched by Hertz Car Sales with the purpose of selling high-quality, active rental cars online at very competitive prices. Hertz Rent2Buy\u00ae provides customers with the opportunity to test the vehicle for 3 full days by renting it at a low special rate, which will be waived if the car is purchased.\n\nResponsibilities:\nCreated components using SOAP wire protocol for the communication between Hertz partners and the Rent2Buy\nDeveloped Solr based search service for cars in the rent2buy database.\nConsumed and exposed data to and from several clients using Web Service (SOAP and JSON) \nDesigned and developed the messaging framework for communication between Rent2Buy, other Enterprise Applications and client data feeds over XML and MQ-Series \nDeveloped\u00a0web interface using JSP, Servlets, JavaScript, CSS and JDBC for administering and managing users, vehicles and clients\nDeveloped and deployed various Entity EJBs and session EJBs\nImplemented a responsive website which can run on Desktops, Tablets and Smartphones using HTML5, JavaScript and CSS3\nMaintained and enhanced Rent2Buy website and Database\nAdded and maintained various data consistency checks and alerts to the Rent2Buy Database\nCreated Middle Tier to enable data transfer between Frontend and Database\n\nEnvironment: \nEnvironment: Java, JSP, Java Server Faces, Servlets, EJB, JDBC, ASP.NET MVC, SQL Server 2008, HTML, CSS 3, HTML5, XML, JSON, Subversion, WebSphere Application Server 6.1, jQuery, GWT, Backbone, Handlebar, Web services (SOAP, WSDL), JIRA, JSON, JUnit,\n\nIPSOS, Parsippany NJ\nAnalyst/Programmer\t\t\t\t\t\t\t\tAug 2011- Feb 2012 \n\nIPSOS is a global market research company focusing on the production, interpretation and distribution of information gathered from individuals about their opinions, desires, attitudes and behaviors. IPSOS operates over 80 countries worldwide. The Project I am working on is for Walgreens Customer Loyalty Research.\n\n\nResponsibilities:\nMaintained and enhanced Walgreens Customer Loyalty website and Database\nAdded and maintained various data consistency checks and alerts to the survey database\nAdded or Removed questions to the customer survey database and website\nCleaned, Transformed and Imported data from the survey to the Data warehouse using SSIS \nCreated various data consistency checks and alerts on the imported data\nPrepared stored procedures to pre-calculate data for reporting\nCreated and maintained ASP.NET Web forms based Website for reporting\nCreated and Maintained various SQL Server 2008 Jobs for data processing and SSIS\nDeveloped ASP.net, Windows Forms and Service applications using Vb.Net and C#\nDeveloped a flex based application for Mobiles with .net Service based backend\n\nEnvironment: \nASP.NET Web forms, Windows Forms, WPF, JavaScript, jQuery, Dojo, ASP.NET AJAX, JSON, C#, Visual Studio.NET 2008, Visual Source Safe, Share point 2007, XML, XSLT, CSS, SQL Server 2008, Windows Services, SSIS, Flex, ActionScript.\n\n\nUnited Nations, New York, NY\t\t\t\t\nInformation Systems Assistant\t\t\t\t\t\tAug 2010- Jul 2011\nThe Headquarters of the United Nations in New York City employs a variety of technologies to fulfill the mandates of different departments and enable effective communications between its branches all over the world. \n\nResponsibilities:\nActively participated in United Nations Non-Governmental Organizations (NGO) Branch website usability enhancement studies and design\nDeveloped and maintained NGO branch website using Java, HTML, JavaScript and Servlets\nCreated and maintained various scripts to host the application in Apache Tomcat container\nMaintained and enhanced custom MVC framework using JSP's, internally developed Custom Tag Libraries, JSP Standard Tag Libraries (JSTL), HTML, and JavaScript\nCreated User interface using JSP, HTML, and JavaScript\nMaintained and enhanced NGO Registration Portal\nCreated and generated reports using Pentaho reporting tool\nMaintained and enhanced event management system for UN agencies in New York and Geneva\nCreated and maintained Tables, Stored Procedures, Triggers and Indexes for Oracle 10g database\nEnhanced the Paperless website to handle the volume of information submitted by Non-Governmental Organizations around the world\n\nEnvironment: \nJAVA, JSP, Servlets, Tomcat, JDBC, Spring Framework, JPA, Hibernate, JavaScript, jQuery, Dojo, Oracle, Eclipse, NetBeans, SQL Developer\n\nEmblem Health, New York, NY\t\t\t\t\nAnalyst/Developer\t\t\t\t\t\t\t\tJuly 2009 \u2013 Aug 2010\nEmblem Health provides health insurance to over 3.4 million members across New York, New Jersey and Connecticut.\n\nResponsibilities:\nCreate ASP.NET website for data gathering and presentation\nMaintain and support existing WinForms application\nAnalyze requirements for the Request tracking intranet website\nPrepare Project Documentation\nDesign SQL Server 2008 database for the request tracking application website\nCreate database objects such as tables, views, stored procedures and indexes\nCreate complex yet efficient stored procedures for Search and Reporting\nCreated AJAX enabled User Interfaces for the Request Management Application using XHTML, ASP.NET MVC and jQuery\nImplement a Data Repository using ADO.net Entity Framework\nTest Driven Development using ASP.NET Test Framework\nCreate SSIS packages for Extracting and loading data from legacy applications\nDesign and Create Local and Remote reports using SSRS\nDeploy SSRS and consume reports in the web application\n\nEnvironment: \nASP.NET, WinForms, WPF, JavaScript, jQuery, Dojo, Structuremap, ASP.NET AJAX, JSON, C#, Visual Studio.NET 2008, Visual Source Safe, Share point 2007, XML, XSLT, CSS, Active Reports, SQL Server 2008, Windows Services, SSIS, SSRS, ADO.NET Entity Framework, LINQ\n\nI3 Solutions, Sterling, VA\t\t\t\t\t\nSenior Developer\t\t\t\t\t\t\t\tMar 2009 \u2013 Jul 2009\nI3solutions is an IT consulting firm specializing in strategic application development, data management and systems integration, and collaborative solutions.\n\nResponsibilities:\nCreated ASP.NET web forms for data gathering and presentation\nCreated and scripted Database components such as tables, functions and stored procedures to store data for effective retrieval and manipulation\nCreated SSIS packages to extract from Excel and Text files, Cleanse and Upload to Oracle 10g database\nCreated and consumed WCF services, web services and HTTP Handlers for asynchronous communication between the server and client\nCreated detailed technical specification and documentation of the project\nCreated AJAX user interface using DOJO Library\nAuthentication using Active directory \nCustomization ASP.NET Membership API to facilitate form based authentication and rule based authorization\n\nEnvironment: ASP.NET, JavaScript, ASP.NET AJAX, WCF, JSON, VB.NET, Oracle 11.i, 10g, Visual Studio.NET 2008, Visual Source Safe, SharePoint 2007, XML, XSLT, CSS, Active Reports.\n\nFINRA, Rockville, MD \t\t\t\t                         \t\nSenior Web Developer \t\t\t\t\t\t\tJul 2008 \u2013 Feb 2009\nFinancial Industry Regulatory Authority (FINRA) is the largest independent regulator for all securities firms doing business in the United States. FINRA operates Web CRD\u00ae, the central licensing and registration system for the U.S. securities industry and its regulators.  It contains the registration records of more than 6,800 registered broker-dealers and the qualification, employment, and disclosure histories of more than 660,000 active registered individuals, making it the world\u2019s largest and most sophisticated online registration and reporting system\n\t\t \t\t         \nResponsibilities:\nCreated ASP.NET components and pages to add new functionality to the WebCRD application\nAnalyzed their impact to the application\nCreated and maintained XML documents and support functions for legacy application\nCreated and maintained XSL Transformations to consume XML data\nCreated and maintained oracle functions and procedures in packages to incorporated changes to the schema\nCreated complex PL/SQL Stored Procedures to check the completeness of form filings\nExtensively used PL/SQL tables, cursors and exception handling\nCreated database scripts to delete or update inconsistent data to maintain data Integrity\nFixed performance issues and bugs within packages, forms, and reports using DBMS_OUTPUT, TOAD and SQL Developer debuggers, explain plan and TKPROF\nImplemented AJAX Control extensions to enhance user experience\nMaintained and implemented Java Script functions for improved user experience\nSharePoint customization and development\nMaintain and debug old codebase\n\nEnvironment: ASP.NET, ASP, C#, VB Script, JavaScript, AJAX, JSON, Oracle 11i, 10g, Eclipse, CVS, SharePoint 2007 XML, XSLT, CSS.\n\n\nMIC BUSINESS SOLUTIONS, INC, New York, NY           \nSenior Web Developer \t\t\t\t\t\t\tJan 2008 \u2013 Jul 2008\nMIC Business Solutions, Inc. (MIBS), a wholly owned subsidiary of Mitsubishi International Corporation. MIBS provides information and communications technology consulting, as well as system and network integration, focusing mainly on medium-sized Japanese corporations with a presence in the U.S. The project was to design and implement a warehouse management system using barcode to manage Inventory.\n\nResponsibilities:\nParticipate in Requirements Analysis\nCreated Database diagrams for the database\nCreated Class diagrams, sequence diagrams and use case diagrams for the project\nCreated tables, stored procedures and functions for the warehouse management system\nCreate SSIS packages to Extract, standardized and upload product delivery notes from different suppliers to the warehouse database\nDesigned and implemented User interface for the web application using server side AJAX\nDeveloped user interface for the WinCE based barcode reader\nDevelopment and customization of SharePoint Site\nImplemented Barcode data manipulation and database interaction\nImplemented barcode reader behavior for the handheld device\nCreate reports using Crystal reports\nImplemented modules to interact with SAP Web module to update stock\nHosted site on client intranet\n\nEnvironment: ASP.NET, JavaScript, ASP.NET AJAX, C#, SQL Server 2005, Visual Studio .NET 2005, Visual SourceSafe, Crystal Reports and Win CE 5.\n\nWORLD TRAVEL HOLDINGS, Woburn MA \t       \nAnalyst Programmer\t\t\t\t\t\t\t\tJul 2007 \u2013 Dec 2007\nWorld Travel Holdings is a multi-brand travel distributor with executive offices in Port Washington, New York and corporate offices in Woburn, Massachusetts. The company's owned brands include CruisesOnly, Cruises.com, Rooms.com, Vacation Outlet, CruiseOne, Cruises Inc., and Villas of Distinction. WTH's portfolio of licensed partner brands includes BJ's Vacations, Hotwire.com Cruises, Overstock.com Travel, Priceline.com Cruises, American Airlines Cruises, continental.com cruises, and many others.\n\nResponsibilities:\nMaintain the existing ASP code base, with an emphasis on JavaScript\nCreated Class diagrams, sequence diagrams and use case diagrams for the project from the specification provided\nImplemented .NET libraries to interact with client\u2019s web services\nImplemented COM+ wrapper classes for this library to interact with legacy ASP code base\nImplemented Http based routines to interact with legacy web resources\nCreated PL/SQL packages to incorporate changes to the database\nCreated PL/SQL stored procedures and Functions\nFixed performance issues and bugs within packages, forms, and reports using DBMS_OUTPUT, TOAD and SQL Developer debuggers, explain plan and TKPROF\nImplemented testing cases for classes used for the gateway\nImplemented a gateway to Interact with WTH partners for data exchange using XML and HTTP\nImplemented web trends and 7 billion people analytic tools for the website\nDeveloped Java based middleware libraries to interact with Sabre web services\nDeveloped, debugged and maintained JSP and Servlet code\n\nEnvironment: ASP, ASP.NET, JavaScript, VBScript, C#, Oracle, Visual Studio.NET 2005, Visual Source Safe, Visual C++, MFC, COM+.\n\t\n\nINPHONIC INC. Largo, MD\t\t\t\t\t\tDec 2006 \u2013 Jun 2007\nAnalyst Programmer\nInPhonic is one of the Internet's No. 1 authorized retailers of cell phones, cell phone plans, and other wireless devices. InPhonic operates Wirefly.com, one of the 5 most visited electronics shopping sites in the world, and manages private label e-commerce sites for national retailers (such as RadioShack and Best Buy), internet retailers (such as Overstock.com and Buy.com), cell phone manufacturers (such as Motorola and LG), membership organizations (such as airlines and associations), and thousands of other partners and affiliates. The aim of the project I was involved was to enhance their Back-Office application with new functionalities called Invitation Codes.\n\nResponsibilities:\nDesigning and developing modules for the application\nMaintenance of legacy application in ASP code \nDesigning application interface for Customer promotions and Invitation codes\nImplemented application logic for product return and damaged Items handling\nCoding application logic in ASP.NET using VB.NET\nEvaluating the impact of the new functionalities on the existing Application\n\nEnvironment: ASP.NET, C#, ASP, VB Script, VB.NET, SQL Server 2000, 2005, Windows XP Professional, Visual Studio .NET, Visual Source Safe, MS Application blocks, SQL Server Reporting Services, Crystal Reports.\n\nSURFPIN LTD., London, UK\t\t\t\t\nSoftware Engineer\t\t\t\t\t\t\t\tApr 2004 \u2013 Nov2006\nSurfpin Ltd, UK is the market leader in providing Micropayment services in Europe, North America and Australia. Their business model includes mobile phone companies or aggregator, ecommerce and gaming websites and customers. The complexity involved in successfully completing a transaction in enormous. Also, there is a pressing need to provide the partner website with state of the art business intelligence about their payment pattern. Projects included designing and implementing transactional, highly available web services, business intelligence and customer service solutions. \n\nResponsibilities: \nCreated views, stored procedures and functions for the database\nCreated COM+ components\nCreated MSMQ components\nCreated Windows services to manage Message queues\nConverted XML to HTML mark-up using XSLT\nImplemented Data marts for faster report processing\nCreated MDX queries\nConverted Multidimensional queries to XML using ADOMD for reporting tools\nCreated Smart client based customer support package using WebDAV and Exchange Server\n\nEnvironment: ASP.NET, C#, COM+, MSMQ, SQL Server 2000, SQL Server 2005, SQL Server Business Intelligence Studio, Crystal Reports, SQL Server Reporting Services, WebDAV, XML, XSLT, XPATH, Web Services, Crystal Reports, WinForms, Smart Client Applications, SSIS, SSAS, SSRS, MS CRM.\n\n\nPet Projects\nCoupon Application \t\t\t\t\t\tDev 2017 - Present\nSmall pet project for small businesses to issue discount coupons and track customer reviews.\nResponsibilities:\nDevelop NodeJS and Angular 6 based application to offer coupons\nImplement and consume REST services \nImplement IONIC based mobile application for IOS and Android\nImplement Firebase datastore\n\n\nEnvironment:\nJavaScript, Nodejs, Angular 6, Firebase, Mongo,  Ionic, TypeScript, Linux, Docker, AWS, Microservices\n\n\n\nBinary Sort (ongoing startup pet project), Frederick MD\nSenior Software Developer\t\t\t\t\t\t\tJan 2015 - Present\n\nBinarySort is an early stage start-up where responsibilities include, design and development of modular and scalable N-tier enterprise application. The application performs real-time social media monitoring. The application monitors and aggregates popular and trending articles from various social media platforms and present to the user via responsive web page.\n\nEnvironment:\nNode.js, .NET, Angularjs, Bootstrap, MongoDB, OAuth 2.0, CORs, Cloud9 IDE\n\n\n\n\n\nMMDB (My Movie DataBase)  ongoing fun/ learning project \t\t\nDeveloper\t\t\t\t\t\t\t\t\tMar 2017 - Present\n\nThe MMDB aims to create an index of publicly available movie datasets. The project aims to combine sophisticated search on a number of criteria such as name, year, genre, studio, actor, country, language. For Elasticsearch is used for indexing the data. The project exposes a number of REST APIs for querying, adding and updating the movie data. The project consumes APIs from IMDB and TMDB to query movie details and reviews. In addition, the project exposes its API using Azure API Management Gateway. The APIs are designed as microservices using Spring boot and .NET core, \n\nEnvironment\n.NET core, Java, Nodejs, MongoDB, Elasticsearch, Azure, Azure Storage, Azure Functions, Angular, Jenkins, GitHub, Docker.\n\nCustom Album Designer (ongoing startup pet project)\ncustomalbumdesigner.com\nSenior Software Developer\t\t\t\t\t\t\tJan 2016 - Present\n\nCustom album designer helps professional and amateurs to create stunning albums with their photos for any occasion or event. The site gives option to upload photos, choose designs, binding types and previews and suggestions. The project is aimed to move the entire operation to AWS with modern JavaScript based user interface and cloud storage and hosting.\n\nEnvironment:\nFlask, React, AWS, Python, JavaScript, S3, DynamoDB, Bootstrap, OAuth 2.0, CORs, Cloud9 IDE, CI CD, Jenkins\n",
          "confidence": 1.0,
          "method": "full_text",
          "structured_data": null
        },
        "tax_term": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "source_by": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "designation": {
          "value": "Senior Software Developer",
          "confidence": 0.8,
          "method": "regex",
          "structured_data": null
        },
        "experience": {
          "value": "13+",
          "confidence": 0.9,
          "method": "regex_total_experience_summary",
          "structured_data": null
        }
      }
    },
    {
      "resume_path": "data\\input\\ABDIRAHIM MASLAH   - SD - OH.docx",
      "confidence_score": 0.56,
      "used_ocr": false,
      "extracted_fields": {
        "skills": {},
        "first_name": "ABDIRAHIM",
        "last_name": "MASLAH",
        "primary_email": {
          "value": "abdirahim008@gmail.com",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "secondary_email": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "phone": {
          "value": "6143774454",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "city": {
          "value": "Columbus",
          "confidence": 0.7,
          "method": "ner",
          "structured_data": null
        },
        "state": {
          "value": "OH",
          "confidence": 0.8,
          "method": "city_database",
          "structured_data": null
        },
        "zip": {
          "value": "",
          "confidence": 0.0,
          "method": "regex",
          "structured_data": null
        },
        "work_authority": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "resume_link": {
          "value": "data\\input\\ABDIRAHIM MASLAH   - SD - OH.docx",
          "confidence": 1.0,
          "method": "file_path",
          "structured_data": null
        },
        "raw_resume": {
          "value": "ABDIRAHIM MASLAH  \n\n                                           Columbus Ohio\nHome: 614-377-4454 Cell: 614-377-4454 abdirahim008@gmail.com \nPROFILE\nAspiring IT professional actively pursuing an aggressive education to obtain Industry specific certifications.  Highly experienced in customer service protocols and processes.  Comfortable working directly with the customer/client to resolve their issues.  Able to de-escalate problematic situations whether face-to-face or via phone.  Called upon to take the lead in multiple situations to train and mentor peers.  \nEDUCATION\nMy Computer career, Columbus, Ohio \t\t\t\t\t\tGraduated June 2016\nInformation Technology system Administration \nCurrent Certification: MTA Network fundamentals, CompTIA Network+, CompTIA A+, CompTIA Server Plus and MTA Windows OS. \nCertification in progress: Mobility + CompTIA, CNNA, MCTS (Window 7), MCTS (Window server Enterprise Administrator), CompTIA Security+, and CCNP \nTECHNICAL HIGHLIGHTS\nResponds to customer technical problems/issues related to various OEM hardware  and software platforms\nAssists customers by diagnosing problems and providing resolutions for technical issues\nUses troubleshooting techniques and tools to identify products that are defective and follow guidelines in issuing service calls/contacts\nUtilized Service Now to open, document, and resolve helpdesk tickets in accordance procedures.\n\u00a0Performs PC, laptop, and printer moves and installations. \n\u00a0Advises/educates customers within procedural guidelines to ensure a complete solution to their technical or service questions\nUtilized ServiceNow ticketing system.\n\nproviding phone support for Windows 7 & 8 desktop environment including Active Directory (AD), within a Retail Financial Services environment\nIn-depth knowledge of several technology disciplines including PC, Server, and Network infrastructure as well as Mobile Messaging/Communication software and hardware such as smart phones, broadband cards and other wireless technologies\nUtilized ServiceNow ticketing system.\nStrong customer service skills via phone, instant message (IM), along with strong documentation skills\nConfigure and troubleshoot UPS\u2019s and Micro pods. \n\nField Nation \t\t\t\t\t\t\t\t          5/25/2015 to present \nFreelance contractor/ Help Desk Analyst\nColumbus, Ohio \nInstalling CAT 5 and CAT 6\u00a0cables, Testing\u00a0and terminating cables, Patching\u00a0panels, \nSolving\u00a0issues related to cables and networks\nTerminate phone jacks\nDemonstrate proper procedures, encourage safe use, document in ServiceNow.\nWorked with TCP/IP, assigning/reassigning IP addresses and with Novell Servers, Windows NT and DNS.\nInstall and trouble PC, Printers, router, servers, and switches\n     \n SomTech    \n Network Technician  \n Columbus,Ohio  \t\t\t\t\t\t\t\t02/15/2014 to 1/30/2016\nFamiliarity with servers - Virtual, blade and physical.\t \t\t\t    \nInstalling router and switches                                                               \nSetting up computer security measures \t\nTroubleshot software and hardware issues via phone such as Windows 7, XP operating systems, Microsoft standard desktop applications including Word, Excel, PowerPoint, Exchange and Visio, Firefox and Chrome, McAfee, Kapersky and Norton anti-virus software\t\t\nbackup processes and retention periods\nRacking, un-racking, and installing servers, UPS, and surge protectors \nHands-on experience with pulling cable\nVery strong part of any data center support team\n\nAFRICAN COMMUNITY CENTER \t\t\t\t\t       07/2008 to 01/2013\nFront desk and Tech support\nMinneapolis, Minnesota\nMade outgoing/received incoming calls to clients to request and clarify information. \nHelped elderly and non-English speaking customers to fill out job, housing, medical, or Immigration applications. \nReceived Company invoices, scanned and emailed to Microsoft Outlook.\nRecord keeping and maintaining an organized work environment \nTroubleshoot, diagnose, repair/replace computer issues. ",
          "confidence": 1.0,
          "method": "full_text",
          "structured_data": null
        },
        "tax_term": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "source_by": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "designation": {
          "value": "system Administration",
          "confidence": 0.8,
          "method": "regex",
          "structured_data": null
        },
        "experience": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        }
      }
    },
    {
      "resume_path": "data\\input\\Abhishek-devops-mig.docx",
      "confidence_score": 0.5750000000000001,
      "used_ocr": false,
      "extracted_fields": {
        "skills": {},
        "first_name": "ABHISHEK",
        "last_name": "Heera",
        "primary_email": {
          "value": "abhishek.heera11@gmail.com",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "secondary_email": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "phone": {
          "value": "7814864747",
          "confidence": 0.9,
          "method": "regex",
          "structured_data": null
        },
        "city": {
          "value": "",
          "confidence": 0.0,
          "method": "ner",
          "structured_data": null
        },
        "state": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "zip": {
          "value": "",
          "confidence": 0.0,
          "method": "regex",
          "structured_data": null
        },
        "work_authority": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "resume_link": {
          "value": "data\\input\\Abhishek-devops-mig.docx",
          "confidence": 1.0,
          "method": "file_path",
          "structured_data": null
        },
        "raw_resume": {
          "value": "ABHISHEK Heera\nDevOps/Migration Engineer\nPhone: (781)-486-4747 | Email: abhishek.heera11@gmail.com\n\nPROFESSIONAL SUMMARY\nAround 7 years of experience in Cloud Infrastructure and DevOps experience building CI/CD pipelines and automation.\nHands on experience with configuration management tools such as Chef, Ansible. Version control using GIT, SVN. Containerization using Docker and Kubernetes, continuous integration using Jenkins, Bamboo and worked on build tools like ANT and Maven.\nExperience in Continuous Integration and deployment using various like Jenkins, Hudson, Bamboo, Chef, and Puppet.\nExpert in Jira Atlassian admin. Have worked extensively with Jira plugins like Tempo, WBS, Jira Automation, Docker.\nConfigures and administers\u00a0Jira\u00a0Software,\u00a0Jira\u00a0Core and/or\u00a0Jira\u00a0Service Desk, and related products such as apps.\nExperience in working on AWS and its services like\u00a0AWS IAM, VPC, EC2, ECS, EBS, RDS, S3, Lambda, ELB, Auto Scaling, Route 53, Cloud Front, Cloud Watch, Cloud Trail, SQS, and SNS.\nExperienced in Cloud automation using AWS Cloud Formation templates to create custom sized VPC, subnets, NAT, EC2 instances, ELB and Security groups.\nManages global settings including users and groups,\u00a0roles, global permissions, and schemes.\nCreates projects, sets up project permissions, and assigns\u00a0Jira\u00a0project\u00a0administrators.\nExpertise in all areas of SCM processes, including version, build\u00a0and issue management, build and release management.\u00a0\nExpert in deploying the code through web application servers like WebSphere/Web Logic/ Apache Tomcat/JBOSS \nGood experience in middleware services JDBC, SSL certificates in WebLogic.\nExpertise in Deploying and troubleshooting the J2EE Applications (WAR & EAR) in JBOSS Application Server\nExperience with maintaining the 24x7 production environments on public Cloud environments (AWS, Azure) and       on prem.\nExtensive experience in setting up the CI/CD pipelines using\u00a0Jenkins, Maven, Nexus, GitHub, CHEF, Terraform and AWS.\nDeveloped\u00a0Python\u00a0scripts for software build conduct and management support.\nExtensive experience using MAVEN and ANT as build tools for the building of deployable artifacts (jar, war, ear) from raw source code.\nExtensively worked on Jenkins for continuous integration and End-to-End automation for all build and deployments.\nExperience working on several Docker\u00a0components like Docker\u00a0Engine, Hub, Machine, creating Docker\u00a0images, Compose, Docker\u00a0Registry and handling multiple images primarily for middleware installations and domain configurations.\nBuilding/Maintaining Docker container clusters managed by\u00a0Kubernetes. \nWorking on On-Premises and AWS platforms utilizing DevOps/Agile operational processes.\nExperience in designing, configuring and deploying solutions on Microsoft Azure using ARM Templates, AZURE PowerShell Module and, Azure CLI focusing on high-availability and auto-scaling.\nGood working knowledge of Creating, the AWS VPC network for installed instances and configured security groups and Elastic IP\u2019s accordingly.\nExperience in installing Logstash, Elastic search, Kibana on containers and creating Logstash config file to get all the logs from the server.  \nExperience in using Bug tracking tools like JIRA, and HP Quality Centre.\nExperience in working with configuration management tools ANSIBLE.\nCreated\u00a0Ansible playbooks to automatically install packages from a repository, to change the configuration of remotely configured machines and to deploy new builds.\nProficient in tracing complex build problems, release issues and environment issues in a multi-component environment.\nExperience in configuring clusters and managing domains in Application Servers\nKnowledge of using Routed Protocols: FTP, SSH, HTTP, TCP/IP, and HTTPS.\nExperience in deploying JDBC pool connections using SSI\u2019s.\nExcellent communication skills, leadership abilities, strong architectural skills, hardworking and a very good team worker. \nEDUCATION\nMasters in Computer Science - Colorado Technical\u00a0University\u00a0\u2013 2017 \n\nTECHNICAL SKILLS\n\n\n                                                               \nEXPERIENCE SUMMARY\n\n\nState of TN, Nashville, TN\nDepartment: HealthCare                                                                                                                    MAY 2021 \u2013 Current\nRole: Sr DevOps Engineer \nProject: TFS Migration\n\nResponsibilities\nConfigured and Managed repositories for various components of application using Bitbucket for Source code management, version control and managing various requirements for code promotion.\nCreated various plans and tasks using bamboo to build and deploy code to various environments and configured it to be automated based on the requirements.\nMonitor various application logs using Splunk to handle or resolve any issues real time.\nResponsible for administration, maintenance, upgrades and enhancements to JIRA, Confluence, Bitbucket and integrations with other tools or plugins like Tempo.\nDevelop & maintain custom JIRA workflows based upon business requirements\nMigration of code from TFS repository to GitHub repository for flexible version control.\nConverted various version control system specific directives and necessary mappings as required.\nTask management and workflow to track issues, bugs and various application requests using the Jira.\nDevelop & maintain custom JIRA workflows based upon business requirements\nPerformed migration of repositories from TF to GitHub.\nPerformed various builds on a regular basis and patches required by the application teams.\nRaised firewall rules and user access requests for various servers for targeted and destination and made use of Jira for tracking progress.\nUsed NuGet packaging for versioning support.\nMaintained servers and provided on the go support to application teams to resolve issues.\nStarting and stopping various nodes and slaves for environments with appropriate changes.\nInvolved in merging code for different components of application including enhancements.\nManaged Jboss servers to configure and resolve issues. \n\nUPS, Lutherville- Timonium, MD     \t\t\t\t\t\t\t              JUL 2019\u2013APR 2021\nRole: Systems Migration Engineer\n\nSYNOPSIS: Migrating various applications from tomcat and JBOSS to OpenShift container platform using azure and Jenkins pipelines to build and deploy.   \n\nResponsibilities\nSource code management using Azure DevOps server to perform various tasks including version control project management builds and reporting.\nBuilt multi branch pipelines using Jenkins to build and deploy various applications to OpenShift.\nScaling pods according to requirements and performing restarts when needed for apps in OpenShift to resolve any issues.\nResponsible for administration, maintenance, upgrades and enhancements to JIRA, Confluence, Bitbucket and integrations with other tools or plugins like Tempo.\nJIRA Agile (formerly green hopper), installed plugins for Confluence, experience maintaining custom Java custom reporting in JIRA\nAdministration of Atlassian Add-ons and 3rd party integrations, plugins, and extensions\nVet the security of those add-ons and integrations, etc.\nGather requirements for business processes, and determine ways to optimize/improve JIRA/Confluence build and configure complex workflows and screen, field, security, and notification schemes\nDevelop metrics dashboards and advanced filters in JIRA to provide end-users and business leadership with meaningful operational/performance metrics and status reports\nImplement Atlassian Tools upgrades, and partner with other IT staff to coordinate infrastructure maintenance and system migrations.\nDevelop & maintain custom JIRA workflows based upon business requirements\nCreated custom permission schemes, notification schemes, screens, and similar configuration changes on a project-by-project basis.\nProvided User Management and support for 1000+ local and remote users, manage system access across groups to ensure compliance, and maintain best practices.\nPerformed soft launches for various application using the stage and live routes implementation in OpenShift, which would only be toggled upon successful testing of the staged application.\nInvolved in raising firewall rules by providing necessary source, destination, and ports for various apps.\nMonitoring applications using APM in OpenShift which would require filtering at the pod\u2019s environment variables level. \nMaintained high availability clustered and standalone server environments and refined automation components with scripting and configuration management (Ansible) and experienced in writing Ansible scripts.\nUsed Ansible tower to run various job templates for onboarding applications and to generate the required com configs where necessary resources can be set for each environment for an app.\nInvolved in integrating taurus plugin with Jenkins for functional and performance test automation.\nInvolved in editing the existing ANT/MAVEN files in case of errors or changes in the project requirements.\u00a0\nUsed visual studio code for integrated development environment to run and execute apps.\nPerformed route switching from stage to live using job templates in ansible tower to switch traffic.\nWork with development/testing, deployment, systems/infrastructure and project teams to ensure continuous operation of build and test systems.\n\nEnvironment: Azure, Openshift, Ansible, JBOSS, ANT/MAVEN, JIRA and Confluence, Bamboo, SVN, GIT, Bitbucket, SDK.\n\nFreddie Mac, McLean, VA                                 \t\t\t\t\t\t\tJUN 2018 \u2013 JUN 2019\n\t\t\t\t\t\t                                                  \nRole: Sr. Systems Administrator\nResponsibilities\nResponsible for Installation, Configuration and Administration of WebSphere Process Server 7.0 including tracking of where the current business process is and troubleshooting issues with other services.\nInstalled, configured, administered and supported WebSphere Application Servers 7.0 on Linux and Windows.\nInstalled Fix packs, Cumulative Fixes and Patches on the Base and ND Versions.\nInstallation of WebSphere and IHS through silent installation using response files.\nCreated the WebSphere Process Server Cluster (Golden topology), ME Cluster, and CEI Cluster and performed a manual install of CEI Applications and changes on data sources.\nResponsible for day-to-day build and deployments in pre-production and production environments.\nOperating and maintaining multiple Data Centers including data clusters across data centers.\nBuild out server automation with Continuous Integration - Continuous Deployment tools like Jenkins/Maven for deployment and build management system.\u00a0\nConfigure Elastic Load balancer (ELB) including high availability of ELB using various subnets in various availability zones, configured security settings and health check for application.\nActing as coordinator during the process of Troubleshooting.\nInvolved in AWS EC2, VPC, S3, SQS, SNS based automation through Terraform, Ansible, Python, and Bash Scripts. Adopted new features as they were released by Amazon, including ELB & EBS.\nInvolved in configuring AWS S3 versioning, lifecycle policies, backup files and archive files in the glacier.\nWritten Templates for AWS infrastructure as a code using Terraform and CloudFormation to build staging and production environments.\nWorked on Azure Fabric, Micro services & Docker Containers in Azure. Ability to create scripts using Azure PowerShell during automation and built-in process.\nUsed cloud\u00a0providers and API\u2019s for Amazon (AWS) with Microsoft Azure and expertise to orchestrate the jobs to and from Azure Automation.\nHands-on AZURE, Migration of all servers from on-premises to Kubernetes containers, writing the scripts in Shell Scripts for managing various enterprise applications.\nWorked on middleware services JDBC, SSL certificates in JBOSS.\nInstalled, configured, and maintained web servers like JBOSS, Apache Web Server.\u00a0\u00a0\nCoordinate/assist developers with establishing and applying appropriate branching, Labelling /naming conventions using GIT source control.\nDesigning and implementing container orchestration systems with Docker Swarm and Kubernetes.\u00a0\nWorked on Docker hub, creating Docker\u00a0images and handling multiple images primarily for middleware installations and domain configurations.\nWorked on Docker\u00a0container snapshots, attaching to a running container, removing images, managing directory structures, and managing containers.\u00a0\t\n\nEnvironment: GIT, Jenkins, JBOSS, AWS, Azure, Shell script, Terraform, Maven, Docker, Kubernetes, Apache Tomcat, New Relic, Splunk, Jira, Confluence.\n\t\t\t                                             \t\nClient: SunTrust Bank      \t\t\t\t\t\t\t\t          SEP 2017 \u2013 APR 2018                                                                       \nSystems Administrator\nResponsibilities\nResponsible for Installation, Configuration and Administration of WebSphere Process Server v6.2 and WebSphere Application Server v7.0  \nInstalled and configured clustered 64bit-silent WebSphere Process Server v6.2.0.3 successfully on Red Hat Linux.\nInstalled and Configured IHS 6.1 and ran multiple instance of IHS for Log Viewer and \t\t       Application Requests\nUsing Build Forge 6.1 (Automation tool from IBM) involved in creating the WebSphere Process Server Cluster (Golden topology), ME Cluster, and CEI Cluster and Application cluster.\nConfiguring WebSphere resources like J2C, JMS, JDBC, Resource adapters, mail providers and shared libraries.\nDeveloped Jython scripts on performing the configuration\u2019s like MQ Queues and Queues Connection Factories, JVM Custom Properties\nTuned Activation specifications, JDBC Connection Factory\u2019s for long running BPEL\u2019s in WebSphere Process Server clustered environment.\n Configured of BPE container and Task Container using JACL Scripts in WebSphere Process Server clustered environment.\nConfigured Business Rules Manager, Relationship Manager and Task manager in WebSphere Process Server clustered environment.\nConfigured Business Process Choreographer and Common Event Infrastructure in WebSphere Process Server clustered environment.\nConfigured of Business Rules using Scripts in WebSphere Process Server clustered environment.\nPrepared BPC Task Exceptions using BPC Explorer in WebSphere Process Server clustered environment.\n Handled long running BPEL and Human Task instances by using BPC Explorer in WebSphere Process Server clustered environment.\nUsing Tivoli Provisioning Manager (TPM) performance the maintenance operation\u2019s as Installation of WPS and WAS and IHS, Also for Installing Fix packs, Cumulative Fixes and Patches on the WPS and WAS.\nDeveloped deployment scripts with support from IBM specialist for automating deployment of SOA applications on WebSphere Process Server and WebSphere Application Server environments.\nConfigured message logging and error processing using mediation modules components in WPS.\nResponsible for migration of WPS instance to new multiple Golden Topology environments.\nConfigured Monitoring agents on various Environments\u2019 like ITCAM for WebSphere, CA Wily Introscope and HP Diagnostics.\nInvolved in performance load testing for various applications. As part of the PLT, I was involved in Identifying Application bottle necks using Monitoring tools, Identifying System Bottleneck using NMON and Monitoring tools, Along with new recommendations for Connection Pool size and thread pool size , Analyzing heap dumps and Thread Dumps for any application related issue during PLT\u2019s \nExtensive troubleshooting experience using ISA and Application relevant logs.\nInvolved in Capacity Planning and Architectural decision on Security configuration and SSO.\nWorked in troubleshooting issues with prod and non-prod of WebSphere Process Server logs.\nWorked on SIB Explorer for identifying the queue depth and purging queue data in WebSphere Process Server default buses queues and customized MQ queues.\n\n\nHSBC Group, Hyderabad India     \t\t\t\t\t\t\t         SEP 2014\u2013 AUG 2015\nRole: Java Developer\n\nSYNOPSIS: The Company planned to move all of its public web assets supporting 50 percent of customer traffic and internet banking workloads to AWS in the coming years. The organization also started its examination with AWS Lambda, machine learning, grid computing and data-analytics workloads as part of its digital-transformation journey. \n\nResponsibilities:\nActively participated in the complete Software development life cycle starting from design phase to the implementation phase. Involved in requirements gathering and designed high and low-level designs using UML.\nDeveloped PL/SQL procedures, Java and Backbone Js, JavaScript, JQuery code.\nAnalysis Design and Development, Testing and Production Support.\nDeveloped Web Services for sending and getting data from different applications using SOAP messages, such as Loan Applications to transfer data from Branch Server to Head office Server, then used SAX and DOM XML parsers for data retrieval.\nDeveloped functional model, object model and dynamic model using UML.\nExtensively worked on generating complex reports.\nDeveloped code for various activities using MVC architecture.\nDeveloped client side screen using JSP, HTML and DHTML. \nWorked on development of Hibernate, including mapping files, configuration file and classes to interact with the database.\nInvolved in injecting dependencies into code using spring core module concepts like IOC of Spring Framework\nDeveloped web service (which talks with web method) for uploading CSV (using web methods) files and validating and later inserting into the corresponding tables.\n\nEnvironment: J2EE, MySQL, Spring, Hibernate, JSP, HTML, JavaScript, JQuery, XML (SAX and DOM), RAD, JUNIT, and JNDI.\n",
          "confidence": 1.0,
          "method": "full_text",
          "structured_data": null
        },
        "tax_term": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "source_by": {
          "value": "",
          "confidence": 0.0,
          "method": "none",
          "structured_data": null
        },
        "designation": {
          "value": "Cloud Infrastructure",
          "confidence": 0.8,
          "method": "regex",
          "structured_data": null
        },
        "experience": {
          "value": "7",
          "confidence": 0.9,
          "method": "regex_total_experience_summary",
          "structured_data": null
        }
      }
    }
  ]
}